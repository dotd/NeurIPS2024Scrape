
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .paper { 
                margin-bottom: 30px;
                padding: 20px;
                border: 1px solid #ccc;
                border-radius: 5px;
            }
            .conference { 
                font-size: 24px;
                color: #2c3e50;
                margin: 20px 0;
                padding-bottom: 10px;
                border-bottom: 2px solid #3498db;
            }
            .field-name {
                font-weight: bold;
                color: #2980b9;
            }
            .field-value {
                margin-bottom: 10px;
                line-break: anywhere;
            }
            a { color: #3498db; }
            a:hover { color: #2980b9; }
            .paper-counter {
                font-size: 18px;
                color: #2c3e50;
                margin: 20px 0;
                padding-bottom: 10px;
                border-bottom: 2px solid #3498db;
            }
        </style>
    </head>
    <body>
    <div class="paper">
<div class="field-name">id:</div>
<div class="field-value">zr2GPi3DSb</div>
<div class="field-name">title:</div>
<div class="field-value">Gentle Manipulation of Tree Branches: A Contact-Aware Policy Learning Approach</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jay Jacob', 'Shizhe Cai', 'Paulo Vinicius Koerich Borges', 'Tirthankar Bandyopadhyay', 'Fabio Ramos']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jay_Jacob1', '~Shizhe_Cai2', '~Paulo_Vinicius_Koerich_Borges1', '~Tirthankar_Bandyopadhyay1', '~Fabio_Ramos1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Sim-to-Real', 'Deformable Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A reinforcement learning approach using a contact detection classifier and sim-to-real transfer for gentle interaction with tree branches.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to interact with deformable tree branches with minimal damage is challenging due to their intricate geometry and inscrutable dynamics. Furthermore, traditional vision-based modelling systems suffer from implicit occlusions in dense foliage, severely changing lighting conditions, and limited field of view, in addition to having a significant computational burden preventing real-time deployment.In this work, we simulate a procedural forest with realistic, self-similar branching structures derived from a parametric L-system model, actuated with crude spring abstractions, mirroring real-world variations with domain randomisation over the morphological and dynamic attributes. We then train a novel Proprioceptive Contact-Aware Policy (PCAP) for a reach task using reinforcement learning, aided by a whole-arm contact detection classifier and reward engineering, without external vision, tactile, or torque sensing. The agent deploys novel strategies to evade and mitigate contact impact, favouring a reactive exploration of the task space. Finally, we demonstrate that the learned behavioural patterns can be transferred zero-shot from simulation to real, allowing the arm to navigate around real branches with unseen topology and variable occlusions while minimising the contact forces and expected ruptures.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/acb4f627444a21ed044b98dd63fa61857d1488d8.pdf" target="_blank">/pdf/acb4f627444a21ed044b98dd63fa61857d1488d8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/88ce7ae955b2cce40cd7c2db09e29da9c5c89100.zip" target="_blank">/attachment/88ce7ae955b2cce40cd7c2db09e29da9c5c89100.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jacob2024gentle,
title={Gentle Manipulation of Tree Branches: A Contact-Aware Policy Learning Approach},
author={Jay Jacob and Shizhe Cai and Paulo Vinicius Koerich Borges and Tirthankar Bandyopadhyay and Fabio Ramos},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=zr2GPi3DSb}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/pcap/home</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/9af58f6db4423dc30109182735c2ce06ca52ac0f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=zr2GPi3DSb&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jacob|gentle_manipulation_of_tree_branches_a_contactaware_policy_learning_approach</div>
</div>
<div class='paper-counter'>1/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">zeYaLS2tw5</div>
<div class="field-name">title:</div>
<div class="field-value">Sparse Diffusion Policy:  A Sparse, Reusable, and Flexible Policy for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yixiao Wang', 'Yifei Zhang', 'Mingxiao Huo', 'Thomas Tian', 'Xiang Zhang', 'Yichen Xie', 'Chenfeng Xu', 'Pengliang Ji', 'Wei Zhan', 'Mingyu Ding', 'Masayoshi Tomizuka']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yixiao_Wang3', '~Yifei_Zhang14', '~Mingxiao_Huo1', '~Thomas_Tian1', '~Xiang_Zhang20', '~Yichen_Xie1', '~Chenfeng_Xu1', '~Pengliang_Ji1', '~Wei_Zhan2', '~Mingyu_Ding1', '~Masayoshi_Tomizuka2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Policy', 'Multitask', 'Continual learning', 'Mixture of Experts']</div>
<div class="field-name">abstract:</div>
<div class="field-value">The increasing complexity of tasks in robotics demands efficient strategies for multitask and continual learning. Traditional models typically rely on a universal policy for all tasks, facing challenges such as high computational costs and catastrophic forgetting when learning new tasks. To address these issues, we introduce a sparse, reusable, and flexible policy, Sparse Diffusion Policy (SDP). By adopting Mixture of Experts (MoE) within a transformer-based diffusion policy, SDP selectively activates experts and skills, enabling task-specific learning without retraining the entire model. It not only reduces the burden of active parameters but also facilitates the seamless integration and reuse of experts across various tasks. Extensive experiments on diverse tasks in both simulators and the real world show that SDP 1) excels in multitask scenarios with negligible increases in active parameters, 2) prevents forgetting in continual learning new tasks, and 3) enables efficient task transfer, offering a promising solution for advanced robotic applications. More demos and codes can be found on our https://anonymous.4open.science/w/sparse_diffusion_policy-24E7/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f18366e33a7c3c4cd395c8dd92f50ce602cbb3e1.pdf" target="_blank">/pdf/f18366e33a7c3c4cd395c8dd92f50ce602cbb3e1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/97018e5cf3834ea2fd72fade1fd7b6e6a6d1eb89.zip" target="_blank">/attachment/97018e5cf3834ea2fd72fade1fd7b6e6a6d1eb89.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024sparse,
title={Sparse Diffusion Policy:  A Sparse, Reusable, and Flexible Policy for Robot Learning},
author={Yixiao Wang and Yifei Zhang and Mingxiao Huo and Thomas Tian and Xiang Zhang and Yichen Xie and Chenfeng Xu and Pengliang Ji and Wei Zhan and Mingyu Ding and Masayoshi Tomizuka},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=zeYaLS2tw5}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://forrest-110.github.io/sparse_diffusion_policy/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7538a2611d2c5d1441476de9db061590c24b000d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=zeYaLS2tw5&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|sparse_diffusion_policy_a_sparse_reusable_and_flexible_policy_for_robot_learning</div>
</div>
<div class='paper-counter'>2/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">zIWu9Kmlqk</div>
<div class="field-name">title:</div>
<div class="field-value">LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Video</div>
<div class="field-name">authors:</div>
<div class="field-value">['Noriaki Hirose', 'Catherine Glossop', 'Ajay Sridhar', 'Oier Mees', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Noriaki_Hirose1', '~Catherine_Glossop1', '~Ajay_Sridhar1', '~Oier_Mees1', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Language-conditioned navigation policy', 'data augmentation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Data augmentation for in-the-wild video for learning language-conditioned navigation policy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present our method, LeLaN, which uses action-free egocentric data to learn robust language-conditioned object navigation. By leveraging the knowledge of large vision and language models and grounding this knowledge using pre-trained segmentation and depth estimation models, we can label in-the-wild data from a variety of indoor and outdoor environments with diverse instructions that capture a range of objects with varied granularity and noise in their descriptions. Leveraging this method to label over 50 hours of data collected in indoor and outdoor environments, including robot observations, YouTube video tours, and human-collected walking data allows us to train a policy that can outperform state-of-the-art methods on the zero-shot object navigation task in both success rate and precision.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c363806fbba8628b3d0535951a0ece787d64cfcc.pdf" target="_blank">/pdf/c363806fbba8628b3d0535951a0ece787d64cfcc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6ea7d11f58c0a455af3898cfd142b748f2726d8a.zip" target="_blank">/attachment/6ea7d11f58c0a455af3898cfd142b748f2726d8a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hirose2024lelan,
title={LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Video},
author={Noriaki Hirose and Catherine Glossop and Ajay Sridhar and Oier Mees and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=zIWu9Kmlqk}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://learning-language-navigation.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a6701e51af3fafaa1cf49ab7827ff40fcf404080.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=zIWu9Kmlqk&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hirose|lelan_learning_a_languageconditioned_navigation_policy_from_inthewild_video</div>
</div>
<div class='paper-counter'>3/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">yqLFb0RnDW</div>
<div class="field-name">title:</div>
<div class="field-value">Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress</div>
<div class="field-name">authors:</div>
<div class="field-value">['Christopher Agia', 'Rohan Sinha', 'Jingyun Yang', 'Ziang Cao', 'Rika Antonova', 'Marco Pavone', 'Jeannette Bohg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Christopher_Agia1', '~Rohan_Sinha1', '~Jingyun_Yang1', '~Ziang_Cao2', '~Rika_Antonova1', '~Marco_Pavone1', '~Jeannette_Bohg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Failure Detection', 'Generative Policies', 'Vision Language Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework for runtime monitoring of generative policies that uses 1) temporal consistency checks to detect erratic policy behavior and 2) Vision Language Models to detect failure to make progress on the task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot behavior policies trained via imitation learning are prone to failure under conditions that deviate from their training data. Thus, algorithms that monitor learned policies at test time and provide early warnings of failure are necessary to facilitate scalable deployment. We propose Sentinel, a runtime monitoring framework that splits the detection of failures into two complementary categories: 1) Erratic failures, which we detect using statistical measures of temporal action consistency, and 2) task progression failures, where we use Vision Language Models (VLMs) to detect when the policy confidently and consistently takes actions that do not solve the task. Our approach has two key strengths. First, because learned policies exhibit diverse failure modes, combining complementary detectors leads to significantly higher accuracy at failure detection. Second, using a statistical temporal action consistency measure ensures that we quickly detect when multimodal, generative policies exhibit erratic behavior at negligible computational cost. In contrast, we only use VLMs to detect modes that are less time-sensitive. We demonstrate our approach in the context of diffusion policies trained on robotic mobile manipulation domains in both simulation and the real world. By unifying temporal consistency detection and VLM runtime monitoring, Sentinel detects 18% more failures than using either of the two detectors alone and significantly outperforms baselines, thus highlighting the importance of assigning specialized detectors to complementary categories of failure. Qualitative results are made available at sites.google.com/stanford.edu/sentinel.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a85030b2d21fd9500496afa583867bc122d503ee.pdf" target="_blank">/pdf/a85030b2d21fd9500496afa583867bc122d503ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d127df801ca6c5255c43ef736d1e127ba465514b.zip" target="_blank">/attachment/d127df801ca6c5255c43ef736d1e127ba465514b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
agia2024unpacking,
title={Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress},
author={Christopher Agia and Rohan Sinha and Jingyun Yang and Ziang Cao and Rika Antonova and Marco Pavone and Jeannette Bohg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=yqLFb0RnDW}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/stanford.edu/sentinel</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/099772e74ba14c9daf0d824b3c60d86db95061d2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=yqLFb0RnDW&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">agia|unpacking_failure_modes_of_generative_policies_runtime_monitoring_of_consistency_and_progress</div>
</div>
<div class='paper-counter'>4/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ypaYtV1CoG</div>
<div class="field-name">title:</div>
<div class="field-value">Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jennifer Grannen', 'Siddharth Karamcheti', 'Suvir Mirchandani', 'Percy Liang', 'Dorsa Sadigh']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jennifer_Grannen1', '~Siddharth_Karamcheti1', '~Suvir_Mirchandani1', '~Percy_Liang1', '~Dorsa_Sadigh1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Continual Learning', 'Multimodal Teaching', 'Human-Robot Interaction']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Vocal Sandbox: a framework for situated human-robot collaboration that adapts to users in real-time, learning new behaviors and low-level skills from mixed-modality feedback.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Vocal Sandbox, a framework for enabling seamless human-robot collaboration in situated environments. Systems in our framework are characterized by their ability to *adapt and continually learn* at multiple levels of abstraction from diverse teaching modalities such as spoken dialogue, object keypoints, and kinesthetic demonstrations. To enable such adaptation, we design lightweight and interpretable learning algorithms that allow users to build an understanding and co-adapt to a robot's capabilities in real-time, as they teach new behaviors. For example, after demonstrating a new low-level skill for "tracking around" an object, users are provided with trajectory visualizations of the robot's intended motion when asked to track a new object. Similarly, users teach high-level planning behaviors through spoken dialogue, using pretrained language models to synthesize behaviors such as "packing an object away" as compositions of low-level skills -- concepts that can be reused and built upon. We evaluate Vocal Sandbox in two settings: collaborative gift bag assembly and LEGO stop-motion animation. In the first setting, we run systematic ablations and user studies with 8 non-expert participants, highlighting the impact of multi-level teaching. Across 23 hours of total robot interaction time, users teach 17 new high-level behaviors with an average of 16 novel low-level skills, requiring 22.1% less active supervision compared to baselines. Qualitatively, users strongly prefer Vocal Sandbox systems due to their ease of use (+31.2%), helpfulness (+13.0%), and overall performance (+18.2%). Finally, we pair an experienced system-user with a robot to film a stop-motion animation; over two hours of continuous collaboration, the user teaches progressively more complex motion skills to produce a 52 second (232 frame) movie. Videos & Supplementary Material: https://vocal-sandbox.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/40a6edc92058f81bb82e974e2d253153eb69f25d.pdf" target="_blank">/pdf/40a6edc92058f81bb82e974e2d253153eb69f25d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/50dba6172f63822834309d43227c36b3f2274b1e.zip" target="_blank">/attachment/50dba6172f63822834309d43227c36b3f2274b1e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
grannen2024vocal,
title={Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration},
author={Jennifer Grannen and Siddharth Karamcheti and Suvir Mirchandani and Percy Liang and Dorsa Sadigh},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ypaYtV1CoG}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://vocal-sandbox.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/01b2cb1c945c39341b8a5f67f3b0a6a8538da12d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ypaYtV1CoG&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">grannen|vocal_sandbox_continual_learning_and_adaptation_for_situated_humanrobot_collaboration</div>
</div>
<div class='paper-counter'>5/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ylZHvlwUcI</div>
<div class="field-name">title:</div>
<div class="field-value">Theia: Distilling Diverse Vision Foundation Models for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jinghuan Shang', 'Karl Schmeckpeper', 'Brandon B. May', 'Maria Vittoria Minniti', 'Tarik Kelestemur', 'David Watkins', 'Laura Herlant']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jinghuan_Shang1', '~Karl_Schmeckpeper1', '~Brandon_B._May1', '~Maria_Vittoria_Minniti1', '~Tarik_Kelestemur1', '~David_Watkins2', '~Laura_Herlant1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['visual representation', 'robot learning', 'distillation', 'foundation model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We build a robot vision foundation model by distilling existing vision foundation models, which improves downstream robot learning performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-based robot policy learning, which maps visual inputs to actions, necessitates a holistic understanding of diverse visual tasks beyond single-task needs like classification or segmentation. Inspired by this, we introduce Theia, a vision foundation model for robot learning that distills multiple off-the-shelf vision foundation models trained on varied vision tasks. Theia's rich visual representations encode diverse visual knowledge, enhancing downstream robot learning. Extensive experiments demonstrate that Theia outperforms its teacher models and prior robot learning models using less training data and smaller model sizes. Additionally, we quantify the quality of pre-trained visual representations and hypothesize that higher entropy in feature norm distributions leads to improved robot learning performance. Code, models, and demo are available at https://theia.theaiinstitute.com.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d9e3232040c4709cb355f8e79aab6a98dc97a26e.pdf" target="_blank">/pdf/d9e3232040c4709cb355f8e79aab6a98dc97a26e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6aa31483363bdd12c99f85ed1987e95849de1b21.zip" target="_blank">/attachment/6aa31483363bdd12c99f85ed1987e95849de1b21.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
shang2024theia,
title={Theia: Distilling Diverse Vision Foundation Models for Robot Learning},
author={Jinghuan Shang and Karl Schmeckpeper and Brandon B. May and Maria Vittoria Minniti and Tarik Kelestemur and David Watkins and Laura Herlant},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ylZHvlwUcI}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://theia.theaiinstitute.com/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/476a7427ee105504231023437576c9808218825a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ylZHvlwUcI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">shang|theia_distilling_diverse_vision_foundation_models_for_robot_learning</div>
</div>
<div class='paper-counter'>6/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">yYujuPxjDK</div>
<div class="field-name">title:</div>
<div class="field-value">Language-guided Manipulator Motion Planning with Bounded Task Space</div>
<div class="field-name">authors:</div>
<div class="field-value">['Thies Oelerich', 'Christian Hartl-Nesic', 'Andreas Kugi']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Thies_Oelerich1', 'christian.hartl@tuwien.ac.at', '~Andreas_Kugi1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Vision Language Models', 'Manipulation Planning', 'Path-following MPC']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Language-based robot control is a powerful and versatile method to control a robot manipulator where large language models (LLMs) are used to reason about the environment. However, the generated robot motions by these controllers often lack safety and performance, resulting in jerky movements. In this work, a novel modular framework for zero-shot motion planning for manipulation tasks is developed. The modular components do not require any motion-planning-specific training. An LLM is combined with a vision model to create Python code that interacts with a novel path planner, which creates a piecewise linear reference path with bounds around the path that ensure safety. An optimization-based planner, the BoundMPC framework, is utilized to execute optimal, safe, and collision-free trajectories along the reference path. The effectiveness of the approach is shown on various everyday manipulation tasks in simulation and experiment, shown in the video at www.acin.tuwien.ac.at/42d2.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/063cd4a9f235bb20dc7acf54f3157fab462f80a1.pdf" target="_blank">/pdf/063cd4a9f235bb20dc7acf54f3157fab462f80a1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9b09a989259134ec163cfb083205fa877c664d34.zip" target="_blank">/attachment/9b09a989259134ec163cfb083205fa877c664d34.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
oelerich2024languageguided,
title={Language-guided Manipulator Motion Planning with Bounded Task Space},
author={Thies Oelerich and Christian Hartl-Nesic and Andreas Kugi},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=yYujuPxjDK}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4db123bf1312f3e0279e14947084e731c6d31fa4.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=yYujuPxjDK&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">oelerich|languageguided_manipulator_motion_planning_with_bounded_task_space</div>
</div>
<div class='paper-counter'>7/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ySI0tBYxpz</div>
<div class="field-name">title:</div>
<div class="field-value">Gaitor: Learning a Unified Representation Across Gaits for Real-World Quadruped Locomotion</div>
<div class="field-name">authors:</div>
<div class="field-value">['Alexander Luis Mitchell', 'Wolfgang Merkt', 'Aristotelis Papatheodorou', 'Ioannis Havoutis', 'Ingmar Posner']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Alexander_Luis_Mitchell1', '~Wolfgang_Merkt1', '~Aristotelis_Papatheodorou2', '~Ioannis_Havoutis1', '~Ingmar_Posner1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Representation Learning', 'Learning for Control', 'Quadruped Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We create a structured latent representation across locomotion skills and show that continuous gait transitions emerge in the space automatically.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The current state-of-the-art in quadruped locomotion is able to produce a variety of complex motions. These methods either rely on switching between a discrete set of skills or learn a distribution across gaits using complex black-box models. Alternatively, we present Gaitor, which learns a disentangled and 2D representation across locomotion gaits. This learnt representation forms a planning space for closed-loop control delivering continuous gait transitions and perceptive terrain traversal. Gaitor’s latent space is readily interpretable and we discover that during gait transitions, novel unseen gaits emerge. The latent space is disentangled with respect to footswing heights and lengths. This means that these gait characteristics can be varied independently in the 2D latent representation. Together with a simple terrain encoding and a learnt planner operating in the latent space, Gaitor can take motion commands including desired gait type and swing characteristics all while reacting to uneven terrain. We evaluate Gaitor in both simulation and the real world on the ANYmal C platform. To the best of our knowledge, this is the first work learning a unified and interpretable latent space for multiple gaits, resulting in continuous blending between different locomotion modes on a real quadruped robot. An overview of the methods and results in this paper is found at https://youtu.be/eVFQbRyilCA.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/465127d4f9c0edb3343faeba09257660df93e40c.pdf" target="_blank">/pdf/465127d4f9c0edb3343faeba09257660df93e40c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9d4c06c35c636e9babe1fd00198d30396d48bda4.zip" target="_blank">/attachment/9d4c06c35c636e9babe1fd00198d30396d48bda4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
mitchell2024gaitor,
title={Gaitor: Learning a Unified Representation Across Gaits for Real-World Quadruped Locomotion},
author={Alexander Luis Mitchell and Wolfgang Merkt and Aristotelis Papatheodorou and Ioannis Havoutis and Ingmar Posner},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ySI0tBYxpz}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b0600db5a8bd61c4e6bcf15f29c35fffc83ea30d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ySI0tBYxpz&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">mitchell|gaitor_learning_a_unified_representation_across_gaits_for_realworld_quadruped_locomotion</div>
</div>
<div class='paper-counter'>8/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">yNQu9zqx6X</div>
<div class="field-name">title:</div>
<div class="field-value">Robust Manipulation Primitive Learning via Domain Contraction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Teng Xue', 'Amirreza Razmjoo', 'Suhan Shetty', 'Sylvain Calinon']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Teng_Xue1', '~Amirreza_Razmjoo1', '~Suhan_Shetty1', '~Sylvain_Calinon1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robust policy learning', 'Contact-rich manipulation', 'Sim-to-real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a bi-level approach to learn parameter-conditioned manipulation policies using multiple models and domain contraction.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Contact-rich manipulation plays an important role in everyday life, but uncertain parameters pose significant challenges to model-based planning and control. To address this issue, domain adaptation and domain randomization have been proposed to learn robust policies. However, they either lose the generalization ability to diverse instances or perform conservatively due to neglecting instance-specific information. In this paper, we propose a bi-level approach to learn robust manipulation primitives, including parameter-augmented policy learning using multiple models with tensor approximation, and parameter-conditioned policy retrieval through domain contraction. This approach unifies domain randomization and domain adaptation, providing optimal behaviors while keeping generalization ability. We validate the proposed method on three contact-rich manipulation primitives: hitting, pushing, and reorientation. The experimental results showcase the superior performance of our approach in generating robust policies for instances with diverse physical parameters.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ccea66ebae00a2b0bbb5af2596615eafa179cd16.pdf" target="_blank">/pdf/ccea66ebae00a2b0bbb5af2596615eafa179cd16.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3ffea8c73040e3b4d4bef8f33ffc8f3400e381a9.zip" target="_blank">/attachment/3ffea8c73040e3b4d4bef8f33ffc8f3400e381a9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xue2024robust,
title={Robust Manipulation Primitive Learning via Domain Contraction},
author={Teng Xue and Amirreza Razmjoo and Suhan Shetty and Sylvain Calinon},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=yNQu9zqx6X}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/robustpl</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2657b8097ec5d4e9b8dbaf5d343259286e886630.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=yNQu9zqx6X&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xue|robust_manipulation_primitive_learning_via_domain_contraction</div>
</div>
<div class='paper-counter'>9/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">y8XkuQIrvI</div>
<div class="field-name">title:</div>
<div class="field-value">MILES: Making Imitation Learning Easy with Self-Supervision</div>
<div class="field-name">authors:</div>
<div class="field-value">['Georgios Papagiannis', 'Edward Johns']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Georgios_Papagiannis1', '~Edward_Johns1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Robotic Manipulation', 'Self-Supervised Data Collection']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">MILES is an imitation learning method that collects data in a self-supervised manner to train policies that can learn complex manipulation skills from a single demonstration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Data collection in imitation learning often requires significant, laborious human supervision, such as numerous demonstrations, and/or frequent environment resets for methods that incorporate reinforcement learning. In this work, we propose an alternative approach, MILES: a fully autonomous, self-supervised data collection paradigm, and we show that this enables efficient policy learning from just a single demonstration and a single environment reset. MILES autonomously learns a policy for returning to and then following the single demonstration, whilst being self-guided during data collection, eliminating the need for additional human interventions. We evaluated MILES across several realworld tasks, including tasks that require precise contact-rich manipulation such as locking a lock with a key. We found that, under the constraints of a single demonstration and no repeated environment resetting, MILES significantly outperforms state-of-the-art alternatives like imitation learning methods that leverage reinforcement learning. Videos of our experiments and code can be found on our webpage: www.robot-learning.uk/miles.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9f82f00a94fbeb218f28dcd30335f754336d9445.pdf" target="_blank">/pdf/9f82f00a94fbeb218f28dcd30335f754336d9445.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e51da0efd5963d23dfbe28f0a79566121c62c8fb.zip" target="_blank">/attachment/e51da0efd5963d23dfbe28f0a79566121c62c8fb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
papagiannis2024miles,
title={{MILES}: Making Imitation Learning Easy with Self-Supervision},
author={Georgios Papagiannis and Edward Johns},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=y8XkuQIrvI}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://www.robot-learning.uk/miles</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/08691c3da87a79188e6878dd670856297d8be20e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=y8XkuQIrvI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">papagiannis|miles_making_imitation_learning_easy_with_selfsupervision</div>
</div>
<div class='paper-counter'>10/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">xeFKtSXPMd</div>
<div class="field-name">title:</div>
<div class="field-value">OCCAM: Online Continuous Controller Adaptation with Meta-Learned Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hersh Sanghvi', 'Spencer Folk', 'Camillo Jose Taylor']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hersh_Sanghvi1', '~Spencer_Folk1', '~Camillo_Jose_Taylor2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Controller Adaptation', 'Robot Model Learning', 'Meta-Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a framework for online controller adaptation that learns prior predictive models of system performance that quickly adapt to online data and can be used to to optimize control parameters for diverse physical and simulated robots.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control tuning and adaptation present a significant challenge to the usage of robots in diverse environments. It is often nontrivial to find a single set of control parameters by hand that work well across the broad array of environments and conditions that a robot might encounter. Automated adaptation approaches must utilize prior knowledge about the system while adapting to significant domain shifts to find new control parameters quickly. In this work, we present a general framework for online controller adaptation that deals with these challenges. We combine meta-learning with Bayesian recursive estimation to learn prior predictive models of system performance that quickly adapt to online data, even when there is significant domain shift. These predictive models can be used as cost functions within efficient sampling-based optimization routines to find new control parameters online that maximize system performance. Our framework is powerful and flexible enough to adapt controllers for four diverse systems: a simulated race car, a simulated quadrupedal robot, and a simulated and physical quadrotor.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8f22a11bdf2a75e01a8c6a45dd12be0e62c2be4e.pdf" target="_blank">/pdf/8f22a11bdf2a75e01a8c6a45dd12be0e62c2be4e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dc60e7428b40010cc1050acb1e70188ff827096e.zip" target="_blank">/attachment/dc60e7428b40010cc1050acb1e70188ff827096e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sanghvi2024occam,
title={{OCCAM}: Online Continuous Controller Adaptation with Meta-Learned Models},
author={Hersh Sanghvi and Spencer Folk and Camillo Jose Taylor},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=xeFKtSXPMd}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://hersh500.github.io/occam</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/55d9f3187c4928d142f065804919dc87857b4e2a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=xeFKtSXPMd&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sanghvi|occam_online_continuous_controller_adaptation_with_metalearned_models</div>
</div>
<div class='paper-counter'>11/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">xcBH8Jhmbi</div>
<div class="field-name">title:</div>
<div class="field-value">Discovering Robotic Interaction Modes with Discrete Representation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Liquan Wang', 'Ankit Goyal', 'Haoping Xu', 'Animesh Garg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Liquan_Wang2', '~Ankit_Goyal1', '~Haoping_Xu1', '~Animesh_Garg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Discovering Robotic Interaction Modes with Discrete Representation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents ActAIM2, a system that unsupervisedly learns discrete robot manipulation modes representation, improving manipulability and generalizability with articulated objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Abstract: Human actions manipulating articulated objects, such as opening and closing a drawer, can be categorized into multiple modalities we define as interaction modes. Traditional robot learning approaches lack discrete representations of these modes, which are crucial for empirical sampling and grounding. In this paper, we present ActAIM2, which learns a discrete representation of robot manipulation interaction modes in a purely unsupervised fashion, without the use of expert labels or simulator-based privileged information. Utilizing novel data collection methods involving simulator rollouts, ActAIM2 consists of an interaction mode selector and a low-level action predictor. The selector generates discrete representations of potential interaction modes with self-supervision, while the predictor outputs
corresponding action trajectories. Our method is validated through its success rate in manipulating articulated objects and its robustness in sampling meaningful actions from the discrete representation. Extensive experiments demonstrate ActAIM2’s effectiveness in enhancing manipulability and generalizability over baselines and ablation studies. For videos and additional results, see our website: https://actaim2.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8d159fef7f92364a44f382d60f1219f9768544e8.pdf" target="_blank">/pdf/8d159fef7f92364a44f382d60f1219f9768544e8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/74dc322dfb8f588a91b7d18f0b1e68c3c03b1ac2.zip" target="_blank">/attachment/74dc322dfb8f588a91b7d18f0b1e68c3c03b1ac2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024discovering,
title={Discovering Robotic Interaction Modes with Discrete Representation Learning},
author={Liquan Wang and Ankit Goyal and Haoping Xu and Animesh Garg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=xcBH8Jhmbi}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://actaim2.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1a4d7a65b5a174b90515d5d441e9fda17ec933e5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=xcBH8Jhmbi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|discovering_robotic_interaction_modes_with_discrete_representation_learning</div>
</div>
<div class='paper-counter'>12/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">xYleTh2QhS</div>
<div class="field-name">title:</div>
<div class="field-value">Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Youwei Yu', 'Junhong Xu', 'Lantao Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Youwei_Yu1', '~Junhong_Xu1', '~Lantao_Liu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Curriculum Reinforcement Learning', 'Diffusion Model', 'Field Robotics']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our work introduces Adaptive Diffusion Terrain Generator, leveraging DDPMs to dynamically generate complex, realistic terrains for improved policy training and generalization to real-world environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Model-free reinforcement learning has emerged as a powerful method for developing robust robot control policies capable of navigating through complex and unstructured terrains.
The effectiveness of these methods hinges on two essential elements: 
(1) the use of massively parallel physics simulations to expedite policy training, 
and
(2) the deployment of an environment generator tasked with crafting terrains that are sufficiently challenging yet attainable, thereby facilitating continuous policy improvement. 
Existing methods of environment generation often rely on heuristics constrained by a set of parameters, limiting the diversity and realism.
In this work, we introduce the Adaptive Diffusion Terrain Generator (ADTG), a novel method that leverages Denoising Diffusion Probabilistic Models (DDPMs) to dynamically expand an existing training environment by adding more diverse and complex terrains tailored to the current policy.
Unlike conventional methods, ADTG adapts the terrain complexity and variety based on the evolving capabilities of the current policy.
This is achieved through two primary mechanisms:
First, by blending terrains from the initial dataset within their latent spaces using performance-informed weights, ADTG creates terrains that suitably challenge the policy. 
Secondly, by manipulating the initial noise in the diffusion process, ADTG seamlessly shifts between creating similar terrains for fine-tuning the current policy and entirely novel ones for expanding training diversity.
Our experiments show that the policy trained by ADTG outperforms both procedural generated and natural environments, along with popular navigation methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f9e9a3fced13f1301eb0b2f74f196885e765f787.pdf" target="_blank">/pdf/f9e9a3fced13f1301eb0b2f74f196885e765f787.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/afd2a43c61d2c994c167cf282f758b98b0246643.zip" target="_blank">/attachment/afd2a43c61d2c994c167cf282f758b98b0246643.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yu2024adaptive,
title={Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation},
author={Youwei Yu and Junhong Xu and Lantao Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=xYleTh2QhS}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://adtg-sim-to-real.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/03f725a46c2f9a60cc953ab5e4758e9abcd29b7c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=xYleTh2QhS&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yu|adaptive_diffusion_terrain_generator_for_autonomous_uneven_terrain_navigation</div>
</div>
<div class='paper-counter'>13/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">xYJn2e1uu8</div>
<div class="field-name">title:</div>
<div class="field-value">Sparsh: Self-supervised touch representations for vision-based tactile sensing</div>
<div class="field-name">authors:</div>
<div class="field-value">['Carolina Higuera', 'Akash Sharma', 'Chaithanya Krishna Bodduluri', 'Taosha Fan', 'Patrick Lancaster', 'Mrinal Kalakrishnan', 'Michael Kaess', 'Byron Boots', 'Mike Lambeta', 'Tingfan Wu', 'Mustafa Mukadam']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Carolina_Higuera1', '~Akash_Sharma1', '~Chaithanya_Krishna_Bodduluri1', '~Taosha_Fan1', '~Patrick_Lancaster1', '~Mrinal_Kalakrishnan1', '~Michael_Kaess1', '~Byron_Boots1', '~Mike_Lambeta1', '~Tingfan_Wu2', '~Mustafa_Mukadam1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Tactile sensing', 'Pre-trained representations', 'Self-supervised learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">General touch representations trained with self-supervision, evaluated on a new touch-centric benchmark, outperform custom task and sensor specific models</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we introduce general purpose touch representations for the increasingly accessible class of vision-based tactile sensors. Such sensors have led to many recent advances in robot manipulation as they markedly complement vision, yet solutions today often rely on task and sensor specific handcrafted perception models. Collecting real data at scale with task centric ground truth labels, like contact forces and slip, is a challenge further compounded by sensors of various form factor differing in aspects like lighting and gel markings. To tackle this, we turn to self-supervised learning (SSL) that has demonstrated remarkable performance in computer vision. We present Sparsh, a family of SSL models that can support various vision-based tactile sensors, alleviating the need for custom labels through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. We also build TacBench, to facilitate standardized benchmarking across sensors and models, comprising of six tasks ranging from comprehending tactile properties to enabling physical perception and manipulation planning. In evaluations, we find that SSL pre-training for touch representation outperforms task and sensor-specific end-to-end training by 95.1% on average over TacBench, and Sparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the merits of learning in latent space for tactile images. Project page: https://sparsh-ssl.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3242f6c62688073a2b3443361d07d8d165c242a8.pdf" target="_blank">/pdf/3242f6c62688073a2b3443361d07d8d165c242a8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/41861ffc2064440bab92607fd7b5bfcbaa773504.zip" target="_blank">/attachment/41861ffc2064440bab92607fd7b5bfcbaa773504.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
higuera2024sparsh,
title={Sparsh: Self-supervised touch representations for vision-based tactile sensing},
author={Carolina Higuera and Akash Sharma and Chaithanya Krishna Bodduluri and Taosha Fan and Patrick Lancaster and Mrinal Kalakrishnan and Michael Kaess and Byron Boots and Mike Lambeta and Tingfan Wu and Mustafa Mukadam},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=xYJn2e1uu8}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sparsh-ssl.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/eefe84f338fef0d91967d89643b24de893d3e705.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=xYJn2e1uu8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">higuera|sparsh_selfsupervised_touch_representations_for_visionbased_tactile_sensing</div>
</div>
<div class='paper-counter'>14/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wzojxM4DmJ</div>
<div class="field-name">title:</div>
<div class="field-value">Optimizing DLO Instance Segmentation with Lightweight Adapters and Text Prompts</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shir Kozlovsky', 'Omkar Joglekar', 'Dotan Di Castro']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shir_Kozlovsky1', '~Omkar_Joglekar2', '~Dotan_Di_Castro1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Instance Segmentation', 'Adapters', 'Deformable Linear Objects (DLOs)']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A method for instance segmentation Deformable Linear Objects (DLOs) using lightweight adapters and text prompts, combining CLIPSeg and SAM models. This approach surpasses state-of-the-art baselines, achieving an mIoU=91.21% DLO-specific dataset.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a method for cable instance segmentation using Adapters for Foundation Models that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of the CLIPSeg model with the zero-shot generalization capabilities of the Segment Anything Model (SAM). Our method exceeds the SOTA baselines with a zero-shot transfer from Sim2Real. We also introduce a rich and diverse DLO-specific generated dataset for instance segmentation of DLOs, which will be made publicly available upon acceptance. Our model also achieves an $mIoU = 91.21\%$ on this dataset.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/dfa7500d95a668eca8e5ec493f558cb79a545e6d.pdf" target="_blank">/pdf/dfa7500d95a668eca8e5ec493f558cb79a545e6d.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">Submitted to CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference/Rejected_Submission</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@misc{
anonymous2024optimizing,
title={Optimizing {DLO} Instance Segmentation with Lightweight Adapters and Text Prompts},
author={Anonymous},
year={2024},
url={https://openreview.net/forum?id=wzojxM4DmJ}
}</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wzojxM4DmJ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kozlovsky|optimizing_dlo_instance_segmentation_with_lightweight_adapters_and_text_prompts</div>
</div>
<div class='paper-counter'>15/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wcbrhPnOei</div>
<div class="field-name">title:</div>
<div class="field-value">RobotKeyframing: Learning Locomotion with High-Level Objectives via Mixture of Dense and Sparse Rewards</div>
<div class="field-name">authors:</div>
<div class="field-value">['Fatemeh Zargarbashi', 'Jin Cheng', 'Dongho Kang', 'Robert Sumner', 'Stelian Coros']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Fatemeh_Zargarbashi1', '~Jin_Cheng1', '~Dongho_Kang1', '~Robert_Sumner1', '~Stelian_Coros1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Legged robots', 'Multi-Critic Reinforcement Learning', 'Motion Imitation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a novel learning-based control framework that incorporates high-level objectives for the natural locomotion of legged robots through keyframing.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents a novel learning-based control framework that uses keyframing to incorporate high-level objectives in natural locomotion for legged robots. These high-level objectives are specified as a variable number of partial or complete pose targets that are spaced arbitrarily in time. Our proposed framework utilizes a multi-critic reinforcement learning algorithm to effectively handle the mixture of dense and sparse rewards. Additionally, it employs a transformer-based encoder to accommodate a variable number of input targets, each associated with specific time-to-arrivals. Throughout simulation and hardware experiments, we demonstrate that our framework can effectively satisfy the target keyframe sequence at the required times. In the experiments, the multi-critic method significantly reduces the effort of hyperparameter tuning compared to the standard single-critic alternative. Moreover, the proposed transformer-based architecture enables robots to anticipate future goals, which results in quantitative improvements in their ability to reach their targets.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/706acbedecf6787e3ca6cc3766777e90c26865b4.pdf" target="_blank">/pdf/706acbedecf6787e3ca6cc3766777e90c26865b4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/699f68a71dac78f35e395ae87229c704c0a61e98.zip" target="_blank">/attachment/699f68a71dac78f35e395ae87229c704c0a61e98.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zargarbashi2024robotkeyframing,
title={RobotKeyframing: Learning Locomotion with High-Level Objectives via Mixture of Dense and Sparse Rewards},
author={Fatemeh Zargarbashi and Jin Cheng and Dongho Kang and Robert Sumner and Stelian Coros},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=wcbrhPnOei}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/robot-keyframing</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1822e308a5c5892f19b0928cbd75214c59c32914.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wcbrhPnOei&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zargarbashi|robotkeyframing_learning_locomotion_with_highlevel_objectives_via_mixture_of_dense_and_sparse_rewards</div>
</div>
<div class='paper-counter'>16/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wTKJge0PTq</div>
<div class="field-name">title:</div>
<div class="field-value">HiRT: Enhancing Robotic Control with Hierarchical  Robot Transformers</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jianke Zhang', 'Yanjiang Guo', 'Xiaoyu Chen', 'Yen-Jen Wang', 'Yucheng Hu', 'Chengming Shi', 'Jianyu Chen']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jianke_Zhang1', '~Yanjiang_Guo1', '~Xiaoyu_Chen4', '~Yen-Jen_Wang1', '~Yucheng_Hu1', '~Chengming_Shi1', '~Jianyu_Chen1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Robots', 'Vision Language Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes HiRT, a Hierarchical  Robot Transformer framework that enables flexible frequency and performance  trade-off.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Vision-Language-Action (VLA) models, leveraging powerful pre-trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes \textbf{HiRT}, a \textbf{Hi}erarchical \textbf{R}obot \textbf{T}ransformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, we achieve a 58\% reduction in inference time delay while maintaining comparable success rates. Additionally, on novel dynamic manipulation benchmarks which are challenging for previous VLA models, HiRT improves the success rate from 48% to 75%.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ef04bc30420aa4f2611c49ff222098b5a059c8a6.pdf" target="_blank">/pdf/ef04bc30420aa4f2611c49ff222098b5a059c8a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0297ccb815516249c73b927a9208ee77839035d7.zip" target="_blank">/attachment/0297ccb815516249c73b927a9208ee77839035d7.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024hirt,
title={Hi{RT}: Enhancing Robotic Control with Hierarchical  Robot Transformers},
author={Jianke Zhang and Yanjiang Guo and Xiaoyu Chen and Yen-Jen Wang and Yucheng Hu and Chengming Shi and Jianyu Chen},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=wTKJge0PTq}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/41a4901c3875846becd88ab577b1ae5459d2d99e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wTKJge0PTq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|hirt_enhancing_robotic_control_with_hierarchical_robot_transformers</div>
</div>
<div class='paper-counter'>17/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wSWMsjuMTI</div>
<div class="field-name">title:</div>
<div class="field-value">ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zeyi Liu', 'Cheng Chi', 'Eric Cousineau', 'Naveen Kuppuswamy', 'Benjamin Burchfiel', 'Shuran Song']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zeyi_Liu1', '~Cheng_Chi4', '~Eric_Cousineau1', '~Naveen_Kuppuswamy1', '~Benjamin_Burchfiel1', '~Shuran_Song3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Manipulation', 'Imitation Learning', 'Audio']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce ManiWAV: a data collection device to collect in-the-wild human demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn robot manipulation policy directly from the demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Audio signals provide rich information for the robot interaction and object properties through contact. These information can surprisingly ease the learning of contact-rich robot manipulation skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio data in robot manipulation has been constrained to teleoperated demonstrations collected by either attaching a microphone to the robot or object, which significantly limits its usage in robot learning pipelines. In this work, we introduce ManiWAV: an 'ear-in-hand' data collection device to collect in-the-wild human demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn robot manipulation policy directly from the demonstrations. We demonstrate the capabilities of our system through four contact-rich manipulation tasks that require either passively sensing the contact events and modes, or actively sensing the object surface materials and states. In addition, we show that our system can generalize to unseen in-the-wild environments, by learning from diverse in-the-wild human demonstrations. All data, code, and policy will be public.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6ad87570710d11d4686466d263a435a596581547.pdf" target="_blank">/pdf/6ad87570710d11d4686466d263a435a596581547.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/90b20669b0344cdc27bda78be5ae4aad3cdba626.zip" target="_blank">/attachment/90b20669b0344cdc27bda78be5ae4aad3cdba626.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024maniwav,
title={Mani{WAV}: Learning Robot Manipulation from In-the-Wild Audio-Visual Data},
author={Zeyi Liu and Cheng Chi and Eric Cousineau and Naveen Kuppuswamy and Benjamin Burchfiel and Shuran Song},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=wSWMsjuMTI}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://maniwav.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/47f55602e807ad443b8e69d457a1343b4e0c9416.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wSWMsjuMTI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|maniwav_learning_robot_manipulation_from_inthewild_audiovisual_data</div>
</div>
<div class='paper-counter'>18/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wH7Wv0nAm8</div>
<div class="field-name">title:</div>
<div class="field-value">Bi-Level Motion Imitation for Humanoid Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">['Wenshuai Zhao', 'Yi Zhao', 'Joni Pajarinen', 'Michael Muehlebach']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Wenshuai_Zhao1', '~Yi_Zhao6', '~Joni_Pajarinen2', '~Michael_Muehlebach1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoid Robots', 'Imitation Learning', 'Latent Dynamics Model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a bi-level motion imitation framework based on a novel self-consistent latent dynamics model to improve humanoid learning from MoCap data</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/10607bffe01b6de6343e3fe24a2ba1abe0fdcd65.pdf" target="_blank">/pdf/10607bffe01b6de6343e3fe24a2ba1abe0fdcd65.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/8051ce1a305a792b766a2b296ac5ab41f3f1a7f8.zip" target="_blank">/attachment/8051ce1a305a792b766a2b296ac5ab41f3f1a7f8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhao2024bilevel,
title={Bi-Level Motion Imitation for Humanoid Robots},
author={Wenshuai Zhao and Yi Zhao and Joni Pajarinen and Michael Muehlebach},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=wH7Wv0nAm8}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/bmi-corl2024</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b5b9c6f56b21dbb7455699a5be66a8ebc097e70c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wH7Wv0nAm8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhao|bilevel_motion_imitation_for_humanoid_robots</div>
</div>
<div class='paper-counter'>19/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">wD2kUVLT1g</div>
<div class="field-name">title:</div>
<div class="field-value">Equivariant Diffusion Policy</div>
<div class="field-name">authors:</div>
<div class="field-value">['Dian Wang', 'Stephen Hart', 'David Surovik', 'Tarik Kelestemur', 'Haojie Huang', 'Haibo Zhao', 'Mark Yeatman', 'Jiuguang Wang', 'Robin Walters', 'Robert Platt']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Dian_Wang1', '~Stephen_Hart3', '~David_Surovik1', '~Tarik_Kelestemur1', '~Haojie_Huang1', '~Haibo_Zhao2', 'myeatman@theaiinstitute.com', '~Jiuguang_Wang1', '~Robin_Walters1', '~Robert_Platt1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Equivariance', 'Diffusion Model', 'Robotic Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Equivariant Diffusion Policy that leverages symmetries in diffusion policy learning to improve sample efficiency.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent work has shown diffusion models are an effective approach to learning the multimodal distributions arising from demonstration data in behavior cloning. However, a drawback of this approach is the need to learn a denoising function, which is significantly more complex than learning an explicit policy. In this work, we propose Equivariant Diffusion Policy, a novel diffusion policy learning method that leverages domain symmetries to obtain better sample efficiency and generalization in the denoising function. We theoretically analyze the $\mathrm{SO}(2)$ symmetry of full 6-DoF control and characterize when a diffusion model is $\mathrm{SO}(2)$-equivariant. We furthermore evaluate the method empirically on a set of 12 simulation tasks in MimicGen, and show that it obtains a success rate that is, on average, 21.9\% higher than the baseline Diffusion Policy. We also evaluate the method on a real-world system to show that effective policies can be learned with relatively few training samples, whereas the baseline Diffusion Policy cannot.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/195f7770e9185dedebdc5f1a6760b8d8cb6f43ca.pdf" target="_blank">/pdf/195f7770e9185dedebdc5f1a6760b8d8cb6f43ca.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e6ffe9e59c9704d5270ab60801113a687cb1dc7f.zip" target="_blank">/attachment/e6ffe9e59c9704d5270ab60801113a687cb1dc7f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024equivariant,
title={Equivariant Diffusion Policy},
author={Dian Wang and Stephen Hart and David Surovik and Tarik Kelestemur and Haojie Huang and Haibo Zhao and Mark Yeatman and Jiuguang Wang and Robin Walters and Robert Platt},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=wD2kUVLT1g}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://equidiff.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3e254cf5ab37277a49607e7d20b6880dd2165ad2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=wD2kUVLT1g&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|equivariant_diffusion_policy</div>
</div>
<div class='paper-counter'>20/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">vtEn8NJWlz</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching</div>
<div class="field-name">authors:</div>
<div class="field-value">['Eugenio Chisari', 'Nick Heppert', 'Max Argus', 'Tim Welschehold', 'Thomas Brox', 'Abhinav Valada']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Eugenio_Chisari1', '~Nick_Heppert1', '~Max_Argus2', '~Tim_Welschehold1', '~Thomas_Brox1', '~Abhinav_Valada1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Manipulation', 'Conditional Flow Matching']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Imitation learning via conditional flow matching, and an investigation of the rotation vector space.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning from expert demonstrations is a popular approach to train
robotic manipulation policies from limited data. However, imitation learning
algorithms require a number of design choices ranging from the input modality,
training objective, and 6-DoF end-effector pose representation. Diffusion-based
methods have gained popularity as they allow to predict long horizon trajectories
and handle multimodal action distributions. Recently, Conditional Flow Matching
(CFM) (or Rectified Flow) has been proposed as a more flexible generalization
of diffusion models. In this paper we investigate the application of CFM in the
context of robotic policy learning, and specifically study the interplay with the
other design choices required to build an imitation learning algorithm. We show
that CFM gives the best performance when combined with point cloud input
observations. Additionally, we study the feasibility of a CFM formulation on
the SO(3) manifold and evaluate its suitability with a simplified example. We
perform extensive experiments on RLBench which demonstrate that our proposed
PointFlowMatch approach achieves a state-of-the-art average success rate of 67.8%
over eight tasks, double the performance of the next best method.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e3d7f128340f76b4c56b89bea2ba69e190b2602f.pdf" target="_blank">/pdf/e3d7f128340f76b4c56b89bea2ba69e190b2602f.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chisari2024learning,
title={Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching},
author={Eugenio Chisari and Nick Heppert and Max Argus and Tim Welschehold and Thomas Brox and Abhinav Valada},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=vtEn8NJWlz}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://pointflowmatch.cs.uni-freiburg.de</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/32b80e67b12b66a6258285a1478296d218e59961.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=vtEn8NJWlz&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chisari|learning_robotic_manipulation_policies_from_point_clouds_with_conditional_flow_matching</div>
</div>
<div class='paper-counter'>21/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">vobaOY0qDl</div>
<div class="field-name">title:</div>
<div class="field-value">Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jan Bruedigam', 'Ali Adeeb Abbas', 'Maks Sorokin', 'Kuan Fang', 'Brandon Hung', 'Maya Guru', 'Stefan Georg Sosnowski', 'Jiuguang Wang', 'Sandra Hirche', "Simon Le Cleac'h"]</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jan_Bruedigam1', '~Ali_Adeeb_Abbas1', '~Maks_Sorokin1', '~Kuan_Fang3', 'bhung@theaiinstitute.com', 'mguru@theaiinstitute.com', '~Stefan_Georg_Sosnowski1', '~Jiuguang_Wang1', '~Sandra_Hirche1', "~Simon_Le_Cleac'h1"]</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous Manipulation Planning', 'Learning with Demonstrations']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a manipulation planner that enables bootstrapped reinforcement learning of dexterous and whole-body manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic manipulation is challenging due to discontinuous dynamics, as well as high-dimensional state and action spaces. Data-driven approaches that succeed in manipulation tasks require large amounts of data and expert demonstrations, typically from humans. Existing planners are restricted to specific systems and often depend on specialized algorithms for using demonstrations. Therefore, we introduce a flexible motion planner tailored to dexterous and whole-body manipulation tasks. Our planner creates readily usable demonstrations for reinforcement learning algorithms, eliminating the need for additional training pipeline complexities. With this approach, we can efficiently learn policies for complex manipulation tasks, where traditional reinforcement learning alone only makes little progress. Furthermore, we demonstrate that learned policies are transferable to real robotic systems for solving complex dexterous manipulation tasks.

Project website: https://jacta-manipulation.github.io/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/984cad9b8fd77b527f00b0e468815348bb64a6b8.pdf" target="_blank">/pdf/984cad9b8fd77b527f00b0e468815348bb64a6b8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a231db1f927dc40ce83481bb02129e97da7f2f86.zip" target="_blank">/attachment/a231db1f927dc40ce83481bb02129e97da7f2f86.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
bruedigam2024jacta,
title={Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation},
author={Jan Bruedigam and Ali Adeeb Abbas and Maks Sorokin and Kuan Fang and Brandon Hung and Maya Guru and Stefan Georg Sosnowski and Jiuguang Wang and Sandra Hirche and Simon Le Cleac'h},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=vobaOY0qDl}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://jacta-manipulation.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1944e1f8020635f05c04eccce27bbf2832bb5abe.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=vobaOY0qDl&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">bruedigam|jacta_a_versatile_planner_for_learning_dexterous_and_wholebody_manipulation</div>
</div>
<div class='paper-counter'>22/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">vhGkyWgctu</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Decentralized Multi-Biped Control for Payload Transport</div>
<div class="field-name">authors:</div>
<div class="field-value">['Bikram Pandit', 'Ashutosh Gupta', 'Mohitvishnu S. Gadde', 'Addison Johnson', 'Aayam Kumar Shrestha', 'Helei Duan', 'Jeremy Dao', 'Alan Fern']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Bikram_Pandit1', '~Ashutosh_Gupta3', '~Mohitvishnu_S._Gadde1', '~Addison_Johnson1', '~Aayam_Kumar_Shrestha1', '~Helei_Duan1', '~Jeremy_Dao1', '~Alan_Fern1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-robot Transport', 'Bipedal locomotion', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A straightforward yet highly effective decentralized approach for collaborative payload transport using multiple bipedal robots is proposed, demonstrating strong generalization in simulations and successful sim-to-real transfer.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Payload transport over flat terrain via multi-wheel robot carriers is well-understood, highly effective, and configurable. In this paper, our goal is to provide similar effectiveness and configurability for transport over rough terrain that is more suitable for legs rather than wheels. For this purpose, we consider multi-biped robot carriers, where wheels are replaced by multiple bipedal robots attached to the carrier. Our main contribution is to design a decentralized controller for such systems that can be effectively applied to varying numbers and configurations of rigidly attached bipedal robots without retraining. We present a reinforcement learning approach for training the controller in simulation that supports transfer to the real world. Our experiments in simulation provide quantitative metrics showing the effectiveness of the approach over a wide variety of simulated transport scenarios. In addition, we demonstrate the controller in the real-world for systems composed of two and three Cassie robots. To our knowledge, this is the first example of a scalable multi-biped payload transport system.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/019ac7dc342315ecac281ac0ea8b92067d5a88e6.pdf" target="_blank">/pdf/019ac7dc342315ecac281ac0ea8b92067d5a88e6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a32cf900eff0cad82f64697d90d185edb95ffb3a.zip" target="_blank">/attachment/a32cf900eff0cad82f64697d90d185edb95ffb3a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
pandit2024learning,
title={Learning Decentralized Multi-Biped Control for Payload Transport},
author={Bikram Pandit and Ashutosh Gupta and Mohitvishnu S. Gadde and Addison Johnson and Aayam Kumar Shrestha and Helei Duan and Jeremy Dao and Alan Fern},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=vhGkyWgctu}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://decmbc.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7c6ab5e25963f752fa436cbf1d7bb38b72ad44d6.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=vhGkyWgctu&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">pandit|learning_decentralized_multibiped_control_for_payload_transport</div>
</div>
<div class='paper-counter'>23/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">vBj5oC60Lk</div>
<div class="field-name">title:</div>
<div class="field-value">Lifelong Autonomous Improvement of Navigation Foundation Models in the Wild</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kyle Stachowicz', 'Lydia Ignatova', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kyle_Stachowicz1', '~Lydia_Ignatova1', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Navigation', 'Reinforcement Learning', 'Lifelong Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework for autonomously improving offline RL navigation policies with online RL with minimal-to-no human supervision</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent works have proposed a number of general-purpose robotic foundation models that can control a variety of robotic platforms to perform a range of different tasks, including in the domains of navigation and manipulation. However, such models are typically trained via imitation learning, which precludes the ability to improve autonomously through experience that the robot gathers on the job. In this work, our aim is to train general-purpose robotic foundation models in the domain of robotic navigation specifically with the aim of enabling autonomous self-improvement. We show that a combination of pretraining with offline reinforcement learning and a complete system for continual autonomous operation leads to a robotic learning framework that not only starts off with broad and diverse capabilities, but can further improve and adapt those capabilities in the course of carrying out navigational tasks in a given deployment location. To our knowledge, our model LiReN is the first navigation robot foundation model that is capable of fine-tuning with autonomous online data in open-world settings.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e5fed2a18686895d4bc43dda8a9487b7eb8910e6.pdf" target="_blank">/pdf/e5fed2a18686895d4bc43dda8a9487b7eb8910e6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/61425bc3c502d7782e8969c2d767d1821fd17091.zip" target="_blank">/attachment/61425bc3c502d7782e8969c2d767d1821fd17091.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
stachowicz2024lifelong,
title={Lifelong Autonomous Improvement of Navigation Foundation Models in the Wild},
author={Kyle Stachowicz and Lydia Ignatova and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=vBj5oC60Lk}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://kylestach.github.io/lifelong-nav-rl/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7657078309ff674d29a47e1b40838b078bd9270d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=vBj5oC60Lk&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">stachowicz|lifelong_autonomous_improvement_of_navigation_foundation_models_in_the_wild</div>
</div>
<div class='paper-counter'>24/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ueBmGhLOXP</div>
<div class="field-name">title:</div>
<div class="field-value">EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jingyun Yang', 'Ziang Cao', 'Congyue Deng', 'Rika Antonova', 'Shuran Song', 'Jeannette Bohg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jingyun_Yang1', '~Ziang_Cao2', '~Congyue_Deng1', '~Rika_Antonova1', '~Shuran_Song3', '~Jeannette_Bohg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Equivariance', 'Data Efficiency']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a visuomotor policy learning method that by combining equivariance with diffusion policies is capable of generalizable and data-efficient policy learning in a wide range of robot manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Building effective imitation learning methods that enable robots to learn from limited data and still generalize across diverse real-world environments is a long-standing problem in robot learning. We propose EquiBot, a robust, data-efficient, and generalizable approach for robot manipulation task learning. Our approach combines SIM(3)-equivariant neural network architectures with diffusion models. This ensures that our learned policies are invariant to changes in scale, rotation, and translation, enhancing their applicability to unseen environments while retaining the benefits of diffusion-based policy learning such as multi-modality and robustness. We show on a suite of 6 simulation tasks that our proposed method reduces the data requirements and improves generalization to novel scenarios. In the real world, with 10 variations of 6 mobile manipulation tasks, we show that our method can easily generalize to novel objects and scenes after learning from just 5 minutes of human demonstrations in each task.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c8d48eda897ebcb7713b57be20fb90ed889aca0d.pdf" target="_blank">/pdf/c8d48eda897ebcb7713b57be20fb90ed889aca0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/34ae72727ed89e2d69fe2287a71bd4b8d99e1300.zip" target="_blank">/attachment/34ae72727ed89e2d69fe2287a71bd4b8d99e1300.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yang2024equibot,
title={EquiBot: {SIM}(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning},
author={Jingyun Yang and Ziang Cao and Congyue Deng and Rika Antonova and Shuran Song and Jeannette Bohg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ueBmGhLOXP}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://equi-bot.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/42e5723e7522fcaf79ae23f7a2e0abf2a49b8387.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ueBmGhLOXP&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yang|equibot_sim3equivariant_diffusion_policy_for_generalizable_and_data_efficient_learning</div>
</div>
<div class='paper-counter'>25/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ubq7Co6Cbv</div>
<div class="field-name">title:</div>
<div class="field-value">Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks</div>
<div class="field-name">authors:</div>
<div class="field-value">['Alex Quach', 'Makram Chahine', 'Alexander Amini', 'Ramin Hasani', 'Daniela Rus']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Alex_Quach1', '~Makram_Chahine1', '~Alexander_Amini1', '~Ramin_Hasani1', '~Daniela_Rus1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['End-to-end learning', 'Gaussian Splatting', 'Sim-to-real transfer']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work enhances sim-to-real quadrotor navigation by combining Gaussian Splatting with Liquid neural networks, achieving robust real-world performance and adaptability to environmental changes.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. 
However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target maneuvers curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3b4801a01c54302e09d1377f68c7b29475f41980.pdf" target="_blank">/pdf/3b4801a01c54302e09d1377f68c7b29475f41980.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ca8f4d520c912e4c2f28d03751fc935009861969.zip" target="_blank">/attachment/ca8f4d520c912e4c2f28d03751fc935009861969.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
quach2024gaussian,
title={Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks},
author={Alex Quach and Makram Chahine and Alexander Amini and Ramin Hasani and Daniela Rus},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ubq7Co6Cbv}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/gs2real-flight/home</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/26acc9581f2078ccc88ad8d61f1740899c292ef7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ubq7Co6Cbv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">quach|gaussian_splatting_to_real_world_flight_navigation_transfer_with_liquid_networks</div>
</div>
<div class='paper-counter'>26/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">uMZ2jnZUDX</div>
<div class="field-name">title:</div>
<div class="field-value">Learning H-Infinity Locomotion Control</div>
<div class="field-name">authors:</div>
<div class="field-value">['Junfeng Long', 'Wenye Yu', 'Quanyi Li', 'ZiRui Wang', 'Dahua Lin', 'Jiangmiao Pang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Junfeng_Long1', '~Wenye_Yu1', '~Quanyi_Li1', '~ZiRui_Wang8', '~Dahua_Lin1', '~Jiangmiao_Pang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Learning', 'Quadrupedal Robot', 'Robust Locomotion']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">The actor acts, the disturber disturbs, and the H-Infinity constraint helps them enhance the robot's disturbance resistance capability.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Stable locomotion in precipitous environments is an essential task for quadruped robots, requiring the ability to resist various external disturbances. Recent neural policies enhance robustness against disturbances by learning to resist external forces sampled from a fixed distribution in the simulated environment. However, the force generation process doesn’t consider the robot’s current state, making it difficult to identify the most effective direction and magnitude that can push the robot to the most unstable but recoverable state. Thus, challenging cases in the buffer are insufficient to optimize robustness. In this paper, we propose to model the robust locomotion learning process as an adversarial interaction between the locomotion policy and a learnable disturbance that is conditioned on the robot state to generate appropriate external forces. To make the joint optimization stable, our novel $H_{\infty}$ constraint mandates the bound of the ratio between the cost and the intensity of the external forces. We verify the robustness of our approach in both simulated environments and real-world deployment, on quadrupedal locomotion tasks and a more challenging task where the quadruped performs locomotion merely on hind legs. Training and deployment code will be made public.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3b87d7d12cceb240fe5f085d14ab910756675e6d.pdf" target="_blank">/pdf/3b87d7d12cceb240fe5f085d14ab910756675e6d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/fa0dda93cb0a787e8b186a014216092af3430f0e.zip" target="_blank">/attachment/fa0dda93cb0a787e8b186a014216092af3430f0e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
long2024learning,
title={Learning H-Infinity Locomotion Control},
author={Junfeng Long and Wenye Yu and Quanyi Li and ZiRui Wang and Dahua Lin and Jiangmiao Pang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=uMZ2jnZUDX}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://junfeng-long.github.io/HINF/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3df5143ce7553c17ac98a016359e88592d30e004.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=uMZ2jnZUDX&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">long|learning_hinfinity_locomotion_control</div>
</div>
<div class='paper-counter'>27/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">uJBMZ6S02T</div>
<div class="field-name">title:</div>
<div class="field-value">Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jia-Feng Cai', 'Zibo Chen', 'Xiao-Ming Wu', 'Jian-Jian Jiang', 'Yi-Lin Wei', 'Wei-Shi Zheng']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jia-Feng_Cai1', '~Zibo_Chen1', '~Xiao-Ming_Wu5', '~Jian-Jian_Jiang1', '~Yi-Lin_Wei1', '~Wei-Shi_Zheng3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Grasp pose detection', 'simulated datasets', 'sim-to-real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A sim-to-real mehod for 6-DoF grasp poses detection</div>
<div class="field-name">abstract:</div>
<div class="field-value">For 6-DoF grasp detection, simulated data is expandable to train more powerful model, but it faces the challenge of the large gap between simulation and real world. Previous works bridge this gap with a sim-to-real way. However, this way explicitly or implicitly forces the simulated data to adapt to the noisy real data when training grasp detectors, where the positional drift and structural distortion within the camera noise will harm the grasp learning. In this work, we propose a Real-to-Sim framework for 6-DoF Grasp detection, named R2SGrasp, with the key insight of bridging this gap in a real-to-sim way, which directly bypasses the camera noise in grasp detector training through an inference-time real-to-sim adaption. To achieve this real-to-sim adaptation, our R2SGrasp designs the Real-to-Sim Data Repairer (R2SRepairer) to mitigate the camera noise of real depth maps in data-level, and the Real-to-Sim Feature Enhancer (R2SEnhancer) to enhance real features with precise simulated geometric primitives in feature-level. To endow our framework with the generalization ability, we construct a large-scale simulated dataset cost-efficiently to train our grasp detector, which includes 64,000 RGB-D images with 14.4 million grasp annotations. Sufficient experiments show that R2SGrasp is powerful and our real-to-sim perspective is effective. The real-world experiments further show great generalization ability of R2SGrasp. Project page is available on https://isee-laboratory.github.io/R2SGrasp.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5c562d18ba61e63d5c7e145c801efb8b122bfd6a.pdf" target="_blank">/pdf/5c562d18ba61e63d5c7e145c801efb8b122bfd6a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ed7f1698ed24313ab2d3cc0ba6bfd21fbc299385.zip" target="_blank">/attachment/ed7f1698ed24313ab2d3cc0ba6bfd21fbc299385.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cai2024realtosim,
title={Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection},
author={Jia-Feng Cai and Zibo Chen and Xiao-Ming Wu and Jian-Jian Jiang and Yi-Lin Wei and Wei-Shi Zheng},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=uJBMZ6S02T}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://isee-laboratory.github.io/R2SGrasp</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1fe3aae153349f15d9eef5125b48e82f1e9fee3d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=uJBMZ6S02T&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cai|realtosim_grasp_rethinking_the_gap_between_simulation_and_real_world_in_grasp_detection</div>
</div>
<div class='paper-counter'>28/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">uHdVI3QMr6</div>
<div class="field-name">title:</div>
<div class="field-value">A Dual Approach to Imitation Learning from Observations with Offline Datasets</div>
<div class="field-name">authors:</div>
<div class="field-value">['Harshit Sikchi', 'Caleb Chuck', 'Amy Zhang', 'Scott Niekum']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Harshit_Sikchi1', '~Caleb_Chuck1', '~Amy_Zhang1', '~Scott_Niekum1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Learning from Observations', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A dual approach to LfO that is principled, computationally efficient, and empirically performant.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Demonstrations are an effective alternative to task specification for learning agents in settings where designing a reward function is difficult. However, demonstrating expert behavior in the action space of the agent becomes unwieldy when robots have complex, unintuitive morphologies. We consider the practical setting where an agent has a dataset of prior interactions with the environment and is provided with observation-only expert demonstrations. Typical learning from observations approaches have required either learning an inverse dynamics model or a discriminator as intermediate steps of training. Errors in these intermediate one-step models compound during downstream policy learning or deployment. We overcome these limitations by directly learning a multi-step utility function that quantifies how each action impacts the agent's divergence from the expert's visitation distribution. Using the principle of duality, we derive DILO (Dual Imitation Learning from Observations), an algorithm that can leverage arbitrary suboptimal data to learn imitating policies without requiring expert actions. DILO reduces the learning from observations problem to that of simply learning an actor and a critic, bearing similar complexity to vanilla offline RL. This allows DILO to gracefully scale to high dimensional observations, and demonstrate improved performance across the board.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1e226e1b3a473ea1b57031201ded3d739d7ffe24.pdf" target="_blank">/pdf/1e226e1b3a473ea1b57031201ded3d739d7ffe24.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/19300e04358cc22fa507ba7409740c27b9d19488.zip" target="_blank">/attachment/19300e04358cc22fa507ba7409740c27b9d19488.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sikchi2024a,
title={A Dual Approach to Imitation Learning from Observations with Offline Datasets},
author={Harshit Sikchi and Caleb Chuck and Amy Zhang and Scott Niekum},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=uHdVI3QMr6}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://hari-sikchi.github.io/dilo/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ac3fc1ecd96347e61acdb40ddfb66364eac96785.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=uHdVI3QMr6&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sikchi|a_dual_approach_to_imitation_learning_from_observations_with_offline_datasets</div>
</div>
<div class='paper-counter'>29/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">uEbJXWobif</div>
<div class="field-name">title:</div>
<div class="field-value">EXTRACT: Efficient Policy Learning by Extracting Transferable Robot Skills from Offline Data</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jesse Zhang', 'Minho Heo', 'Zuxin Liu', 'Erdem Biyik', 'Joseph J Lim', 'Yao Liu', 'Rasool Fakoor']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jesse_Zhang3', '~Minho_Heo1', '~Zuxin_Liu1', '~Erdem_Biyik1', '~Joseph_J_Lim1', '~Yao_Liu1', '~Rasool_Fakoor1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['reinforcement learning', 'skill-based reinformement learning', 'skill learning', 'transfer learning', 'foundation models for robotics', 'robot learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We extract discrete, meaningfully-aligned skills from offline data for efficient reinforcement learning of new tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Most reinforcement learning (RL) methods focus on learning optimal policies over low-level action spaces. While these methods can perform well in their training environments, they lack the flexibility to transfer to new tasks. 
Instead, RL agents that can act over useful, temporally extended skills rather than low-level actions can learn new tasks more easily.
Prior work in skill-based RL either requires expert supervision to define useful skills, which is hard to scale, or learns a skill-space from offline data with heuristics that limit the adaptability of the skills, making them difficult to transfer during downstream RL.
Our approach, EXTRACT, instead utilizes pre-trained vision language models to extract a discrete set of semantically meaningful skills from offline data, each of which is parameterized by continuous arguments, without human supervision. 
This skill parameterization allows robots to learn new tasks by only needing to learn when to select a specific skill and how to modify its arguments for the specific task.
We demonstrate through experiments in sparse-reward, image-based, robot manipulation environments that EXTRACT can more quickly learn new tasks than prior works, with major gains in sample efficiency and performance over prior skill-based RL.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/881f064e65935a3f9336654ed7fc4a4ace26b828.pdf" target="_blank">/pdf/881f064e65935a3f9336654ed7fc4a4ace26b828.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/5cbb40be809950cf4c5521430c449fc41b52ccc2.zip" target="_blank">/attachment/5cbb40be809950cf4c5521430c449fc41b52ccc2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024extract,
title={{EXTRACT}: Efficient Policy Learning by Extracting Transferable Robot Skills from Offline Data},
author={Jesse Zhang and Minho Heo and Zuxin Liu and Erdem Biyik and Joseph J Lim and Yao Liu and Rasool Fakoor},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=uEbJXWobif}
}</div>
<div class="field-name">website:</div>
<div class="field-value">jessezhang.net/projects/extract</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/14630377089ea66c2d5b3d24b6570eb70bee002b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=uEbJXWobif&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|extract_efficient_policy_learning_by_extracting_transferable_robot_skills_from_offline_data</div>
</div>
<div class='paper-counter'>30/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ty1cqzTtUv</div>
<div class="field-name">title:</div>
<div class="field-value">RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches</div>
<div class="field-name">authors:</div>
<div class="field-value">['Priya Sundaresan', 'Quan Vuong', 'Jiayuan Gu', 'Peng Xu', 'Ted Xiao', 'Sean Kirmani', 'Tianhe Yu', 'Michael Stark', 'Ajinkya Jain', 'Karol Hausman', 'Dorsa Sadigh', 'Jeannette Bohg', 'Stefan Schaal']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Priya_Sundaresan1', '~Quan_Vuong2', '~Jiayuan_Gu1', '~Peng_Xu9', '~Ted_Xiao1', '~Sean_Kirmani1', '~Tianhe_Yu1', '~Michael_Stark5', '~Ajinkya_Jain1', '~Karol_Hausman2', '~Dorsa_Sadigh1', '~Jeannette_Bohg1', '~Stefan_Schaal1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Visual Imitation Learning', 'Goal-Conditioned Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel framework for goal-conditioned imitation learning based on hand-drawn sketches, which offer convenience without compromising on goal specificity.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Natural language and images are commonly used as goal representations in goal-conditioned imitation learning. However, language can be ambiguous and images can be over-specified. In this work, we study hand-drawn sketches as a modality for goal specification. Sketches can be easy to provide on the fly like language, but like images they can also help a downstream policy to be spatially-aware. By virtue of being minimal, sketches can further help disambiguate task-relevant from irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of trajectories paired with synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch performs comparably to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch handles sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please visit http://rt-sketch.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/23c7a6c13cc39c2f73c31dfe42cc249bfae0acb3.pdf" target="_blank">/pdf/23c7a6c13cc39c2f73c31dfe42cc249bfae0acb3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9a8a9b17c9de38e1bcf1b4c3c880964abb2c09b1.zip" target="_blank">/attachment/9a8a9b17c9de38e1bcf1b4c3c880964abb2c09b1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sundaresan2024rtsketch,
title={{RT}-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches},
author={Priya Sundaresan and Quan Vuong and Jiayuan Gu and Peng Xu and Ted Xiao and Sean Kirmani and Tianhe Yu and Michael Stark and Ajinkya Jain and Karol Hausman and Dorsa Sadigh and Jeannette Bohg and Stefan Schaal},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ty1cqzTtUv}
}</div>
<div class="field-name">website:</div>
<div class="field-value">rt-sketch.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c048730dda6b803afcb8795e91dc07615b655439.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ty1cqzTtUv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sundaresan|rtsketch_goalconditioned_imitation_learning_from_handdrawn_sketches</div>
</div>
<div class='paper-counter'>31/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">tqsQGrmVEu</div>
<div class="field-name">title:</div>
<div class="field-value">View-Invariant Policy Learning via Zero-Shot Novel View Synthesis</div>
<div class="field-name">authors:</div>
<div class="field-value">['Stephen Tian', 'Blake Wulfe', 'Kyle Sargent', 'Katherine Liu', 'Sergey Zakharov', 'Vitor Campagnolo Guizilini', 'Jiajun Wu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Stephen_Tian1', '~Blake_Wulfe1', '~Kyle_Sargent1', '~Katherine_Liu1', '~Sergey_Zakharov1', '~Vitor_Campagnolo_Guizilini2', '~Jiajun_Wu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['generalization', 'visual imitation learning', 'view synthesis']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We describe and evaluate a simple yet effective method for making visuomotor policies robust to changes in camera pose by leveraging zero-shot novel view synthesis methods.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. 
In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image.
For practical application to diverse robotic data, these models must operate *zero-shot*, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/67f19a80ddd979dde451b97e3e0079c69bda8445.pdf" target="_blank">/pdf/67f19a80ddd979dde451b97e3e0079c69bda8445.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/20fc8ff578ff6ddd248d5ec31a7b451ab93f060d.zip" target="_blank">/attachment/20fc8ff578ff6ddd248d5ec31a7b451ab93f060d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tian2024viewinvariant,
title={View-Invariant Policy Learning via Zero-Shot Novel View Synthesis},
author={Stephen Tian and Blake Wulfe and Kyle Sargent and Katherine Liu and Sergey Zakharov and Vitor Campagnolo Guizilini and Jiajun Wu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=tqsQGrmVEu}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://s-tian.github.io/projects/vista/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2d9a94976ce283ab589bf4d2e1f6ef68e9b90198.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=tqsQGrmVEu&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tian|viewinvariant_policy_learning_via_zeroshot_novel_view_synthesis</div>
</div>
<div class='paper-counter'>32/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">t0LkF9JnVb</div>
<div class="field-name">title:</div>
<div class="field-value">PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Cheng Qian', 'Julen Urain', 'Kevin Zakka', 'Jan Peters']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Cheng_Qian8', '~Julen_Urain2', '~Kevin_Zakka1', '~Jan_Peters3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Imitation Learning', 'Robotics', 'Dexterous Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train a generalist policy for controlling dexterous robot hands to play any songs, using human pianist demonstration videos from internet.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we introduce PianoMime, a framework for training a piano-playing agent using internet demonstrations.
The internet is a promising source of large-scale demonstrations for training our robot agents. 
In particular, for the case of piano-playing, Youtube is full of videos of professional pianists playing a wide myriad of songs.
In our work, we leverage these demonstrations to learn a generalist piano-playing agent capable of playing any arbitrary song.
Our framework is divided into three parts: a data preparation phase to extract the informative features from the Youtube videos,
a policy learning phase to train song-specific expert policies from the demonstrations and a policy distillation phase to distil the policies into a single generalist agent.
We explore different policy designs to represent the agent and evaluate the influence of the amount of training data on the generalization capability of the agent to novel songs not available in the dataset.
We show that we are able to learn a policy with up to 57% F1 score on unseen songs.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1a212880e992488cf2f96101541dba801529b41f.pdf" target="_blank">/pdf/1a212880e992488cf2f96101541dba801529b41f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/44e35fccb0c7c2c86043b2b40f7361f0c0abb012.zip" target="_blank">/attachment/44e35fccb0c7c2c86043b2b40f7361f0c0abb012.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
qian2024pianomime,
title={PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations},
author={Cheng Qian and Julen Urain and Kevin Zakka and Jan Peters},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=t0LkF9JnVb}
}</div>
<div class="field-name">website:</div>
<div class="field-value">pianomime.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5488ccab30be6f8dbe70207ad746cf43955510f0.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=t0LkF9JnVb&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">qian|pianomime_learning_a_generalist_dexterous_piano_player_from_internet_demonstrations</div>
</div>
<div class='paper-counter'>33/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">s31IWg2kN5</div>
<div class="field-name">title:</div>
<div class="field-value">Exploring Under Constraints with Model-Based Actor-Critic and Safety Filters</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ahmed Agha', 'Baris Kayalibay', 'Atanas Mirchev', 'Patrick van der Smagt', 'Justin Bayer']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ahmed_Agha1', '~Baris_Kayalibay1', '~Atanas_Mirchev1', '~Patrick_van_der_Smagt1', '~Justin_Bayer1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Model-based RL', 'Safe RL', 'Safety Filter', 'Exploration']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We combine constrained model-based policy optimization with planning-based safety filters as a backup policy to reduce constraint violation rates during exploration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Applying reinforcement learning (RL) to learn effective policies on physical robots without supervision remains challenging when it comes to tasks where safe exploration is critical. Constrained model-based RL (CMBRL) presents a promising approach to this problem. These methods are designed to learn constraint-adhering policies through constrained optimization approaches. Yet, such policies often fail to meet stringent safety requirements during learning and exploration. Our solution ``CASE'' aims to reduce the instances where constraints are breached during the learning phase. Specifically, CASE integrates techniques for optimizing constrained policies and employs planning-based safety filters as backup policies, effectively lowering constraint violations during learning and making it a more reliable option than other recent constrained model-based policy optimization methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/67fe940272135897fe2b7a68647fa9a22382212d.pdf" target="_blank">/pdf/67fe940272135897fe2b7a68647fa9a22382212d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3a76327920b0715267c764c54afd4ba00b79dc87.zip" target="_blank">/attachment/3a76327920b0715267c764c54afd4ba00b79dc87.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
agha2024exploring,
title={Exploring Under Constraints with Model-Based Actor-Critic and Safety Filters},
author={Ahmed Agha and Baris Kayalibay and Atanas Mirchev and Patrick van der Smagt and Justin Bayer},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=s31IWg2kN5}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/128939421defb59db2d262439a97d7f7e28a88fd.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=s31IWg2kN5&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">agha|exploring_under_constraints_with_modelbased_actorcritic_and_safety_filters</div>
</div>
<div class='paper-counter'>34/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">s0vHSq5QEv</div>
<div class="field-name">title:</div>
<div class="field-value">Generalizing End-To-End Autonomous Driving In Real-World Environments Using Zero-Shot LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zeyu Dong', 'Yimin Zhu', 'Yansong Li', 'Kevin Mahon', 'Yu Sun']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zeyu_Dong3', '~Yimin_Zhu2', '~Yansong_Li2', '~Kevin_Mahon1', '~Yu_Sun22']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['End-to-end Autonomous Driving', 'Large Vision-Language Model', 'Generalization']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We proposed an architecture that combines LLMs with end-to-end driving models, characterized by low data requirements, strong generalization ability, and suitability for real-world closed-loop inference.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Traditional autonomous driving methods adopt modular design, decomposing tasks into sub-tasks, including perception, prediction, planning, and control. In contrast, end-to-end autonomous driving directly outputs actions from raw sensor data, avoiding error accumulation. However, training an end-to-end model requires a comprehensive dataset. Without adequate data, the end-to-end model exhibits poor generalization capabilities. Recently, large language models (LLMs) have been applied to enhance the generalization property of end-to-end driving models. Most studies explore LLMs in an open-loop manner, where the output actions are compared to those of experts without direct activation in the real world. Other studies in closed-loop settings examine their results in simulated environments. In comparison, this paper proposes an efficient architecture that integrates multimodal LLMs into end-to-end real-world driving models in a closed-loop setting. The LLM periodically takes raw sensor data to generate high-level driving instructions. In our architecture, LLMs can effectively guide the end-to-end model, even at a slower rate than the raw sensor data, because updates aren't needed every time frame. This architecture relaxes the trade-off between the latency and inference quality of the LLM. It also allows us to choose a wide variety of LLMs to improve high-level driving instructions and minimize fine-tuning costs. Consequently, our architecture reduces the data collection requirements because the LLMs do not directly output actions, and we only need to train a simple imitation learning model to output actions. In our experiments, the training data for the end-to-end model in a real-world environment consists of only simple obstacle configurations with one traffic cone, while the test environment is more complex and contains different types of obstacles. Experiments show that the proposed architecture enhances the generalization capabilities of the end-to-end model even without fine-tuning the LLM.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a68cb7d6cab94ea9513433184808e5cce17f3405.pdf" target="_blank">/pdf/a68cb7d6cab94ea9513433184808e5cce17f3405.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/14b81f9ac412465053abe9250c7dcf63d193456f.zip" target="_blank">/attachment/14b81f9ac412465053abe9250c7dcf63d193456f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
dong2024generalizing,
title={Generalizing End-To-End Autonomous Driving In Real-World Environments Using Zero-Shot {LLM}s},
author={Zeyu Dong and Yimin Zhu and Yansong Li and Kevin Mahon and Yu Sun},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=s0vHSq5QEv}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8afe1e66a2a474274f8a5e4b18d0ac11aed64000.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=s0vHSq5QEv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">dong|generalizing_endtoend_autonomous_driving_in_realworld_environments_using_zeroshot_llms</div>
</div>
<div class='paper-counter'>35/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">s0VNSnPeoA</div>
<div class="field-name">title:</div>
<div class="field-value">Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jakob Thumm', 'Christopher Agia', 'Marco Pavone', 'Matthias Althoff']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jakob_Thumm1', '~Christopher_Agia1', '~Marco_Pavone1', '~Matthias_Althoff1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Human-Robot Interaction', 'Human Preference Learning', 'Task and Motion Planning', 'Safe Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Text2Interaction integrates human preferences into the task, motion, and control level of the robotic stack.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Adjusting robot behavior to human preferences can require intensive human feedback, preventing quick adaptation to new users and changing circumstances. Moreover, current approaches typically treat user preferences as a reward, which requires a manual balance between task success and user satisfaction. To integrate new user preferences in a zero-shot manner, our proposed Text2Interaction framework invokes large language models to generate a task plan, motion preferences as Python code, and parameters of a safety controller. By maximizing the combined probability of task completion and user satisfaction instead of a weighted sum of rewards, we can reliably find plans that fulfill both requirements. We find that 83% of users working with Text2Interaction agree that it integrates their preferences into the plan of the robot, and 94% prefer Text2Interaction over the baseline. Our ablation study shows that Text2Interaction aligns better with unseen preferences than other baselines while maintaining a high success rate. Real-world demonstrations and code are made available at [sites.google.com/view/text2interaction](sites.google.com/view/text2interaction).</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9917027112c158b943243d5cf4a0247b35d52eab.pdf" target="_blank">/pdf/9917027112c158b943243d5cf4a0247b35d52eab.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f990f8fd68e717207c76d29b9901bf49ddb94ee0.zip" target="_blank">/attachment/f990f8fd68e717207c76d29b9901bf49ddb94ee0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
thumm2024textinteraction,
title={Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction},
author={Jakob Thumm and Christopher Agia and Marco Pavone and Matthias Althoff},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=s0VNSnPeoA}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/text2interaction/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/747b242afae70f3745296820bb415b297ff310bd.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=s0VNSnPeoA&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">thumm|text2interaction_establishing_safe_and_preferable_humanrobot_interaction</div>
</div>
<div class='paper-counter'>36/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">rvKWXxIvj0</div>
<div class="field-name">title:</div>
<div class="field-value">Non-rigid Relative Placement through 3D Dense Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">['Eric Cai', 'Octavian Donca', 'Ben Eisner', 'David Held']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Eric_Cai1', 'odonca@andrew.cmu.edu', '~Ben_Eisner1', '~David_Held1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Deformable', 'Non-rigid', 'Manipulation', 'Relative Placement']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We expand the "relative placement" formulation beyond rigid body manipulation, allowing for generalization to novel object instances, unseen scene configurations, and multimodal placements for highly deformable tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The task of "relative placement" is to predict the placement of one object in relation to another, e.g. placing a mug on a mug rack. Recent methods for relative placement have made tremendous progress towards data-efficient learning for robot manipulation; using explicit object-centric geometric reasoning, these approaches enable generalization to unseen task variations from a small number of demonstrations. State-of-the-art works in this area, however, have yet to represent deformable transformations, despite the ubiquity of non-rigid bodies in real world settings. As a first step towards bridging this gap, we propose "cross-displacement" - an extension of the principles of relative placement to geometric relationships between deformable objects - and present a novel vision-based method to learn cross-displacement for a non-rigid task through dense diffusion. To this end, we demonstrate our method's ability to generalize to unseen object instances, out-of-distribution scene configurations, and multimodal goals on a highly deformable cloth-hanging task beyond the scope of prior works.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f197bce3e78538ba039227fcde0e119b95a05301.pdf" target="_blank">/pdf/f197bce3e78538ba039227fcde0e119b95a05301.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/7581c4eca3e8c6623af83bc7eeb71a5bea434aae.zip" target="_blank">/attachment/7581c4eca3e8c6623af83bc7eeb71a5bea434aae.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cai2024nonrigid,
title={Non-rigid Relative Placement through 3D Dense Diffusion},
author={Eric Cai and Octavian Donca and Ben Eisner and David Held},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=rvKWXxIvj0}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/tax3d-corl-2024</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/25e98761f91ffa103a2fb45966c83195b9e2a0aa.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=rvKWXxIvj0&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cai|nonrigid_relative_placement_through_3d_dense_diffusion</div>
</div>
<div class='paper-counter'>37/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">rY5T2aIjPZ</div>
<div class="field-name">title:</div>
<div class="field-value">DeliGrasp: Inferring Object Properties with LLMs for Adaptive Grasp Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">['William Xie', 'Maria Valentini', 'Jensen Lavering', 'Nikolaus Correll']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~William_Xie1', '~Maria_Valentini1', '~Jensen_Lavering1', '~Nikolaus_Correll1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['contact-rich manipulation', 'adaptive grasping', 'force control', 'produce manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We prompt GPT-4 to infer object mass, friction, and compliance terms and to translate them into grasp policies that outperform traditional grasps on delicate objects; we also improve property estimation and grasping for atypical objects</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping. We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics-mass $m$, friction coefficient $\mu$, and spring constant $k$-from a semantic description, and then translate those characteristics into an executable adaptive grasp policy. Using a two-finger gripper with a built-in depth camera that can control its torque by limiting motor current, we demonstrate that LLM-parameterized but first-principles grasp policies outperform both traditional adaptive grasp policies and direct LLM-as-code policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force. We then improve property estimation and grasp performance on variable size objects with model finetuning on property-based comparisons and eliciting such comparisons via chain-of-thought prompting. We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness. Our code and videos are available at: https://deligrasp.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9bbfc280da4bb85461a1cd2e2abb14ecda97abd4.pdf" target="_blank">/pdf/9bbfc280da4bb85461a1cd2e2abb14ecda97abd4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c29195e3f38a478e8783685621d1b04b23b97c8f.zip" target="_blank">/attachment/c29195e3f38a478e8783685621d1b04b23b97c8f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xie2024deligrasp,
title={DeliGrasp: Inferring Object Properties with {LLM}s for Adaptive Grasp Policies},
author={William Xie and Maria Valentini and Jensen Lavering and Nikolaus Correll},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=rY5T2aIjPZ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">deligrasp.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ad1e00bab33b7dc2f81920b474adcf8aad93f554.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=rY5T2aIjPZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xie|deligrasp_inferring_object_properties_with_llms_for_adaptive_grasp_policies</div>
</div>
<div class='paper-counter'>38/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">rThtgkXuvZ</div>
<div class="field-name">title:</div>
<div class="field-value">NOD-TAMP: Generalizable Long-Horizon Planning with Neural Object Descriptors</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shuo Cheng', 'Caelan Reed Garrett', 'Ajay Mandlekar', 'Danfei Xu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shuo_Cheng1', '~Caelan_Reed_Garrett1', '~Ajay_Mandlekar1', '~Danfei_Xu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Learning', 'Robot Planning', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce NOD-TAMP, a TAMP-based framework that can solve broad long-horizon manipulation tasks by adapting and composing short manipulation trajectories from a handful of human demonstration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Solving complex manipulation tasks in household and factory settings remains challenging due to long-horizon reasoning, fine-grained interactions, and broad object and scene diversity. Learning skills from demonstrations can be an effective strategy, but such methods often have limited generalizability beyond training data and struggle to solve long-horizon tasks. To overcome this, we propose to synergistically combine two paradigms: Neural Object Descriptors (NODs) that produce generalizable object-centric features and Task and Motion Planning (TAMP) frameworks that chain short-horizon skills to solve multi-step tasks. We introduce NOD-TAMP, a TAMP-based framework that extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon, contact-rich tasks. NOD-TAMP solves existing manipulation benchmarks with a handful of demonstrations and significantly outperforms prior NOD-based approaches on new tabletop manipulation tasks that require diverse generalization. Finally, we deploy NOD-TAMP on a number of real-world tasks, including tool-use and high-precision insertion. For more details, please visit https://nodtamp.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ac8a42bacb318cf68f5bd4aab26c3470c69c8b3a.pdf" target="_blank">/pdf/ac8a42bacb318cf68f5bd4aab26c3470c69c8b3a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/901cdbc3b253f90ebbb02b9ca89523b6f9610146.zip" target="_blank">/attachment/901cdbc3b253f90ebbb02b9ca89523b6f9610146.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cheng2024nodtamp,
title={{NOD}-{TAMP}: Generalizable Long-Horizon Planning with Neural Object Descriptors},
author={Shuo Cheng and Caelan Reed Garrett and Ajay Mandlekar and Danfei Xu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=rThtgkXuvZ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://nodtamp.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/38338e3659c8f0d8347725e38dc299149c1a1a9d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=rThtgkXuvZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cheng|nodtamp_generalizable_longhorizon_planning_with_neural_object_descriptors</div>
</div>
<div class='paper-counter'>39/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">rRpmVq6yHv</div>
<div class="field-name">title:</div>
<div class="field-value">SELFI: Autonomous Self-Improvement with RL for Vision-Based Navigation around People</div>
<div class="field-name">authors:</div>
<div class="field-value">['Noriaki Hirose', 'Dhruv Shah', 'Kyle Stachowicz', 'Ajay Sridhar', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Noriaki_Hirose1', '~Dhruv_Shah1', '~Kyle_Stachowicz1', '~Ajay_Sridhar1', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['online reinforcement learning', 'vision-based navigation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Online model-free RL coorperating offline model-based learning objective and its application to vision-based navigation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-emptive behavior for the pedestrians, collision avoidance for small and transparent objects, and avoiding travel on uneven floor surfaces. We provide supplementary videos to demonstrate the performance of our fine-tuned policy.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ed4d721396d1fbb1c012a87a82a928ee42dbe9ed.pdf" target="_blank">/pdf/ed4d721396d1fbb1c012a87a82a928ee42dbe9ed.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a0c83fc92a77565ad579025bafff96cc88c6d7a2.zip" target="_blank">/attachment/a0c83fc92a77565ad579025bafff96cc88c6d7a2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hirose2024selfi,
title={{SELFI}: Autonomous Self-Improvement with {RL} for Vision-Based Navigation around People},
author={Noriaki Hirose and Dhruv Shah and Kyle Stachowicz and Ajay Sridhar and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=rRpmVq6yHv}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/selfi-rl/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5b578c4130b62107182fed0391646addd55da026.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=rRpmVq6yHv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hirose|selfi_autonomous_selfimprovement_with_rl_for_visionbased_navigation_around_people</div>
</div>
<div class='paper-counter'>40/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">rEteJcq61j</div>
<div class="field-name">title:</div>
<div class="field-value">Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ziwei Liao', 'Binbin Xu', 'Steven L. Waslander']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ziwei_Liao1', '~Binbin_Xu1', '~Steven_L._Waslander1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Mapping', 'Objects Reconstruction', 'Pose Estimation', 'Diffusion']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Object-level mapping builds a 3D map of objects in a scene with detailed shapes and poses from multi-view sensor observations.   Conventional methods struggle to build complete shapes and estimate accurate poses due to partial occlusions and sensor noise. They require dense observations to cover all objects, which is challenging to achieve in robotics trajectories.  Recent work introduces generative shape priors for object-level mapping from sparse views, but is limited to single-category objects. In this work, we propose a General Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape prior with multi-category support and outputs Neural Radiance Fields (NeRFs) for both texture and geometry for all objects in a scene. 
GOM includes an effective formulation to guide a pre-trained diffusion model with extra nonlinear constraints from sensor measurements without finetuning. We also develop a probabilistic optimization formulation to fuse multi-view sensor observations and diffusion priors for joint 3D object pose and shape estimation. 
Our GOM system demonstrates superior multi-category mapping performance from sparse views, and achieves more accurate mapping results compared to state-of-the-art methods on the real-world benchmarks. 
We will release our code and model upon publication.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0d429edeb107268ff741043de5f3d11e14801dfd.pdf" target="_blank">/pdf/0d429edeb107268ff741043de5f3d11e14801dfd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/627510437d3c805795cfc92aed2fd72b5e89364f.zip" target="_blank">/attachment/627510437d3c805795cfc92aed2fd72b5e89364f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liao2024toward,
title={Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors},
author={Ziwei Liao and Binbin Xu and Steven L. Waslander},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=rEteJcq61j}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/fb2e10c542d15d9202bb51e6e2c52bc81781c4f4.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=rEteJcq61j&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liao|toward_general_objectlevel_mapping_from_sparse_views_with_3d_diffusion_priors</div>
</div>
<div class='paper-counter'>41/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">r6ZhiVYriY</div>
<div class="field-name">title:</div>
<div class="field-value">Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Aidan Curtis', 'Nishanth Kumar', 'Jing Cao', 'Tomás Lozano-Pérez', 'Leslie Pack Kaelbling']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Aidan_Curtis2', '~Nishanth_Kumar1', '~Jing_Cao3', '~Tomás_Lozano-Pérez1', '~Leslie_Pack_Kaelbling1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['LLMs for planning', 'task and motion planning', 'constraint satisfaction']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">having LLM's produce code that corresponds to a CSP, then solving that CSP separately, enables solving long-horizon robotic manipulation tasks with complex and realistic constratins</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent developments in pretrained large language models (LLMs) applied to robotics have demonstrated their capacity for sequencing a set of discrete skills to achieve open-ended goals in simple robotic tasks. In this paper, we examine the topic of LLM planning for a set of *continuously parameterized* skills whose execution must avoid violations of a set of kinematic, geometric, and physical constraints. We prompt the LLM to output code for a function with open parameters, which, together with environmental constraints, can be viewed as a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved through sampling or optimization to find a skill sequence and continuous parameter settings that achieve the goal while avoiding constraint violations. Additionally, we consider cases where the LLM proposes unsatisfiable CCSPs, such as those that are kinematically infeasible, dynamically unstable, or lead to collisions, and re-prompt the LLM to form a new CCSP accordingly. Experiments across simulated and real-world domains demonstrate that our proposed strategy, \OursNoSpace, is capable of solving a wide range of complex manipulation tasks with realistic constraints much more efficiently and effectively than existing baselines.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/7f6f365981e04ed2fd4324bef702d5067ae0af47.pdf" target="_blank">/pdf/7f6f365981e04ed2fd4324bef702d5067ae0af47.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/89f842d5fef3d4807da35c7552ec41cc702dafeb.zip" target="_blank">/attachment/89f842d5fef3d4807da35c7552ec41cc702dafeb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
curtis2024trust,
title={Trust the {PR}oC3S: Solving Long-Horizon Robotics Problems with {LLM}s and Constraint Satisfaction},
author={Aidan Curtis and Nishanth Kumar and Jing Cao and Tom{\'a}s Lozano-P{\'e}rez and Leslie Pack Kaelbling},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=r6ZhiVYriY}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://proc3s.csail.mit.edu</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b303d7ad9d210b9f647cd2f41e3d8b006f81b15a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=r6ZhiVYriY&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">curtis|trust_the_proc3s_solving_longhorizon_robotics_problems_with_llms_and_constraint_satisfaction</div>
</div>
<div class='paper-counter'>42/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">qoebyrnF36</div>
<div class="field-name">title:</div>
<div class="field-value">Control with Patterns: A D-learning Method</div>
<div class="field-name">authors:</div>
<div class="field-value">['Quan Quan', 'Kai-Yuan Cai', 'Chenyu Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Quan_Quan1', '~Kai-Yuan_Cai1', '~Chenyu_Wang10']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Lyapunov Methods', 'Reinforcement Learning', 'Control with Patterns', 'D-learning', 'Visual Servoing']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">The proposed control with patterns based on D-learning is a type of Lyapunov function learning methods without requring any knowledge of the system dynamics, which parallels to Q-learning in reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning-based control policies are widely used in various tasks in the field of robotics and control. However, formal (Lyapunov) stability guarantees for learning-based controllers with nonlinear dynamical systems are challenging to obtain.
	We propose a novel control approach, namely Control with Patterns (CWP), to address the stability issue over data sets corresponding to nonlinear dynamical systems.
	For data sets of this kind, we introduce a new definition, namely exponential attraction on data sets, to describe nonlinear dynamical systems under consideration. The problem of exponential attraction on data sets is converted to a pattern classification one based on the data sets and parameterized Lyapunov functions. Furthermore, D-learning is proposed as a method for performing CWP without knowledge of the system dynamics. 
	Finally, the effectiveness of CWP based on D-learning is demonstrated through simulations and real flight experiments. In these experiments, the position of the multicopter is stabilized using only real-time images as feedback, which can be considered as an Image-Based Visual Servoing (IBVS) problem.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4d5ef75103c34e3a387547fc744c4344d07fbc46.pdf" target="_blank">/pdf/4d5ef75103c34e3a387547fc744c4344d07fbc46.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/95833e0ac95135b5fe29a8a2f9e5e2e085ad7ed9.zip" target="_blank">/attachment/95833e0ac95135b5fe29a8a2f9e5e2e085ad7ed9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
quan2024control,
title={Control with Patterns: A D-learning Method},
author={Quan Quan and Kai-Yuan Cai and Chenyu Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=qoebyrnF36}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4585cd9f97d02ff5b7a02cc133c72a19124ab409.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=qoebyrnF36&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">quan|control_with_patterns_a_dlearning_method</div>
</div>
<div class='paper-counter'>43/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">qUSa3F79am</div>
<div class="field-name">title:</div>
<div class="field-value">Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Vivek Myers', 'Chunyuan Zheng', 'Oier Mees', 'Kuan Fang', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Vivek_Myers1', '~Chunyuan_Zheng2', '~Oier_Mees1', '~Kuan_Fang3', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Vision-Language Models', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show decomposing tasks with language can enable few-shot adaptation to OOD manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learned language-conditioned robot policies often struggle to effectively adapt to new real-world tasks even when pre-trained across a diverse set of instructions. We propose a novel approach for few-shot adaptation to unseen tasks that exploits the semantic understanding of task decomposition provided by vision-language models (VLMs). Our method, Policy Adaptation via Language Optimization (PALO), combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset. We evaluate PALO on extensive real-world experiments consisting of challenging unseen, long-horizon robot manipulation tasks. We find that PALO is able of consistently complete long-horizon, multi-tier tasks in the real world, outperforming state of the art pre-trained generalist policies, and methods that have access to the same demonstrations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/81bd646a6f76fbb687413a5832857da2e5f13bc8.pdf" target="_blank">/pdf/81bd646a6f76fbb687413a5832857da2e5f13bc8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e6707b1952488ab2893d76f3ff3bd0ff8565310c.zip" target="_blank">/attachment/e6707b1952488ab2893d76f3ff3bd0ff8565310c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
myers2024policy,
title={Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation},
author={Vivek Myers and Chunyuan Zheng and Oier Mees and Kuan Fang and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=qUSa3F79am}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://palo-website.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/aded8988429985f72d7164b860cdf2861f14d701.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=qUSa3F79am&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">myers|policy_adaptation_via_language_optimization_decomposing_tasks_for_fewshot_imitation</div>
</div>
<div class='paper-counter'>44/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">pcPSGZFaCH</div>
<div class="field-name">title:</div>
<div class="field-value">Modeling the Real World with High-Density Visual Particle Dynamics</div>
<div class="field-name">authors:</div>
<div class="field-value">['William F Whitney', 'Jake Varley', 'Deepali Jain', 'Krzysztof Marcin Choromanski', 'Sumeet Singh', 'Vikas Sindhwani']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~William_F_Whitney1', '~Jake_Varley1', '~Deepali_Jain1', '~Krzysztof_Marcin_Choromanski1', '~Sumeet_Singh3', '~Vikas_Sindhwani1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['point clouds', 'particle dynamics', 'world models for control', 'Performers']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Scalable robotics world models with learned particle dynamics trained from RGB-D</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present High-Density Visual Particle Dynamics (HD-VPD), a  learned world model that can emulate the physical dynamics of real scenes by processing massive latent point clouds containing 100K+ particles. To enable efficiency at this scale, we introduce a novel family of Point Cloud Transformers (PCTs) called Interlacers leveraging intertwined linear-attention Performer layers and graph-based neighbour attention layers. We demonstrate the capabilities of HD-VPD by modeling the dynamics of high degree-of-freedom bi-manual robots with two RGB-D cameras. Compared to the previous graph neural network approach, our Interlacer dynamics is twice as fast with the same prediction quality, and can achieve higher quality using 4x as many particles. We illustrate how HD-VPD can evaluate motion plan quality with robotic box pushing and can grasping tasks. See videos and particle dynamics rendered by HD-VPD at https://sites.google.com/view/hd-vpd.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4b902af6684a60f8c4213bd65877bb77af22b3c3.pdf" target="_blank">/pdf/4b902af6684a60f8c4213bd65877bb77af22b3c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cbd1b85382eed1ace5a0ddea5e3c71527d757b69.zip" target="_blank">/attachment/cbd1b85382eed1ace5a0ddea5e3c71527d757b69.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
whitney2024modeling,
title={Modeling the Real World with High-Density Visual Particle Dynamics},
author={William F Whitney and Jake Varley and Deepali Jain and Krzysztof Marcin Choromanski and Sumeet Singh and Vikas Sindhwani},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=pcPSGZFaCH}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/hd-vpd</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3678b2b044b1e13d57128dd61613aafd921e1372.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=pcPSGZFaCH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">whitney|modeling_the_real_world_with_highdensity_visual_particle_dynamics</div>
</div>
<div class='paper-counter'>45/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">pPhTsonbXq</div>
<div class="field-name">title:</div>
<div class="field-value">GraspSplats: Efficient Manipulation with 3D Feature Splatting</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mazeyu Ji', 'Ri-Zhao Qiu', 'Xueyan Zou', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mazeyu_Ji1', '~Ri-Zhao_Qiu1', '~Xueyan_Zou1', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Zero-shot manipulation', 'Gaussian Splatting', 'Keypoint Tracking']</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to its implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats can generate high-quality scene representations under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers.
With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods. The code will be released.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4dad7d796c45a36089bf92b84f9a00be84310fbc.pdf" target="_blank">/pdf/4dad7d796c45a36089bf92b84f9a00be84310fbc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f9f249cb9a5bf09fd470047cee738f0561fc3e77.zip" target="_blank">/attachment/f9f249cb9a5bf09fd470047cee738f0561fc3e77.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ji2024graspsplats,
title={GraspSplats: Efficient Manipulation with 3D Feature Splatting},
author={Mazeyu Ji and Ri-Zhao Qiu and Xueyan Zou and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=pPhTsonbXq}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://graspsplats.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/de53b3635c3cb604f40c86c37543299c207dc3e1.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=pPhTsonbXq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ji|graspsplats_efficient_manipulation_with_3d_feature_splatting</div>
</div>
<div class='paper-counter'>46/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">p6Wq6TjjHH</div>
<div class="field-name">title:</div>
<div class="field-value">Generative Factor Chaining: Coordinated Manipulation with Diffusion-based Factor Graph</div>
<div class="field-name">authors:</div>
<div class="field-value">['Utkarsh Aashu Mishra', 'Yongxin Chen', 'Danfei Xu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Utkarsh_Aashu_Mishra2', '~Yongxin_Chen1', '~Danfei_Xu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Task and Motion Planning', 'Manipulation Planning', 'Bimanual Manipulation', 'Generative Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Generative Factor Chaining (GFC), a novel approach based on modularized generative models for learning and composing skills in complex tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to plan for multi-step, multi-manipulator tasks is notoriously difficult because of the large search space and the complex constraint satisfaction problems. We present Generative Factor Chaining (GFC), a composable generative model for planning. GFC represents a planning problem as a spatial-temporal factor graph, where nodes represent objects and robots in the scene, spatial factors capture the distributions of valid relationships among nodes, and temporal factors represent the distributions of skill transitions. Each factor is implemented as a modular diffusion model, which are composed during inference to generate feasible long-horizon plans through bi-directional message passing. We show that GFC can solve complex bimanual manipulation tasks and exhibits strong generalization to unseen planning tasks with novel combinations of objects and constraints. More details can be found at: https://sites.google.com/view/generative-factor-chaining</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/83c84821eba178867b7282ab4a79b2595839c331.pdf" target="_blank">/pdf/83c84821eba178867b7282ab4a79b2595839c331.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c825f127d3ce16cf9a8207c98da7df458c942ac1.zip" target="_blank">/attachment/c825f127d3ce16cf9a8207c98da7df458c942ac1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
mishra2024generative,
title={Generative Factor Chaining: Coordinated Manipulation with Diffusion-based Factor Graph},
author={Utkarsh Aashu Mishra and Yongxin Chen and Danfei Xu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=p6Wq6TjjHH}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://generative-fc.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/497ca193e0853003ed0e2da676fae7c43588d807.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=p6Wq6TjjHH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">mishra|generative_factor_chaining_coordinated_manipulation_with_diffusionbased_factor_graph</div>
</div>
<div class='paper-counter'>47/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ovjxugn9Q2</div>
<div class="field-name">title:</div>
<div class="field-value">SoftManiSim: A Fast Simulation Framework for Multi-Segment Continuum Manipulators Tailored for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mohammadreza Kasaei', 'Hamidreza Kasaei', 'Mohsen Khadem']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mohammadreza_Kasaei1', '~Hamidreza_Kasaei1', '~Mohsen_Khadem1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Simulation Framework', 'Soft Robotics', 'Mathematical Modelling', 'Robot Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces SoftManiSim, a novel simulation framework for multi-segment continuum manipulators.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper introduces SoftManiSim, a novel simulation framework for multi-segment continuum manipulators. Existing continuum robot simulators often rely on simplifying assumptions, such as constant curvature bending or ignoring contact forces, to meet real-time simulation and training demands. To bridge this gap, we propose a robust and rapid mathematical model for continuum robots at the core of SoftManiSim, ensuring precise and adaptable simulations. The framework can integrate with various rigid-body robots, increasing its utility across different robotic platforms. SoftManiSim supports parallel operations for simultaneous simulations of multiple robots and generates synthetic data essential for training deep reinforcement learning models. This capability enhances the development and optimization of control strategies in dynamic environments. Extensive simulations validate the framework's effectiveness, demonstrating its capabilities in handling complex robotic interactions and tasks. We also present real robot validation to showcase the simulator's practical applicability and accuracy in real-world settings. To our knowledge, SoftManiSim is the first open-source real-time simulator capable of modeling continuum robot behavior under dynamic point/distributed loading. It enables rapid deployment in reinforcement learning and machine learning applications. 
This simulation framework can be downloaded from https://github.com/MohammadKasaei/SoftManiSim.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0b44d9931567970feeede9f4ed01d19237c21b2c.pdf" target="_blank">/pdf/0b44d9931567970feeede9f4ed01d19237c21b2c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ba445556f94a82d4fb9f2bc59b374f6423a5ae2d.zip" target="_blank">/attachment/ba445556f94a82d4fb9f2bc59b374f6423a5ae2d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kasaei2024softmanisim,
title={SoftManiSim: A Fast Simulation Framework for Multi-Segment Continuum Manipulators Tailored for Robot Learning},
author={Mohammadreza Kasaei and Hamidreza Kasaei and Mohsen Khadem},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ovjxugn9Q2}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2b3eab541dfbfb3b6f4b10a2a8a67562c7f6978b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ovjxugn9Q2&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kasaei|softmanisim_a_fast_simulation_framework_for_multisegment_continuum_manipulators_tailored_for_robot_learning</div>
</div>
<div class='paper-counter'>48/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">oSU7M7MK6B</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Visuotactile Estimation and Control for Non-prehensile Manipulation under Occlusions</div>
<div class="field-name">authors:</div>
<div class="field-value">['Juan Del Aguila Ferrandis', 'Joao Moura', 'Sethu Vijayakumar']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Juan_Del_Aguila_Ferrandis1', '~Joao_Moura1', 'sethu.vijayakumar@ed.ac.uk']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['State Estimation', 'Reinforcement Learning with Tactile Sensing', 'Non-prehensile Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a method to learn visuotactile state estimators and uncertainty-aware control policies for non-prehensile manipulation under occlusions and extensively validate it in simulation and robotic hardware.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulation without grasping, known as non-prehensile manipulation, is essential for dexterous robots in contact-rich environments, but presents many challenges relating with underactuation, hybrid-dynamics, and frictional uncertainty. Additionally, object occlusions in a scenario of contact uncertainty and where the motion of the object evolves independently from the robot becomes a critical problem, which previous literature fails to address. We present a method for learning visuotactile state estimators and uncertainty-aware control policies for non-prehensile manipulation under occlusions, by leveraging diverse interaction data from privileged policies trained in simulation. We formulate the estimator within a Bayesian deep learning framework, to model its uncertainty, and then train uncertainty-aware control policies by incorporating the pre-learned estimator into the reinforcement learning (RL) loop, both of which lead to significantly improved estimator and policy performance. Therefore, unlike prior non-prehensile research that relies on complex external perception set-ups, our method successfully handles occlusions after sim-to-real transfer to robotic hardware with a simple onboard camera.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2c8018619ea5a5eee508d3ac678b640fd463e259.pdf" target="_blank">/pdf/2c8018619ea5a5eee508d3ac678b640fd463e259.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3185cc01ca1604415c9bc9caff55d2728c3e2e2e.zip" target="_blank">/attachment/3185cc01ca1604415c9bc9caff55d2728c3e2e2e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ferrandis2024learning,
title={Learning Visuotactile Estimation and Control for Non-prehensile Manipulation under Occlusions},
author={Juan Del Aguila Ferrandis and Joao Moura and Sethu Vijayakumar},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=oSU7M7MK6B}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/cda00c2a6a2abb13fa30a171c63a88f6541225be.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=oSU7M7MK6B&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ferrandis|learning_visuotactile_estimation_and_control_for_nonprehensile_manipulation_under_occlusions</div>
</div>
<div class='paper-counter'>49/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">oL1WEZQal8</div>
<div class="field-name">title:</div>
<div class="field-value">OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tairan He', 'Zhengyi Luo', 'Xialin He', 'Wenli Xiao', 'Chong Zhang', 'Weinan Zhang', 'Kris M. Kitani', 'Changliu Liu', 'Guanya Shi']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tairan_He1', '~Zhengyi_Luo1', '~Xialin_He1', '~Wenli_Xiao1', '~Chong_Zhang6', '~Weinan_Zhang1', '~Kris_M._Kitani1', '~Changliu_Liu1', '~Guanya_Shi1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoid Teleoperation', 'Humanoid Loco-Manipulation', 'RL']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose OmniH2O, a learning-based system for whole-body humanoid teleoperation and autonomy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets. Videos at the anonymous website [https://anonymous-omni-h2o.github.io/](https://anonymous-omni-h2o.github.io/)</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d79b8e14c6dc300b22ed8e1074c081ca282f80e0.pdf" target="_blank">/pdf/d79b8e14c6dc300b22ed8e1074c081ca282f80e0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/27d72e18044605ffbf97902f7a2db53f960e8f1f.zip" target="_blank">/attachment/27d72e18044605ffbf97902f7a2db53f960e8f1f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
he2024omniho,
title={OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning},
author={Tairan He and Zhengyi Luo and Xialin He and Wenli Xiao and Chong Zhang and Weinan Zhang and Kris M. Kitani and Changliu Liu and Guanya Shi},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=oL1WEZQal8}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://omni.human2humanoid.com/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/be338b3fbc2f1a7492d740dc389142913815c48b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=oL1WEZQal8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">he|omnih2o_universal_and_dexterous_humantohumanoid_wholebody_teleoperation_and_learning</div>
</div>
<div class='paper-counter'>50/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">nmEt0ci8hi</div>
<div class="field-name">title:</div>
<div class="field-value">General Flow as Foundation Affordance for Scalable Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chengbo Yuan', 'Chuan Wen', 'Tong Zhang', 'Yang Gao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chengbo_Yuan2', '~Chuan_Wen1', '~Tong_Zhang23', '~Yang_Gao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Flow', 'Transferable Affordance', 'Scalability']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A flow-based framework enables real-world zero-shot human-to-robot skill transfer while providing scalability, universality, and stability for robot learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We address the challenge of acquiring real-world manipulation skills with a scalable framework. We hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning.
Therefore, we propose to utilize 3D flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target. 

To exploit scalable data resources, we turn our attention to human videos. We develop, for the first time, a language-conditioned 3D flow prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable guidance, thus facilitating zero-shot skill transfer in real-world scenarios.
We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any in-domain finetuning, our method achieves an impressive 81\% success rate in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. 

Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) wide application: multiple object categories, including rigid, articulated, and soft bodies;
(3) stable skill transfer: providing actionable guidance with a small inference domain-gap.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8c0e8be6743bb0f736ffaa604665c636e64e9491.pdf" target="_blank">/pdf/8c0e8be6743bb0f736ffaa604665c636e64e9491.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/08239f069172d0f3e5e95ce6d86aacaae573cb2f.zip" target="_blank">/attachment/08239f069172d0f3e5e95ce6d86aacaae573cb2f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yuan2024general,
title={General Flow as Foundation Affordance for Scalable Robot Learning},
author={Chengbo Yuan and Chuan Wen and Tong Zhang and Yang Gao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=nmEt0ci8hi}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://general-flow.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4503ae14a5f8088bcc9c974eddd1befbc6ffdb8d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=nmEt0ci8hi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yuan|general_flow_as_foundation_affordance_for_scalable_robot_learning</div>
</div>
<div class='paper-counter'>51/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">nVJm2RdPDu</div>
<div class="field-name">title:</div>
<div class="field-value">DiffuseLoco: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xiaoyu Huang', 'Yufeng Chi', 'Ruofeng Wang', 'Zhongyu Li', 'Xue Bin Peng', 'Sophia Shao', 'Borivoje Nikolic', 'Koushil Sreenath']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xiaoyu_Huang1', '~Yufeng_Chi1', '~Ruofeng_Wang1', '~Zhongyu_Li3', '~Xue_Bin_Peng1', '~Sophia_Shao1', '~Borivoje_Nikolic1', '~Koushil_Sreenath1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Offline Learning', 'Bipedal Walking', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work introduces DiffuseLoco, a scalable framework for training diverse multi-skill diffusion models for dynamic legged locomotion control from offline datasets.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline learning at scale has led to breakthroughs in computer vision, natural language processing, and robotic manipulation domains. However, scaling up learning for legged robot locomotion, especially with multiple skills in a single policy, presents significant challenges for prior online reinforcement learning (RL) methods. To address this challenge, we propose DiffuseLoco, a novel, scalable framework that leverages diffusion models to directly learn from offline multimodal datasets with a diverse set of locomotion skills. With design choices tailored for real-time control in dynamical systems, including receding horizon control and delayed inputs, DiffuseLoco is capable of reproducing multimodality in performing various locomotion skills, zero-shot transferred to real quadruped robots and deployed on edge computes. Through extensive real-world benchmarking, DiffuseLoco exhibits better stability and velocity tracking performance compared to prior RL and non-diffusion-based behavior cloning baselines. This work opens new possibilities for scaling up learning-based legged locomotion control through the scaling of large, expressive models and diverse offline datasets.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5d7b411b2ae6a0447c0b9fee86c17a991f5356ec.pdf" target="_blank">/pdf/5d7b411b2ae6a0447c0b9fee86c17a991f5356ec.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a35e4368e350efe4d114ef905241f57474cb7348.zip" target="_blank">/attachment/a35e4368e350efe4d114ef905241f57474cb7348.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024diffuseloco,
title={DiffuseLoco: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets},
author={Xiaoyu Huang and Yufeng Chi and Ruofeng Wang and Zhongyu Li and Xue Bin Peng and Sophia Shao and Borivoje Nikolic and Koushil Sreenath},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=nVJm2RdPDu}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://diffuselo.co/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3dbd2874fdf3cac920b7d3c2983fc5b69b60e2db.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=nVJm2RdPDu&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|diffuseloco_realtime_legged_locomotion_control_with_diffusion_from_offline_datasets</div>
</div>
<div class='paper-counter'>52/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">nQslM6f7dW</div>
<div class="field-name">title:</div>
<div class="field-value">APRICOT: Active Preference Learning and Constraint-Aware Task Planning with LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huaxiaoyue Wang', 'Nathaniel Chin', 'Gonzalo Gonzalez-Pumariega', 'Xiangwan Sun', 'Neha Sunkara', 'Maximus Adrian Pace', 'Jeannette Bohg', 'Sanjiban Choudhury']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huaxiaoyue_Wang1', '~Nathaniel_Chin1', '~Gonzalo_Gonzalez-Pumariega1', '~Xiangwan_Sun1', '~Neha_Sunkara1', '~Maximus_Adrian_Pace1', '~Jeannette_Bohg1', '~Sanjiban_Choudhury3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Active Preference Learning', 'Task Planning', 'Large Language Models']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Home robots performing personalized tasks must adeptly balance user preferences with environmental affordances.
We focus on organization tasks within constrained spaces, such as arranging items into a refrigerator, where preferences for placement collide with physical limitations.
The robot must infer user preferences based on a small set of demonstrations, which is easier for users to provide than extensively defining all their requirements.
While recent works use Large Language Models (LLMs) to learn preferences from user demonstrations, they encounter two fundamental challenges.
First, there is inherent ambiguity in interpreting user actions, as multiple preferences can often explain a single observed behavior.
Second, not all user preferences are practically feasible due to geometric constraints in the environment.
To address these challenges, we introduce APRICOT, a novel approach that merges LLM-based Bayesian active preference learning with constraint-aware task planning. 
APRICOT refines its generated preferences by actively querying the user and dynamically adapts its plan to respect environmental constraints.
We evaluate APRICOT on a dataset of diverse organization tasks and demonstrate its effectiveness in real-world scenarios, showing significant improvements in both preference satisfaction and plan feasibility.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/503811e3b5edf278f94e54921d99d5d54067f554.pdf" target="_blank">/pdf/503811e3b5edf278f94e54921d99d5d54067f554.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/07c61986734c8fe1d2d20a551e810ad902325576.zip" target="_blank">/attachment/07c61986734c8fe1d2d20a551e810ad902325576.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024apricot,
title={{APRICOT}: Active Preference Learning and Constraint-Aware Task Planning with {LLM}s},
author={Huaxiaoyue Wang and Nathaniel Chin and Gonzalo Gonzalez-Pumariega and Xiangwan Sun and Neha Sunkara and Maximus Adrian Pace and Jeannette Bohg and Sanjiban Choudhury},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=nQslM6f7dW}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://portal-cornell.github.io/apricot/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d213270b93ef20058906bf7f46e29874d43e584d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=nQslM6f7dW&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|apricot_active_preference_learning_and_constraintaware_task_planning_with_llms</div>
</div>
<div class='paper-counter'>53/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ma7McOiCZY</div>
<div class="field-name">title:</div>
<div class="field-value">HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jin Wang', 'Rui Dai', 'Weijie Wang', 'Luca Rossini', 'Francesco Ruscelli', 'Nikos Tsagarakis']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jin_Wang26', '~Rui_Dai7', '~Weijie_Wang3', 'luca.rossini@iit.it', 'francescoruscelli24@gmail.com', 'nikos.tsagarakis@iit.it']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Loco-manipulation', 'Large Language Models', 'Humanoid Robot Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling robots to autonomously perform hybrid motions in diverse environments can be beneficial for long-horizon tasks such as material handling, household chores, and work assistance. This requires extensive exploitation of intrinsic motion capabilities, extraction of affordances from rich environmental information, and planning of physical interaction behaviors. Despite recent progress has demonstrated impressive humanoid whole-body control abilities, they struggle to achieve versatility and adaptability for new tasks. In this work, we propose HYPERmotion, a framework that learns, selects and plans behaviors based on tasks in different scenarios. We combine reinforcement learning with whole-body optimization to generate motion for 38 actuated joints and create a motion library to store the learned skills. We apply the planning and reasoning features of the large language models (LLMs) to complex loco-manipulation tasks, constructing a hierarchical task graph that comprises a series of primitive behaviors to bridge lower-level execution with higher-level planning. By leveraging the interaction of distilled spatial geometry and 2D observation with a visual language model (VLM) to ground knowledge into a robotic morphology selector to choose appropriate actions in single- or dual-arm, legged or wheeled locomotion.  Experiments in simulation and real-world show that learned motions can efficiently adapt to new tasks, demonstrating high autonomy from free-text commands in unstructured scenes. Videos and website: hy-motion.github.io//</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/b57174ebd5140f91dd7592ce927eb3c64c1b8d56.pdf" target="_blank">/pdf/b57174ebd5140f91dd7592ce927eb3c64c1b8d56.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ea2905c4b12331fef10f0a56de3eb64e5c8f3d66.zip" target="_blank">/attachment/ea2905c4b12331fef10f0a56de3eb64e5c8f3d66.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024hypermotion,
title={{HYPER}motion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation},
author={Jin Wang and Rui Dai and Weijie Wang and Luca Rossini and Francesco Ruscelli and Nikos Tsagarakis},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ma7McOiCZY}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://hy-motion.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3345786e91c50fdf267dedcdec0ee27365317b62.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ma7McOiCZY&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|hypermotion_learning_hybrid_behavior_planning_for_autonomous_locomanipulation</div>
</div>
<div class='paper-counter'>54/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">lyhS75loxe</div>
<div class="field-name">title:</div>
<div class="field-value">A3VLM: Actionable Articulation-Aware Vision Language Model</div>
<div class="field-name">authors:</div>
<div class="field-value">['Siyuan Huang', 'Haonan Chang', 'Yuhan Liu', 'Yimeng Zhu', 'Hao Dong', 'Abdeslam Boularias', 'Peng Gao', 'Hongsheng Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Siyuan_Huang4', '~Haonan_Chang1', '~Yuhan_Liu2', '~Yimeng_Zhu1', '~Hao_Dong3', '~Abdeslam_Boularias1', '~Peng_Gao3', '~Hongsheng_Li3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['LLM', 'VLM', 'Manipulation', 'Articulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introudce the A3VLM, focusing on the articulation structure and action affordances of objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision Language Models (VLMs) for robotics have received significant attention in recent years. As a VLM can understand robot observations and perform complex visual reasoning, it is regarded as a potential universal solution for general robotics challenges such as manipulation and navigation. However, previous robotics VLMs such as RT-1, RT-2, and ManipLLM have focused on directly learning robot actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0cf7bb8c6119c410341d935d4f30225445f5400c.pdf" target="_blank">/pdf/0cf7bb8c6119c410341d935d4f30225445f5400c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/889fcc4e31c99faeee423575a206c7feba170a43.zip" target="_blank">/attachment/889fcc4e31c99faeee423575a206c7feba170a43.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024avlm,
title={A3{VLM}: Actionable Articulation-Aware Vision Language Model},
author={Siyuan Huang and Haonan Chang and Yuhan Liu and Yimeng Zhu and Hao Dong and Abdeslam Boularias and Peng Gao and Hongsheng Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=lyhS75loxe}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/99ca94bb1f9d3ed195ee4989db073b894b9484fd.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=lyhS75loxe&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|a3vlm_actionable_articulationaware_vision_language_model</div>
</div>
<div class='paper-counter'>55/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">lt0Yf8Wh5O</div>
<div class="field-name">title:</div>
<div class="field-value">Differentiable Robot Rendering</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ruoshi Liu', 'Alper Canberk', 'Shuran Song', 'Carl Vondrick']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ruoshi_Liu2', '~Alper_Canberk1', '~Shuran_Song3', '~Carl_Vondrick2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Representation', 'Visual Foundation Model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a differentiable robot rendering method based on deformable Gaussians splattings and show many downstream applications. Abstract:</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision foundation models trained on massive amounts of visual data have shown unprecedented reasoning and planning skills in open-world settings. A key challenge in applying them to robotic tasks is the modality gap between visual data and action data. We introduce differentiable robot rendering, a method allowing the visual appearance of a robot body to be directly differentiable with respect to its control parameters. Our model integrates a kinematics-aware deformable model and Gaussians Splatting and is compatible with any robot form factors and degrees of freedom. We demonstrate its capability and usage in applications including reconstruction of robot poses from images and controlling robots through vision language models. Quantitative and qualitative results show that our differentiable rendering model provides effective gradients for robotic control directly from pixels, setting the foundation for the future applications of vision foundation models in robotics.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/00ad78ab581d6f22d38d1735bf6860fc95836ba8.pdf" target="_blank">/pdf/00ad78ab581d6f22d38d1735bf6860fc95836ba8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f15657e46e2023ed65e4145d4cbed90b5774ae1c.zip" target="_blank">/attachment/f15657e46e2023ed65e4145d4cbed90b5774ae1c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024differentiable,
title={Differentiable Robot Rendering},
author={Ruoshi Liu and Alper Canberk and Shuran Song and Carl Vondrick},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=lt0Yf8Wh5O}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://drrobot.cs.columbia.edu/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/86fae71c0fed2e61f8698d5f451627dcd8b5ac1d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=lt0Yf8Wh5O&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|differentiable_robot_rendering</div>
</div>
<div class='paper-counter'>56/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">lpjPft4RQT</div>
<div class="field-name">title:</div>
<div class="field-value">TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yunfan Jiang', 'Chen Wang', 'Ruohan Zhang', 'Jiajun Wu', 'Li Fei-Fei']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yunfan_Jiang1', '~Chen_Wang16', '~Ruohan_Zhang1', '~Jiajun_Wu1', '~Li_Fei-Fei1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Sim-to-Real Transfer', 'Human-in-the-Loop', 'Robot Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present TRANSIC, a holistic human-in-the-loop method to tackle sim-to-real transfer for contact-rich robotic arm manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots. The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps. Previous methods often require domain-specific knowledge *a priori*. We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world. The robots can then learn from humans to close various sim-to-real gaps. We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework. TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction. Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution. We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly. Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps. It displays attractive properties such as scaling with human effort. Videos and code are available at https://transic-robot.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fec002deb330174a19e233010917894a7c54ab8f.pdf" target="_blank">/pdf/fec002deb330174a19e233010917894a7c54ab8f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/35a55f55af5226d6b231a795b3fa5302fc265497.zip" target="_blank">/attachment/35a55f55af5226d6b231a795b3fa5302fc265497.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jiang2024transic,
title={{TRANSIC}: Sim-to-Real Policy Transfer by Learning from Online Correction},
author={Yunfan Jiang and Chen Wang and Ruohan Zhang and Jiajun Wu and Li Fei-Fei},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=lpjPft4RQT}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://transic-robot.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/411482516e5544efd27da6b50d731f7805cb8095.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=lpjPft4RQT&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jiang|transic_simtoreal_policy_transfer_by_learning_from_online_correction</div>
</div>
<div class='paper-counter'>57/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">lKGRPJFPCM</div>
<div class="field-name">title:</div>
<div class="field-value">InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Andrew Choong-Won Lee', 'Ian Chuang', 'Ling-Yuan Chen', 'Iman Soltani']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Andrew_Choong-Won_Lee1', '~Ian_Chuang1', '~Ling-Yuan_Chen1', '~Iman_Soltani1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robotics', 'Imitation Learning', 'Bimanual Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We presented InterACT, a framework for robust bimanual manipulation, which integrates hierarchical attention transformers to capture inter-dependencies between dual-arm joint states and visual inputs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/70c09731009fc463b246ad14a602a332bded2320.pdf" target="_blank">/pdf/70c09731009fc463b246ad14a602a332bded2320.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/28294eff7826122805c5292c3bacb402599789a0.zip" target="_blank">/attachment/28294eff7826122805c5292c3bacb402599789a0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lee2024interact,
title={Inter{ACT}: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation},
author={Andrew Choong-Won Lee and Ian Chuang and Ling-Yuan Chen and Iman Soltani},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=lKGRPJFPCM}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://soltanilara.github.io/interact/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e3d0623bde12fa5e2617789f871766312b9d70cb.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=lKGRPJFPCM&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lee|interact_interdependency_aware_action_chunking_with_hierarchical_attention_transformers_for_bimanual_manipulation</div>
</div>
<div class='paper-counter'>58/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">kEZXeaMrkD</div>
<div class="field-name">title:</div>
<div class="field-value">Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">['RenMing Huang', 'Shaochong Liu', 'Yunqiang Pei', 'Peng Wang', 'Guoqing Wang', 'Yang Yang', 'Heng Tao Shen']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~RenMing_Huang1', '~Shaochong_Liu1', '~Yunqiang_Pei1', '~Peng_Wang19', '~Guoqing_Wang2', '~Yang_Yang37', '~Heng_Tao_Shen3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Goal-Reaching', 'Long-Horizon', 'Non-Expert Observation Data']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we address the challenging problem of long-horizon goal-reaching policy learning from non-expert, action-free observation data. Unlike fully labeled expert data, our data is more accessible and avoids the costly process of action labeling. Additionally, compared to online learning, which often involves aimless exploration, our data provides useful guidance for more efficient exploration. To achieve our goal, we propose a novel subgoal guidance learning strategy. The motivation behind this strategy is that long-horizon goals offer limited guidance for efficient exploration and accurate state transition. We develop a diffusion strategy-based high-level policy to generate reasonable subgoals as waypoints, preferring states that more easily lead to the final goal. Additionally, we learn state-goal value functions to encourage efficient subgoal reaching. These two components naturally integrate into the off-policy actor-critic framework, enabling efficient goal attainment through informative exploration. We evaluate our method on complex robotic navigation and manipulation tasks, demonstrating a significant performance advantage over existing methods. Our ablation study further shows that our method is robust to observation data with various corruptions.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/614a4d80b0ecbdf7ca09a39e5b39f11b3ed6fc7e.pdf" target="_blank">/pdf/614a4d80b0ecbdf7ca09a39e5b39f11b3ed6fc7e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e0a185786e3f36ce2ae6779d97feb20f5a15bed0.zip" target="_blank">/attachment/e0a185786e3f36ce2ae6779d97feb20f5a15bed0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024goalreaching,
title={Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance},
author={RenMing Huang and Shaochong Liu and Yunqiang Pei and Peng Wang and Guoqing Wang and Yang Yang and Heng Tao Shen},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=kEZXeaMrkD}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/27fa521c50af7c94465c1b2e35b05ecf8e235aa5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=kEZXeaMrkD&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|goalreaching_policy_learning_from_nonexpert_observations_via_effective_subgoal_guidance</div>
</div>
<div class='paper-counter'>59/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">k4Nnxqcwt8</div>
<div class="field-name">title:</div>
<div class="field-value">Q-SLAM: Quadric Representations for Monocular SLAM</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chensheng Peng', 'Chenfeng Xu', 'Yue Wang', 'Mingyu Ding', 'Heng Yang', 'Masayoshi Tomizuka', 'Kurt Keutzer', 'Marco Pavone', 'Wei Zhan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chensheng_Peng1', '~Chenfeng_Xu1', '~Yue_Wang2', '~Mingyu_Ding1', '~Heng_Yang4', '~Masayoshi_Tomizuka2', '~Kurt_Keutzer1', '~Marco_Pavone1', '~Wei_Zhan2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Neural Radiance Fields', 'Simultaneous Localization and Mapping']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we reimagine volumetric representations through the lens of quadrics. We posit that rigid scene components can be effectively decomposed into quadric surfaces. Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which results in more accurate and efficient modeling of 3D scenes in SLAM contexts. First, we use the quadric assumption to rectify noisy depth estimations from RGB inputs. This step significantly improves depth estimation accuracy, and allows us to efficiently sample ray points around quadric planes instead of the entire volume space in previous NeRF-SLAM systems. Second, we introduce a novel quadric-decomposed transformer to aggregate information across quadrics. The quadric semantics are not only explicitly used for depth correction and scene decomposition, but also serve as an implicit supervision signal for the mapping network. Through rigorous experimental evaluation, our method exhibits superior performance over other approaches relying on estimated depth, and achieves comparable accuracy to methods utilizing ground truth depth on both synthetic and real-world datasets.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0e3bb99a49a4f94869a5a1dbe63c173c19f84e43.pdf" target="_blank">/pdf/0e3bb99a49a4f94869a5a1dbe63c173c19f84e43.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/42421fba54bbc6b8e5bfaa29407401e38019f158.zip" target="_blank">/attachment/42421fba54bbc6b8e5bfaa29407401e38019f158.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
peng2024qslam,
title={Q-{SLAM}: Quadric Representations for Monocular {SLAM}},
author={Chensheng Peng and Chenfeng Xu and Yue Wang and Mingyu Ding and Heng Yang and Masayoshi Tomizuka and Kurt Keutzer and Marco Pavone and Wei Zhan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=k4Nnxqcwt8}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/71f68b65eb2753e62c24fefd9cbe55aa04e237b3.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=k4Nnxqcwt8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">peng|qslam_quadric_representations_for_monocular_slam</div>
</div>
<div class='paper-counter'>60/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">k0ogr4dnhG</div>
<div class="field-name">title:</div>
<div class="field-value">ClutterGen: A Cluttered Scene Generator for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yinsen Jia', 'Boyuan Chen']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yinsen_Jia2', '~Boyuan_Chen1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Simulation Scene Generation', 'Manipulation', 'Robot Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">ClutterGen, a physically compliant simulation scene generator capable of producing highly diverse, cluttered, and stable scenes for robot learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce ClutterGen, a physically compliant simulation scene generator capable of producing highly diverse, cluttered, and stable scenes for robot learning. Generating such scenes is challenging as each object must adhere to physical laws like gravity and collision. As the number of objects increases, finding valid poses becomes more difficult, necessitating significant human engineering effort, which limits the diversity of the scenes. To overcome these challenges, we propose a reinforcement learning method that can be trained with physics-based reward signals provided by the simulator. Our experiments demonstrate that ClutterGen can generate cluttered object layouts with up to ten objects on confined table surfaces. Additionally, our policy design explicitly encourages the diversity of the generated scenes for open-ended generation. Our real-world robot results show that ClutterGen can be directly used for clutter rearrangement and stable placement policy training.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/aa577d740ba9c778a5d712028cf916ef46739df4.pdf" target="_blank">/pdf/aa577d740ba9c778a5d712028cf916ef46739df4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/51bc627a59657d6a185a6e009e60efbce8f74092.zip" target="_blank">/attachment/51bc627a59657d6a185a6e009e60efbce8f74092.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jia2024cluttergen,
title={ClutterGen: A Cluttered Scene Generator for Robot Learning},
author={Yinsen Jia and Boyuan Chen},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=k0ogr4dnhG}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://generalroboticslab.com/ClutterGen</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/71cf7ccf3d59147c99f0aed272de9d132f9694ec.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=k0ogr4dnhG&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jia|cluttergen_a_cluttered_scene_generator_for_robot_learning</div>
</div>
<div class='paper-counter'>61/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">jnubz7wB2w</div>
<div class="field-name">title:</div>
<div class="field-value">Verification of Neural Control Barrier Functions with Symbolic Derivative Bounds Propagation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hanjiang Hu', 'Yujie Yang', 'Tianhao Wei', 'Changliu Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hanjiang_Hu1', '~Yujie_Yang1', '~Tianhao_Wei1', '~Changliu_Liu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Learning for control', 'control barrier function', 'formal verification']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new neural CBF verification framework using symbolic derivative bound propagation to give tight and efficient bounds for verifying forward invariance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control barrier functions (CBFs) are important in safety-critical systems and robot control applications. Neural networks have been used to parameterize and synthesize CBFs with bounded control input for complex systems. However, it is still challenging to verify pre-trained neural networks CBFs (neural CBFs) in an efficient symbolic manner. To this end, we propose a new efficient verification framework for ReLU-based neural CBFs through symbolic derivative bound propagation by combining the linearly bounded nonlinear dynamic system and the gradient bounds of neural CBFs. Specifically, with Heaviside step function form for derivatives of activation functions, we show that the symbolic bounds can be propagated through the inner product of neural CBF Jacobian and nonlinear system dynamics. Through extensive experiments on different robot dynamics, our results outperform the interval arithmetic-based baselines in verified rate and verification time along the CBF boundary, validating the effectiveness and efficiency of the proposed method with different model complexity. The code can be found at https://github.com/intelligent-control-lab/verify-neural-CBF.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/302d25e55405cc3eae415f011ceeee34bb06370e.pdf" target="_blank">/pdf/302d25e55405cc3eae415f011ceeee34bb06370e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a708cba3424add9bdb18b763484259786bc6a3d8.zip" target="_blank">/attachment/a708cba3424add9bdb18b763484259786bc6a3d8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hu2024verification,
title={Verification of Neural Control Barrier Functions with Symbolic Derivative Bounds Propagation},
author={Hanjiang Hu and Yujie Yang and Tianhao Wei and Changliu Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=jnubz7wB2w}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e2a898528825618f655054ecaf6ec06935502011.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=jnubz7wB2w&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hu|verification_of_neural_control_barrier_functions_with_symbolic_derivative_bounds_propagation</div>
</div>
<div class='paper-counter'>62/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">jart4nhCQr</div>
<div class="field-name">title:</div>
<div class="field-value">Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhecheng Yuan', 'Tianming Wei', 'Shuiqi Cheng', 'Gu Zhang', 'Yuanpei Chen', 'Huazhe Xu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhecheng_Yuan1', '~Tianming_Wei1', '~Shuiqi_Cheng2', '~Gu_Zhang1', '~Yuanpei_Chen2', '~Huazhe_Xu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Visual Generalization', 'Sim2real', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A generalizable framework for visual RL, and can facilitate the sim2real transfer.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Can we endow visuomotor robots with generalization capabilities to operate in diverse open-world scenarios? In this paper, we propose Maniwhere, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types. Specifically, we introduce a multi-view representation learning approach fused with Spatial Transformer Network (STN) module to capture shared semantic information and correspondences among different viewpoints. In addition, we employ a curriculum-based randomization and augmentation approach to stabilize the RL training process and strengthen the visual generalization ability.  To exhibit the effectiveness of Maniwhere,  we meticulously design **8** tasks encompassing articulate objects, bi-manual, and dexterous hand manipulation tasks, demonstrating Maniwhere's strong visual generalization and sim2real transfer abilities across **3** hardware platforms. Our experiments show that Maniwhere significantly outperforms existing state-of-the-art methods. Videos are provided at https://maniwhere.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6d3ec7a12f43eeda14089de7b25bfd2d3093253e.pdf" target="_blank">/pdf/6d3ec7a12f43eeda14089de7b25bfd2d3093253e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/fca58ed42365c5dcca6cff0edc948e4db41fdf09.zip" target="_blank">/attachment/fca58ed42365c5dcca6cff0edc948e4db41fdf09.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yuan2024learning,
title={Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning},
author={Zhecheng Yuan and Tianming Wei and Shuiqi Cheng and Gu Zhang and Yuanpei Chen and Huazhe Xu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=jart4nhCQr}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gemcollector.github.io/maniwhere/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/fc0096e8455a57d116c30c46503f69462c2755a7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=jart4nhCQr&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yuan|learning_to_manipulate_anywhere_a_visual_generalizable_framework_for_reinforcement_learning</div>
</div>
<div class='paper-counter'>63/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">jPkOFAiOzf</div>
<div class="field-name">title:</div>
<div class="field-value">Region-aware Grasp Framework with Normalized Grasp Space for Efficient 6-DoF Grasping</div>
<div class="field-name">authors:</div>
<div class="field-value">['Siang Chen', 'Pengwei Xie', 'Wei Tang', 'Dingchang Hu', 'Yixiang Dai', 'Guijin Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Siang_Chen1', '~Pengwei_Xie1', '~Wei_Tang22', '~Dingchang_Hu1', 'daiyx23@mails.tsinghua.edu.cn', '~Guijin_Wang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['6-DoF Grasping', 'RGBD Perception', 'Normalized Space', 'Heatmap']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">For 6-DoF grasping, we propose Region-aware Grasp Framework consisting of Normalized Grasp Space and a efficient Region-aware Normalized Grasp Network, achieving best grasp detection performance with high efficiency and generalization capability.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A series of region-based methods succeed in extracting regional features and enhancing grasp detection quality. However, faced with a cluttered scene with potential collision, the definition of the grasp-relevant region stays inconsistent. In this paper, we propose Normalized Grasp Space (NGS) from a novel region-aware viewpoint, unifying the grasp representation within a normalized regional space and benefiting the generalizability of methods. Leveraging the NGS, we find that CNNs are underestimated for 3D feature extraction and 6-DoF grasp detection in clutter scenes and build a highly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on the public benchmark show that our method achieves significant >20 % performance gains while attaining a real-time inference speed of approximately 50 FPS. Real-world cluttered scene clearance experiments underscore the effectiveness of our method. Further, human-to-robot handover and dynamic object grasping experiments demonstrate the potential of our proposed method for closed-loop grasping in dynamic scenarios.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/848e8b5b69d126c2baf49538360befbfe4a82465.pdf" target="_blank">/pdf/848e8b5b69d126c2baf49538360befbfe4a82465.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a7f4d16873e3c51b9ad97ca8ee51d74b26e856dd.zip" target="_blank">/attachment/a7f4d16873e3c51b9ad97ca8ee51d74b26e856dd.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024regionaware,
title={Region-aware Grasp Framework with Normalized Grasp Space for Efficient 6-DoF Grasping},
author={Siang Chen and Pengwei Xie and Wei Tang and Dingchang Hu and Yixiang Dai and Guijin Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=jPkOFAiOzf}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://github.com/THU-VCLab/RegionNormalizedGrasp</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8e333719c7cea78e87d2a2a029cbfc1b0d227a75.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=jPkOFAiOzf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|regionaware_grasp_framework_with_normalized_grasp_space_for_efficient_6dof_grasping</div>
</div>
<div class='paper-counter'>64/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">itKJ5uu1gW</div>
<div class="field-name">title:</div>
<div class="field-value">Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mingtong Zhang', 'Kaifeng Zhang', 'Yunzhu Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mingtong_Zhang1', '~Kaifeng_Zhang2', '~Yunzhu_Li1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dynamics Model', '3D Gaussian Splatting', 'Action-Conditioned Video Prediction', 'Model-Based Planning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at \url{https://gaussian-gbnd.github.io/}.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e91fb0e197b8a8f70b9bd9bb0d52b16b432b1809.pdf" target="_blank">/pdf/e91fb0e197b8a8f70b9bd9bb0d52b16b432b1809.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ccaea26a97dad4b7da3492b27bea75cfe47656b9.zip" target="_blank">/attachment/ccaea26a97dad4b7da3492b27bea75cfe47656b9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024dynamic,
title={Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling},
author={Mingtong Zhang and Kaifeng Zhang and Yunzhu Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=itKJ5uu1gW}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gs-dynamics.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/aa9ce411be7272bf97d027170f220cc808c72b57.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=itKJ5uu1gW&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|dynamic_3d_gaussian_tracking_for_graphbased_neural_dynamics_modeling</div>
</div>
<div class='paper-counter'>65/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">iZF0FRPgfq</div>
<div class="field-name">title:</div>
<div class="field-value">I Can Tell What I am Doing: Toward Real-World Natural Language Grounding of Robot Experiences</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zihan Wang', 'Brian Liang', 'Varad Dhat', 'Zander Brumbaugh', 'Nick Walker', 'Ranjay Krishna', 'Maya Cakmak']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zihan_Wang14', '~Brian_Liang1', '~Varad_Dhat1', '~Zander_Brumbaugh1', '~Nick_Walker1', '~Ranjay_Krishna1', '~Maya_Cakmak1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Large Language Model', 'Explainable AI', 'Failure Analysis']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Understanding robot behaviors and experiences through natural language is crucial for developing intelligent and transparent robotic systems. Recent advancement in large language models (LLMs) makes it possible to translate complex, multi-modal robotic experiences into coherent, human-readable narratives. However, grounding real-world robot experiences into natural language is challenging due to many reasons, such as multi-modal nature of data, differing sample rates, and data volume. We introduce RONAR, an LLM-based system that generates natural language narrations from robot experiences, aiding in behavior announcement, failure analysis, and human interaction to recover failure. Evaluated across various scenarios, RONAR outperforms state-of-the-art methods and improves failure recovery efficiency. Our contributions include a multi-modal framework for robot experience narration, a comprehensive real-robot dataset, and empirical evidence of RONAR's effectiveness in enhancing user experience in system transparency and failure analysis.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c71631b91e3843f83db14a7b7f9d9be35a44bc4c.pdf" target="_blank">/pdf/c71631b91e3843f83db14a7b7f9d9be35a44bc4c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/85e1a3fe3ca04237f38c31c39ae6eeddd1b91910.zip" target="_blank">/attachment/85e1a3fe3ca04237f38c31c39ae6eeddd1b91910.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024i,
title={I Can Tell What I am Doing: Toward Real-World Natural Language Grounding of Robot Experiences},
author={Zihan Wang and Brian Liang and Varad Dhat and Zander Brumbaugh and Nick Walker and Ranjay Krishna and Maya Cakmak},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=iZF0FRPgfq}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/real-world-robot-narration/home</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b4c834e48150fe904419ad017c4855f2bef0dfa8.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=iZF0FRPgfq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|i_can_tell_what_i_am_doing_toward_realworld_natural_language_grounding_of_robot_experiences</div>
</div>
<div class='paper-counter'>66/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">hV97HJm7Ag</div>
<div class="field-name">title:</div>
<div class="field-value">Task-Oriented Hierarchical Object Decomposition for Visuomotor Control</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jianing Qian', 'Yunshuang Li', 'Bernadette Bucher', 'Dinesh Jayaraman']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jianing_Qian2', '~Yunshuang_Li1', '~Bernadette_Bucher1', '~Dinesh_Jayaraman2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Visual Representations', 'Entities', 'Imitation', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show how a visual scene can be represented by task-oriented entity tree structure, and enables efficient and generalizable imitation policy learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Good pre-trained visual representations could enable robots to learn visuomotor policy efficiently. Still, existing representations take a one-size-fits-all-tasks approach that comes with two important drawbacks: (1) Being completely task-agnostic, these representations cannot effectively ignore any task-irrelevant information in the scene,  and (2) They often lack the representational capacity to handle unconstrained/complex real-world scenes. Instead, we propose to train a large combinatorial family of representations organized by scene entities: objects and object parts. This hierarchical object decomposition for task-oriented representations (HODOR) permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task. In our experiments, we find that HODOR outperforms prior pre-trained representations, both scene vector representations and object-centric representations, for sample-efficient imitation learning across 5 simulated and 5 real-world manipulation tasks. We further find that the invariances captured in HODOR are inherited into downstream policies, which can robustly generalize to out-of-distribution test conditions, permitting zero-shot skill chaining. Appendix and videos: https://sites.google.com/view/
hodor-corl24</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4c76ec6d6d6863beea17c22f4a19c3201b3d5c85.pdf" target="_blank">/pdf/4c76ec6d6d6863beea17c22f4a19c3201b3d5c85.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/93c59a938fb52831b0c036ae88add8584cbc74a9.zip" target="_blank">/attachment/93c59a938fb52831b0c036ae88add8584cbc74a9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
qian2024taskoriented,
title={Task-Oriented Hierarchical Object Decomposition for Visuomotor Control},
author={Jianing Qian and Yunshuang Li and Bernadette Bucher and Dinesh Jayaraman},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=hV97HJm7Ag}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/hodor-corl24</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4bbf82d679b9c35ebab28a6ff16f8301b6f8dd78.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=hV97HJm7Ag&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">qian|taskoriented_hierarchical_object_decomposition_for_visuomotor_control</div>
</div>
<div class='paper-counter'>67/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">gvdXE7ikHI</div>
<div class="field-name">title:</div>
<div class="field-value">ALOHA Unleashed: A Simple Recipe for Robot Dexterity</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tony Z. Zhao', 'Jonathan Tompson', 'Danny Driess', 'Pete Florence', 'Seyed Kamyar Seyed Ghasemipour', 'Chelsea Finn', 'Ayzaan Wahid']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tony_Z._Zhao1', '~Jonathan_Tompson1', '~Danny_Driess1', '~Pete_Florence1', '~Seyed_Kamyar_Seyed_Ghasemipour1', '~Chelsea_Finn1', '~Ayzaan_Wahid1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A simple recipe for dexterous autonomous policies with scalable data collection and Diffusion Policies</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent work has shown promising results for learning end-to-end robot policies using imitation learning. In this work we address the question of how far can we push imitation learning for challenging dexterous manipulation tasks. We show that a simple recipe of large scale data collection on the ALOHA 2 platform, combined with expressive models such as Diffusion Policies, can be effective in learning challenging bimanual manipulation tasks involving deformable objects and complex contact rich dynamics. We demonstrate our recipe on 5 challenging real-world and 3 simulated tasks and demonstrate improved performance over state-of-the-art baselines.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/b9f31099808bf92a0ea5ecf01d38fed9b0283259.pdf" target="_blank">/pdf/b9f31099808bf92a0ea5ecf01d38fed9b0283259.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dd205fcc0ed6b3d3a3e4b7fff38b459b9590cdcd.zip" target="_blank">/attachment/dd205fcc0ed6b3d3a3e4b7fff38b459b9590cdcd.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhao2024aloha,
title={{ALOHA} Unleashed: A Simple Recipe for Robot Dexterity},
author={Tony Z. Zhao and Jonathan Tompson and Danny Driess and Pete Florence and Seyed Kamyar Seyed Ghasemipour and Chelsea Finn and Ayzaan Wahid},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=gvdXE7ikHI}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://aloha-unleashed.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0a1ad3f7870a2e376740c224bd0410367b7d7b3a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=gvdXE7ikHI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhao|aloha_unleashed_a_simple_recipe_for_robot_dexterity</div>
</div>
<div class='paper-counter'>68/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">gqFIybpsLX</div>
<div class="field-name">title:</div>
<div class="field-value">Avoid Everything: Model-Free Collision Avoidance with Expert-Guided Fine-Tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Adam Fishman', 'Aaron Walsman', 'Mohak Bhardwaj', 'Wentao Yuan', 'Balakumar Sundaralingam', 'Byron Boots', 'Dieter Fox']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Adam_Fishman1', '~Aaron_Walsman1', '~Mohak_Bhardwaj1', '~Wentao_Yuan1', '~Balakumar_Sundaralingam1', '~Byron_Boots1', '~Dieter_Fox1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Robotics', 'Collision Avoidance', 'Fine Tuning', 'Motion Planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Avoid Everything is a novel end-to-end system for generating collision-free motion toward a goal that successfully solves 63% of challenging problems where the previous state of the art method fails.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The world is full of clutter. In order to operate effectively in uncontrolled, real world spaces, robots must navigate safely by executing tasks around obstacles while in proximity to hazards. Creating safe movement for robotic manipulators remains a long-standing challenge in robotics, particularly in environments with partial observability. In partially observed settings, classical techniques often fail. Learned end-to-end motion policies can infer correct solutions in these settings, but are as-yet unable to produce reliably safe movement when close to obstacles. In this work, we introduce Avoid Everything, a novel end-to-end system for generating collision-free motion toward a target, even targets close to obstacles. Avoid Everything consists of two parts: 1) Motion Policy Transformer (M$\pi$Former), a transformer architecture for end-to-end joint space control from point clouds, trained on over 1,000,000 expert trajectories and 2) a fine-tuning procedure we call Refining on Optimized Policy Experts (ROPE), which uses optimization to provide demonstrations of safe behavior in challenging states. With these techniques, we are able to successfully solve over 63% of reaching problems that caused the previous state of the art method to fail, resulting in an overall success rate of over 91\% in challenging manipulation settings.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d04a076c1b1b0e51cf759c1e124928c58b22657e.pdf" target="_blank">/pdf/d04a076c1b1b0e51cf759c1e124928c58b22657e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dbfc7a276ee597d81a547352236e8562caa6e67d.zip" target="_blank">/attachment/dbfc7a276ee597d81a547352236e8562caa6e67d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
fishman2024avoid,
title={Avoid Everything: Model-Free Collision Avoidance with Expert-Guided Fine-Tuning},
author={Adam Fishman and Aaron Walsman and Mohak Bhardwaj and Wentao Yuan and Balakumar Sundaralingam and Byron Boots and Dieter Fox},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=gqFIybpsLX}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://avoid-everything.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b95c6f15d482b6e84fc329fb65800d5c873561ef.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=gqFIybpsLX&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">fishman|avoid_everything_modelfree_collision_avoidance_with_expertguided_finetuning</div>
</div>
<div class='paper-counter'>69/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">gqCQxObVz2</div>
<div class="field-name">title:</div>
<div class="field-value">3D Diffuser Actor: Policy Diffusion with 3D Scene Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tsung-Wei Ke', 'Nikolaos Gkanatsios', 'Katerina Fragkiadaki']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tsung-Wei_Ke2', '~Nikolaos_Gkanatsios1', '~Katerina_Fragkiadaki1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Diffusion models', '3D representations', 'manipulation', 'imitation learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion policies are conditional diffusion models that learn robot action distributions conditioned on the robot and environment state. They have recently shown to outperform both deterministic and alternative action distribution learning formulations. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor , a neural policy equipped with a novel 3D denoising transformer that fuses information from the 3D visual scene, a language instruction and proprioception to predict the noise in noised 3D robot pose trajectories. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 18.1% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it improves over the current SOTA by a 9% relative increase. It also learns to control a robot manipulator in the real world from a handful of demonstrations. Through thorough comparisons with the current SOTA policies and ablations of our model, we show 3D Diffuser Actor ’s design choices dramatically outperform 2D representations, regression and classification objectives, absolute attentions, and holistic non-tokenized 3D scene embeddings.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/342311a6eadf314eb070e7b379e819b3678bc82b.pdf" target="_blank">/pdf/342311a6eadf314eb070e7b379e819b3678bc82b.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ke2024d,
title={3D Diffuser Actor: Policy Diffusion with 3D Scene Representations},
author={Tsung-Wei Ke and Nikolaos Gkanatsios and Katerina Fragkiadaki},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=gqCQxObVz2}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://3d-diffuser-actor.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f29dab8c51a679aa047f3606aefd876085834032.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=gqCQxObVz2&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ke|3d_diffuser_actor_policy_diffusion_with_3d_scene_representations</div>
</div>
<div class='paper-counter'>70/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fs7ia3FqUM</div>
<div class="field-name">title:</div>
<div class="field-value">Humanoid Parkour Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ziwen Zhuang', 'Shenzhe Yao', 'Hang Zhao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ziwen_Zhuang1', '~Shenzhe_Yao1', '~Hang_Zhao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoid Agile Locomotion', 'Visuomotor Control', 'Sim-to-Real Transfer']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a single vision-based end-to-end whole-body-control parkour policy for humanoid robots that can jump on 0.42m platforms, leap over hurdles, 0.8m gaps, and overcome various terrains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Parkour is a grand challenge for legged locomotion, even for quadruped robots, requiring active perception and various maneuvers to overcome multiple challenging obstacles. Existing methods for humanoid locomotion either optimize a trajectory for a single parkour track or train a reinforcement learning policy only to walk with a significant amount of motion references. In this work, we propose a framework for learning an end-to-end vision-based whole-body-control parkour policy for humanoid robots that overcomes multiple parkour skills without any motion prior. Using the parkour policy, the humanoid robot can jump on a 0.42m platform, leap over hurdles, 0.8m gaps, and much more. It can also run at 1.8m/s in the wild and walk robustly on different terrains. We test our policy in indoor and outdoor environments to demonstrate that it can autonomously select parkour skills while following the rotation command of the joystick. We override the arm actions and show that this framework can easily transfer to humanoid mobile manipulation tasks. Videos can be found at https://humanoid4parkour.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e8baa8e6dcf503aac29ff423a87db6803986449e.pdf" target="_blank">/pdf/e8baa8e6dcf503aac29ff423a87db6803986449e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a902a0c633c765e158749f1b988a4421be268674.zip" target="_blank">/attachment/a902a0c633c765e158749f1b988a4421be268674.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhuang2024humanoid,
title={Humanoid Parkour Learning},
author={Ziwen Zhuang and Shenzhe Yao and Hang Zhao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fs7ia3FqUM}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://humanoid4parkour.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5313851e666015ac49e0a15872a138a814cbeaae.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fs7ia3FqUM&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhuang|humanoid_parkour_learning</div>
</div>
<div class='paper-counter'>71/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fR1rCXjCQX</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Compositional Behaviors from Demonstration and Language</div>
<div class="field-name">authors:</div>
<div class="field-value">['Weiyu Liu', 'Neil Nie', 'Ruohan Zhang', 'Jiayuan Mao', 'Jiajun Wu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Weiyu_Liu1', '~Neil_Nie1', '~Ruohan_Zhang1', '~Jiayuan_Mao1', '~Jiajun_Wu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Manipulation', 'Planning Abstractions', 'Learning from Language']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a new framework for long-horizon robotic manipulation by integrating imitation learning and planning with models automatically constructed from language.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Behavior from Language and Demonstration (BLADE), a framework for long-horizon robotic manipulation by integrating imitation learning and model-based planning. BLADE leverages language-annotated demonstrations, extracts abstract action knowledge from large language models (LLMs), and constructs a library of structured, high-level action representations. These representations include preconditions and effects grounded in visual perception for each high-level action, along with corresponding controllers implemented as neural network-based policies. BLADE can recover such structured representations automatically, without manually labeled states or symbolic definitions. BLADE shows significant capabilities in generalizing to novel situations, including novel initial states, external state perturbations, and novel goals. We validate the effectiveness of our approach both in simulation and on real robots with a diverse set of objects with articulated parts, partial observability, and geometric constraints.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1e1aa7380ea488090293d0b43384450b59952516.pdf" target="_blank">/pdf/1e1aa7380ea488090293d0b43384450b59952516.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/18369eb8c1d0317551593f4fc6d6dca9c91afb81.zip" target="_blank">/attachment/18369eb8c1d0317551593f4fc6d6dca9c91afb81.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024learning,
title={Learning Compositional Behaviors from Demonstration and Language},
author={Weiyu Liu and Neil Nie and Ruohan Zhang and Jiayuan Mao and Jiajun Wu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fR1rCXjCQX}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://blade-bot.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/de51bba25ee930a5d1a2ca928ada7e06a2079318.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fR1rCXjCQX&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|learning_compositional_behaviors_from_demonstration_and_language</div>
</div>
<div class='paper-counter'>72/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fNBbEgcfwO</div>
<div class="field-name">title:</div>
<div class="field-value">Surgical Robot Transformer (SRT): Imitation Learning for Surgical Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ji Woong Kim', 'Tony Z. Zhao', 'Samuel Schmidgall', 'Anton Deguet', 'Marin Kobilarov', 'Chelsea Finn', 'Axel Krieger']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ji_Woong_Kim2', '~Tony_Z._Zhao1', '~Samuel_Schmidgall1', 'anton.deguet@jhu.edu', '~Marin_Kobilarov1', '~Chelsea_Finn1', '~Axel_Krieger1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Manipulation', 'Medical Robotics']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Imitation learning on the da Vinci robot without needing hand-eye calibration</div>
<div class="field-name">abstract:</div>
<div class="field-value">We explore whether surgical manipulation tasks can be learned on the da Vinci robot via imitation learning.
However, the da Vinci system presents unique challenges which hinder straight-forward implementation of imitation learning. Notably, its forward kinematics is inconsistent due to imprecise joint measurements, and naively training a policy using such approximate kinematics data often leads to task failure. To overcome this limitation, we introduce a relative action formulation 
which enables successful policy training and deployment using its approximate kinematics data. A promising outcome of this approach is that the large repository of clinical data, which contains approximate kinematics, may be directly utilized for robot learning without further corrections. We demonstrate our findings through successful execution of three fundamental surgical tasks, including tissue manipulation, needle handling, and knot-tying.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/cec419e45ddbb0680ba2cba407bd4cb2498caebb.pdf" target="_blank">/pdf/cec419e45ddbb0680ba2cba407bd4cb2498caebb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/216db7e885d750343169c5d03cf2e45c1d118b2a.zip" target="_blank">/attachment/216db7e885d750343169c5d03cf2e45c1d118b2a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kim2024surgical,
title={Surgical Robot Transformer ({SRT}): Imitation Learning for Surgical Tasks},
author={Ji Woong Kim and Tony Z. Zhao and Samuel Schmidgall and Anton Deguet and Marin Kobilarov and Chelsea Finn and Axel Krieger},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fNBbEgcfwO}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://surgical-robot-transformer.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f68c08ca7a815f7d8413a77983c4f1e47679d219.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fNBbEgcfwO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kim|surgical_robot_transformer_srt_imitation_learning_for_surgical_tasks</div>
</div>
<div class='paper-counter'>73/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fIj88Tn3fc</div>
<div class="field-name">title:</div>
<div class="field-value">ReMix: Optimizing Data Mixtures for Large Scale Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Joey Hejna', 'Chethan Anand Bhateja', 'Yichen Jiang', 'Karl Pertsch', 'Dorsa Sadigh']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Joey_Hejna1', '~Chethan_Anand_Bhateja1', 'ycjiang@stanford.edu', '~Karl_Pertsch1', '~Dorsa_Sadigh1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Data Curation', 'Data Quality', 'Robot Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use techniques from robust optimization to learn data mixture weights for Bridge and RT-X datasets, and show they improve downstream performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Increasingly large robotics datasets are being collected to train larger foundation models in robotics. However, despite the fact that data selection has been of utmost importance to scaling in vision and natural language processing (NLP), little work in robotics has questioned what data such models should actually be trained on. In this work we investigate how to weigh different subsets or ``domains'' of robotics datasets during pre-training to maximize worst-case performance across all possible downstream domains using distributionally robust optimization (DRO). Unlike in NLP, we find that these methods are hard to apply out of the box due to varying action spaces and dynamics across robots. Our method, ReMix, employs early stopping and action normalization and discretization to counteract these issues. Through extensive experimentation on both the Bridge and OpenX datasets, we demonstrate that data curation can have an outsized impact on downstream performance. Specifically, domain weights learned by ReMix outperform uniform weights by over 40\% on average and human-selected weights by over 20\% on datasets used to train the RT-X models.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6b72813c13466a1efcd4cecb8403ab639179ec95.pdf" target="_blank">/pdf/6b72813c13466a1efcd4cecb8403ab639179ec95.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/37287fc82d9dd25944e1776d48d8a5752f0da03a.zip" target="_blank">/attachment/37287fc82d9dd25944e1776d48d8a5752f0da03a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hejna2024remix,
title={ReMix: Optimizing Data Mixtures for Large Scale Imitation Learning},
author={Joey Hejna and Chethan Anand Bhateja and Yichen Jiang and Karl Pertsch and Dorsa Sadigh},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fIj88Tn3fc}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/68e61f8077edffacd8d20736c98a10b0bf6ce450.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fIj88Tn3fc&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hejna|remix_optimizing_data_mixtures_for_large_scale_imitation_learning</div>
</div>
<div class='paper-counter'>74/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fDRO4NHEwZ</div>
<div class="field-name">title:</div>
<div class="field-value">VIRL: Self-Supervised Visual Graph Inverse Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Lei Huang', 'Weijia Cai', 'Zihan Zhu', 'Chen Feng', 'Helge Rhodin', 'Zhengbo Zou']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Lei_Huang11', '~Weijia_Cai1', 'zzhu12@student.ubc.ca', '~Chen_Feng2', '~Helge_Rhodin5', '~Zhengbo_Zou1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Inverse Reinforcement Learning', 'Learning from Video', 'Graph Network']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">An inverse reinforcement learning method that leverages visual features and graph abstractions from videos to learn dense reward functions for reinforcement learning, demonstrating generalization to extrapolation tasks and unseen domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning dense reward functions from unlabeled videos for reinforcement learning exhibits scalability due to the vast diversity and quantity of video resources. Recent works use visual features or graph abstractions in videos to measure task progress as rewards, which either deteriorate in unseen domains or capture spatial information while overlooking visual details. We propose $\textbf{V}$isual-Graph $\textbf{I}$nverse $\textbf{R}$einforcement $\textbf{L}$earning (VIRL), a self-supervised method that synergizes low-level visual features and high-level graph abstractions from frames to graph representations for reward learning. VIRL utilizes a visual encoder that extracts object-wise features for graph nodes and a graph encoder that derives properties from graphs constructed from detected objects in each frame. The encoded representations are enforced to align videos temporally and reconstruct in-scene objects. The pretrained visual graph encoder is then utilized to construct a dense reward function for policy learning by measuring latent distances between current frames and the goal frame. Our empirical evaluation on the X-MAGICAL and Robot Visual Pusher benchmark demonstrates that VIRL effectively handles tasks necessitating both granular visual attention and broader global feature consideration, and exhibits robust generalization to $\textit{extrapolation}$ tasks and domains not seen in demonstrations. Our policy for the robotic task also achieves the highest success rate in real-world robot experiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6c2209ac6017064c51e181a56a27ccac550dc559.pdf" target="_blank">/pdf/6c2209ac6017064c51e181a56a27ccac550dc559.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e0514d57cc9862e7756e4cb7ad68ac5a19a74922.zip" target="_blank">/attachment/e0514d57cc9862e7756e4cb7ad68ac5a19a74922.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024virl,
title={{VIRL}: Self-Supervised Visual Graph Inverse Reinforcement Learning},
author={Lei Huang and Weijia Cai and Zihan Zhu and Chen Feng and Helge Rhodin and Zhengbo Zou},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fDRO4NHEwZ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://leihhhuang.github.io/VIRL/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1232339e6610c004d38b3ed5029a96adfa1e9847.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fDRO4NHEwZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|virl_selfsupervised_visual_graph_inverse_reinforcement_learning</div>
</div>
<div class='paper-counter'>75/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fCDOfpTCzZ</div>
<div class="field-name">title:</div>
<div class="field-value">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yuxing Long', 'Wenzhe Cai', 'Hongcheng Wang', 'Guanqi Zhan', 'Hao Dong']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yuxing_Long1', '~Wenzhe_Cai1', '~Hongcheng_Wang6', '~Guanqi_Zhan1', '~Hao_Dong3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Generic Instruction Navigation', 'Zero-shot', 'Unexplored Environment']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling robots to navigate following diverse language instructions in unexplored environments is an attractive goal for human-robot interaction. However, this goal is challenging because different navigation tasks require different strategies. The scarcity of instruction navigation data hinders training an instruction navigation model with varied strategies. Therefore, previous methods are all constrained to one specific type of navigation instruction. In this work, we propose InstructNav, a generic instruction navigation system. InstructNav makes the first endeavor to handle various instruction navigation tasks without any navigation training or pre-built maps. To reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify the planning process for different types of navigation instructions. Furthermore, we propose Multi-sourced Value Maps to model key elements in instruction navigation so that linguistic DCoN planning can be converted into robot actionable trajectories. With InstructNav, we complete the R2R-CE task in a zero-shot way for the first time and outperform many task-training methods. Besides, InstructNav also surpasses the previous SOTA method by 10.48% on the zero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN. Real robot experiments on diverse indoor scenes further demonstrate our method's robustness in coping with the environment and instruction variations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a874a10e64c744f030562e878e2d2a0c59a256c3.pdf" target="_blank">/pdf/a874a10e64c744f030562e878e2d2a0c59a256c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3633c15f8475c2c4dfcf9e23cf0f833c64c49a0d.zip" target="_blank">/attachment/3633c15f8475c2c4dfcf9e23cf0f833c64c49a0d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
long2024instructnav,
title={InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment},
author={Yuxing Long and Wenzhe Cai and Hongcheng Wang and Guanqi Zhan and Hao Dong},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fCDOfpTCzZ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/instructnav</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5925619eba2954251000f8be5e09b5b5a976e233.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fCDOfpTCzZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">long|instructnav_zeroshot_system_for_generic_instruction_navigation_in_unexplored_environment</div>
</div>
<div class='paper-counter'>76/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">fC0wWeXsVm</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Dhruva Tirumala', 'Markus Wulfmeier', 'Ben Moran', 'Sandy Huang', 'Jan Humplik', 'Guy Lever', 'Tuomas Haarnoja', 'Leonard Hasenclever', 'Arunkumar Byravan', 'Nathan Batchelor', 'Neil sreendra', 'Kushal Patel', 'Marlon Gwira', 'Francesco Nori', 'Martin Riedmiller', 'Nicolas Heess']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Dhruva_Tirumala1', '~Markus_Wulfmeier1', '~Ben_Moran2', '~Sandy_Huang1', '~Jan_Humplik1', '~Guy_Lever1', '~Tuomas_Haarnoja1', '~Leonard_Hasenclever1', '~Arunkumar_Byravan1', '~Nathan_Batchelor1', '~Neil_sreendra1', '~Kushal_Patel1', '~Marlon_Gwira1', '~Francesco_Nori2', '~Martin_Riedmiller1', '~Nicolas_Heess1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['robotics', 'deep reinforcement learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We apply multi-agent deep reinforcement learning (RL) to train end-to-end robot soccer policies with fully onboard computation and sensing via egocentric RGB vision. This setting reflects many challenges of real-world robotics, including active perception, agile full-body control, and long-horizon planning in a dynamic, partially-observable, multi-agent domain. We rely on large-scale, simulation-based data generation to obtain complex behaviors from egocentric vision which can be successfully transferred to physical robots using low-cost sensors.
To achieve adequate visual realism, our simulation combines rigid-body physics with learned, realistic rendering via multiple Neural Radiance Fields (NeRFs). We combine teacher-based multi-agent RL and cross-experiment data reuse to enable the discovery of sophisticated soccer strategies. We analyze active-perception behaviors including object tracking and ball seeking that emerge when simply optimizing perception-agnostic soccer play. The agents display equivalent levels of performance and agility as policies with access to privileged, ground-truth state. To our knowledge, this paper constitutes a first demonstration of end-to-end training for multi-agent robot soccer, mapping raw pixel observations to joint-level actions that can be deployed in the real world.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0fb3371771518b43abc8fb8a60ff46df666a6d26.pdf" target="_blank">/pdf/0fb3371771518b43abc8fb8a60ff46df666a6d26.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2e99ba7e5712f61a4994b88bfdfdbb76a4561c8e.zip" target="_blank">/attachment/2e99ba7e5712f61a4994b88bfdfdbb76a4561c8e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tirumala2024learning,
title={Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning},
author={Dhruva Tirumala and Markus Wulfmeier and Ben Moran and Sandy Huang and Jan Humplik and Guy Lever and Tuomas Haarnoja and Leonard Hasenclever and Arunkumar Byravan and Nathan Batchelor and Neil sreendra and Kushal Patel and Marlon Gwira and Francesco Nori and Martin Riedmiller and Nicolas Heess},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=fC0wWeXsVm}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/vision-soccer</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7cb6e32853bc622d3ac5c1655b8f3e477f31d1c7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=fC0wWeXsVm&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tirumala|learning_robot_soccer_from_egocentric_vision_with_deep_reinforcement_learning</div>
</div>
<div class='paper-counter'>77/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">evCXwlCMIi</div>
<div class="field-name">title:</div>
<div class="field-value">Learning to Walk from Three Minutes of Real-World Data with Semi-structured Dynamics Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jacob Levy', 'Tyler Westenbroek', 'David Fridovich-Keil']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jacob_Levy1', '~Tyler_Westenbroek1', '~David_Fridovich-Keil1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Model-Based Reinforcement Learning', 'Physics-Based Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a novel framework for model-based reinforcement learning, which leverages physics-informed, semi-structured dynamics models to enable highly sample-efficient policy learning in the real world.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Traditionally, model-based reinforcement learning (MBRL) methods exploit neural networks as flexible function approximators to represent $\textit{a priori}$ unknown environment dynamics. However, training data are typically scarce in practice, and these black-box models often fail to generalize. Modeling architectures that leverage known physics can substantially reduce the complexity of system-identification, but break down in the face of complex phenomena such as contact. We introduce a novel framework for learning semi-structured dynamics models for contact-rich systems which seamlessly integrates structured first principles modeling techniques with black-box auto-regressive models. Specifically, we develop an ensemble of probabilistic models to estimate external forces, conditioned on historical observations and actions, and integrate these predictions using known Lagrangian dynamics. With this semi-structured approach, we can make accurate long-horizon predictions with substantially less data than prior methods. We leverage this capability and propose Semi-Structured Reinforcement Learning ($\texttt{SSRL}$) a simple model-based learning framework which pushes the sample complexity boundary for real-world learning. We validate our approach on a real-world Unitree Go1 quadruped robot, learning dynamic gaits  -- from scratch -- on both hard and soft surfaces with just a few minutes of real-world data. Video and code are available at: https://sites.google.com/utexas.edu/ssrl</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/39e6775e6c68c4b82f819595fb3bfbed27eddd9b.pdf" target="_blank">/pdf/39e6775e6c68c4b82f819595fb3bfbed27eddd9b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2f995359edb81aae5a7d9ee211e0070b483fa4c8.zip" target="_blank">/attachment/2f995359edb81aae5a7d9ee211e0070b483fa4c8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
levy2024learning,
title={Learning to Walk from Three Minutes of Real-World Data with Semi-structured Dynamics Models},
author={Jacob Levy and Tyler Westenbroek and David Fridovich-Keil},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=evCXwlCMIi}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/utexas.edu/ssrl</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/846362cc98a53e4a52e5e62e2f43bd6174ddee82.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=evCXwlCMIi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">levy|learning_to_walk_from_three_minutes_of_realworld_data_with_semistructured_dynamics_models</div>
</div>
<div class='paper-counter'>78/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">eeoX7tCoK2</div>
<div class="field-name">title:</div>
<div class="field-value">Shelf-Supervised Cross-Modal Pre-Training for 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mehar Khurana', 'Neehar Peri', 'James Hays', 'Deva Ramanan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['mehar21541@iiitd.ac.in', '~Neehar_Peri1', '~James_Hays1', '~Deva_Ramanan1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Shelf-Supervised 3D Object Detection', 'Vision-Language Models', 'Autonomous Vehicles']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate that pre-training detectors with zero-shot 3D bounding box pseudo-labels achieves higher semi-supervised detection accuracy than prior contrastive learning objectives.</div>
<div class="field-name">abstract:</div>
<div class="field-value">State-of-the-art 3D object detectors are often trained on massive labeled datasets. However, annotating 3D bounding boxes remains prohibitively expensive and time-consuming, particularly for LiDAR. Instead, recent works demonstrate that self-supervised pre-training with unlabeled data can improve detection accuracy with limited labels. Contemporary methods adapt best-practices for self-supervised learning from the image domain to point clouds (such as contrastive learning). However, publicly available 3D datasets are considerably smaller and less diverse than those used for image-based self-supervised learning, limiting their effectiveness. We do note, however, that such data is naturally collected in a multimodal fashion, often paired with images. Rather than pre-training with only self-supervised objectives, we argue that it is better to bootstrap point cloud representations using image-based foundation models trained on internet-scale image data. Specifically, we propose a shelf-supervised approach (e.g. supervised with off-the-shelf image foundation models) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR data. Pre-training 3D detectors with such pseudo-labels yields significantly better semi-supervised detection accuracy than prior self-supervised pretext tasks. Importantly, we show that image-based shelf-supervision is helpful for training LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the effectiveness of our approach on nuScenes and WOD, significantly improving over prior work in limited data settings.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/aa2e2f6ac2681cc938e0da493766f56d2a88786a.pdf" target="_blank">/pdf/aa2e2f6ac2681cc938e0da493766f56d2a88786a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/051ca3f45fdfec1075196256f2320406acec64cc.zip" target="_blank">/attachment/051ca3f45fdfec1075196256f2320406acec64cc.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
khurana2024shelfsupervised,
title={Shelf-Supervised Cross-Modal Pre-Training for 3D Object Detection},
author={Mehar Khurana and Neehar Peri and James Hays and Deva Ramanan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=eeoX7tCoK2}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://meharkhurana03.github.io/cm3d/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f575290b51bd3c650145b5a12294514531a7abb3.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=eeoX7tCoK2&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">khurana|shelfsupervised_crossmodal_pretraining_for_3d_object_detection</div>
</div>
<div class='paper-counter'>79/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">edP2dmingV</div>
<div class="field-name">title:</div>
<div class="field-value">Large Scale Mapping of Indoor Magnetic Field by Local and Sparse Gaussian Processes</div>
<div class="field-name">authors:</div>
<div class="field-value">['Iad ABDUL-RAOUF', 'Vincent Gay-Bellile', 'Cyril JOLY', 'Steve Bourgeois', 'Alexis Paljic']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Iad_ABDUL-RAOUF1', '~Vincent_Gay-Bellile1', '~Cyril_JOLY1', '~Steve_Bourgeois1', 'alexis.paljic@mines-paristech.fr']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Gaussian process regression', 'magnetic field maps', 'indoor localization']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a new large-scale magnetic field map model based on a combination of a sparse approximation of Gaussian process regression and a local expert aggregation method.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Magnetometer-based indoor navigation uses variations in the magnetic field to determine the robot's location. For that, a magnetic map of the environment has to be built beforehand from a collection of localized magnetic measurements. Existing solutions built on sparse Gaussian Process (GP) regression do not scale well to large environments, being either slow or resulting in discontinuous prediction. In this paper, we propose to model the magnetic field of large environments based on GP regression. We first modify a deterministic training conditional sparse GP by accounting for magnetic field physics to map small environments efficiently. We then scale the model on larger scenes by introducing a local expert aggregation framework. It splits the scene into subdomains, fits a local expert on each, and then aggregates expert predictions in a differentiable and probabilistic way. We evaluate our model on real and simulated data and show that we can smoothly map a three-story building in a few hundred milliseconds.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/b0265f0414f30972cd3151f13360aa9fe4bb4703.pdf" target="_blank">/pdf/b0265f0414f30972cd3151f13360aa9fe4bb4703.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/5b675296e73d73b231aae0c4bdc56f9ccada4aca.zip" target="_blank">/attachment/5b675296e73d73b231aae0c4bdc56f9ccada4aca.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
abdul-raouf2024large,
title={Large Scale Mapping of Indoor Magnetic Field by Local and Sparse Gaussian Processes},
author={Iad ABDUL-RAOUF and Vincent Gay-Bellile and Cyril JOLY and Steve Bourgeois and Alexis Paljic},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=edP2dmingV}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://github.com/CEA-LIST/large-scale-magnetic-mapping</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6b72fa8002f133fe63ee97f167e47c65d5e559ce.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=edP2dmingV&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">abdulraouf|large_scale_mapping_of_indoor_magnetic_field_by_local_and_sparse_gaussian_processes</div>
</div>
<div class='paper-counter'>80/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">eU5E0oTtpS</div>
<div class="field-name">title:</div>
<div class="field-value">Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mike Zhang', 'Kaixian Qu', 'Vaishakh Patil', 'Cesar Cadena', 'Marco Hutter']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mike_Zhang2', '~Kaixian_Qu1', '~Vaishakh_Patil1', '~Cesar_Cadena1', '~Marco_Hutter1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Scene Understanding', 'Large Language Models']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLM) have emerged as a tool for robots to generate task plans using common sense reasoning. For the LLM to generate actionable plans, scene context must be provided, often through a map. Recent works have shifted from explicit maps with fixed semantic classes to implicit open vocabulary maps based on queryable embeddings capable of representing any semantic class. However, embeddings cannot directly report the scene context as they are implicit, requiring further processing for LLM integration. To address this, we propose an explicit text-based map that can represent thousands of semantic classes while easily integrating with LLMs due to their text-based nature by building upon large-scale image recognition models. We study how entities in our map can be localized and show through evaluations that our text-based map localizations perform comparably to those from open vocabulary maps while using two to four orders of magnitude less memory. Real-robot experiments demonstrate the grounding of an LLM with the text-based map to solve user tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/bcc72098bf8decfafae97a654034cc698a9f1947.pdf" target="_blank">/pdf/bcc72098bf8decfafae97a654034cc698a9f1947.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2f75a3e2174832a0ee231d4b6a6c35ca8982b753.zip" target="_blank">/attachment/2f75a3e2174832a0ee231d4b6a6c35ca8982b753.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024tag,
title={Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models},
author={Mike Zhang and Kaixian Qu and Vaishakh Patil and Cesar Cadena and Marco Hutter},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=eU5E0oTtpS}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://tag-mapping.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5a074d997a942eca5beef9679177878650f4c293.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=eU5E0oTtpS&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|tag_map_a_textbased_map_for_spatial_reasoning_and_navigation_with_large_language_models</div>
</div>
<div class='paper-counter'>81/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">eTRncsYYdv</div>
<div class="field-name">title:</div>
<div class="field-value">Solving Offline Reinforcement Learning with Decision Tree Regression</div>
<div class="field-name">authors:</div>
<div class="field-value">['Prajwal Koirala', 'Cody Fleming']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Prajwal_Koirala1', '~Cody_Fleming2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Offline Reinforcement Learning', 'Decision Trees']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">The research introduces Decision Tree-based frameworks for Offline RL, achieving faster training times and comparable performance to traditional methods across various robotic tasks, while emphasizing the interpretability of learned policies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This study presents a novel approach to addressing offline reinforcement learning (RL) problems by reframing them as regression tasks that can be effectively solved using Decision Trees. Mainly, we introduce two distinct frameworks: return-conditioned and return-weighted decision tree policies (RCDTP and RWDTP), both of which achieve notable speed in agent training as well as inference, with training typically lasting less than a few minutes. Despite the simplification inherent in this reformulated approach to offline RL, our agents demonstrate performance that is at least on par with the established methods. We evaluate our methods on D4RL datasets for locomotion and manipulation, as well as other robotic tasks involving wheeled and flying robots. Additionally, we assess performance in delayed/sparse reward scenarios and highlight the explainability of these policies through action distribution and feature importance.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6f0d4d9a0d27a3a1489e2acf2efe6b6555feff5d.pdf" target="_blank">/pdf/6f0d4d9a0d27a3a1489e2acf2efe6b6555feff5d.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
koirala2024solving,
title={Solving Offline Reinforcement Learning with Decision Tree Regression},
author={Prajwal Koirala and Cody Fleming},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=eTRncsYYdv}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8038e8fbf0fd86eeba8e0ec563e9a13cb6c1aaa5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=eTRncsYYdv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">koirala|solving_offline_reinforcement_learning_with_decision_tree_regression</div>
</div>
<div class='paper-counter'>82/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">eJHy0AF5TO</div>
<div class="field-name">title:</div>
<div class="field-value">RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chongkai Gao', 'Zhengrong Xue', 'Shuying Deng', 'Tianhai Liang', 'Siqi Yang', 'Lin Shao', 'Huazhe Xu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chongkai_Gao1', '~Zhengrong_Xue1', '~Shuying_Deng1', '~Tianhai_Liang1', '~Siqi_Yang4', '~Lin_Shao2', '~Huazhe_Xu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['SE(3)-Equivariance', 'Manipulation', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes RiEMann, an efficient and near real-time SE(3)-equivariant robot imitation learning framework for manipulation tasks framework without point cloud segmentation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target actions for manipulation without any object segmentation. RiEMann can efficiently train the visuomotor policy from scratch with 5 to 10 demonstrations for a manipulation task, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable SE(3)-equivariant action space of RiEMann supports both pick-and-place tasks and articulated object manipulation tasks. In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic distance errors (reduced by 68.6%), and achieves 5.4 frames per second (fps) network inference speed.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fe323d553de306eff1a3253310958a44866af585.pdf" target="_blank">/pdf/fe323d553de306eff1a3253310958a44866af585.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ba3c6f63a45ee79506abf72f9244f4c3d38a0a19.zip" target="_blank">/attachment/ba3c6f63a45ee79506abf72f9244f4c3d38a0a19.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
gao2024riemann,
title={Ri{EM}ann: Near Real-Time {SE}(3)-Equivariant Robot Manipulation without Point Cloud Segmentation},
author={Chongkai Gao and Zhengrong Xue and Shuying Deng and Tianhai Liang and Siqi Yang and Lin Shao and Huazhe Xu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=eJHy0AF5TO}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://riemann-web.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/443025ae68730f14af350f6dba6ac2cc8ab192fa.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=eJHy0AF5TO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">gao|riemann_near_realtime_se3equivariant_robot_manipulation_without_point_cloud_segmentation</div>
</div>
<div class='paper-counter'>83/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">dsxmR6lYlg</div>
<div class="field-name">title:</div>
<div class="field-value">Reinforcement Learning with Foundation Priors: Let Embodied Agent Efficiently Learn on Its Own</div>
<div class="field-name">authors:</div>
<div class="field-value">['Weirui Ye', 'Yunsheng Zhang', 'Haoyang Weng', 'Xianfan Gu', 'Shengjie Wang', 'Tong Zhang', 'Mengchen Wang', 'Pieter Abbeel', 'Yang Gao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Weirui_Ye1', '~Yunsheng_Zhang1', '~Haoyang_Weng1', '~Xianfan_Gu1', '~Shengjie_Wang2', '~Tong_Zhang23', '~Mengchen_Wang3', '~Pieter_Abbeel2', '~Yang_Gao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Foundation Models', 'Robotics', 'VLMs']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) is a promising approach for solving robotic manipulation tasks.
However, it is challenging to apply the RL algorithms directly in the real world.
For one thing, RL is data-intensive and typically requires millions of interactions with environments, which are impractical in real scenarios. 
For another, it is necessary to make heavy engineering efforts to design reward functions manually. 
To address these issues, we leverage foundation models in this paper. 
We propose Reinforcement Learning with Foundation Priors (RLFP) to utilize guidance and feedback from policy, value, and success-reward foundation models.
Within this framework, we introduce the Foundation-guided Actor-Critic (FAC) algorithm, which enables embodied agents to explore more efficiently with automatic reward functions.
The benefits of our framework are threefold: (1) \textit{sample efficient}; (2) \textit{minimal and effective reward engineering}; (3) \textit{agnostic to foundation model forms and robust to noisy priors}. Our method achieves remarkable performances in various manipulation tasks on both real robots and in simulation. Across 5 dexterous tasks with real robots, FAC achieves an average success rate of 86\% after one hour of real-time learning. 
Across 8 tasks in the simulated Meta-world, FAC achieves 100\% success rates in 7/8 tasks under less than 100k frames (about 1-hour training), outperforming baseline methods with manual-designed rewards in 1M frames. 
We believe the RLFP framework can enable future robots to explore and learn autonomously in the physical world for more tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/81f2c8370ba2b70e11f87f3d09f8ca04b34b1cb7.pdf" target="_blank">/pdf/81f2c8370ba2b70e11f87f3d09f8ca04b34b1cb7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d8fe88f6b7c288490c28705d7ab48fa23ec00eff.zip" target="_blank">/attachment/d8fe88f6b7c288490c28705d7ab48fa23ec00eff.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ye2024reinforcement,
title={Reinforcement Learning with Foundation Priors: Let Embodied Agent Efficiently Learn on Its Own},
author={Weirui Ye and Yunsheng Zhang and Haoyang Weng and Xianfan Gu and Shengjie Wang and Tong Zhang and Mengchen Wang and Pieter Abbeel and Yang Gao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=dsxmR6lYlg}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://yewr.github.io/rlfp</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a0e69f0ed5480ebf2d3dcb106e87b0d38e7a26cb.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=dsxmR6lYlg&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ye|reinforcement_learning_with_foundation_priors_let_embodied_agent_efficiently_learn_on_its_own</div>
</div>
<div class='paper-counter'>84/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">deywgeWmL5</div>
<div class="field-name">title:</div>
<div class="field-value">TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Junik Bae', 'Kwanyoung Park', 'Youngwoon Lee']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Junik_Bae1', '~Kwanyoung_Park1', '~Youngwoon_Lee1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Unsupervised Goal-Conditioned Reinforcement Learning', 'Temporal Distance-Aware Representations']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising paradigm for developing diverse robotic skills without external supervision. However, existing unsupervised GCRL methods often struggle to cover a wide range of states in complex environments due to their limited exploration and sparse or noisy rewards for GCRL. To overcome these challenges, we propose a novel unsupervised GCRL method that leverages TemporaL Distance-aware Representations (TLDR). Based on temporal distance, TLDR selects faraway goals to initiate exploration and computes intrinsic exploration rewards and goal-reaching rewards. Specifically, our exploration policy seeks states with large temporal distances (i.e. covering a large state space), while the goal-conditioned policy learns to minimize the temporal distance to the goal (i.e. reaching the goal). Our results in six simulated locomotion environments demonstrate that TLDR significantly outperforms prior unsupervised GCRL methods in achieving a wide range of states.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/174b69202b58cd003e3302a4cde2b1ebcd1e8a18.pdf" target="_blank">/pdf/174b69202b58cd003e3302a4cde2b1ebcd1e8a18.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a7c0a81a9dad166e605dd8046602746c210cf526.zip" target="_blank">/attachment/a7c0a81a9dad166e605dd8046602746c210cf526.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
bae2024tldr,
title={{TLDR}: Unsupervised Goal-Conditioned {RL} via Temporal Distance-Aware Representations},
author={Junik Bae and Kwanyoung Park and Youngwoon Lee},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=deywgeWmL5}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://heatz123.github.io/tldr/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c50392db67cf28464056acab24e85f1a18d514fc.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=deywgeWmL5&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">bae|tldr_unsupervised_goalconditioned_rl_via_temporal_distanceaware_representations</div>
</div>
<div class='paper-counter'>85/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">dXSGw7Cy55</div>
<div class="field-name">title:</div>
<div class="field-value">Contrast Sets for Evaluating Language-Guided Robot Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">['Abrar Anwar', 'Rohan Gupta', 'Jesse Thomason']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Abrar_Anwar1', '~Rohan_Gupta3', '~Jesse_Thomason1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Evaluation', 'Language-guided robots']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">we introduce contrast sets for robotics as an approach to make small, but specific, perturbations to otherwise independent, identically distributed (i.i.d.) test instances for better evaluating language-guided robots.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot evaluations in language-guided, real world settings are time-consuming and often sample only a small space of potential instructions across complex scenes. In this work, we introduce contrast sets for robotics as an approach to make small, but specific, perturbations to otherwise independent, identically distributed (i.i.d.) test instances. We investigate the relationship between experimenter effort to carry out an evaluation and the resulting estimated test performance as well as the insights that can be drawn from performance on perturbed instances. We use contrast sets to characterize policies at reduced experimenter effort in both a simulated manipulation task and a physical robot vision-and-language navigation task. We encourage the use of contrast set evaluations as a more informative alternative to small scale, i.i.d. demonstrations on physical robots, and as a scalable alternative to industry-scale real world evaluations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/096eeab8e865e628ec628de3e71b81fa939f490b.pdf" target="_blank">/pdf/096eeab8e865e628ec628de3e71b81fa939f490b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/7297b8bc75f5b1459b09916b97dadf6d36b50972.zip" target="_blank">/attachment/7297b8bc75f5b1459b09916b97dadf6d36b50972.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
anwar2024contrast,
title={Contrast Sets for Evaluating Language-Guided Robot Policies},
author={Abrar Anwar and Rohan Gupta and Jesse Thomason},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=dXSGw7Cy55}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a449be0934b070130ff5a9073b090cfdc9c99a0b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=dXSGw7Cy55&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">anwar|contrast_sets_for_evaluating_languageguided_robot_policies</div>
</div>
<div class='paper-counter'>86/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">dUo6j3YURS</div>
<div class="field-name">title:</div>
<div class="field-value">MOSAIC: Modular Foundation Models for Assistive and Interactive Cooking</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huaxiaoyue Wang', 'Kushal Kedia', 'Juntao Ren', 'Rahma Abdullah', 'Atiksh Bhardwaj', 'Angela Chao', 'Kelly Y Chen', 'Nathaniel Chin', 'Prithwish Dan', 'Xinyi Fan', 'Gonzalo Gonzalez-Pumariega', 'Aditya Kompella', 'Maximus Adrian Pace', 'Yash Sharma', 'Xiangwan Sun', 'Neha Sunkara', 'Sanjiban Choudhury']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huaxiaoyue_Wang1', '~Kushal_Kedia1', '~Juntao_Ren1', '~Rahma_Abdullah1', '~Atiksh_Bhardwaj1', '~Angela_Chao1', '~Kelly_Y_Chen1', '~Nathaniel_Chin1', '~Prithwish_Dan1', '~Xinyi_Fan3', '~Gonzalo_Gonzalez-Pumariega1', '~Aditya_Kompella1', '~Maximus_Adrian_Pace1', '~Yash_Sharma3', '~Xiangwan_Sun1', '~Neha_Sunkara1', '~Sanjiban_Choudhury3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Foundation Models', 'Human-Robot Interaction', 'Model Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">MOSAIC is a modular architecture that enables multiple home robots to collaboratively cook with humans.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present MOSAIC, a modular architecture for coordinating multiple robots to (a) interact with users using natural language and (b) manipulate an open vocabulary of everyday objects. At several levels, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for high-level tasks like language and image recognition, while using streamlined modules designed for low-level task-specific control. This decomposition allows us to reap the complementary benefits of foundation models and precise, more specialized models, enabling our system to scale to complex tasks that involve coordinating multiple robots and humans. First, we unit-test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. We then extensively evaluate MOSAIC with 60 end-to-end trials. We discuss crucial design decisions, limitations of the current system, and open challenges in this domain</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/59fb87379352794f3d56625a3a3f32abcbffb3aa.pdf" target="_blank">/pdf/59fb87379352794f3d56625a3a3f32abcbffb3aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/8bf0c8480cff9ddc5516008d09bc9b271efb4b0d.zip" target="_blank">/attachment/8bf0c8480cff9ddc5516008d09bc9b271efb4b0d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024mosaic,
title={{MOSAIC}: Modular Foundation Models for Assistive and Interactive Cooking},
author={Huaxiaoyue Wang and Kushal Kedia and Juntao Ren and Rahma Abdullah and Atiksh Bhardwaj and Angela Chao and Kelly Y Chen and Nathaniel Chin and Prithwish Dan and Xinyi Fan and Gonzalo Gonzalez-Pumariega and Aditya Kompella and Maximus Adrian Pace and Yash Sharma and Xiangwan Sun and Neha Sunkara and Sanjiban Choudhury},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=dUo6j3YURS}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://portal-cornell.github.io/MOSAIC/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f4a3d8872668fb31ed89d2b958b08d42c59c03f7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=dUo6j3YURS&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|mosaic_modular_foundation_models_for_assistive_and_interactive_cooking</div>
</div>
<div class='paper-counter'>87/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cvVEkS5yij</div>
<div class="field-name">title:</div>
<div class="field-value">Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tianhao Wei', 'Liqian Ma', 'Rui Chen', 'Weiye Zhao', 'Changliu Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tianhao_Wei1', 'mlq19@mails.tsinghua.edu.cn', '~Rui_Chen11', '~Weiye_Zhao1', '~Changliu_Liu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['embodied agent', 'model-based control', 'LLM', 'manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">a meta-control system can be built to automate the thought process that human experts use to customize control systems for heterogeneous robot skills.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The requirements for real-world manipulation tasks are diverse and often conflicting; some tasks require precise motion while others require force compliance; some tasks require avoidance of certain regions while others require convergence to certain states. Satisfying these varied requirements with a fixed state-action representation and control strategy is challenging, impeding the development of a universal robotic foundation model. In this work, we propose Meta-Control, the first LLM-enabled automatic control synthesis approach that creates customized state representations and control strategies tailored to specific tasks. Our core insight is that a meta-control system can be built to automate the thought process that human experts use to design control systems. Specifically, human experts heavily use a model-based, hierarchical (from abstract to concrete) thought model, then compose various dynamic models and controllers together to form a control system. Meta-Control mimics the thought model and harnesses LLM's extensive control knowledge with Socrates' "art of midwifery" to automate the thought process. Meta-Control stands out for its fully model-based nature, allowing rigorous analysis, generalizability, robustness, efficient parameter tuning, and reliable real-time execution.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5f0bf078fb067022909b8cac559c6ac689f4ea61.pdf" target="_blank">/pdf/5f0bf078fb067022909b8cac559c6ac689f4ea61.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wei2024metacontrol,
title={Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills},
author={Tianhao Wei and Liqian Ma and Rui Chen and Weiye Zhao and Changliu Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cvVEkS5yij}
}</div>
<div class="field-name">website:</div>
<div class="field-value">meta-control-paper.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a97df4a12232bb5b68f510b2be390c7af22e95a1.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cvVEkS5yij&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wei|metacontrol_automatic_modelbased_control_synthesis_for_heterogeneous_robot_skills</div>
</div>
<div class='paper-counter'>88/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cvUXoou8iz</div>
<div class="field-name">title:</div>
<div class="field-value">SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zihan Zhou', 'Animesh Garg', 'Dieter Fox', 'Caelan Reed Garrett', 'Ajay Mandlekar']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zihan_Zhou1', '~Animesh_Garg1', '~Dieter_Fox1', '~Caelan_Reed_Garrett1', '~Ajay_Mandlekar1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Manipulation Planning', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Adding RL to Planner guided subtasks in long-horizon BC improves robustness and performance multifold.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot learning has proven to be a general and effective technique for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations. Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch. And for both techniques, the learning difficulty increases proportional to the length of the manipulation task. Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/32aa1d389a73c76a6385f6a5d54b170f3f950a6d.pdf" target="_blank">/pdf/32aa1d389a73c76a6385f6a5d54b170f3f950a6d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/aa4f183e3aef8b55057e81a5c3296dc7adef91c3.zip" target="_blank">/attachment/aa4f183e3aef8b55057e81a5c3296dc7adef91c3.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhou2024spire,
title={{SPIRE}: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation},
author={Zihan Zhou and Animesh Garg and Dieter Fox and Caelan Reed Garrett and Ajay Mandlekar},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cvUXoou8iz}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/spire-corl-2024</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5e71f78eaceb12d051089c14619c4b69fbcef244.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cvUXoou8iz&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhou|spire_synergistic_planning_imitation_and_reinforcement_learning_for_longhorizon_manipulation</div>
</div>
<div class='paper-counter'>89/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cvAIaS6V2I</div>
<div class="field-name">title:</div>
<div class="field-value">OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Aadhithya Iyer', 'Zhuoran Peng', 'Yinlong Dai', 'Irmak Guzey', 'Siddhant Haldar', 'Soumith Chintala', 'Lerrel Pinto']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Aadhithya_Iyer1', '~Zhuoran_Peng1', '~Yinlong_Dai1', '~Irmak_Guzey1', '~Siddhant_Haldar1', '~Soumith_Chintala1', '~Lerrel_Pinto1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Teleoperation', 'Robot Learning', 'Robotic Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Versatile Teleoperation System for Robotic Manipulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines. The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics. However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control. built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands, bimanual arms, and mobile manipulators, through an easy-to-use app. Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views. We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework. Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, Allegro, and Hello Stretch platforms, OPEN TEACH is fully open-sourced to promote broader adoption. Videos are available at https://anon-open-teach.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c796f0cbd9940b109ec45ca4a7df134204817d5a.pdf" target="_blank">/pdf/c796f0cbd9940b109ec45ca4a7df134204817d5a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/4ff5fb81021e3b49e1212edf692b8fa63c5a02b6.zip" target="_blank">/attachment/4ff5fb81021e3b49e1212edf692b8fa63c5a02b6.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
iyer2024open,
title={{OPEN} {TEACH}: A Versatile Teleoperation System for Robotic Manipulation},
author={Aadhithya Iyer and Zhuoran Peng and Yinlong Dai and Irmak Guzey and Siddhant Haldar and Soumith Chintala and Lerrel Pinto},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cvAIaS6V2I}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://open-teach.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/baa451f2799beb10a35262787f2349da3a9772ea.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cvAIaS6V2I&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">iyer|open_teach_a_versatile_teleoperation_system_for_robotic_manipulation</div>
</div>
<div class='paper-counter'>90/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ctzBccpolr</div>
<div class="field-name">title:</div>
<div class="field-value">RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Lawrence Yunliang Chen', 'Chenfeng Xu', 'Karthik Dharmarajan', 'Richard Cheng', 'Kurt Keutzer', 'Masayoshi Tomizuka', 'Quan Vuong', 'Ken Goldberg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Lawrence_Yunliang_Chen1', '~Chenfeng_Xu1', '~Karthik_Dharmarajan1', '~Richard_Cheng1', '~Kurt_Keutzer1', '~Masayoshi_Tomizuka2', '~Quan_Vuong2', '~Ken_Goldberg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Cross-Embodiment Learning', 'Viewpoint Robust', 'Data Augmentation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">data augmentation that uses diffusion models to generate novel robots and viewpoints</div>
<div class="field-name">abstract:</div>
<div class="field-value">Scaling up robot learning requires large and diverse datasets, and how to efficiently reuse collected data and transfer policies to new embodiments remains an open question. Emerging research such as the Open-X Embodiment (OXE) project has shown promise in leveraging skills by combining datasets including different robots. However, imbalances in the distribution of robot types and camera angles in many datasets make policies prone to overfit. To mitigate this issue, we propose RoVi-Aug, which leverages state-of-the-art image-to-image generative models to augment robot data by synthesizing demonstrations with different robots and camera views. Through extensive physical experiments, we show that, by training on robot- and viewpoint-augmented data, RoVi-Aug can zero-shot deploy on an unseen robot with significantly different camera angles. Compared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires no extra processing at test time, does not assume known camera angles, and allows policy fine-tuning. Moreover, by co-training on both the original and augmented robot datasets, RoVi-Aug can learn multi-robot and multi-task policies, enabling more efficient transfer between robots and skills and improving success rates by up to 30%.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/7bbbbe13b778f9a8cc189a143673067fed188c78.pdf" target="_blank">/pdf/7bbbbe13b778f9a8cc189a143673067fed188c78.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/1e73036120daaaf7b91b335669d551bca4c1498b.zip" target="_blank">/attachment/1e73036120daaaf7b91b335669d551bca4c1498b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024roviaug,
title={RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning},
author={Lawrence Yunliang Chen and Chenfeng Xu and Karthik Dharmarajan and Richard Cheng and Kurt Keutzer and Masayoshi Tomizuka and Quan Vuong and Ken Goldberg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ctzBccpolr}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://rovi-aug.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/44f978289fc5cf3f48290893468995530c0e08e9.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ctzBccpolr&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|roviaug_robot_and_viewpoint_augmentation_for_crossembodiment_robot_learning</div>
</div>
<div class='paper-counter'>91/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cq2uB30uBM</div>
<div class="field-name">title:</div>
<div class="field-value">Pre-emptive Action Revision by Environmental Feedback for Embodied Instruction Following Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jinyeon Kim', 'Cheolhong Min', 'Byeonghwi Kim', 'Jonghyun Choi']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jinyeon_Kim1', '~Cheolhong_Min1', '~Byeonghwi_Kim1', '~Jonghyun_Choi1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Replanning', 'Environmental Feedback', 'Brain plasticity', 'Embodied AI']</div>
<div class="field-name">abstract:</div>
<div class="field-value">When we, humans, perform a task, we consider changes in environments such as objects' arrangement due to interactions with objects and other reasons; e.g., when we find a mug to clean, if it is already clean, we skip cleaning it. But even the state-of-the-art embodied agents often ignore changed environments when performing a task, leading to failure to complete the task, executing unnecessary actions, or fixing the mistake after it was made. Here, we propose Pre-emptive Action Revision by Environmental feeDback (PRED) that allows an embodied agent to revise their action in response to the perceived environmental status before it makes mistakes. We empirically validate PRED and observe that it outperforms the prior art on two challenging benchmarks in the virtual environment, TEACh and ALFRED, by noticeable margins in most metrics, including unseen success rates, with shorter execution time, implying an efficiently behaved agent. Furthermore, we demonstrate the effectiveness of the proposed method with real robot experiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5b883ce9e55b1f414936aa8f0a6b5d1ef64a2a1c.pdf" target="_blank">/pdf/5b883ce9e55b1f414936aa8f0a6b5d1ef64a2a1c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/523872a0a61b6d0804815a396f157f02f023b2c8.zip" target="_blank">/attachment/523872a0a61b6d0804815a396f157f02f023b2c8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kim2024preemptive,
title={Pre-emptive Action Revision by Environmental Feedback for Embodied Instruction Following Agents},
author={Jinyeon Kim and Cheolhong Min and Byeonghwi Kim and Jonghyun Choi},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cq2uB30uBM}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://pred-agent.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/98f4d8869f9713a5d3f2d2ff208137b96eb3d07b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cq2uB30uBM&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kim|preemptive_action_revision_by_environmental_feedback_for_embodied_instruction_following_agents</div>
</div>
<div class='paper-counter'>92/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cocHfT7CEs</div>
<div class="field-name">title:</div>
<div class="field-value">Generative Image as Action Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mohit Shridhar', 'Yat Long Lo', 'Stephen James']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mohit_Shridhar1', '~Yat_Long_Lo1', '~Stephen_James1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Diffusion Models', 'Image Generation', 'Behavior Cloning', 'Visuomotor']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Stable Diffusion can be fine-tuned to draw joint-actions for visuomotor control.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Image-generation diffusion models have been fine-tuned to unlock new capabilities such as image-editing and novel view synthesis. Can we similarly unlock image-generation models for visuomotor control? We present GENIMA, a behavior-cloning agent that fine-tunes Stable Diffusion to “draw joint-actions” as targets on RGB images. These images are fed into a controller that maps the visual targets into a sequence of joint-positions. We study GENIMA on 25 RLBench and 9 real-world manipulation tasks. We find that, by lifting actions into image-space, internet pre-trained diffusion models can generate policies that outperform state- of-the-art visuomotor approaches, especially in robustness to scene perturbations and generalizing to novel objects. Our method is also competitive with 3D agents, despite lacking priors such as depth, keypoints, or motion-planners.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e24718f3633b28f3854ba6231e7228120e9b28df.pdf" target="_blank">/pdf/e24718f3633b28f3854ba6231e7228120e9b28df.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a60f26cb7651092ca7855076290bfec905a65d85.zip" target="_blank">/attachment/a60f26cb7651092ca7855076290bfec905a65d85.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
shridhar2024generative,
title={Generative Image as Action Models},
author={Mohit Shridhar and Yat Long Lo and Stephen James},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cocHfT7CEs}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://genima-robot.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1c4ff155f1bcdb1442ffe7793cb974ff86bc151a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cocHfT7CEs&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">shridhar|generative_image_as_action_models</div>
</div>
<div class='paper-counter'>93/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">clqzoCrulY</div>
<div class="field-name">title:</div>
<div class="field-value">OrbitGrasp: SE(3)-Equivariant Grasp Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Boce Hu', 'Xupeng Zhu', 'Dian Wang', 'Zihao Dong', 'Haojie Huang', 'Chenghao Wang', 'Robin Walters', 'Robert Platt']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Boce_Hu1', '~Xupeng_Zhu1', '~Dian_Wang1', '~Zihao_Dong2', '~Haojie_Huang1', '~Chenghao_Wang1', '~Robin_Walters1', '~Robert_Platt1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Grasp Detection', 'Equivariance', 'Symmetry', 'Grasp Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an SE(3)-equivariant grasp learning algorithm based on spherical harmonics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">While grasp detection is an important part of any robotic manipulation pipeline, reliable and accurate grasp detection in $\\mathrm{SE}(3)$ remains a research challenge. Many robotics applications in unstructured environments such as the home or warehouse would benefit a lot from better grasp performance. This paper proposes a novel framework for detecting $\mathrm{SE}(3)$ grasp poses based on point cloud input. Our main contribution is to propose an $\mathrm{SE}(3)$-equivariant model that maps each point in the cloud to a continuous grasp quality function over the 2-sphere $S^2$ using a spherical harmonic basis. Compared with reasoning about a finite set of samples, this formulation improves the accuracy and efficiency of our model when a large number of samples would otherwise be needed. In order to accomplish this, we propose a novel variation on EquiFormerV2 that leverages a UNet-style backbone to enlarge the number of points the model can handle. Our resulting method, which we name OrbitGrasp, significantly outperforms baselines in both simulation and physical experiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/be1f3a6b7998cacdb6864a9158a5252d6e463776.pdf" target="_blank">/pdf/be1f3a6b7998cacdb6864a9158a5252d6e463776.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/65890f068643ce22c87937ffedc93be018511d5b.zip" target="_blank">/attachment/65890f068643ce22c87937ffedc93be018511d5b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hu2024orbitgrasp,
title={OrbitGrasp: {SE}(3)-Equivariant Grasp Learning},
author={Boce Hu and Xupeng Zhu and Dian Wang and Zihao Dong and Haojie Huang and Chenghao Wang and Robin Walters and Robert Platt},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=clqzoCrulY}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://orbitgrasp.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6d3379c38d2523ebcd28b936a20bcd69addc1048.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=clqzoCrulY&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hu|orbitgrasp_se3equivariant_grasp_learning</div>
</div>
<div class='paper-counter'>94/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cT2N3p1AcE</div>
<div class="field-name">title:</div>
<div class="field-value">Visual Whole-Body Control for Legged Loco-Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Minghuan Liu', 'Zixuan Chen', 'Xuxin Cheng', 'Yandong Ji', 'Ri-Zhao Qiu', 'Ruihan Yang', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Minghuan_Liu1', '~Zixuan_Chen9', '~Xuxin_Cheng2', '~Yandong_Ji1', '~Ri-Zhao_Qiu1', '~Ruihan_Yang2', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Learning; Reinforcement Learning; Imitation Learning; Mobile Loco-Manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely Visual Whole-Body Control (VBC), is composed of a low-level policy using all degrees of freedom to track the body velocities along with the end-effector position, and a high-level policy proposing the velocities and end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights, locations, orientations) and environments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a60b553b3ea69ee9d85eef95adda177be0d8b5be.pdf" target="_blank">/pdf/a60b553b3ea69ee9d85eef95adda177be0d8b5be.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3f1a773bc8fb3ba3e1274599403c95f1a17b976e.zip" target="_blank">/attachment/3f1a773bc8fb3ba3e1274599403c95f1a17b976e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024visual,
title={Visual Whole-Body Control for Legged Loco-Manipulation},
author={Minghuan Liu and Zixuan Chen and Xuxin Cheng and Yandong Ji and Ri-Zhao Qiu and Ruihan Yang and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cT2N3p1AcE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://wholebody-b1.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1460a067f51e5fd7d55d1a0461518fc110b0f6a4.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cT2N3p1AcE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|visual_wholebody_control_for_legged_locomanipulation</div>
</div>
<div class='paper-counter'>95/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cNI0ZkK1yC</div>
<div class="field-name">title:</div>
<div class="field-value">Flow as the Cross-domain Manipulation Interface</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mengda Xu', 'Zhenjia Xu', 'Yinghao Xu', 'Cheng Chi', 'Gordon Wetzstein', 'Manuela Veloso', 'Shuran Song']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mengda_Xu1', '~Zhenjia_Xu1', '~Yinghao_Xu1', '~Cheng_Chi4', '~Gordon_Wetzstein3', '~Manuela_Veloso1', '~Shuran_Song3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robots', 'Learning', 'cross-domain', 'cross-embodiment']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Im2Flow2Act, a scalable learning framework that enables robots to acquire manipulation skills from diverse data sources.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present Im2Flow2Act, a scalable learning framework that enables robots to acquire real-world manipulation skills without the need of real-world robot training data. The key idea behind Im2Flow2Act is to use object flow as the manipulation interface, bridging domain gaps between different embodiments (i.e., human and robot) and training environments (i.e., real-world and simulated). Im2Flow2Act comprises two components: a flow generation network and a flow-conditioned policy. The flow generation network, trained on human demonstration videos, generates object flow from the initial scene image, conditioned on the task description. The flow-conditioned policy, trained on simulated robot play data, maps the generated object flow to robot actions to realize the desired object movements. By using flow as input, this policy can be directly deployed in the real world with a minimal sim-to-real gap. By leveraging real-world human videos and simulated robot play data, we bypass the challenges of teleoperating physical robots in the real world, resulting in a scalable system for diverse tasks. We demonstrate Im2Flow2Act's capabilities in a variety of real-world tasks, including the manipulation of rigid, articulated, and deformable objects.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/368c85e3328b77830ade29cc1e281924078e31fe.pdf" target="_blank">/pdf/368c85e3328b77830ade29cc1e281924078e31fe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/43ac432add5b5f104f63de2dd96472ef1bd2c5a1.zip" target="_blank">/attachment/43ac432add5b5f104f63de2dd96472ef1bd2c5a1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xu2024flow,
title={Flow as the Cross-domain Manipulation Interface},
author={Mengda Xu and Zhenjia Xu and Yinghao Xu and Cheng Chi and Gordon Wetzstein and Manuela Veloso and Shuran Song},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cNI0ZkK1yC}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://im-flow-act.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/fd3f9ce3da4b8724d105a83cdce099669dbb9a13.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cNI0ZkK1yC&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xu|flow_as_the_crossdomain_manipulation_interface</div>
</div>
<div class='paper-counter'>96/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cGswIOxHcN</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Visual Parkour from Generated Images</div>
<div class="field-name">authors:</div>
<div class="field-value">['Alan Yu', 'Ge Yang', 'Ran Choi', 'Yajvan Ravan', 'John Leonard', 'Phillip Isola']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Alan_Yu2', '~Ge_Yang1', '~Ran_Choi1', '~Yajvan_Ravan1', '~John_Leonard1', '~Phillip_Isola1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Generative AI', 'Simulation', 'Legged Locomotion', 'Sensory Motor-learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Train a State-of-the-art quadruped parkour policy on generated data</div>
<div class="field-name">abstract:</div>
<div class="field-value">Fast and accurate physics simulation is an essential component of robot learning, where robots can explore failure scenarios that are difficult to produce in the real world and learn from unlimited on-policy data. Yet, it remains challenging to incorporate RGB-color perception into the sim-to-real pipeline that matches the real world in its richness and realism. In this work, we train a robot dog in simulation for visual parkour. We propose a way to use generative models to synthesize diverse and physically accurate image sequences of the scene from the robot's ego-centric perspective. We present demonstrations of zero-shot transfer to the RGB-only observations of the real world on a robot equipped with a low-cost, off-the-shelf color camera.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/43a5f40b4d4559630fe97c8a7bf79d756005b44d.pdf" target="_blank">/pdf/43a5f40b4d4559630fe97c8a7bf79d756005b44d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/afec99dcc2e2b282b42906972b9cce6165911844.zip" target="_blank">/attachment/afec99dcc2e2b282b42906972b9cce6165911844.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yu2024learning,
title={Learning Visual Parkour from Generated Images},
author={Alan Yu and Ge Yang and Ran Choi and Yajvan Ravan and John Leonard and Phillip Isola},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cGswIOxHcN}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://lucidsim.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/58512e4479f5089a6875fb440467efb9baf99163.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cGswIOxHcN&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yu|learning_visual_parkour_from_generated_images</div>
</div>
<div class='paper-counter'>97/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">cDXnnOhNrF</div>
<div class="field-name">title:</div>
<div class="field-value">Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception</div>
<div class="field-name">authors:</div>
<div class="field-value">['Anushri Dixit', 'Zhiting Mei', 'Meghan Booker', 'Mariko Storey-Matsutani', 'Allen Z. Ren', 'Anirudha Majumdar']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Anushri_Dixit1', '~Zhiting_Mei1', '~Meghan_Booker1', 'ms8364@princeton.edu', '~Allen_Z._Ren1', '~Anirudha_Majumdar1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Uncertainty quantification', 'occupancy prediction', 'robot navigation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">An uncertainty quantification framework for perception-based navigation tasks that provides formal assurances for end-to-end safety.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Rapid advances in perception have enabled large pre-trained models to be used out of the box for transforming high-dimensional, noisy, and partial observations of the world into rich occupancy representations. However, the reliability of these models and consequently their safe integration onto robots remains unknown when deployed in environments unseen during training. In this work, we address this challenge by rigorously quantifying the uncertainty of pre-trained perception systems for object detection via a novel calibration technique based on conformal prediction. Crucially, this procedure guarantees robustness to distribution shifts in states when perceptual outputs are used in conjunction with a planner. As a result, the calibrated perception system can be used in combination with any safe planner to provide an end-to-end statistical assurance on safety in unseen environments. We evaluate the resulting approach, Perceive with Confidence (PwC), with experiments in simulation and on hardware where a quadruped robot navigates through previously unseen indoor, static environments. These experiments validate the safety assurances for obstacle avoidance provided by PwC and demonstrate up to 40% improvements in empirical safety compared to baselines.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/343e132ff0b10a58e1b4358b246287ad980fcdda.pdf" target="_blank">/pdf/343e132ff0b10a58e1b4358b246287ad980fcdda.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f81f91f0bc68c715733e1648a95d64c5b2d175c7.zip" target="_blank">/attachment/f81f91f0bc68c715733e1648a95d64c5b2d175c7.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
dixit2024perceive,
title={Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception},
author={Anushri Dixit and Zhiting Mei and Meghan Booker and Mariko Storey-Matsutani and Allen Z. Ren and Anirudha Majumdar},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=cDXnnOhNrF}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://perceive-with-confidence.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a41ca34182509a686ea36df6af6c1cd52f057086.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=cDXnnOhNrF&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">dixit|perceive_with_confidence_statistical_safety_assurances_for_navigation_with_learningbased_perception</div>
</div>
<div class='paper-counter'>98/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">bt0PX0e4rE</div>
<div class="field-name">title:</div>
<div class="field-value">Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jiaxu Xing', 'Angel Romero', 'Leonard Bauersfeld', 'Davide Scaramuzza']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jiaxu_Xing1', '~Angel_Romero1', '~Leonard_Bauersfeld1', '~Davide_Scaramuzza1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Quadrotor', 'Visuomotor Control', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate a visuomotor policy capable of navigating through a sequence of gates using only gate corners or RGB images combining the strengths of RL and IL..</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning visuomotor policies for agile quadrotor flight presents significant difficulties, primarily from inefficient policy exploration caused by high-dimensional visual inputs and the need for precise and low-latency control.
To address these challenges, we propose a novel approach that combines the performance of Reinforcement Learning (RL) and the sample efficiency of Imitation Learning (IL) in the task of vision-based autonomous drone racing.
While RL provides a framework for learning high-performance controllers through trial and error, it faces challenges with sample efficiency and computational demands due to the high dimensionality of visual inputs.
Conversely, IL efficiently learns from visual expert demonstrations, but it remains limited by the expert's performance and state distribution.
To overcome these limitations, our policy learning framework integrates the strengths of both approaches.
Our framework contains three phases: training a teacher policy using RL with privileged state information, distilling it into a student policy via IL, and adaptive fine-tuning via RL.
Testing in both simulated and real-world scenarios shows our approach can not only learn in scenarios where RL from scratch fails but also outperforms existing IL methods in both robustness and performance, successfully navigating a quadrotor through a race course using only visual information.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/77a0b4c8cf66af14e246bdf897c7e017b1f1585b.pdf" target="_blank">/pdf/77a0b4c8cf66af14e246bdf897c7e017b1f1585b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/bc8a218b8942509581912fd9e86da361f6b1b7f6.zip" target="_blank">/attachment/bc8a218b8942509581912fd9e86da361f6b1b7f6.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xing2024bootstrapping,
title={Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight},
author={Jiaxu Xing and Angel Romero and Leonard Bauersfeld and Davide Scaramuzza},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=bt0PX0e4rE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://bootstrap-rl-with-il.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/921cb39e00921347c453a6b7fd15ca38e57ab1ac.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=bt0PX0e4rE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xing|bootstrapping_reinforcement_learning_with_imitation_for_visionbased_agile_flight</div>
</div>
<div class='paper-counter'>99/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">bk28WlkqZn</div>
<div class="field-name">title:</div>
<div class="field-value">3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing</div>
<div class="field-name">authors:</div>
<div class="field-value">['Binghao Huang', 'Yixuan Wang', 'Xinyi Yang', 'Yiyue Luo', 'Yunzhu Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Binghao_Huang1', '~Yixuan_Wang2', '~Xinyi_Yang6', '~Yiyue_Luo1', '~Yunzhu_Li1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Contact-Rich Manipulation', 'Multi-Modal Perception', 'Tactile Sensing', 'Imitation Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Tactile and visual perception are both crucial for humans to perform fine-grained interactions with their environment. Developing similar multi-modal sensing capabilities for robots can significantly enhance and expand their manipulation skills. This paper introduces **3D-ViTac**, a multi-modal sensing and learning system designed for dexterous bimanual manipulation. Our system features tactile sensors equipped with dense sensing units, each covering an area of 3$mm^2$. These sensors are low-cost and flexible, providing detailed and extensive coverage of physical contacts, effectively complementing visual information. To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. Through concrete hardware experiments, we demonstrate that even low-cost robots can perform precise manipulations and significantly outperform vision-only policies, particularly in safe interactions with fragile items and executing long-horizon tasks involving in-hand manipulation. Our project page is available at https://binghao-huang.github.io/3D-ViTac/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/132f61b0a1ec09cab551c71a9ce6d7263fbdda09.pdf" target="_blank">/pdf/132f61b0a1ec09cab551c71a9ce6d7263fbdda09.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/202df8734ce55e32687b9a98852e10bd8190a209.zip" target="_blank">/attachment/202df8734ce55e32687b9a98852e10bd8190a209.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024dvitac,
title={3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing},
author={Binghao Huang and Yixuan Wang and Xinyi Yang and Yiyue Luo and Yunzhu Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=bk28WlkqZn}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://binghao-huang.github.io/3D-ViTac/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/22018508223d9eaeadde70ed73aa79f5f6f14e2f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=bk28WlkqZn&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|3dvitac_learning_finegrained_manipulation_with_visuotactile_sensing</div>
</div>
<div class='paper-counter'>100/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">bftFwjSJxk</div>
<div class="field-name">title:</div>
<div class="field-value">Rate-Informed Discovery via Bayesian Adaptive Multifidelity Sampling</div>
<div class="field-name">authors:</div>
<div class="field-value">['Aman Sinha', 'Payam Nikdel', 'Supratik Paul', 'Shimon Whiteson']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Aman_Sinha1', '~Payam_Nikdel1', '~Supratik_Paul1', '~Shimon_Whiteson1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Autonomous Driving', 'Rare-event Simulation', 'Adaptive Sampling']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a method that efficiently estimates autonomous vehicle safety and performs targeted discovery of high-impact failure cases.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Ensuring the safety of autonomous vehicles (AVs) requires both accurate estimation of their performance and efficient discovery of potential failure cases. This paper introduces Bayesian adaptive multifidelity sampling (BAMS), which leverages the power of adaptive Bayesian sampling to achieve efficient discovery while simultaneously estimating the rate of adverse events. BAMS prioritizes exploration of regions with potentially low performance, leading to the identification of novel and critical scenarios that traditional methods might miss. Using real-world AV data we demonstrate that BAMS discovers 10 times as many issues as Monte Carlo (MC) and importance sampling (IS) baselines, while at the same time generating rate estimates with variances 15 and 6 times narrower than MC and IS baselines respectively.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d23f7842ad619c1acaf6bab4ac79923b907e5d26.pdf" target="_blank">/pdf/d23f7842ad619c1acaf6bab4ac79923b907e5d26.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cc112e67be112c6cbd5028360608710fd491f4fb.zip" target="_blank">/attachment/cc112e67be112c6cbd5028360608710fd491f4fb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sinha2024rateinformed,
title={Rate-Informed Discovery via Bayesian Adaptive Multifidelity Sampling},
author={Aman Sinha and Payam Nikdel and Supratik Paul and Shimon Whiteson},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=bftFwjSJxk}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/dda3e9072ddab12034a744a38e559b0dec53f6df.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=bftFwjSJxk&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sinha|rateinformed_discovery_via_bayesian_adaptive_multifidelity_sampling</div>
</div>
<div class='paper-counter'>101/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">adf3pO9baG</div>
<div class="field-name">title:</div>
<div class="field-value">Dreaming to Assist: Learning to Align with Human Objectives for Shared Control in High-Speed Racing</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jonathan DeCastro', 'Andrew Silva', 'Deepak Gopinath', 'Emily Sumner', 'Thomas Matrai Balch', 'Laporsha Dees', 'Guy Rosman']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jonathan_DeCastro1', '~Andrew_Silva1', 'deepak.gopinath@tri.global', 'emily.sumner@tri.global', '~Thomas_Matrai_Balch1', 'laporsha.dees.ctr@tri.global', '~Guy_Rosman2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Recurrent State-Space Models', 'Human-Robot Interactions', 'Shared-Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a recurrent state space model capable of reasoning over human intents, enabling an assistive agent to select actions that align with the human and enabling a fluid teaming interaction, and demonstrate it in a high-speed racing domain.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Tight coordination is required for effective human-robot teams in domains involving fast dynamics and tactical decisions, such as multi-car racing.  In such settings, robot teammates must react to cues of a human teammate's tactical objective to assist in a way that is consistent with the objective (e.g., navigating left or right around an obstacle). To address this challenge, we present _Dream2Assist_,  a framework that combines a rich world model able to infer human objectives and value functions, and an assistive agent that provides appropriate expert assistance to a given human teammate. Our approach builds on a recurrent state space model to explicitly infer human intents, enabling the assistive agent to select actions that align with the human and enabling a fluid teaming interaction. We demonstrate our approach in a high-speed racing domain with a population of synthetic human drivers pursuing mutually exclusive objectives, such as "stay-behind" and "overtake". We show that the combined human-robot team, when blending its actions with those of the human, outperforms synthetic humans alone and several baseline assistance strategies, and that intent-conditioning enables adherence to human preferences during task execution, leading to improved performance while satisfying the human's objective.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8a7ba35067077a2ed7e62a4ba908074792a942aa.pdf" target="_blank">/pdf/8a7ba35067077a2ed7e62a4ba908074792a942aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cbc872db82e387580cb7f22c6dc82f452fb88d5c.zip" target="_blank">/attachment/cbc872db82e387580cb7f22c6dc82f452fb88d5c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
decastro2024dreaming,
title={Dreaming to Assist: Learning to Align with Human Objectives for Shared Control in High-Speed Racing},
author={Jonathan DeCastro and Andrew Silva and Deepak Gopinath and Emily Sumner and Thomas Matrai Balch and Laporsha Dees and Guy Rosman},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=adf3pO9baG}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://dream2assist.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5acdf703ecf7f25711afd139197c7f9e817b6da7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=adf3pO9baG&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">decastro|dreaming_to_assist_learning_to_align_with_human_objectives_for_shared_control_in_highspeed_racing</div>
</div>
<div class='paper-counter'>102/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">aaY5fVFMVf</div>
<div class="field-name">title:</div>
<div class="field-value">Conformal Prediction for Semantically-Aware Autonomous Perception in Urban Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">['Achref Doula', 'Tobias Güdelhöfer', 'Max Mühlhäuser', 'Alejandro Sanchez Guinea']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Achref_Doula1', '~Tobias_Güdelhöfer1', '~Max_Mühlhäuser1', '~Alejandro_Sanchez_Guinea1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Uncertainty in Robotics', 'Robot Perception', 'Semantics for Robotics']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Knowledge-Refined Prediction Sets (KRPS), a novel approach that performs semantically-aware uncertainty quantification for multitask-based autonomous perception in urban environments. KRPS extends conformal prediction (CP) to ensure 2 properties not typically addressed by CP frameworks: semantic label consistency and true label coverage, across multiple perception tasks. We elucidate the capability of KRPS through high-level classification tasks crucial for semantically-aware autonomous perception in urban environments, including agent classification, agent location classification, and agent action classification. In a theoretical analysis, we introduce the concept of semantic label consistency among tasks and prove the semantic consistency and marginal coverage properties of the produced sets by KRPS. The results of our evaluation on the ROAD dataset and the Waymo/ROAD++ dataset show that KRPS outperforms state-of-the-art CP methods in reducing uncertainty by up to 80\% and increasing the semantic consistency by up to 30\%, while maintaining the coverage guarantees.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4f2e203c23eb140de85df389f4cc8d56b24f09a0.pdf" target="_blank">/pdf/4f2e203c23eb140de85df389f4cc8d56b24f09a0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0691d8ad64adc804ba0004338636f671a9c6695d.zip" target="_blank">/attachment/0691d8ad64adc804ba0004338636f671a9c6695d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
doula2024conformal,
title={Conformal Prediction for Semantically-Aware Autonomous Perception in Urban Environments},
author={Achref Doula and Tobias G{\"u}delh{\"o}fer and Max M{\"u}hlh{\"a}user and Alejandro Sanchez Guinea},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=aaY5fVFMVf}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c36028ed5ec0490b314b4ef1d6311b0f44388fb4.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=aaY5fVFMVf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">doula|conformal_prediction_for_semanticallyaware_autonomous_perception_in_urban_environments</div>
</div>
<div class='paper-counter'>103/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ZdgaF8fOc0</div>
<div class="field-name">title:</div>
<div class="field-value">Bridging the gap between Learning-to-plan, Motion Primitives and Safe Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Piotr Kicki', 'Davide Tateo', 'Puze Liu', 'Jonas Günster', 'Jan Peters', 'Krzysztof Walas']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Piotr_Kicki1', '~Davide_Tateo2', '~Puze_Liu1', '~Jonas_Günster1', '~Jan_Peters3', '~Krzysztof_Walas2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['safe reinforcement learning', 'motion planning', 'motion primitives']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a new SafeRL approach that draws an inspiration from learning-to-plan method combined with motion primitives.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Trajectory planning under kinodynamic constraints is fundamental for advanced robotics applications that require dexterous, reactive, and rapid skills in complex environments. These constraints, which may represent task, safety, or actuator limitations, are essential for ensuring the proper functioning of robotic platforms and preventing unexpected behaviors. Recent advances in kinodynamic planning demonstrate that learning-to-plan techniques can generate complex and reactive motions under intricate constraints. However, these techniques necessitate the analytical modeling of both the robot and the entire task, a limiting assumption when systems are extremely complex or when constructing accurate task models is prohibitive.
This paper addresses this limitation by combining learning-to-plan methods with reinforcement learning, resulting in a novel integration of black-box learning of motion primitives and optimization. We evaluate our approach against state-of-the-art safe reinforcement learning methods, showing that our technique, particularly when exploiting task structure, outperforms baseline methods in challenging scenarios such as planning to hit in robot air hockey. This work demonstrates the potential of our integrated approach to enhance the performance and safety of robots operating under complex kinodynamic constraints.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f5f91cad827b6e10858985c1b07c0323bcf57614.pdf" target="_blank">/pdf/f5f91cad827b6e10858985c1b07c0323bcf57614.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3cb8db246c064dc081b012545bc89af765ce5854.zip" target="_blank">/attachment/3cb8db246c064dc081b012545bc89af765ce5854.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kicki2024bridging,
title={Bridging the gap between Learning-to-plan, Motion Primitives and Safe Reinforcement Learning},
author={Piotr Kicki and Davide Tateo and Puze Liu and Jonas G{\"u}nster and Jan Peters and Krzysztof Walas},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ZdgaF8fOc0}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://pkicki.github.io/CNP3O/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/9d19b2e33c5a6dd626d66a5bd6ce0a1cfa1ffbc7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ZdgaF8fOc0&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kicki|bridging_the_gap_between_learningtoplan_motion_primitives_and_safe_reinforcement_learning</div>
</div>
<div class='paper-counter'>104/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">ZMnD6QZAE6</div>
<div class="field-name">title:</div>
<div class="field-value">OpenVLA: An Open-Source Vision-Language-Action Model</div>
<div class="field-name">authors:</div>
<div class="field-value">['Moo Jin Kim', 'Karl Pertsch', 'Siddharth Karamcheti', 'Ted Xiao', 'Ashwin Balakrishna', 'Suraj Nair', 'Rafael Rafailov', 'Ethan P Foster', 'Pannag R Sanketi', 'Quan Vuong', 'Thomas Kollar', 'Benjamin Burchfiel', 'Russ Tedrake', 'Dorsa Sadigh', 'Sergey Levine', 'Percy Liang', 'Chelsea Finn']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Moo_Jin_Kim1', '~Karl_Pertsch1', '~Siddharth_Karamcheti1', '~Ted_Xiao1', '~Ashwin_Balakrishna1', '~Suraj_Nair1', '~Rafael_Rafailov1', '~Ethan_P_Foster1', '~Pannag_R_Sanketi1', '~Quan_Vuong2', '~Thomas_Kollar1', '~Benjamin_Burchfiel1', '~Russ_Tedrake1', '~Dorsa_Sadigh1', '~Sergey_Levine1', '~Percy_Liang1', '~Chelsea_Finn1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Vision-Language-Action Models', 'Generalist Policies', 'Large-scale Robot Learning', 'Robotic Manipulation', 'Robotics', 'Vision-Language Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce OpenVLA, a state-of-the-art, open-source 7B-parameter VLA model that obtains strong performance for cross-embodiment robot control out-of-the-box and can be easily adapted to new robot setups via parameter-efficient fine-tuning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, where we outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/abf32802eb323064805ffb39688f56254876b4be.pdf" target="_blank">/pdf/abf32802eb323064805ffb39688f56254876b4be.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/1318377860aa083955bff5f25bdc309a2124d6b1.zip" target="_blank">/attachment/1318377860aa083955bff5f25bdc309a2124d6b1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kim2024openvla,
title={Open{VLA}: An Open-Source Vision-Language-Action Model},
author={Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan P Foster and Pannag R Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=ZMnD6QZAE6}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://openvla.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f960a7e41adfc235ae2586471632c5cf8606d7f7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=ZMnD6QZAE6&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kim|openvla_an_opensource_visionlanguageaction_model</div>
</div>
<div class='paper-counter'>105/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Yw5QGNBkEN</div>
<div class="field-name">title:</div>
<div class="field-value">Scaling Manipulation Learning with Visual Kinematic Chain Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xinyu Zhang', 'Yuhan Liu', 'Haonan Chang', 'Abdeslam Boularias']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xinyu_Zhang7', '~Yuhan_Liu2', '~Haonan_Chang1', '~Abdeslam_Boularias1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-Task Robot Learning', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose visual kinematics transformer, a convolution-free architecture trained with forecasting kinematic structures through optimal point set matching, and demonstrate its superior performance as a general agent in diverse environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning general-purpose models from diverse datasets has achieved great success in machine learning. In robotics, however, existing methods in multi-task learning are typically constrained to a single robot and workspace, while recent work such as RT-X requires a non-trivial action normalization procedure to manually bridge the gap between different action spaces in diverse environments. In this paper, we propose the visual kinematics chain as a precise and universal representation of quasi-static actions for robot learning over diverse environments, which requires no manual adjustment since the visual kinematic chains can be automatically obtained from the robot’s model and camera parameters. We propose the Visual Kinematics Transformer (VKT),  a convolution-free architecture that supports an arbitrary number of camera viewpoints, and that is trained with a single objective of forecasting kinematic structures through optimal point-set matching. We demonstrate the superior performance of VKT over BC transformers as a general agent on Calvin, RLBench, ALOHA, Open-X, and real robot manipulation tasks. Video demonstrations and source code can be found at https://mlzxy.github.io/visual-kinetic-chain.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/03d49d1e0fd0333d071fe1ee2c8f4258f4bf546b.pdf" target="_blank">/pdf/03d49d1e0fd0333d071fe1ee2c8f4258f4bf546b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/57f505106ae306faf96a28e95a3ac0bdf8bffc2a.zip" target="_blank">/attachment/57f505106ae306faf96a28e95a3ac0bdf8bffc2a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024scaling,
title={Scaling Manipulation Learning with Visual Kinematic Chain Prediction},
author={Xinyu Zhang and Yuhan Liu and Haonan Chang and Abdeslam Boularias},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Yw5QGNBkEN}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://mlzxy.github.io/visual-kinetic-chain/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/974989de1ec84f78881cbf9365350dda6f7cc961.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Yw5QGNBkEN&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|scaling_manipulation_learning_with_visual_kinematic_chain_prediction</div>
</div>
<div class='paper-counter'>106/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Yce2jeILGt</div>
<div class="field-name">title:</div>
<div class="field-value">Open-TeleVision: Teleoperation with Immersive Active Visual Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xuxin Cheng', 'Jialong Li', 'Shiqi Yang', 'Ge Yang', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xuxin_Cheng2', '~Jialong_Li3', '~Shiqi_Yang2', '~Ge_Yang1', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Teleoperation', 'VR/AR', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">An immersive teleoperation system with active visual feedback</div>
<div class="field-name">abstract:</div>
<div class="field-value">Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system $\textbf{Open-TeleVision}$ that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (can sorting, can insertion, folding, and unloading) for 2 different humanoid robots and deploy them in the real world. The entire system will be open-sourced.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2e5a06a7523966f81d5b65b188fb77d4ba66b39c.pdf" target="_blank">/pdf/2e5a06a7523966f81d5b65b188fb77d4ba66b39c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/63b2250c42c34db55237cf049dc8c5e054140db1.zip" target="_blank">/attachment/63b2250c42c34db55237cf049dc8c5e054140db1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cheng2024opentelevision,
title={Open-TeleVision: Teleoperation with Immersive Active Visual Feedback},
author={Xuxin Cheng and Jialong Li and Shiqi Yang and Ge Yang and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Yce2jeILGt}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robot-tv.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/9f213d37555f2839216ad2af1e78316e29f1d223.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Yce2jeILGt&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cheng|opentelevision_teleoperation_with_immersive_active_visual_feedback</div>
</div>
<div class='paper-counter'>107/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">YOFrRTDC6d</div>
<div class="field-name">title:</div>
<div class="field-value">SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment</div>
<div class="field-name">authors:</div>
<div class="field-value">['Caelan Reed Garrett', 'Ajay Mandlekar', 'Bowen Wen', 'Dieter Fox']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Caelan_Reed_Garrett1', '~Ajay_Mandlekar1', '~Bowen_Wen1', '~Dieter_Fox1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Manipulation', 'Planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a skill-based approach to automated data generation for imitation learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks. To address this issue, we propose SkillGen, an automated system for generating demonstration datasets from a few human demos. SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion. We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time. We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability produce data for large scene variations, including clutter, and agents that are on average 24% more successful. We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task. Videos, and more at https://skillgen.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a3d8665a779be6563a062685046e77b0c49d23c7.pdf" target="_blank">/pdf/a3d8665a779be6563a062685046e77b0c49d23c7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c42e45e9f6e1a333eb88563c91536165d152d80e.zip" target="_blank">/attachment/c42e45e9f6e1a333eb88563c91536165d152d80e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
garrett2024skillmimicgen,
title={SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment},
author={Caelan Reed Garrett and Ajay Mandlekar and Bowen Wen and Dieter Fox},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=YOFrRTDC6d}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://skillgen.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3a77b4009dec57e1050f7870cb81c9ac66513208.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=YOFrRTDC6d&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">garrett|skillmimicgen_automated_demonstration_generation_for_efficient_skill_learning_and_deployment</div>
</div>
<div class='paper-counter'>108/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">XrxLGzF0lJ</div>
<div class="field-name">title:</div>
<div class="field-value">So You Think You Can Scale Up Autonomous Robot Data Collection?</div>
<div class="field-name">authors:</div>
<div class="field-value">['Suvir Mirchandani', 'Suneel Belkhale', 'Joey Hejna', 'Evelyn Choi', 'Md Sazzad Islam', 'Dorsa Sadigh']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Suvir_Mirchandani1', '~Suneel_Belkhale1', '~Joey_Hejna1', '~Evelyn_Choi1', '~Md_Sazzad_Islam1', '~Dorsa_Sadigh1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['autonomous data collection', 'imitation learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We systematically study the increasingly popular paradigm of autonomous imitation learning, and find that it faces challenges in scaling due to both environment design costs and human supervision costs, while observing only modest performance gains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A long-standing goal in robot learning is to develop methods for robots to acquire new skills autonomously. While reinforcement learning (RL) comes with the promise of enabling autonomous data collection, it remains challenging to scale in the real-world partly due to the significant effort required for environment design and instrumentation, including the need for designing reset functions or accurate success detectors. On the other hand, imitation learning (IL) methods require little to no environment design effort, but instead require significant human supervision in the form of collected demonstrations. To address these shortcomings, recent works in autonomous IL start with an initial seed dataset of human demonstrations that an autonomous policy can bootstrap from. While autonomous IL approaches come with the promise of addressing the challenges of autonomous RL—environment design challenges—as well as the challenges of pure IL strategies—extensive human supervision—in this work, we posit that such techniques do not deliver on this promise and are still unable to scale up autonomous data collection in the real world.  Through a series of targeted real-world experiments, we demonstrate that these approaches, when scaled up to realistic settings, face much of the same scaling challenges as prior attempts in RL in terms of environment design. Further, we perform a rigorous study of various autonomous IL methods across different data scales and 7 simulation and real-world tasks, and demonstrate that while autonomous data collection can modestly improve performance (on the order of 10%), simply collecting more human data often provides significantly more improvement. Our work suggests a negative result: that scaling up autonomous data collection for learning robot policies for real-world tasks is more challenging and impractical than what is suggested in prior work. We hope these insights about the core challenges of scaling up data collection help inform future efforts in autonomous learning.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4820414c7d09baa5d0aa0f6fadf390a427bb6e43.pdf" target="_blank">/pdf/4820414c7d09baa5d0aa0f6fadf390a427bb6e43.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0b665ea51eb7feec6726391b25be922f6b936957.zip" target="_blank">/attachment/0b665ea51eb7feec6726391b25be922f6b936957.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
mirchandani2024so,
title={So You Think You Can Scale Up Autonomous Robot Data Collection?},
author={Suvir Mirchandani and Suneel Belkhale and Joey Hejna and Evelyn Choi and Md Sazzad Islam and Dorsa Sadigh},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=XrxLGzF0lJ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://autonomous-data-collection.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/49ce196b53ae979142fe53e878bac49a7b77a2de.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=XrxLGzF0lJ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">mirchandani|so_you_think_you_can_scale_up_autonomous_robot_data_collection</div>
</div>
<div class='paper-counter'>109/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">XopATjibyz</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Quadruped Locomotion Using Differentiable Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yunlong Song', 'Sang bae Kim', 'Davide Scaramuzza']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yunlong_Song1', '~Sang_bae_Kim1', '~Davide_Scaramuzza1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Differentiable Simulation', 'Legged Locomotion', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Differentiable simulation allows for learning quadruped locomotion over challenging terrains effectively.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work explores the potential of using differentiable simulation for learning robot control. Differentiable simulation promises fast convergence and stable training by computing low-variance first-order gradients using the robot model. Still, so far, its usage for legged robots is limited to simulation. The main challenge lies in the complex optimization landscape of robotic tasks due to discontinuous dynamics.  This work proposes a new differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable simulator to maintain sufficient simulation accuracy. Our framework enables learning quadruped walking in simulation in minutes without parallelization. When augmented with GPU parallelization, our approach allows the quadruped robot to master diverse locomotion skills on challenging terrains in minutes. We demonstrate that differentiable simulation outperforms a reinforcement 
learning algorithm (PPO) by achieving significantly better sample efficiency while maintaining its effectiveness in handling large-scale environments. Our policy achieves robust locomotion performance in the real world zero-shot.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/dc74837b9d7da27727b71f9692bd5be9de8adce2.pdf" target="_blank">/pdf/dc74837b9d7da27727b71f9692bd5be9de8adce2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dfba68b8975b51b8548b9307de0a1323234681bb.zip" target="_blank">/attachment/dfba68b8975b51b8548b9307de0a1323234681bb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
song2024learning,
title={Learning Quadruped Locomotion Using Differentiable Simulation},
author={Yunlong Song and Sang bae Kim and Davide Scaramuzza},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=XopATjibyz}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/69cd71a15379addf108c06fb6f06941b98619662.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=XopATjibyz&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">song|learning_quadruped_locomotion_using_differentiable_simulation</div>
</div>
<div class='paper-counter'>110/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">X3OfR3axX4</div>
<div class="field-name">title:</div>
<div class="field-value">Multi-Transmotion: Pre-trained Model for Human Motion Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yang Gao', 'Po-Chien Luan', 'Alexandre Alahi']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yang_Gao15', '~Po-Chien_Luan1', '~Alexandre_Alahi3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Human motion prediction', 'Pre-training', 'Transformer']</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability of intelligent systems to predict human behaviors is essential, particularly in fields such as autonomous vehicle navigation and social robotics. However, the intricacies of human motion have precluded the development of a standardized dataset and model for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to further propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model capable of cross-modality pre-training. Additionally, we devise a novel masking strategy to learn rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code will be made available upon publication.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e6b54b18e7c4e6dcce240bd1965bdfc6cb3ba0d8.pdf" target="_blank">/pdf/e6b54b18e7c4e6dcce240bd1965bdfc6cb3ba0d8.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
gao2024multitransmotion,
title={Multi-Transmotion: Pre-trained Model for Human Motion Prediction},
author={Yang Gao and Po-Chien Luan and Alexandre Alahi},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=X3OfR3axX4}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/85b09d619077d1828738edda728251f645075eee.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=X3OfR3axX4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">gao|multitransmotion_pretrained_model_for_human_motion_prediction</div>
</div>
<div class='paper-counter'>111/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">WnSl42M9Z4</div>
<div class="field-name">title:</div>
<div class="field-value">HumanPlus: Humanoid Shadowing and Imitation from Humans</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zipeng Fu', 'Qingqing Zhao', 'Qi Wu', 'Gordon Wetzstein', 'Chelsea Finn']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zipeng_Fu1', '~Qingqing_Zhao1', 'wuqi23@stanford.edu', '~Gordon_Wetzstein3', '~Chelsea_Finn1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoids', 'Learning from Human Data', 'Whole-Body Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">a full-stack humanoid system for learning motion and autonomous skills from human data</div>
<div class="field-name">abstract:</div>
<div class="field-value">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training.Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1f77b448b6bb24e5e6a2182eec1a88b17e0337ba.pdf" target="_blank">/pdf/1f77b448b6bb24e5e6a2182eec1a88b17e0337ba.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0ab441c33274734f0ff34007aebc7603b4c6b0ec.zip" target="_blank">/attachment/0ab441c33274734f0ff34007aebc7603b4c6b0ec.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
fu2024humanplus,
title={HumanPlus: Humanoid Shadowing and Imitation from Humans},
author={Zipeng Fu and Qingqing Zhao and Qi Wu and Gordon Wetzstein and Chelsea Finn},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=WnSl42M9Z4}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://humanoid-ai.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3b7fcfa7855694fa7c46264897b100b148632262.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=WnSl42M9Z4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">fu|humanplus_humanoid_shadowing_and_imitation_from_humans</div>
</div>
<div class='paper-counter'>112/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">WmWbswjTsi</div>
<div class="field-name">title:</div>
<div class="field-value">Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision</div>
<div class="field-name">authors:</div>
<div class="field-value">['Alberta Longhini', 'Marcel Büsching', 'Bardienus Pieter Duisterhof', 'Jens Lundell', 'Jeffrey Ichnowski', 'Mårten Björkman', 'Danica Kragic']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Alberta_Longhini1', '~Marcel_Büsching1', '~Bardienus_Pieter_Duisterhof1', '~Jens_Lundell1', '~Jeffrey_Ichnowski1', '~Mårten_Björkman2', '~Danica_Kragic1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D State Representations', 'Gaussian Splatting', 'Deformable Objects', 'Vision-based Tracking']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Cloth-Splatting, a method that integrates a pre-trained action-conditioned dynamics model with Gaussian Splatting for 3D state estimation of cloth-like deformable objects from RGB observations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth's state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time by $\sim 85$ \%.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2f4f99f794c998e7f3ad9709944141dd514ba48f.pdf" target="_blank">/pdf/2f4f99f794c998e7f3ad9709944141dd514ba48f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e224dee7107eba7202f8f2820f3c96127ac66071.zip" target="_blank">/attachment/e224dee7107eba7202f8f2820f3c96127ac66071.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
longhini2024clothsplatting,
title={Cloth-Splatting: 3D Cloth State Estimation from {RGB} Supervision},
author={Alberta Longhini and Marcel B{\"u}sching and Bardienus Pieter Duisterhof and Jens Lundell and Jeffrey Ichnowski and M{\r{a}}rten Bj{\"o}rkman and Danica Kragic},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=WmWbswjTsi}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://kth-rpl.github.io/cloth-splatting/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/330e6c2a26bfcd12d7d2e046045738dc8f2fb93e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=WmWbswjTsi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">longhini|clothsplatting_3d_cloth_state_estimation_from_rgb_supervision</div>
</div>
<div class='paper-counter'>113/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">WjDR48cL3O</div>
<div class="field-name">title:</div>
<div class="field-value">Continuous Control with Coarse-to-fine Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Younggyo Seo', 'Jafar Uruç', 'Stephen James']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Younggyo_Seo1', '~Jafar_Uruç1', '~Stephen_James1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Sample-Efficient', 'Action Discretization']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a sample-efficient RL algorithm that can be deployed in real-robot experiments. We train RL agents to zoom-into a continuous action space in a coarse-to-fine manner.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite recent advances in improving the sample-efficiency of reinforcement learning (RL) algorithms, designing an RL algorithm that can be practically deployed in real-world environments remains a challenge. In this paper, we present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner, enabling the use of stable, sample-efficient value-based RL algorithms for fine-grained continuous control tasks. Our key idea is to train agents that output actions by iterating the procedure of (i) discretizing the continuous action space into multiple intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level. We then introduce a concrete, value-based algorithm within the CRL framework called Coarse-to-fine Q-Network (CQN). Our experiments demonstrate that CQN significantly outperforms RL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation tasks with a modest number of environment interactions and expert demonstrations. We also show that CQN robustly learns to solve real-world manipulation tasks within a few minutes of online training.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/021ee9b171c75cea038356100cc342cf5c33eb82.pdf" target="_blank">/pdf/021ee9b171c75cea038356100cc342cf5c33eb82.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/8269450a38625e4e856bd7e24888ae7aa77466b9.zip" target="_blank">/attachment/8269450a38625e4e856bd7e24888ae7aa77466b9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
seo2024continuous,
title={Continuous Control with Coarse-to-fine Reinforcement Learning},
author={Younggyo Seo and Jafar Uru{\c{c}} and Stephen James},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=WjDR48cL3O}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://younggyo.me/cqn/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/bdc6120a62f1244768584a840958c71a621ad495.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=WjDR48cL3O&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">seo|continuous_control_with_coarsetofine_reinforcement_learning</div>
</div>
<div class='paper-counter'>114/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">WLOTZHmmO6</div>
<div class="field-name">title:</div>
<div class="field-value">Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yili Liu', 'Linzhan Mou', 'Xuan Yu', 'Chenrui Han', 'Sitong Mao', 'Rong Xiong', 'Yue Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yili_Liu1', '~Linzhan_Mou1', '~Xuan_Yu1', '~Chenrui_Han1', '~Sitong_Mao1', '~Rong_Xiong1', '~Yue_Wang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D occupancy prediction', 'occupancy flow', 'Neural Radiance Field']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">the first self-supervised work to joint predict the 3D occupancy and occupancy flow</div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurate perception of the dynamic environment is a fundamental task for autonomous driving and robot systems. This paper introduces Let Occ Flow, the first self-supervised work for joint 3D occupancy and occupancy flow prediction using only camera inputs, eliminating the need for 3D annotations. Utilizing TPV for unified scene representation and deformable attention layers for feature aggregation, our approach incorporates a novel attention-based temporal fusion module to capture dynamic object dependencies, followed by a 3D refine module for fine-gained volumetric representation. Besides, our method extends differentiable rendering to 3D volumetric flow fields, leveraging zero-shot 2D segmentation and optical flow cues for dynamic decomposition and motion optimization. Extensive experiments on nuScenes and KITTI datasets demonstrate the competitive performance of our approach over prior state-of-the-art methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a8e713b0ca6144ffa7f04d43b4264b92f1257be8.pdf" target="_blank">/pdf/a8e713b0ca6144ffa7f04d43b4264b92f1257be8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/fe04462d254fa8c923da9a0b389ea8cbc8065e92.zip" target="_blank">/attachment/fe04462d254fa8c923da9a0b389ea8cbc8065e92.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024let,
title={Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction},
author={Yili Liu and Linzhan Mou and Xuan Yu and Chenrui Han and Sitong Mao and Rong Xiong and Yue Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=WLOTZHmmO6}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://eliliu2233.github.io/letoccflow/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/977e77d2a7f526e82ab11bfd2bbfde75e4308567.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=WLOTZHmmO6&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|let_occ_flow_selfsupervised_3d_occupancy_flow_prediction</div>
</div>
<div class='paper-counter'>115/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">VoC3wF6fbh</div>
<div class="field-name">title:</div>
<div class="field-value">Learning to Open and Traverse Doors with a Legged Manipulator</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mike Zhang', 'Yuntao Ma', 'Takahiro Miki', 'Marco Hutter']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mike_Zhang2', '~Yuntao_Ma2', '~Takahiro_Miki1', '~Marco_Hutter1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Mobile Manipulation', 'Legged Manipulator', 'Reinforcement Learning', 'Door Opening']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train a legged manipulator robot to robustly open and pass through a variety of doors without knowing crucial properties such as if the door is push or pull a priori.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Using doors is a longstanding challenge in robotics and is of significant practical interest in giving robots greater access to human-centric spaces. The task is challenging due to the need for online adaptation to varying door properties and precise control in manipulating the door panel and navigating through the confined doorway. To address this, we propose a learning-based controller for a legged manipulator to open and traverse through doors. The controller is trained using a teacher-student approach in simulation to learn robust task behaviors as well as estimate crucial door properties during the interaction. Unlike previous works, our approach is a single control policy that can handle both push and pull doors through learned behaviour which infers the opening direction during deployment without prior knowledge. The policy was deployed on the ANYmal legged robot with an arm and achieved a success rate of 95.0% in repeated trials conducted in an experimental setting. Additional experiments validate the policy's effectiveness and robustness to various doors and disturbances. A video overview of the method and experiments is provided in the supplementary material.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e16ea109868a5f1b4db45f475f90742ab05fbe87.pdf" target="_blank">/pdf/e16ea109868a5f1b4db45f475f90742ab05fbe87.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/65331ae955a63b5aefbe0e3d62c09a5eaadcd68a.zip" target="_blank">/attachment/65331ae955a63b5aefbe0e3d62c09a5eaadcd68a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024learning,
title={Learning to Open and Traverse Doors with a Legged Manipulator},
author={Mike Zhang and Yuntao Ma and Takahiro Miki and Marco Hutter},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=VoC3wF6fbh}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4fe521cf8110351281d317828d036343921eb33d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=VoC3wF6fbh&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|learning_to_open_and_traverse_doors_with_a_legged_manipulator</div>
</div>
<div class='paper-counter'>116/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">VdyIhsh1jU</div>
<div class="field-name">title:</div>
<div class="field-value">Legolas: Deep Leg-Inertial Odometry</div>
<div class="field-name">authors:</div>
<div class="field-value">['Justin Wasserman', 'Ananye Agarwal', 'Rishabh Jangir', 'Girish Chowdhary', 'Deepak Pathak', 'Abhinav Gupta']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Justin_Wasserman1', '~Ananye_Agarwal1', '~Rishabh_Jangir1', '~Girish_Chowdhary1', '~Deepak_Pathak1', '~Abhinav_Gupta1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['State and Odometry Estimation', 'Quadruped robots', 'Sim-to-Real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Legolas learns to predict odometry from simulation and succesfully deploys to multiple real-world quadruped platforms.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Estimating odometry, where an accumulating position and rotation is tracked, has critical applications in many areas of robotics as a form of state estimation such as in SLAM, navigation, and controls. During deployment of a legged robot, a vision system's tracking can easily get lost. Instead, using only the onboard leg and inertial sensor for odometry is a promising alternative. Previous methods in estimating leg-inertial odometry require analytical modeling or collecting high-quality real-world trajectories to train a model. Analytical modeling is specific to each robot, requires manual fine-tuning, and doesn't always capture real-world phenomena such as slippage. Previous work learning legged odometry still relies on collecting real-world data, this has been shown to not perform well out of distribution. In this work, we show that it is possible to estimate the odometry of a legged robot without any analytical modeling or real-world data collection. In this paper, we present Legolas, the first method that accurately estimates odometry in a purely data-driven fashion for quadruped robots. We deploy our method on two real-world quadruped robots in both indoor and outdoor environments. In the indoor scenes, our proposed method accomplishes a relative pose error that is 73% less than an analytical filtering-based approach and 87.5% less than a real-world behavioral cloning approach.
More results are available at: learned-odom.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/574f90b4e6c1e1ac1dda3856caaa59b38249ccef.pdf" target="_blank">/pdf/574f90b4e6c1e1ac1dda3856caaa59b38249ccef.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/8af37d399ad7a121b384ab1a0065d19a28729521.zip" target="_blank">/attachment/8af37d399ad7a121b384ab1a0065d19a28729521.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wasserman2024legolas,
title={Legolas: Deep Leg-Inertial Odometry},
author={Justin Wasserman and Ananye Agarwal and Rishabh Jangir and Girish Chowdhary and Deepak Pathak and Abhinav Gupta},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=VdyIhsh1jU}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://learned-odom.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/989c1cca83e58c1b6e8f7a0842a5ca0bd5c2aa28.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=VdyIhsh1jU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wasserman|legolas_deep_leginertial_odometry</div>
</div>
<div class='paper-counter'>117/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">VUhlMfEekm</div>
<div class="field-name">title:</div>
<div class="field-value">Implicit Grasp Diffusion: Bridging the Gap between Dense Prediction and Sampling-based Grasping</div>
<div class="field-name">authors:</div>
<div class="field-value">['Pinhao Song', 'Pengteng Li', 'Renaud Detry']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Pinhao_Song1', '2110276192@email.szu.edu.cn', 'renaud.detry@kuleuven.be']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Grasping', 'Implicit Neural Representations', 'Diffusion Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Using diffusion models to generate grasps based on local features</div>
<div class="field-name">abstract:</div>
<div class="field-value">There are two dominant approaches in modern robot grasp planning: dense prediction and sampling-based methods. Dense prediction calculates viable grasps across the robot’s view but is limited to predicting one grasp per voxel. Sampling-based methods, on the other hand, encode multi-modal grasp distributions, allowing for different grasp approaches at a point. However, these methods rely on a global latent representation, which struggles to represent the entire field of view, resulting in coarse grasps. To address this, we introduce \emph{Implicit Grasp Diffusion} (IGD), which combines the strengths of both methods by using implicit neural representations to extract detailed local features and sampling grasps from diffusion models conditioned on these features. Evaluations on clutter removal tasks in both simulated and real-world environments show that IGD delivers high accuracy, noise resilience, and multi-modal grasp pose capabilities.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/41586b04c595cf39e8b9207a8f20f285e8aa925a.pdf" target="_blank">/pdf/41586b04c595cf39e8b9207a8f20f285e8aa925a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/351916e6bf36ac3e7d6bb29a0092d1c2440ffc18.zip" target="_blank">/attachment/351916e6bf36ac3e7d6bb29a0092d1c2440ffc18.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
song2024implicit,
title={Implicit Grasp Diffusion: Bridging the Gap between Dense Prediction and Sampling-based Grasping},
author={Pinhao Song and Pengteng Li and Renaud Detry},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=VUhlMfEekm}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a10b9a78611a75966a4b848702e2f369244bc582.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=VUhlMfEekm&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">song|implicit_grasp_diffusion_bridging_the_gap_between_dense_prediction_and_samplingbased_grasping</div>
</div>
<div class='paper-counter'>118/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">VMqg1CeUQP</div>
<div class="field-name">title:</div>
<div class="field-value">DexCatch:  Learning to Catch Arbitrary Objects with Dexterous Hands</div>
<div class="field-name">authors:</div>
<div class="field-value">['Fengbo Lan', 'Shengjie Wang', 'Yunzhe Zhang', 'Haotian Xu', 'Oluwatosin OluwaPelumi Oseni', 'Ziye Zhang', 'Yang Gao', 'Tao Zhang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Fengbo_Lan2', '~Shengjie_Wang2', '~Yunzhe_Zhang3', '~Haotian_Xu6', '~Oluwatosin_OluwaPelumi_Oseni1', '~Ziye_Zhang2', '~Yang_Gao1', '~Tao_Zhang9']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Dexterous Manipulation', 'System Stability']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a Learning-based framework for Throwing-Catching tasks using dexterous hands (LTC).</div>
<div class="field-name">abstract:</div>
<div class="field-value">Achieving human-like dexterous manipulation remains a crucial area of research in robotics. Current research focuses on improving the success rate of pick-and-place tasks. Compared with pick-and-place, throwing-catching behavior has the potential to increase the speed of transporting objects to their destination. However, dynamic dexterous manipulation poses a major challenge for stable control due to a large number of dynamic contacts. In this paper, we propose a Learning-based framework for Throwing-Catching tasks using dexterous hands (LTC). Our method, LTC, achieves a 73% success rate across 45 scenarios (diverse hand poses and objects), and the learned policies demonstrate strong zero-shot transfer performance on unseen objects. Additionally, in tasks where the object in hand faces sideways, an extremely unstable scenario due to the lack of support from the palm, all baselines fail, while our method still achieves a success rate of over 60%.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c50ec8df83a20d14c33f65d2d85795414aa88e5e.pdf" target="_blank">/pdf/c50ec8df83a20d14c33f65d2d85795414aa88e5e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/68b94ce45582f718c05dd0ffe57854df62abdddf.zip" target="_blank">/attachment/68b94ce45582f718c05dd0ffe57854df62abdddf.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lan2024dexcatch,
title={DexCatch:  Learning to Catch Arbitrary Objects with Dexterous Hands},
author={Fengbo Lan and Shengjie Wang and Yunzhe Zhang and Haotian Xu and Oluwatosin OluwaPelumi Oseni and Ziye Zhang and Yang Gao and Tao Zhang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=VMqg1CeUQP}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a258b9ac2d879f9fabc1365148a3578d884f2991.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=VMqg1CeUQP&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lan|dexcatch_learning_to_catch_arbitrary_objects_with_dexterous_hands</div>
</div>
<div class='paper-counter'>119/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">VFs1vbQnYN</div>
<div class="field-name">title:</div>
<div class="field-value">Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zihan Wang', 'Xiangyang Li', 'Jiahao Yang', 'Yeqi Liu', 'Shuqiang Jiang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zihan_Wang11', '~Xiangyang_Li2', '~Jiahao_Yang5', '~Yeqi_Liu2', '~Shuqiang_Jiang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Vision-and-Language Navigation', '3D Feature Fields', 'Semantic Traversable Map']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-and-language navigation (VLN) enables the agent to navigate to a remote location in 3D environments following the natural language instruction. In this field, the agent is usually trained and evaluated in the navigation simulators, lacking effective approaches for sim-to-real transfer. The VLN agents with only a monocular camera exhibit extremely limited performance, while the mainstream VLN models trained with panoramic observation, perform better but are difficult to deploy on most monocular robots. For this case, we propose a sim-to-real transfer approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding, thus smoothly transferring the high-performance panoramic VLN models to the common monocular robots. In this work, the semantic traversable map is proposed to predict agent-centric navigable waypoints, and the novel view representations of these navigable waypoints are predicted through the 3D feature fields. These methods broaden the limited field of view of the monocular robots and significantly improve navigation performance in the real world. Our VLN system outperforms previous SOTA monocular VLN methods in R2R-CE and RxR-CE benchmarks within the simulation environments and is also validated in real-world environments, providing a practical and high-performance solution for real-world VLN.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6f3bf1593bb35fa8975632b5bbde8f6c0779a2fb.pdf" target="_blank">/pdf/6f3bf1593bb35fa8975632b5bbde8f6c0779a2fb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3be17b1353b8a579bb2949f4cd086341b177df87.zip" target="_blank">/attachment/3be17b1353b8a579bb2949f4cd086341b177df87.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024simtoreal,
title={Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation},
author={Zihan Wang and Xiangyang Li and Jiahao Yang and Yeqi Liu and Shuqiang Jiang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=VFs1vbQnYN}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d235f2d47827f2e138f0f911094e54206a284ce6.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=VFs1vbQnYN&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|simtoreal_transfer_via_3d_feature_fields_for_visionandlanguage_navigation</div>
</div>
<div class='paper-counter'>120/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">V5x0m6XDSV</div>
<div class="field-name">title:</div>
<div class="field-value">Differentiable Discrete Elastic Rods for Real-Time Modeling of Deformable Linear Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yizhou Chen', 'Yiting Zhang', 'Zachary Brei', 'Tiancheng Zhang', 'Yuzhen Chen', 'Julie Wu', 'Ram Vasudevan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yizhou_Chen4', '~Yiting_Zhang3', 'breizach@umich.edu', '~Tiancheng_Zhang3', '~Yuzhen_Chen1', 'jwuxx@umich.edu', '~Ram_Vasudevan2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Deformable Linear Objects Modeling', 'Physics-Informed Learning', 'Differentiable Simulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a novel framework that combines a differentiable physics-based model with learning framework to model DLOs accurately and in real-time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper addresses the task of modeling Deformable Linear Objects (DLOs), such as ropes and cables, during dynamic motion over long time horizons. This task presents significant challenges due to the complex dynamics of DLOs. To address these challenges, this paper proposes differentiable Discrete Elastic Rods For deformable linear Objects with Real-time Modeling (DEFORM),  a novel framework that combines a differentiable physics-based model with a learning framework to model DLOs accurately and in real-time.  The performance of DEFORM is evaluated in an experimental setup involving two industrial robots and a variety of sensors. A comprehensive series of experiments demonstrate the efficacy of DEFORM in terms of accuracy, computational speed, and generalizability when compared to state-of-the-art alternatives. To further demonstrate the utility of DEFORM, this paper integrates it into a perception pipeline and illustrates its superior performance when compared to the state-of-the-art methods while tracking a DLO even in the presence of occlusions.  Finally, this paper illustrates the superior performance of DEFORM when compared to state-of-the-art methods when it is applied to perform autonomous planning and control of DLOs.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e48d7c98923bc370a73a465ff5ac2677bf958638.pdf" target="_blank">/pdf/e48d7c98923bc370a73a465ff5ac2677bf958638.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/7d48de4444b5d8e28b85cce26d1bec9477006a92.zip" target="_blank">/attachment/7d48de4444b5d8e28b85cce26d1bec9477006a92.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024differentiable,
title={Differentiable Discrete Elastic Rods for Real-Time Modeling of Deformable Linear Objects},
author={Yizhou Chen and Yiting Zhang and Zachary Brei and Tiancheng Zhang and Yuzhen Chen and Julie Wu and Ram Vasudevan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=V5x0m6XDSV}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://roahmlab.github.io/DEFORM/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/09d218fe20e3a7bebc3027ff63e1df66e2c502e3.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=V5x0m6XDSV&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|differentiable_discrete_elastic_rods_for_realtime_modeling_of_deformable_linear_objects</div>
</div>
<div class='paper-counter'>121/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Uaaj4MaVIQ</div>
<div class="field-name">title:</div>
<div class="field-value">D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Rearrangement</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yixuan Wang', 'Mingtong Zhang', 'Zhuoran Li', 'Tarik Kelestemur', 'Katherine Rose Driggs-Campbell', 'Jiajun Wu', 'Li Fei-Fei', 'Yunzhu Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yixuan_Wang2', '~Mingtong_Zhang1', '~Zhuoran_Li3', '~Tarik_Kelestemur1', '~Katherine_Rose_Driggs-Campbell1', '~Jiajun_Wu1', '~Li_Fei-Fei1', '~Yunzhu_Li1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Implicit 3D Representation', 'Visual Foundational Model', 'Zero-Shot Generalization', 'Robotic Manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Scene representation is a crucial design choice in robotic manipulation systems. An ideal representation is expected to be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields---**dynamic 3D descriptor fields**. These fields are **implicit 3D representations** that take in 3D points and output semantic features and instance masks. They can also capture the dynamics of the underlying 3D environments. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from visual foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to rearrangement tasks in a zero-shot manner. Through extensive evaluation in real worlds and simulations, we demonstrate that D$^3$Fields are effective for **zero-shot generalizable** rearrangement tasks. We also compare D$^3$Fields with state-of-the-art implicit 3D representations and show significant improvements in effectiveness and efficiency. Project page: https://robopil.github.io/d3fields/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e9e372b6f1a724f007fa1d178ad742d8915dddbb.pdf" target="_blank">/pdf/e9e372b6f1a724f007fa1d178ad742d8915dddbb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e38855025ad6430f3c43d09014b4baa9ee805ed4.zip" target="_blank">/attachment/e38855025ad6430f3c43d09014b4baa9ee805ed4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024dfields,
title={D\${\textasciicircum}3\$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Rearrangement},
author={Yixuan Wang and Mingtong Zhang and Zhuoran Li and Tarik Kelestemur and Katherine Rose Driggs-Campbell and Jiajun Wu and Li Fei-Fei and Yunzhu Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Uaaj4MaVIQ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robopil.github.io/d3fields/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/4db26a768840cba0f3b635bb011a3bbb814cdd7e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Uaaj4MaVIQ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|d^3fields_dynamic_3d_descriptor_fields_for_zeroshot_generalizable_rearrangement</div>
</div>
<div class='paper-counter'>122/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">UUZ4Yw3lt0</div>
<div class="field-name">title:</div>
<div class="field-value">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhenyu Jiang', 'Yuqi Xie', 'Jinhan Li', 'Ye Yuan', 'Yifeng Zhu', 'Yuke Zhu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhenyu_Jiang1', '~Yuqi_Xie1', '~Jinhan_Li2', '~Ye_Yuan5', '~Yifeng_Zhu2', '~Yuke_Zhu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoid Robot', 'Whole-Body Motion Generation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Humanoid motion generation with human motion prior and VLM-based motion refinement.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found on our website https://ut-austin-rpl.github.io/Harmon/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/cd8dcc8ab6ba2fb61fced9c9de68168c19c4b305.pdf" target="_blank">/pdf/cd8dcc8ab6ba2fb61fced9c9de68168c19c4b305.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f2e7bcd50854a774d2eec4b21a4d449681f544f1.zip" target="_blank">/attachment/f2e7bcd50854a774d2eec4b21a4d449681f544f1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jiang2024harmon,
title={Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions},
author={Zhenyu Jiang and Yuqi Xie and Jinhan Li and Ye Yuan and Yifeng Zhu and Yuke Zhu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=UUZ4Yw3lt0}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://ut-austin-rpl.github.io/Harmon/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/13c8c14ba38fcfebd781463e2deb6f9185d01177.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=UUZ4Yw3lt0&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jiang|harmon_wholebody_motion_generation_of_humanoid_robots_from_language_descriptions</div>
</div>
<div class='paper-counter'>123/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">URj5TQTAXM</div>
<div class="field-name">title:</div>
<div class="field-value">OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jinhan Li', 'Yifeng Zhu', 'Yuqi Xie', 'Zhenyu Jiang', 'Mingyo Seo', 'Georgios Pavlakos', 'Yuke Zhu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jinhan_Li2', '~Yifeng_Zhu2', '~Yuqi_Xie1', '~Zhenyu_Jiang1', '~Mingyo_Seo1', '~Georgios_Pavlakos1', '~Yuke_Zhu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Humanoid Manipulation', 'Imitation From Videos', 'Motion Retargeting']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately.  Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, outperforming the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of $79.2\%$ without the need for labor-intensive teleoperation. More videos can be found on our 
website https://ut-austin-rpl.github.io/OKAMI/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/04db9778afe6ea49f7bd939035a9c358479f8d05.pdf" target="_blank">/pdf/04db9778afe6ea49f7bd939035a9c358479f8d05.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0add412bd8194e5e7de34a1a5ecbd253c09c65b8.zip" target="_blank">/attachment/0add412bd8194e5e7de34a1a5ecbd253c09c65b8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
li2024okami,
title={{OKAMI}: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation},
author={Jinhan Li and Yifeng Zhu and Yuqi Xie and Zhenyu Jiang and Mingyo Seo and Georgios Pavlakos and Yuke Zhu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=URj5TQTAXM}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://ut-austin-rpl.github.io/OKAMI/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8dde045216b9ba53e9a2288497d65d2be5f715bf.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=URj5TQTAXM&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">li|okami_teaching_humanoid_robots_manipulation_skills_through_single_video_imitation</div>
</div>
<div class='paper-counter'>124/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">UHxPZgK33I</div>
<div class="field-name">title:</div>
<div class="field-value">RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hanxiao Jiang', 'Binghao Huang', 'Ruihai Wu', 'Zhuoran Li', 'Shubham Garg', 'Hooshang Nayyeri', 'Shenlong Wang', 'Yunzhu Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hanxiao_Jiang1', '~Binghao_Huang1', '~Ruihai_Wu1', '~Zhuoran_Li3', '~Shubham_Garg1', '~Hooshang_Nayyeri1', '~Shenlong_Wang1', '~Yunzhu_Li1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Action-Conditioned Scene Graph', 'Foundation Models for Robotics', 'Scene Exploration', 'Robotic Manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information (geometry and semantics) and high-level information (action-conditioned relationships between different entities) in the scene. To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities. The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG.
Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects, and deformable objects. Project Page: https://jianghanxiao.github.io/roboexp-web/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2400047673da874c1262def626279645e521ef51.pdf" target="_blank">/pdf/2400047673da874c1262def626279645e521ef51.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/bff80795c3b8d58cb88027fd345ec7fb0087ec09.zip" target="_blank">/attachment/bff80795c3b8d58cb88027fd345ec7fb0087ec09.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jiang2024roboexp,
title={Robo{EXP}: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation},
author={Hanxiao Jiang and Binghao Huang and Ruihai Wu and Zhuoran Li and Shubham Garg and Hooshang Nayyeri and Shenlong Wang and Yunzhu Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=UHxPZgK33I}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://jianghanxiao.github.io/roboexp-web/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3550ad3749b10a810b6d5658313fbdddfc46cca2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=UHxPZgK33I&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jiang|roboexp_actionconditioned_scene_graph_via_interactive_exploration_for_robotic_manipulation</div>
</div>
<div class='paper-counter'>125/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">U5RPcnFhkq</div>
<div class="field-name">title:</div>
<div class="field-value">FetchBench: A Simulation Benchmark for Robot Fetching</div>
<div class="field-name">authors:</div>
<div class="field-value">['Beining Han', 'Meenal Parakh', 'Derek Geng', 'Jack A Defay', 'Gan Luyang', 'Jia Deng']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Beining_Han1', '~Meenal_Parakh1', '~Derek_Geng1', '~Jack_A_Defay1', '~Gan_Luyang1', '~Jia_Deng1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Grasping; Benchmark; Imitation Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Fetching, which includes approaching, grasping, and retrieving, is a critical challenge for robot manipulation tasks. Existing methods primarily focus on table-top scenarios, which do not adequately capture the complexities of environments where both grasping and planning are essential. To address this gap, we propose a new benchmark FetchBench, featuring diverse procedural scenes that integrate both grasping and motion planning challenges. Additionally, FetchBench includes a data generation pipeline that collects successful fetch trajectories for use in imitation learning methods. We implement multiple baselines from the traditional sense-plan-act pipeline to end-to-end behavior models. Our empirical analysis reveals that these methods achieve a maximum success rate of only 20%, indicating substantial room for improvement. Additionally, we identify key bottlenecks within the sense-plan-act pipeline and make recommendations based on the systematic analysis.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/903a24d3d4839d54ead80ffd41d2c467f9380184.pdf" target="_blank">/pdf/903a24d3d4839d54ead80ffd41d2c467f9380184.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/852513ee9b5026a9d27df51c5e0fd280e7333512.zip" target="_blank">/attachment/852513ee9b5026a9d27df51c5e0fd280e7333512.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
han2024fetchbench,
title={FetchBench: A Simulation Benchmark for Robot Fetching},
author={Beining Han and Meenal Parakh and Derek Geng and Jack A Defay and Gan Luyang and Jia Deng},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=U5RPcnFhkq}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2a203a0059b11c64bf48a91696239ef65e7db7b2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=U5RPcnFhkq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">han|fetchbench_a_simulation_benchmark_for_robot_fetching</div>
</div>
<div class='paper-counter'>126/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">TzqKmIhcwq</div>
<div class="field-name">title:</div>
<div class="field-value">Structured Bayesian Meta-Learning for Data-Efficient Visual-Tactile Model Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shaoxiong Yao', 'Yifan Zhu', 'Kris Hauser']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shaoxiong_Yao1', '~Yifan_Zhu8', '~Kris_Hauser2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multimodal perception', 'tactile sensing', 'few-shot learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our SBML framework enables data-efficient visual-tactile model estimation by learning a prior of visual-tactile model from diverse real world objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Estimating visual-tactile models of deformable objects is challenging because vision suffers from occlusion, while touch data is sparse and noisy.  We propose a novel data-efficient method for dense heterogeneous model estimation by leveraging experience from diverse training objects.  The method is based on Bayesian Meta-Learning (BML), which can mitigate overfitting high-capacity visual-tactile models by meta-learning an informed prior and naturally achieves few-shot online estimation via posterior estimation.  However, BML requires a shared parametric model across tasks but visual-tactile models for diverse objects have different parameter spaces.  To address this issue, we introduce Structured Bayesian Meta-Learning (SBML) that incorporates heterogeneous physics models, enabling learning from training objects with varying appearances and geometries.  SBML performs zero-shot vision-only prediction of deformable model parameters and few-shot adaptation after a handful of touches.  Experiments show that in two classes of heterogeneous objects, namely plants and shoes, SBML outperforms existing approaches in force and torque prediction accuracy in zero- and few-shot settings.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/019b5caaa180030c2272cb8ff4caccbb6461db72.pdf" target="_blank">/pdf/019b5caaa180030c2272cb8ff4caccbb6461db72.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/54fb42774df5ae80aef632d7290e7d5355a7d04f.zip" target="_blank">/attachment/54fb42774df5ae80aef632d7290e7d5355a7d04f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yao2024structured,
title={Structured Bayesian Meta-Learning for Data-Efficient Visual-Tactile Model Estimation},
author={Shaoxiong Yao and Yifan Zhu and Kris Hauser},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=TzqKmIhcwq}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://shaoxiongyao.github.io/SBML</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6e367c3651ff25af92371561c3461056cca863ea.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=TzqKmIhcwq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yao|structured_bayesian_metalearning_for_dataefficient_visualtactile_model_estimation</div>
</div>
<div class='paper-counter'>127/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Si2krRESZb</div>
<div class="field-name">title:</div>
<div class="field-value">TieBot: Learning to Knot a Tie from Visual Demonstration through a Real-to-Sim-to-Real Approach</div>
<div class="field-name">authors:</div>
<div class="field-value">['Weikun Peng', 'Jun Lv', 'Yuwei Zeng', 'Haonan Chen', 'Siheng Zhao', 'Jichen Sun', 'Cewu Lu', 'Lin Shao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Weikun_Peng1', '~Jun_Lv2', '~Yuwei_Zeng1', '~Haonan_Chen4', '~Siheng_Zhao1', '~Jichen_Sun1', '~Cewu_Lu3', '~Lin_Shao2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['cloth manipulation', 'learning from demonstration', 'robot learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">The tie-knotting task is highly challenging due to the tie's high deformation and long-horizon manipulation actions. This work presents TieBot, a Real-to-Sim-to-Real learning from visual demonstration system for the robots to learn to knot a tie. We introduce the Hierarchical Feature Matching approach to estimate a sequence of tie's meshes from the demonstration video. With these estimated meshes used as subgoals, we first learn a teacher policy using privileged information. Then, we learn a student policy with point cloud observation by imitating teacher policy. Lastly, our pipeline applies learned policy to real-world execution. We demonstrate the effectiveness of TieBot in simulation and the real world. In the real-world experiment, a dual-arm robot successfully knots a tie, achieving 50% success rate among 10 trials. Videos can be found on https://tiebots.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/678a508916384038767fdcae89e07f99946fcd30.pdf" target="_blank">/pdf/678a508916384038767fdcae89e07f99946fcd30.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0d528373b86b54d8269529cac4e9d40497df375d.zip" target="_blank">/attachment/0d528373b86b54d8269529cac4e9d40497df375d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
peng2024tiebot,
title={TieBot: Learning to Knot a Tie from Visual Demonstration through a Real-to-Sim-to-Real Approach},
author={Weikun Peng and Jun Lv and Yuwei Zeng and Haonan Chen and Siheng Zhao and Jichen Sun and Cewu Lu and Lin Shao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Si2krRESZb}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://tiebots.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7818e028b83bc72cfc4f901e6ea116a513b282a5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Si2krRESZb&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">peng|tiebot_learning_to_knot_a_tie_from_visual_demonstration_through_a_realtosimtoreal_approach</div>
</div>
<div class='paper-counter'>128/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">SfaB20rjVo</div>
<div class="field-name">title:</div>
<div class="field-value">An Open-Source Soft Robotic Platform for Autonomous Aerial Manipulation in the Wild</div>
<div class="field-name">authors:</div>
<div class="field-value">['Erik Bauer', 'Marc Blöchlinger', 'Pascal Strauch', 'Arman Raayatsanati', 'Cavelti Curdin', 'Robert K. Katzschmann']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Erik_Bauer1', '~Marc_Blöchlinger1', '~Pascal_Strauch1', '~Arman_Raayatsanati1', '~Cavelti_Curdin1', '~Robert_K._Katzschmann1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Aerial Manipulation', 'Learning-Based Grasping', 'Autonomous Flight', 'Robotic Systems', 'Soft Grasping']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an open-source soft robotic platform for autonomous aerial manipulation in the wild that uses only onboard perception for self- and target localization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Aerial manipulation combines the versatility and speed of flying platforms with the functional capabilities of mobile manipulation, which presents significant challenges due to the need for precise localization and control. Traditionally, researchers have relied on off-board perception systems, which are limited to expensive and impractical specially equipped indoor environments. In this work, we introduce a novel platform for autonomous aerial manipulation that exclusively utilizes onboard perception systems. Our platform can perform aerial manipulation in various indoor and outdoor environments without depending on external perception systems. Our experimental results demonstrate the platform's ability to autonomously grasp various objects in diverse settings. This advancement significantly improves the scalability and practicality of aerial manipulation applications by eliminating the need for costly tracking solutions. To accelerate future research, we open source our modern ROS 2 software stack and custom hardware design, making our contributions accessible to the broader research community.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/755e319d88822876bc30a12ffaf7739d6919dc83.pdf" target="_blank">/pdf/755e319d88822876bc30a12ffaf7739d6919dc83.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d0c17be3befaf55bc02e6926bb8402218be065bd.zip" target="_blank">/attachment/d0c17be3befaf55bc02e6926bb8402218be065bd.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
bauer2024an,
title={An Open-Source Soft Robotic Platform for Autonomous Aerial Manipulation in the Wild},
author={Erik Bauer and Marc Bl{\"o}chlinger and Pascal Strauch and Arman Raayatsanati and Cavelti Curdin and Robert K. Katzschmann},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=SfaB20rjVo}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/open-source-soft-platform/open-source-soft-robotic-platform</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6d22a6cea2b8794ade55146394c3ff6906dcf54d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=SfaB20rjVo&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">bauer|an_opensource_soft_robotic_platform_for_autonomous_aerial_manipulation_in_the_wild</div>
</div>
<div class='paper-counter'>129/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">SW8ntpJl0E</div>
<div class="field-name">title:</div>
<div class="field-value">JA-TN: Pick-and-Place Towel Shaping from Crumpled States based on TransporterNet with Joint-Probability Action Inference</div>
<div class="field-name">authors:</div>
<div class="field-value">['Halid Abdulrahim Kadi', 'Kasim Terzić']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Halid_Abdulrahim_Kadi1', '~Kasim_Terzić1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Cloth Manipulation', 'Imitation Learning', 'Sim2Real Transfer']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our JA-TN towel-shaping controller extends the TransporterNet architecture with our joint-probability action inference and achieves state-of-the-art performance for flattening, folding-from-flattened and folding-from-crumpled tasks in simulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Towel manipulation is a crucial step towards more general cloth manipulation. However, folding a towel from an arbitrarily crumpled state and recovering from a failed folding step remain critical challenges in robotics. We propose joint-probability action inference JA-TN, as a way to improve TransporterNet's operational efficiency; to our knowledge, this is the first single data-driven policy to achieve various types of folding from most crumpled states. We present three benchmark domains with a set of shaping tasks and the corresponding oracle policies to facilitate the further development of the field. We also present a simulation-to-reality transfer procedure for vision-based deep learning controllers by processing and augmenting RGB and/or depth images. We also demonstrate JA-TN's ability to integrate with a real camera and a UR3e robot arm, showcasing the method's applicability to real-world tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c6e6c97346b6cdefd7c1fb3f2110d1cdb2a87e1c.pdf" target="_blank">/pdf/c6e6c97346b6cdefd7c1fb3f2110d1cdb2a87e1c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ea32db4181760db3977befc3d2ab1d370e7547c2.zip" target="_blank">/attachment/ea32db4181760db3977befc3d2ab1d370e7547c2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kadi2024jatn,
title={{JA}-{TN}: Pick-and-Place Towel Shaping from Crumpled States based on TransporterNet with Joint-Probability Action Inference},
author={Halid Abdulrahim Kadi and Kasim Terzi{\'c}},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=SW8ntpJl0E}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8d67166cd2662d1208d318c0c1ca477c0a13e1fb.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=SW8ntpJl0E&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kadi|jatn_pickandplace_towel_shaping_from_crumpled_states_based_on_transporternet_with_jointprobability_action_inference</div>
</div>
<div class='paper-counter'>130/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">SFJz5iLvur</div>
<div class="field-name">title:</div>
<div class="field-value">Lessons from Learning to Spin “Pens”</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jun Wang', 'Ying Yuan', 'Haichuan Che', 'Haozhi Qi', 'Yi Ma', 'Jitendra Malik', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jun_Wang60', '~Ying_Yuan2', '~Haichuan_Che1', '~Haozhi_Qi1', '~Yi_Ma4', '~Jitendra_Malik2', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous In-Hand Manipulation', 'Reinforcement Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In-hand manipulation of pen-like objects is a most basic and important skill in our daily lives, as many tools such as hammers and screwdrivers are similarly shaped. However, current learning-based methods struggle with this task due to a lack of high-quality demonstrations and the significant gap between simulation and the real world. In this work, we push the boundaries of learning-based in-hand manipulation systems by demonstrating the capability to spin pen-like objects. We use reinforcement learning to train a policy and generate a high-fidelity trajectory dataset in simulation. This serves two purposes: 1) pre-training a sensorimotor policy in simulation; 2) conducting open-loop trajectory replay in the real world. We then fine-tune the sensorimotor policy using these real-world trajectories to adapt to the real world. With less than 50 trajectories, our policy learns to rotate more than ten pen-like objects with different physical properties for multiple revolutions. We present a comprehensive analysis of our design choices and share the lessons learned during development. Videos are shown on https://corl-2024-dexpen.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2715f3725326b7879f480d18897988565248f9fd.pdf" target="_blank">/pdf/2715f3725326b7879f480d18897988565248f9fd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ca3e4be843c98a124b7ae50e4d8a112dae45f781.zip" target="_blank">/attachment/ca3e4be843c98a124b7ae50e4d8a112dae45f781.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024lessons,
title={Lessons from Learning to Spin {\textquotedblleft}Pens{\textquotedblright}},
author={Jun Wang and Ying Yuan and Haichuan Che and Haozhi Qi and Yi Ma and Jitendra Malik and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=SFJz5iLvur}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://penspin.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/226a08dfb29ddffb4bbbffed03b3f975e507eccc.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=SFJz5iLvur&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|lessons_from_learning_to_spin_pens</div>
</div>
<div class='paper-counter'>131/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">S8jQtafbT3</div>
<div class="field-name">title:</div>
<div class="field-value">Autonomous Interactive Correction MLLM for Robust Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chuyan Xiong', 'Chengyu Shen', 'Xiaoqi Li', 'Kaichen Zhou', 'Jiaming Liu', 'Ruiping Wang', 'Hao Dong']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chuyan_Xiong1', '~Chengyu_Shen1', '~Xiaoqi_Li3', '~Kaichen_Zhou1', '~Jiaming_Liu2', '~Ruiping_Wang1', '~Hao_Dong3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['large language model', 'robotics']</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects. Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly. However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses which is particularly prone to occur during articulated object manipulation. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions for articulated object. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities. We design two types of prompt instructions for interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2) textual descriptions to indicate potential directions for rotation correction. During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts. To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration. Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5484a153c092674c7ad593b458bd10d3fa578a93.pdf" target="_blank">/pdf/5484a153c092674c7ad593b458bd10d3fa578a93.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/407e6cb3c4c9eaf3fd5238be9fa3328318186786.zip" target="_blank">/attachment/407e6cb3c4c9eaf3fd5238be9fa3328318186786.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xiong2024autonomous,
title={Autonomous Interactive Correction {MLLM} for Robust Robotic Manipulation},
author={Chuyan Xiong and Chengyu Shen and Xiaoqi Li and Kaichen Zhou and Jiaming Liu and Ruiping Wang and Hao Dong},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=S8jQtafbT3}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/9b4cfe34649de733755ec19ba26586d8a6c86036.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=S8jQtafbT3&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xiong|autonomous_interactive_correction_mllm_for_robust_robotic_manipulation</div>
</div>
<div class='paper-counter'>132/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">S70MgnIA0v</div>
<div class="field-name">title:</div>
<div class="field-value">Robotic Control via Embodied Chain-of-Thought Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Michał Zawalski', 'William Chen', 'Karl Pertsch', 'Oier Mees', 'Chelsea Finn', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Michał_Zawalski1', '~William_Chen1', '~Karl_Pertsch1', '~Oier_Mees1', '~Chelsea_Finn1', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Vision-Language-Action Models', 'Embodied Chain-of-Thought Reasoning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train vision-language-action models to perform *embodied* chain-of-thought reasoning, substantially improving their success rate without any additional robot data.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A key limitation of learned robot control policies is their inability to generalize outside their training data. 
Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of "chain-of-thought" (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28\% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/be5fc498a7283ba58758a346c6fddc91d7588d08.pdf" target="_blank">/pdf/be5fc498a7283ba58758a346c6fddc91d7588d08.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c69c6e08f4c73715e7e300a83f8384f5e41e6aa7.zip" target="_blank">/attachment/c69c6e08f4c73715e7e300a83f8384f5e41e6aa7.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zawalski2024robotic,
title={Robotic Control via Embodied Chain-of-Thought Reasoning},
author={Micha{\l} Zawalski and William Chen and Karl Pertsch and Oier Mees and Chelsea Finn and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=S70MgnIA0v}
}</div>
<div class="field-name">website:</div>
<div class="field-value">embodied-cot.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a2f89bbb54b68a95a5a443ff65892974fe9a5958.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=S70MgnIA0v&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zawalski|robotic_control_via_embodied_chainofthought_reasoning</div>
</div>
<div class='paper-counter'>133/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">S2Jwb0i7HN</div>
<div class="field-name">title:</div>
<div class="field-value">DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tyler Ga Wei Lum', 'Martin Matak', 'Viktor Makoviychuk', 'Ankur Handa', 'Arthur Allshire', 'Tucker Hermans', 'Nathan D. Ratliff', 'Karl Van Wyk']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tyler_Ga_Wei_Lum1', '~Martin_Matak1', '~Viktor_Makoviychuk1', '~Ankur_Handa1', '~Arthur_Allshire1', '~Tucker_Hermans2', '~Nathan_D._Ratliff1', '~Karl_Van_Wyk1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous Grasping', 'Geometric Fabrics', 'Reinforcement Learning', 'Teacher-Student Distillation', 'Sim-to-Real Transfer']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">DextrAH-G is a safe, continuously reacting pixels-to-action policy that achieves state-of-the-art dexterous grasping in the real world and was trained entirely in simulation using RL and teacher-student distillation with a geometric fabric controller</div>
<div class="field-name">abstract:</div>
<div class="field-value">A pivotal challenge in robotics is achieving fast, safe, and robust dexterous grasping across a diverse range of objects, an important goal within industrial applications. However, existing methods often have very limited speed, dexterity, and generality, along with limited or no hardware safety guarantees. In this work, we introduce DextrAH-G, a depth-based dexterous grasping policy trained entirely in simulation that combines reinforcement learning, geometric fabrics, and teacher-student distillation. We address key challenges in joint arm-hand policy learning, such as high-dimensional observation and action spaces, the sim2real gap, collision avoidance, and hardware constraints. DextrAH-G enables a 23 motor arm-hand robot to safely and continuously grasp and transport a large variety of objects at high speed using multi-modal inputs including depth images, allowing generalization across object geometry. Videos at https://sites.google.com/view/dextrah-g.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0e03187100ea67ce367767a26adf4e495de18ce6.pdf" target="_blank">/pdf/0e03187100ea67ce367767a26adf4e495de18ce6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2c398d9df21839ecaf6f24975854678d392ac1de.zip" target="_blank">/attachment/2c398d9df21839ecaf6f24975854678d392ac1de.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lum2024dextrahg,
title={Dextr{AH}-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics},
author={Tyler Ga Wei Lum and Martin Matak and Viktor Makoviychuk and Ankur Handa and Arthur Allshire and Tucker Hermans and Nathan D. Ratliff and Karl Van Wyk},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=S2Jwb0i7HN}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/dextrah-g</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/dbcfb2e19ee764c2963db4d86ae9f3f2237bb5a0.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=S2Jwb0i7HN&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lum|dextrahg_pixelstoaction_dexterous_armhand_grasping_with_geometric_fabrics</div>
</div>
<div class='paper-counter'>134/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">RMkdcKK7jq</div>
<div class="field-name">title:</div>
<div class="field-value">SLR: Learning Quadruped Locomotion without Privileged Information</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shiyi Chen', 'Zeyu Wan', 'Shiyang Yan', 'Chun Zhang', 'Weiyi Zhang', 'Qiang Li', 'Debing Zhang', 'Fasih Ud Din Farrukh']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shiyi_Chen3', '~Zeyu_Wan2', '~Shiyang_Yan3', '~Chun_Zhang4', '~Weiyi_Zhang3', '~Qiang_Li28', '~Debing_Zhang4', '~Fasih_Ud_Din_Farrukh1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Locomotion', 'Reinforcement Learning', 'Privileged Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Traditional reinforcement learning control for quadruped robots often relies on privileged information, demanding meticulous selection and precise estimation, thereby imposing constraints on the development process. This work proposes a Self-learning Latent Representation (SLR) method, which achieves high-performance control policy learning without the need for privileged information. To enhance the credibility of our proposed method's evaluation, SLR is compared with open-source code repositories of state-of-the-art algorithms, retaining the original authors' configuration parameters. Across four repositories, SLR consistently outperforms the reference results. Ultimately, the trained policy and encoder empower the quadruped robot to navigate steps, climb stairs, ascend rocks, and traverse various challenging terrains.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3ce0d8f18daf0512edc371c142bbf352a011a3a8.pdf" target="_blank">/pdf/3ce0d8f18daf0512edc371c142bbf352a011a3a8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ca059a9b5a2d84d90d08d61406454e2d3fcc47f3.zip" target="_blank">/attachment/ca059a9b5a2d84d90d08d61406454e2d3fcc47f3.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024slr,
title={{SLR}: Learning Quadruped Locomotion without Privileged Information},
author={Shiyi Chen and Zeyu Wan and Shiyang Yan and Chun Zhang and Weiyi Zhang and Qiang Li and Debing Zhang and Fasih Ud Din Farrukh},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=RMkdcKK7jq}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://11chens.github.io/SLR/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/113c9f5e2496c6312b976eeb97e7df9f7a49aa35.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=RMkdcKK7jq&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|slr_learning_quadruped_locomotion_without_privileged_information</div>
</div>
<div class='paper-counter'>135/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Qz2N4lWBk3</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Granular Media Avalanche Behavior for Indirectly Manipulating Obstacles on a Granular Slope</div>
<div class="field-name">authors:</div>
<div class="field-value">['Haodi Hu', 'Feifei Qian', 'Daniel Seita']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Haodi_Hu1', '~Feifei_Qian1', '~Daniel_Seita1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Granular media', 'Avalanche dynamics', 'Legged robots.']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this work, we propose Granular Robotic Avalanche INteraction (GRAIN), a novel learning-based method for leveraging granular avalanche dynamics for indirectly manipulating objects on a granular slope.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Legged robot locomotion on sand slopes is challenging due to the complex dynamics of granular media and how the lack of solid surfaces can hinder locomotion. A promising strategy, inspired by ghost crabs and other organisms in nature, is to strategically interact with rocks, debris, and other obstacles to facilitate movement. To provide legged robots with this ability, we present a novel approach that leverages avalanche dynamics to indirectly manipulate objects on a granular slope. We use a Vision Transformer (ViT) to process image representations of granular dynamics and robot excavation actions. The ViT predicts object movement, which we use to determine which leg excavation action to execute. We collect training data from 100 real physical trials and, at test time, deploy our trained model in novel settings. Experimental results suggest that our model can accurately predict object movements and achieve a success rate ≥ 80% in a variety of manipulation tasks with up to four obstacles, and can also generalize to objects with different physics properties. To our knowledge, this is the first paper to leverage granular media avalanche dynamics to indirectly manipulate objects on granular slopes. Supplementary material is available at https://sites.google.com/view/grain-corl2024/home.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6fb28887c82f787495544d360ce4a80a67a6643b.pdf" target="_blank">/pdf/6fb28887c82f787495544d360ce4a80a67a6643b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/afa298a89ba38944c3dae3eff43a86ef897a3fd9.zip" target="_blank">/attachment/afa298a89ba38944c3dae3eff43a86ef897a3fd9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hu2024learning,
title={Learning Granular Media Avalanche Behavior for Indirectly Manipulating Obstacles on a Granular Slope},
author={Haodi Hu and Feifei Qian and Daniel Seita},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Qz2N4lWBk3}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/grain-corl2024/home</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/12e32c34ff108114f80a6c86e95b7de408cb97e5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Qz2N4lWBk3&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hu|learning_granular_media_avalanche_behavior_for_indirectly_manipulating_obstacles_on_a_granular_slope</div>
</div>
<div class='paper-counter'>136/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">QtCtY8zl2T</div>
<div class="field-name">title:</div>
<div class="field-value">Task Success Prediction for Open-Vocabulary Manipulation Based on Multi-Level Aligned Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Miyu Goko', 'Motonari Kambara', 'Daichi Saito', 'Seitaro Otsuki', 'Komei Sugiura']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Miyu_Goko1', '~Motonari_Kambara1', '~Daichi_Saito1', '~Seitaro_Otsuki1', '~Komei_Sugiura1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Task Success Prediction', 'Open-Vocabulary Manipulation', 'Multi-Level Aligned Visual Representation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a task success prediction model for open-vocabulary manipulation. The model focuses on the differences between multi-level aligned representations of images. It outperformed existing models, including representative multimodal LLMs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this study, we consider the problem of predicting task success for open-vocabulary manipulation by a manipulator, based on instruction sentences and egocentric images before and after manipulation. Conventional approaches, including multimodal large language models (MLLMs), often fail to appropriately understand detailed characteristics of objects and/or subtle changes in the position of objects. We propose Contrastive $\lambda$-Repformer, which predicts task success for table-top manipulation tasks by aligning images with instruction sentences. Our method integrates the following three key types of features into a multi-level aligned representation: features that preserve local image information; features aligned with natural language; and features structured through natural language. This allows the model to focus on important changes by looking at the differences in the representation between two images. We evaluate Contrastive $\lambda$-Repformer on a dataset based on a large-scale standard dataset, the RT-1 dataset, and on a physical robot platform. The results show that our approach outperformed existing approaches including MLLMs. Our best model achieved an improvement of 8.66 points in accuracy compared to the representative MLLM-based model.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d2bc0375e2b8ea495325d156e1c551027e9270e2.pdf" target="_blank">/pdf/d2bc0375e2b8ea495325d156e1c551027e9270e2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/fcdccef46710511cf035198d233ad70b123e6d01.zip" target="_blank">/attachment/fcdccef46710511cf035198d233ad70b123e6d01.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
goko2024task,
title={Task Success Prediction for Open-Vocabulary Manipulation Based on Multi-Level Aligned Representations},
author={Miyu Goko and Motonari Kambara and Daichi Saito and Seitaro Otsuki and Komei Sugiura},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=QtCtY8zl2T}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://5ei74r0.github.io/contrastive-lambda-repformer.page/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c47af76eaab5c6583cd257f78fba10f006bc661b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=QtCtY8zl2T&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">goko|task_success_prediction_for_openvocabulary_manipulation_based_on_multilevel_aligned_representations</div>
</div>
<div class='paper-counter'>137/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Qpjo8l8AFW</div>
<div class="field-name">title:</div>
<div class="field-value">Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tong Zhang', 'Yingdong Hu', 'Jiacheng You', 'Yang Gao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tong_Zhang23', '~Yingdong_Hu1', '~Jiacheng_You1', '~Yang_Gao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robotic Manipulation', 'Sample Efficiency']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias—$\textit{action locality}$, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/b4aa49f9012fe23380bf55e956086507bdb344bf.pdf" target="_blank">/pdf/b4aa49f9012fe23380bf55e956086507bdb344bf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/8a177dc3291f0480ac651b587bd91a02a8f4e771.zip" target="_blank">/attachment/8a177dc3291f0480ac651b587bd91a02a8f4e771.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024leveraging,
title={Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation},
author={Tong Zhang and Yingdong Hu and Jiacheng You and Yang Gao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Qpjo8l8AFW}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sgrv2-robot.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d1d44a7842861ec0f12f2f7091a0e4aa79965b18.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Qpjo8l8AFW&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|leveraging_locality_to_boost_sample_efficiency_in_robotic_manipulation</div>
</div>
<div class='paper-counter'>138/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Qoy12gkH4C</div>
<div class="field-name">title:</div>
<div class="field-value">Progressive Multi-Modal Fusion for Robust 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Rohit Mohan', 'Daniele Cattaneo', 'Florian Drews', 'Abhinav Valada']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Rohit_Mohan1', '~Daniele_Cattaneo1', 'florian.drews@de.bosch.com', '~Abhinav_Valada1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D Object Detection', 'Multimodal Learning', 'Self-Supervised Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-sensor fusion is crucial for accurate 3D object detection in autonomous driving, with cameras and LiDAR being the most commonly used sensors. However, existing methods perform sensor fusion in a single view by projecting features from both modalities either in Bird's Eye View (BEV) or Perspective View (PV), thus sacrificing complementary information such as height or geometric proportions.
To address this limitation, we propose ProFusion3D, a progressive fusion framework that combines features in both BEV and PV at both intermediate and object query levels. Our architecture hierarchically fuses local and global features, enhancing the robustness of 3D object detection. Additionally, we introduce a self-supervised mask modeling pre-training strategy to improve multi-modal representation learning and data efficiency through three novel objectives. Extensive experiments on nuScenes and Argoverse2 datasets conclusively demonstrate the efficacy of ProFusion3D. Moreover, ProFusion3D is robust to sensor failure, showing strong performance when only one modality is available.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0894ee9e5ffe102670cc7ad39c3f77de04154146.pdf" target="_blank">/pdf/0894ee9e5ffe102670cc7ad39c3f77de04154146.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/84d0f071fb9571bf71ec30ff71c769c084c95711.zip" target="_blank">/attachment/84d0f071fb9571bf71ec30ff71c769c084c95711.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
mohan2024progressive,
title={Progressive Multi-Modal Fusion for Robust 3D Object Detection},
author={Rohit Mohan and Daniele Cattaneo and Florian Drews and Abhinav Valada},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Qoy12gkH4C}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://profusion3d.cs.uni-freiburg.de/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0e33be34efe03ee10f2ac5a0069f66f7a738e17e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Qoy12gkH4C&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">mohan|progressive_multimodal_fusion_for_robust_3d_object_detection</div>
</div>
<div class='paper-counter'>139/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">QUzwHYJ9Hf</div>
<div class="field-name">title:</div>
<div class="field-value">Towards Open-World Grasping with Large Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Georgios Tziafas', 'Hamidreza Kasaei']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Georgios_Tziafas1', '~Hamidreza_Kasaei1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Foundation Models for Robotics', 'Open-World Grasping', 'Open-Ended23 Visual Grounding', 'Robot Planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel system formulation for open-world grasping, combining large LLMs as GPT-4v with segmentation and grasp synthesis models for open-ended grounding, planning and grasp ranking.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics.
An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios.
Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and action models to ground such knowledge into the environment and parameterize actuation.
This setup suffers from two major bottlenecks: a) the LLM's reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios.
In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. 
We propose \texttt{OWG}, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms.
We conduct extensive evaluation in cluttered indoor scene datasets to showcase \texttt{OWG}'s robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/544cc251f456a60d0ba5b4060561c507855c74c4.pdf" target="_blank">/pdf/544cc251f456a60d0ba5b4060561c507855c74c4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/010574e4a124d90f687efccc1a51a8cbf12f09be.zip" target="_blank">/attachment/010574e4a124d90f687efccc1a51a8cbf12f09be.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tziafas2024towards,
title={Towards Open-World Grasping with Large Vision-Language Models},
author={Georgios Tziafas and Hamidreza Kasaei},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=QUzwHYJ9Hf}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gtziafas.github.io/OWG_project/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/bcbbd1320ac349eb00181fc594d043ee5713daba.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=QUzwHYJ9Hf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tziafas|towards_openworld_grasping_with_large_visionlanguage_models</div>
</div>
<div class='paper-counter'>140/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Q2lGXMZCv8</div>
<div class="field-name">title:</div>
<div class="field-value">LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Dantong Niu', 'Yuvan Sharma', 'Giscard Biamby', 'Jerome Quenum', 'Yutong Bai', 'Baifeng Shi', 'Trevor Darrell', 'Roei Herzig']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Dantong_Niu1', '~Yuvan_Sharma1', '~Giscard_Biamby1', '~Jerome_Quenum1', '~Yutong_Bai1', '~Baifeng_Shi1', '~Trevor_Darrell2', '~Roei_Herzig2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['LMMs', 'Vision Action Instruction Tuning', 'Robot Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose LLARVA, a model trained with a novel instruction tuning method that leverages structured prompts to unify a range of robotic configurations and introduces the concept of visual traces to align the vision and action spaces further.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In recent years, instruction-tuned Large Multimodal Models (LMMs) have been successful at several tasks, including image captioning and visual question answering; yet leveraging these models remains an open question for robotics. Prior LMMs for robotics applications have been extensively trained on language and action data, but their ability to generalize in different settings has often been less than desired. To address this, we introduce LLARVA, a model trained with a novel instruction tuning method that leverages structured prompts to unify a range of robotic learning tasks, scenarios, and environments. Additionally, we show that predicting intermediate 2-D representations, which we refer to as *visual traces*, can help further align vision and action spaces for robot learning. We generate 8.5M image-visual trace pairs from the Open X-Embodiment dataset in order to pre-train our model, and we evaluate on 12 different tasks in the RLBench simulator as well as a physical Franka Emika Panda 7-DoF robot. Our experiments yield strong performance, demonstrating that LLARVA — using 2-D and language representations — performs well compared to several contemporary baselines, and can generalize across various robot environments and configurations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c3afc36a2cb813b5e423fbf963ff33581b37fa95.pdf" target="_blank">/pdf/c3afc36a2cb813b5e423fbf963ff33581b37fa95.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/322e279e5d2f7e09c2ff86b580ee37d7cf352373.zip" target="_blank">/attachment/322e279e5d2f7e09c2ff86b580ee37d7cf352373.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
niu2024llarva,
title={{LLARVA}: Vision-Action Instruction Tuning Enhances Robot Learning},
author={Dantong Niu and Yuvan Sharma and Giscard Biamby and Jerome Quenum and Yutong Bai and Baifeng Shi and Trevor Darrell and Roei Herzig},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Q2lGXMZCv8}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://llarva24.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/dbc52d5832d23f241c9077e6cc3d49af76f49a8e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Q2lGXMZCv8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">niu|llarva_visionaction_instruction_tuning_enhances_robot_learning</div>
</div>
<div class='paper-counter'>141/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">PbQOZntuXO</div>
<div class="field-name">title:</div>
<div class="field-value">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</div>
<div class="field-name">authors:</div>
<div class="field-value">['Nico Bohlinger', 'Grzegorz Czechmanowski', 'Maciej Piotr Krupka', 'Piotr Kicki', 'Krzysztof Walas', 'Jan Peters', 'Davide Tateo']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Nico_Bohlinger1', '~Grzegorz_Czechmanowski1', '~Maciej_Piotr_Krupka1', '~Piotr_Kicki1', '~Krzysztof_Walas2', '~Jan_Peters3', '~Davide_Tateo2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Locomotion', 'Reinforcement Learning', 'Multi-embodiment Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a neural network architecture that can learn locomotion over multiple legged robot embodiments and morphologies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion.
While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments.
To close this gap, we introduce URMA, the Unified Robot Morphology Architecture. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology.
The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a first step in building a foundation model for legged robot locomotion.
Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6bdf668a75313e227e98d732dcd3c05e71cbc950.pdf" target="_blank">/pdf/6bdf668a75313e227e98d732dcd3c05e71cbc950.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0e07d7c4aec25d5f53e05f2a93bb39ccff8ab039.zip" target="_blank">/attachment/0e07d7c4aec25d5f53e05f2a93bb39ccff8ab039.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
bohlinger2024one,
title={One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion},
author={Nico Bohlinger and Grzegorz Czechmanowski and Maciej Piotr Krupka and Piotr Kicki and Krzysztof Walas and Jan Peters and Davide Tateo},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=PbQOZntuXO}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://nico-bohlinger.github.io/one_policy_to_run_them_all_website/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5ce2fd1a5c627934af9a512bfd64cc35c0d0061a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=PbQOZntuXO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">bohlinger|one_policy_to_run_them_all_an_endtoend_learning_approach_to_multiembodiment_locomotion</div>
</div>
<div class='paper-counter'>142/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">PAtsxVz0ND</div>
<div class="field-name">title:</div>
<div class="field-value">ScissorBot: Learning Generalizable Scissor Skill for Paper Cutting via Simulation, Imitation, and Sim2Real</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jiangran Lyu', 'Yuxing Chen', 'Tao Du', 'Feng Zhu', 'Huiquan Liu', 'Yizhou Wang', 'He Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jiangran_Lyu2', '~Yuxing_Chen3', '~Tao_Du1', '~Feng_Zhu12', '~Huiquan_Liu1', '~Yizhou_Wang1', '~He_Wang5']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Deformable Object Manipulation', 'Imitation Learning', 'Sim-to-Real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">The first learning-based robotic paper-cutting system with scissors</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper tackles the challenging robotic task of generalizable paper cutting using scissors. 
In this task, scissors attached to a robot arm are driven to accurately cut curves drawn on the paper, which is hung with the top edge fixed. 
Due to the frequent paper-scissor contact and consequent fracture, the paper features continual deformation and changing topology, which is diffult for accurate modeling.To deal with such versatile scenarios, we propose ScissorBot, the first learning-based system for robotic paper cutting with scissors via simulation, imitation learning and sim2real. Given the lack of sufficient data for this task, we build PaperCutting-Sim, a paper simulator supporting interactive fracture coupling with scissors, enabling demonstration generation with a heuristic-based oracle policy. 
To ensure effective execution, we customize an action primitive sequence for imitation learning to constrain its action space, thus alleviating potential compounding errors.
Finally, by integrating sim-to-real techniques to bridge the gap between simulation and reality, our policy can be effectively deployed on the real robot.
Experimental results demonstrate that our method surpasses all baselines in both simulation and real-world benchmarks and achives performance comparable to human operation with a single hand under the same conditions.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a3e524e529ca74c6538f40fdbed08b6701715c12.pdf" target="_blank">/pdf/a3e524e529ca74c6538f40fdbed08b6701715c12.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2003474de3f513f90da0595c5f1f7d6cb7337561.zip" target="_blank">/attachment/2003474de3f513f90da0595c5f1f7d6cb7337561.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lyu2024scissorbot,
title={ScissorBot: Learning Generalizable Scissor Skill for Paper Cutting via Simulation, Imitation, and Sim2Real},
author={Jiangran Lyu and Yuxing Chen and Tao Du and Feng Zhu and Huiquan Liu and Yizhou Wang and He Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=PAtsxVz0ND}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://pku-epic.github.io/ScissorBot/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0d9ba76f21bc4918feeb686af2cc3c54feeb1822.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=PAtsxVz0ND&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lyu|scissorbot_learning_generalizable_scissor_skill_for_paper_cutting_via_simulation_imitation_and_sim2real</div>
</div>
<div class='paper-counter'>143/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">OznnnxPLiH</div>
<div class="field-name">title:</div>
<div class="field-value">JointMotion: Joint Self-Supervision for Joint Motion Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Royden Wagner', 'Omer Sahin Tas', 'Marvin Klemp', 'Carlos Fernandez']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Royden_Wagner1', '~Omer_Sahin_Tas1', '~Marvin_Klemp1', '~Carlos_Fernandez1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Self-supervised learning', 'representation learning', 'multimodal pre-training', 'motion prediction', 'data-efficient learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Self-supervised pre-training method for joint motion prediction in self-driving vehicles.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present JointMotion, a self-supervised pre-training method for joint motion prediction in self-driving vehicles. Our method jointly optimizes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Scene-level representations are learned via non-contrastive similarity learning of past motion sequences and environment context. At the instance level, we use masked autoencoding to refine multimodal polyline representations. We complement this with an adaptive pre-training decoder that enables JointMotion to generalize across different environment representations, fusion mechanisms, and dataset characteristics. Notably, our method reduces the joint final displacement error of Wayformer, HPTR, and Scene Transformer models by 3%, 8%, and 12%, respectively; and enables transfer learning between the Waymo Open Motion and the Argoverse 2 Motion Forecasting datasets.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2d89c6686c9939ed1963d3f49d2f36d649603ab0.pdf" target="_blank">/pdf/2d89c6686c9939ed1963d3f49d2f36d649603ab0.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wagner2024jointmotion,
title={JointMotion: Joint Self-Supervision for Joint Motion Prediction},
author={Royden Wagner and Omer Sahin Tas and Marvin Klemp and Carlos Fernandez},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=OznnnxPLiH}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/efc64c38bf259856e9eb49760f8aa35a5220cf59.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=OznnnxPLiH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wagner|jointmotion_joint_selfsupervision_for_joint_motion_prediction</div>
</div>
<div class='paper-counter'>144/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Oce2215aJE</div>
<div class="field-name">title:</div>
<div class="field-value">Body Transformer: Leveraging Robot Embodiment for Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Carmelo Sferrazza', 'Dun-Ming Huang', 'Fangchen Liu', 'Jongmin Lee', 'Pieter Abbeel']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Carmelo_Sferrazza1', '~Dun-Ming_Huang1', '~Fangchen_Liu2', '~Jongmin_Lee1', '~Pieter_Abbeel2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Learning', 'Graph Neural Networks', 'Imitation Learning', 'Reinforcement Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In recent years, the transformer architecture has become the de-facto standard for machine learning algorithms applied to natural language processing and computer vision. Despite notable evidence of successful deployment of this architecture in the context of robot learning, we claim that vanilla transformers do not fully exploit the structure of the robot learning problem. We propose Body Transformer (BoT), an architecture that exploits the robot embodiment by providing an inductive bias that guides the learning process. We represent the robot body as a graph of sensors and actuators, and rely on masked attention to pool information through the architecture. The resulting architecture outperforms the vanilla transformer, as well as the classical multilayer perceptron, with respect to task completion, scaling properties, and computational efficiency when representing either imitation or reinforcement learning policies.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/df86c0af235c5ca657b594dd6231f51809db7c27.pdf" target="_blank">/pdf/df86c0af235c5ca657b594dd6231f51809db7c27.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/b0dc20c7648624720a84fecee640f61b5652a8c4.zip" target="_blank">/attachment/b0dc20c7648624720a84fecee640f61b5652a8c4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sferrazza2024body,
title={Body Transformer: Leveraging Robot Embodiment for Policy Learning},
author={Carmelo Sferrazza and Dun-Ming Huang and Fangchen Liu and Jongmin Lee and Pieter Abbeel},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Oce2215aJE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sferrazza.cc/bot_site/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5551b527eb39189ae65efa8be022d4740e177c36.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Oce2215aJE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sferrazza|body_transformer_leveraging_robot_embodiment_for_policy_learning</div>
</div>
<div class='paper-counter'>145/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">OGjGtN6hoo</div>
<div class="field-name">title:</div>
<div class="field-value">Adaptive Language-Guided Abstraction from Contrastive Explanations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Andi Peng', 'Belinda Z. Li', 'Ilia Sucholutsky', 'Nishanth Kumar', 'Julie Shah', 'Jacob Andreas', 'Andreea Bobu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Andi_Peng1', '~Belinda_Z._Li1', '~Ilia_Sucholutsky1', '~Nishanth_Kumar1', '~Julie_Shah2', '~Jacob_Andreas1', '~Andreea_Bobu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['reward learning', 'language-guided abstraction', 'reward features']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an algorithm to iteratively specify missing features and then update the reward parameters for reward learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations.
To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward.
In particularly complex, high-dimensional environments, human demonstrators often struggle to fully specify their desired behavior from a small number of demonstrations.
End-to-end reward learning methods (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features.
By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest. 
How do we build robots that leverage this kind of background knowledge when learning from new demonstrations?
This paper describes a method named ALGAE which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features.
Experiments across a variety of both simulated and real-world robot environments show that ALGAElearns generalizable reward functions defined on interpretable features using only small numbers of demonstrations.
Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2c3dbfb8df7aff26a19165db6c557614bd550e53.pdf" target="_blank">/pdf/2c3dbfb8df7aff26a19165db6c557614bd550e53.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f846fbe2d170512ca5083aec675cf29cb6698635.zip" target="_blank">/attachment/f846fbe2d170512ca5083aec675cf29cb6698635.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
peng2024adaptive,
title={Adaptive Language-Guided Abstraction from Contrastive Explanations},
author={Andi Peng and Belinda Z. Li and Ilia Sucholutsky and Nishanth Kumar and Julie Shah and Jacob Andreas and Andreea Bobu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=OGjGtN6hoo}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ede67a79ee96fb79c85f9b06763d1b4247b3ad8b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=OGjGtN6hoo&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">peng|adaptive_languageguided_abstraction_from_contrastive_explanations</div>
</div>
<div class='paper-counter'>146/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">O0oK2bVist</div>
<div class="field-name">title:</div>
<div class="field-value">Adapting Humanoid Locomotion over Challenging Terrain via Two-Phase Training</div>
<div class="field-name">authors:</div>
<div class="field-value">['Wenhao Cui', 'Shengtao Li', 'Huaxing Huang', 'Bangyu Qin', 'Tianchu Zhang', 'hanjinchao', 'Liang Zheng', 'Ziyang Tang', 'Chenxu Hu', 'NING Yan', 'Jiahao Chen', 'Zheyuan Jiang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Wenhao_Cui2', '~Shengtao_Li3', '~Huaxing_Huang1', '~Bangyu_Qin1', '~Tianchu_Zhang1', '~hanjinchao1', '~Liang_Zheng7', '~Ziyang_Tang3', '~Chenxu_Hu1', '~NING_Yan2', '~Jiahao_Chen14', '~Zheyuan_Jiang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['humanoid robots', 'locomotion', 'reinforcement learning', 'curriculum', 'sim-to-real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A training framework that leverages a  2 phase reinforcement learning and command curriculum learning, also refining DreamWaQ for mitigating joint oscillations, culminating in the successful sim-to-real transfer.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humanoid robots are a key focus in robotics, with their capacity to navigate tough terrains being essential for many uses. While strides have been made, creating adaptable locomotion for complex environments is still tough. Recent progress in learning-based systems offers hope for robust legged locomotion, but challenges persist, such as tracking accuracy at high speeds and on uneven ground, and joint oscillations in actual robots.
    This paper proposes a novel training framework to address these challenges by employing a two-phase training paradigm with reinforcement learning. The proposed framework is further enhanced through the integration of command curriculum learning, refining the precision and adaptability of our approach. Additionally, we adapt DreamWaQ to our humanoid locomotion system and improve it to mitigate joint oscillations. Finally, we achieve the sim-to-real transfer of our method. A series of empirical results demonstrate the superior performance of our proposed method compared to state-of-the-art methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/aba46fa6f3ba16cd1dcdca0f8cac469c5f6e49a5.pdf" target="_blank">/pdf/aba46fa6f3ba16cd1dcdca0f8cac469c5f6e49a5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9d97097b3a83c622c1d8abaa8e782231ec3904be.zip" target="_blank">/attachment/9d97097b3a83c622c1d8abaa8e782231ec3904be.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cui2024adapting,
title={Adapting Humanoid Locomotion over Challenging Terrain via Two-Phase Training},
author={Wenhao Cui and Shengtao Li and Huaxing Huang and Bangyu Qin and Tianchu Zhang and hanjinchao and Liang Zheng and Ziyang Tang and Chenxu Hu and NING Yan and Jiahao Chen and Zheyuan Jiang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=O0oK2bVist}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/adapting-humanoid-locomotion/two-phase-training</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b6295e71e1adbeb1d95b70b1574f23b89151e034.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=O0oK2bVist&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cui|adapting_humanoid_locomotion_over_challenging_terrain_via_twophase_training</div>
</div>
<div class='paper-counter'>147/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">O05tIQt2d5</div>
<div class="field-name">title:</div>
<div class="field-value">TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Junli Ren', 'Yikai Liu', 'Yingru Dai', 'Junfeng Long', 'Guijin Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Junli_Ren1', '~Yikai_Liu2', '~Yingru_Dai1', '~Junfeng_Long1', '~Guijin_Wang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Navigation', 'Task Planning', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a legged navigation system that achieves open-world navigation surpassing limitations posed by  visual conditions or prior knowledge by integrating terrain, obstacle and proprioception.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Legged navigation is typically examined within open-world, off-road, and challenging environments. In these scenarios, estimating external disturbances requires a complex synthesis of multi-modal information. This underlines a major limitation in existing works that primarily focus on avoiding obstacles. In this work, we propose TOP-Nav, a novel legged navigation framework that integrates a comprehensive path planner with Terrain awareness, Obstacle avoidance and close-loop Proprioception. TOP-Nav underscores the synergies between vision and proprioception in both path and motion planning. Within the path planner, we present a terrain estimator that enables the robot to select waypoints on terrains with higher traversability while effectively avoiding obstacles. In the motion planning level, we construct a proprioception advisor from the learning-based locomotion controller to provide motion evaluations for the path planner. Based on the close-loop motion feedback, we offer online corrections for the vision-based terrain and obstacle estimations. Consequently, TOP-Nav achieves open-world navigation that the robot can handle terrains or disturbances beyond the distribution of prior knowledge and overcomes constraints imposed by visual conditions. Building upon extensive experiments conducted in both simulation and real-world environments, TOP-Nav demonstrates superior performance in open-world navigation compared to existing methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d52f2e2999bc2918949731159418233b864934c8.pdf" target="_blank">/pdf/d52f2e2999bc2918949731159418233b864934c8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d276b9c99edb409a38df56cc742bb099d3c5416a.zip" target="_blank">/attachment/d276b9c99edb409a38df56cc742bb099d3c5416a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ren2024topnav,
title={{TOP}-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation},
author={Junli Ren and Yikai Liu and Yingru Dai and Junfeng Long and Guijin Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=O05tIQt2d5}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://top-nav-legged.github.io/TOP-Nav-Legged-page/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7a49f935e188eba27ed4a084ac1512f2eff03542.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=O05tIQt2d5&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ren|topnav_legged_navigation_integrating_terrain_obstacle_and_proprioception_estimation</div>
</div>
<div class='paper-counter'>148/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">NiA8hVdDS7</div>
<div class="field-name">title:</div>
<div class="field-value">RoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics using Koopman Operator</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hemant Kumawat', 'Biswadeep Chakraborty', 'Saibal Mukhopadhyay']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hemant_Kumawat1', '~Biswadeep_Chakraborty1', '~Saibal_Mukhopadhyay2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Feature extraction', 'Task Feedback', 'Control']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing agents that can perform complex control tasks from high-dimensional observations is a core ability of autonomous agents that requires underlying robust task control policies and adapting the underlying visual representations to the task. Most existing policies need a lot of training samples and treat this problem from the lens of two-stage learning with a controller learned on top of pre-trained vision models. We approach this problem from the lens of Koopman theory and learn visual representations from robotic agents conditioned on specific downstream tasks in the context of learning stabilizing control for the agent. We introduce a Contrastive Spectral Koopman Embedding network that allows us to learn efficient linearized visual representations from the agent's visual data in a high dimensional latent space and utilizes reinforcement learning to perform off-policy control on top of the extracted representations with a linear controller. Our method enhances stability and control in gradient dynamics over time, significantly outperforming existing approaches by improving efficiency and accuracy in learning task policies over extended horizons.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ec552faa3de5c708883db5bd4f3b563c63d40f3a.pdf" target="_blank">/pdf/ec552faa3de5c708883db5bd4f3b563c63d40f3a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/bc4ffb3d2ce6cc0886f7510ec87037527709d169.zip" target="_blank">/attachment/bc4ffb3d2ce6cc0886f7510ec87037527709d169.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kumawat2024robokoop,
title={RoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics using Koopman Operator},
author={Hemant Kumawat and Biswadeep Chakraborty and Saibal Mukhopadhyay},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=NiA8hVdDS7}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ea1a421e2a747148aad8072fa2c01f249104d47f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=NiA8hVdDS7&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kumawat|robokoop_efficient_control_conditioned_representations_from_visual_input_in_robotics_using_koopman_operator</div>
</div>
<div class='paper-counter'>149/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">NCnplCf4wo</div>
<div class="field-name">title:</div>
<div class="field-value">Learning a Distributed Hierarchical Locomotion Controller for Embodied Cooperation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chuye Hong', 'Kangyao Huang', 'Huaping Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chuye_Hong1', '~Kangyao_Huang1', '~Huaping_Liu3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Cooperation', 'Locomotion', 'Hierarchical reinforcement learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we propose a distributed hierarchical locomotion control strategy for whole-body cooperation and demonstrate the potential for migration into large numbers of agents. Our method utilizes a hierarchical structure to break down complex tasks into smaller, manageable sub-tasks. By incorporating spatiotemporal continuity features, we establish the sequential logic necessary for causal inference and cooperative behaviour in sequential tasks, thereby facilitating efficient and coordinated control strategies. Through training within this framework, we demonstrate enhanced adaptability and cooperation, leading to superior performance in task completion compared to the original methods. Moreover, we construct a set of environments as the benchmark for embodied cooperation.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8601663ac8a83b1d78786a4ea33a826cb3a090d8.pdf" target="_blank">/pdf/8601663ac8a83b1d78786a4ea33a826cb3a090d8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/62113bbad3c43bde14b1e0052168a9042947422d.zip" target="_blank">/attachment/62113bbad3c43bde14b1e0052168a9042947422d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hong2024learning,
title={Learning a Distributed Hierarchical Locomotion Controller for Embodied Cooperation},
author={Chuye Hong and Kangyao Huang and Huaping Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=NCnplCf4wo}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://d-hrl.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f4a57e8a40274631cf51178f603b29f04452350d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=NCnplCf4wo&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hong|learning_a_distributed_hierarchical_locomotion_controller_for_embodied_cooperation</div>
</div>
<div class='paper-counter'>150/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">N5IS6DzBmL</div>
<div class="field-name">title:</div>
<div class="field-value">Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ruoxuan Feng', 'Di Hu', 'Wenke Ma', 'Xuelong Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ruoxuan_Feng1', '~Di_Hu1', '~Wenke_Ma1', '~Xuelong_Li2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-Sensory', 'Robotic Manipulation', 'Multi-Stage']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose MS-Bot, a stage-guided dynamic multi-sensory fusion method with coarse-to-fine stage understanding, which dynamically adjusts the priority of modalities based on the fine-grained state within the predicted current stage.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans possess a remarkable talent for flexibly alternating to different senses when interacting with the environment. Picture a chef skillfully gauging the timing of ingredient additions and controlling the heat according to the colors, sounds, and aromas, seamlessly navigating through every stage of the complex cooking process. This ability is founded upon a thorough comprehension of task stages, as achieving the sub-goal within each stage can necessitate the utilization of different senses. In order to endow robots with similar ability, we incorporate the task stages divided by sub-goals into the imitation learning process to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a stage-guided dynamic multi-sensory fusion method with coarse-to-fine stage understanding, which dynamically adjusts the priority of modalities based on the fine-grained state within the predicted current stage. We train a robot system equipped with visual, auditory, and tactile sensors to accomplish challenging robotic manipulation tasks: pouring and peg insertion with keyway. Experimental results indicate that our approach enables more effective and explainable dynamic fusion, aligning more closely with the human fusion process than existing methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6f482928dafcd9b6f5d21bb612bebc48dc11e831.pdf" target="_blank">/pdf/6f482928dafcd9b6f5d21bb612bebc48dc11e831.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/523a75a9876b440fc40eda87b9aa7aab57adfb22.zip" target="_blank">/attachment/523a75a9876b440fc40eda87b9aa7aab57adfb22.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
feng2024play,
title={Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation},
author={Ruoxuan Feng and Di Hu and Wenke Ma and Xuelong Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=N5IS6DzBmL}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gewu-lab.github.io/MS-Bot/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a9243cef764ade37a04a60dea86876188355f2ca.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=N5IS6DzBmL&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">feng|play_to_the_score_stageguided_dynamic_multisensory_fusion_for_robotic_manipulation</div>
</div>
<div class='paper-counter'>151/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">N1K4B8N3n1</div>
<div class="field-name">title:</div>
<div class="field-value">Scaling Safe Multi-Agent Control for Signal Temporal Logic Specifications</div>
<div class="field-name">authors:</div>
<div class="field-value">['Joe Eappen', 'Zikang Xiong', 'Dipam Patel', 'Aniket Bera', 'Suresh Jagannathan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Joe_Eappen2', '~Zikang_Xiong1', '~Dipam_Patel1', '~Aniket_Bera1', '~Suresh_Jagannathan1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-Robot Systems', 'Path Planning for Multiple Mobile Robots or Agents', 'Collision Avoidance', 'Hybrid Logical/Dynamical Planning and Verification', 'Deep Learning Methods']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Graphical Neural Networks (GNNs) can help scale Specification-driven control on Multi-agent systems beyond existing Mixed Integer Linear Programming (MILP)-based planners.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Existing methods for safe multi-agent control using logic specifications like Signal Temporal Logic (STL) often face scalability issues. This is because they rely either on single-agent perspectives or on Mixed Integer Linear Programming (MILP)-based planners, which are complex to optimize. These methods have proven to be computationally expensive and inefficient when dealing with a large number of agents. To address these limitations, we present a new scalable approach to multi-agent control in this setting. Our method treats the relationships between agents using a graph structure rather than in terms of a single-agent perspective. Moreover, it combines a multi-agent collision avoidance controller with a Graph Neural Network (GNN) based planner, models the system in a decentralized fashion, and trains on STL-based objectives to generate safe and efficient plans for multiple agents, thereby optimizing the satisfaction of complex temporal specifications while also facilitating multi-agent collision avoidance. Our experiments show that our approach significantly outperforms existing methods that use a state-of-the-art MILP-based planner in terms of scalability and performance.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d51e958117667bd52e2c45be7ef0c19b439e9a90.pdf" target="_blank">/pdf/d51e958117667bd52e2c45be7ef0c19b439e9a90.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9be50bb7d0ffe90905e8d4c036a141401be48e67.zip" target="_blank">/attachment/9be50bb7d0ffe90905e8d4c036a141401be48e67.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
eappen2024scaling,
title={Scaling Safe Multi-Agent Control for Signal Temporal Logic Specifications},
author={Joe Eappen and Zikang Xiong and Dipam Patel and Aniket Bera and Suresh Jagannathan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=N1K4B8N3n1}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://jeappen.github.io/mastl-gcbf-website/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3a6b27aabcafc528cbe7a700009a7b5a782cc581.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=N1K4B8N3n1&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">eappen|scaling_safe_multiagent_control_for_signal_temporal_logic_specifications</div>
</div>
<div class='paper-counter'>152/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">MyyZZAPgpy</div>
<div class="field-name">title:</div>
<div class="field-value">SHADOW: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer</div>
<div class="field-name">authors:</div>
<div class="field-value">['Marion Lepert', 'Ria Doshi', 'Jeannette Bohg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Marion_Lepert1', '~Ria_Doshi1', '~Jeannette_Bohg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Cross-embodiment learning', 'Imitation Learning', 'Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Shadow, an efficient data editing scheme for robust cross-embodiment learning from a source to a target robot. Shadow overlays composite segmentation masks on the input images to train and evaluate this policy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Data collection in robotics is spread across diverse hardware, and this variation will increase as new hardware is developed. Effective use of this growing body of data requires methods capable of learning from diverse robot embodiments. We consider the setting of training a policy using expert trajectories from a single robot arm (the source), and evaluating on a different robot arm for which no data was collected (the target). We present a data editing scheme termed Shadow, in which the robot during training and evaluation is replaced with a composite segmentation mask of the source and target robots. In this way, the input data distribution at train and test time match closely, enabling robust policy transfer to the new unseen robot while being far more data efficient than approaches that require co-training on large amounts of data from diverse embodiments. We demonstrate that an approach as simple as Shadow is effective both in simulation on varying tasks and robots, and on real robot hardware, where Shadow demonstrates over 2x improvement in success rate compared to the strongest baseline.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/68114dc2dadf430ba3d6a9cc5a4fe128f829d663.pdf" target="_blank">/pdf/68114dc2dadf430ba3d6a9cc5a4fe128f829d663.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/76358074c653b30c5d64d85a3681ff0a73af784a.zip" target="_blank">/attachment/76358074c653b30c5d64d85a3681ff0a73af784a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lepert2024shadow,
title={{SHADOW}: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer},
author={Marion Lepert and Ria Doshi and Jeannette Bohg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=MyyZZAPgpy}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://shadow-cross-embodiment.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b06562e9cabc4c0196ae1a32f2959d966b3a4ef5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=MyyZZAPgpy&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lepert|shadow_leveraging_segmentation_masks_for_crossembodiment_policy_transfer</div>
</div>
<div class='paper-counter'>153/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">MwZJ96Okl3</div>
<div class="field-name">title:</div>
<div class="field-value">Modeling Drivers’ Situational Awareness from Eye Gaze for Driving Assistance</div>
<div class="field-name">authors:</div>
<div class="field-value">['Abhijat Biswas', 'Pranay Gupta', 'Shreeya Khurana', 'David Held', 'Henny Admoni']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Abhijat_Biswas1', '~Pranay_Gupta1', 'srkhuran@andrew.cmu.edu', '~David_Held1', '~Henny_Admoni1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['driver awareness', 'driving assistance', 'situational awareness']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new protocol to record drivers' situational awareness, use it to collect a dataset, and build a predictive SA model</div>
<div class="field-name">abstract:</div>
<div class="field-value">Intelligent driving assistance can alert drivers to objects in their environment; however, such systems require a model of drivers' situational awareness (SA) (what aspects of the scene they are already aware of) to avoid unnecessary alerts. 
Moreover, collecting the data to train such an SA model is challenging: 
being an internal human cognitive state, driver SA is difficult to measure, and non-verbal signals such as eye gaze are some of the only outward manifestations of it. Traditional methods to obtain SA labels rely on probes that result in sparse, intermittent SA labels unsuitable for modeling a dense, temporally correlated process via machine learning. We propose a novel interactive labeling protocol that captures dense, continuous SA labels and use it to collect an object-level SA dataset in a VR driving simulator. Our dataset comprises 20 unique drivers' SA labels, driving data, and gaze (over 320 minutes of driving) which will be made public.
Additionally, we train an SA model from this data, formulating the object-level driver SA prediction problem as a semantic segmentation problem. Our formulation allows all objects in a scene at a timestep to be processed simultaneously, leveraging global scene context and local gaze-object relationships together.
Our experiments show that this formulation leads to improved performance over common sense baselines and prior art on the SA prediction task.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fb1762425c7799eb82b530dbce31165a82ebe04f.pdf" target="_blank">/pdf/fb1762425c7799eb82b530dbce31165a82ebe04f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/90439beb4cce41d508ed86c1d97a121e5c06afeb.zip" target="_blank">/attachment/90439beb4cce41d508ed86c1d97a121e5c06afeb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
biswas2024modeling,
title={Modeling Drivers{\textquoteright} Situational Awareness from Eye Gaze for Driving Assistance},
author={Abhijat Biswas and Pranay Gupta and Shreeya Khurana and David Held and Henny Admoni},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=MwZJ96Okl3}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://harplab.github.io/DriverSA</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/77cd14bd3b7bc438976bb09f571b3475a36f1f17.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=MwZJ96Okl3&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">biswas|modeling_drivers_situational_awareness_from_eye_gaze_for_driving_assistance</div>
</div>
<div class='paper-counter'>154/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">MsCbbIqHRA</div>
<div class="field-name">title:</div>
<div class="field-value">ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yaoyao Qian', 'Xupeng Zhu', 'Ondrej Biza', 'Shuo Jiang', 'Linfeng Zhao', 'Haojie Huang', 'Yu Qi', 'Robert Platt']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yaoyao_Qian1', '~Xupeng_Zhu1', '~Ondrej_Biza1', '~Shuo_Jiang1', '~Linfeng_Zhao1', '~Haojie_Huang1', '~Yu_Qi4', '~Robert_Platt1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robotic Grasping', 'Vision-Language Models', 'Language Conditioned Grasping']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We have developed ThinkGrasp, a plug-and-play vision-language grasping system for heavy clutter environment grasping strategies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic grasping in cluttered environments remains a significant challenge due to occlusions and complex object arrangements. We have developed ThinkGrasp, a plug-and-play vision-language grasping system that makes use of GPT-4o's advanced contextual reasoning for grasping strategies. ThinkGrasp can effectively identify and generate grasp poses for target objects, even when they are heavily obstructed or nearly invisible, by using goal-oriented language to guide the removal of obstructing objects. This approach progressively uncovers the target object and ultimately grasps it with a few steps and a high success rate. In both simulated and real experiments, ThinkGrasp achieved a high success rate and significantly outperformed state-of-the-art methods in heavily cluttered environments or with diverse unseen objects, demonstrating strong generalization capabilities.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/eb2dda0e9c9b0755449a6fe24e7cf0b738913063.pdf" target="_blank">/pdf/eb2dda0e9c9b0755449a6fe24e7cf0b738913063.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/fffa2f32e0bce3dd7856074a3ae35b5afd2e0ffb.zip" target="_blank">/attachment/fffa2f32e0bce3dd7856074a3ae35b5afd2e0ffb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
qian2024thinkgrasp,
title={ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter},
author={Yaoyao Qian and Xupeng Zhu and Ondrej Biza and Shuo Jiang and Linfeng Zhao and Haojie Huang and Yu Qi and Robert Platt},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=MsCbbIqHRA}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://h-freax.github.io/thinkgrasp_page/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/20ec103f36f95304ba30eb51ce90bd3690790e52.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=MsCbbIqHRA&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">qian|thinkgrasp_a_visionlanguage_system_for_strategic_part_grasping_in_clutter</div>
</div>
<div class='paper-counter'>155/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">MfuzopqVOX</div>
<div class="field-name">title:</div>
<div class="field-value">LiDARGrid: Self-supervised 3D Opacity Grid from LiDAR for Scene Forecasting</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chuanyu Pan', 'Aolin Xu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chuanyu_Pan1', '~Aolin_Xu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D perception', 'lidar', 'opacity grid', 'occupancy grid', 'neural rendering', 'self-supervised learning', 'mobile robot', 'autonomous driving']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Timely capturing the dense geometry of the surrounding scene with unlabeled LiDAR data is valuable but under-explored for mobile robotic applications. Its value lies in the huge amount of such unlabeled data, enabling self-supervised learning for various downstream tasks. Current dynamic 3D scene reconstruction approaches however heavily rely on data annotations to tackle the moving objects in the scene. In response, we present LiDARGrid, a 3D opacity grid representation instantly derived from LiDAR points, which captures the dense 3D scene and facilitates scene forecasting. Our method features a novel self-supervised neural volume densification procedure based on an autoencoder and differentiable volume rendering. Leveraging this representation, self-supervised scene forecasting can be performed. Our method is trained on NuScenes dataset for autonomous driving, and is evaluated by predicting future point clouds using the scene forecasting. It notably outperforms state-of-the-art methods in point cloud forecasting in all performance metrics. Beyond scene forecasting, our representation excels in supporting additional tasks such as moving region detection and depth completion, as shown by experiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/401f75be3317afdcf7f3a0ed8a8b9e983b30d602.pdf" target="_blank">/pdf/401f75be3317afdcf7f3a0ed8a8b9e983b30d602.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
pan2024lidargrid,
title={Li{DARG}rid: Self-supervised 3D Opacity Grid from Li{DAR} for Scene Forecasting},
author={Chuanyu Pan and Aolin Xu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=MfuzopqVOX}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ac504bf4395b8a22d4ff8d88926ea4a834fea412.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=MfuzopqVOX&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">pan|lidargrid_selfsupervised_3d_opacity_grid_from_lidar_for_scene_forecasting</div>
</div>
<div class='paper-counter'>156/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">MfIUKzihC8</div>
<div class="field-name">title:</div>
<div class="field-value">CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Luke Rowe', 'Roger Girgis', 'Anthony Gosselin', 'Bruno Carrez', 'Florian Golemo', 'Felix Heide', 'Liam Paull', 'Christopher Pal']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Luke_Rowe1', '~Roger_Girgis1', '~Anthony_Gosselin1', '~Bruno_Carrez1', '~Florian_Golemo1', '~Felix_Heide2', '~Liam_Paull1', '~Christopher_Pal1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['offline reinforcement learning', 'autonomous driving', 'simulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose reactive and controllable agents in a driving simulator using offline reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data are not reactive and hard to intuitively control. Existing approaches address these challenges by proposing methods that rely on heuristics or generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through a physics-enhanced Nocturne simulator to generate a diverse offline reinforcement learning dataset, annotated with various reward terms. We then train a return-conditioned multi-agent behaviour model that allows for fine-grained manipulation of agent behaviours by modifying the desired returns for the various reward components. This capability enables the generation of a wide range of driving behaviours beyond the scope of the initial dataset, including adversarial behaviours. We demonstrate that CtRL-Sim can generate diverse and realistic safety-critical scenarios while providing fine-grained control over agent behaviours.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f0911020539f35ff4c2c738d2fb2c8a38b429761.pdf" target="_blank">/pdf/f0911020539f35ff4c2c738d2fb2c8a38b429761.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0df2437b4276a4ab8834406524c180595d1db532.zip" target="_blank">/attachment/0df2437b4276a4ab8834406524c180595d1db532.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
rowe2024ctrlsim,
title={Ct{RL}-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning},
author={Luke Rowe and Roger Girgis and Anthony Gosselin and Bruno Carrez and Florian Golemo and Felix Heide and Liam Paull and Christopher Pal},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=MfIUKzihC8}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://montrealrobotics.ca/ctrlsim/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/147531dfa6f349c076e38b81d2bd76759b832bd7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=MfIUKzihC8&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">rowe|ctrlsim_reactive_and_controllable_driving_agents_with_offline_reinforcement_learning</div>
</div>
<div class='paper-counter'>157/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">M0JtsLuhEE</div>
<div class="field-name">title:</div>
<div class="field-value">T$^2$SQNet: A Recognition Model for Manipulating Partially Observed Transparent Tableware Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">['Young Hun Kim', 'Seungyeon Kim', 'Yonghyeon Lee', 'Frank C. Park']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Young_Hun_Kim1', '~Seungyeon_Kim2', '~Yonghyeon_Lee2', '~Frank_C._Park1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Transparent objects', 'Shape recognition', 'Object manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a novel framework for recognizing and manipulating partially observed transparent tableware objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recognizing and manipulating transparent tableware from partial view RGB image observations is made challenging by the difficulty in obtaining reliable depth measurements of transparent objects.  In this paper we present the Transparent Tableware SuperQuadric Network (T$^2$SQNet), a neural network model that leverages a family of newly extended deformable superquadrics to produce low-dimensional, instance-wise and accurate 3D geometric representations of transparent objects from partial views.  As a byproduct and contribution of independent interest, we also present TablewareNet, a publicly available toolset of seven parametrized shapes based on our extended deformable superquadrics, that can be used to generate new datasets of tableware objects of diverse shapes and sizes. Experiments with T$^2$SQNet trained with TablewareNet show that T$^2$SQNet outperforms existing methods in recognizing transparent objects, in some cases by significant margins, and can be effectively used in robotic applications like decluttering and target retrieval.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ae050addf0616c6b8c28f49f4962ad947d6476a0.pdf" target="_blank">/pdf/ae050addf0616c6b8c28f49f4962ad947d6476a0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e80a99dfd39e2c594f0a9f14d6306b41d996af0c.zip" target="_blank">/attachment/e80a99dfd39e2c594f0a9f14d6306b41d996af0c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kim2024tsqnet,
title={T\${\textasciicircum}2\${SQN}et: A Recognition Model for Manipulating Partially Observed Transparent Tableware Objects},
author={Young Hun Kim and Seungyeon Kim and Yonghyeon Lee and Frank C. Park},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=M0JtsLuhEE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://t2sqnet.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/97d9a3079f36084c71fa197ce3bab9385b242b1b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=M0JtsLuhEE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kim|t^2sqnet_a_recognition_model_for_manipulating_partially_observed_transparent_tableware_objects</div>
</div>
<div class='paper-counter'>158/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">M0Gv07MUMU</div>
<div class="field-name">title:</div>
<div class="field-value">Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">['Thomas Tian', 'Boyi Li', 'Xinshuo Weng', 'Yuxiao Chen', 'Edward Schmerling', 'Yue Wang', 'Boris Ivanovic', 'Marco Pavone']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Thomas_Tian1', '~Boyi_Li1', '~Xinshuo_Weng3', '~Yuxiao_Chen3', '~Edward_Schmerling1', '~Yue_Wang2', '~Boris_Ivanovic1', '~Marco_Pavone1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-modal LLM', 'Autonomous Driving', 'Representation Alignment']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM’s reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM’s reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by producing condensed and semantically enriched representations of the scene. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios. Additionally, our work highlights the importance of representation alignment and structured reasoning in sparking the common-sense reasoning capabilities of MM-LLMs for effective planning.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/49a78f45ba6be9ebe118f1857ac1688acd3940c3.pdf" target="_blank">/pdf/49a78f45ba6be9ebe118f1857ac1688acd3940c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cfcdc500195242eb15298d1969710be605fc48c5.zip" target="_blank">/attachment/cfcdc500195242eb15298d1969710be605fc48c5.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tian2024tokenize,
title={Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving},
author={Thomas Tian and Boyi Li and Xinshuo Weng and Yuxiao Chen and Edward Schmerling and Yue Wang and Boris Ivanovic and Marco Pavone},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=M0Gv07MUMU}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://thomasrantian.github.io/TOKEN_MM-LLM_for_AutoDriving/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/68440b910c782ec07607650b4e2229b26b679a11.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=M0Gv07MUMU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tian|tokenize_the_world_into_objectlevel_knowledge_to_address_longtail_events_in_autonomous_driving</div>
</div>
<div class='paper-counter'>159/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">LmOF7UAOZ7</div>
<div class="field-name">title:</div>
<div class="field-value">A Planar-Symmetric SO(3) Representation for Learning Grasp Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tianyi Ko', 'Takuya Ikeda', 'Hiroya Sato', 'Koichi Nishiwaki']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tianyi_Ko1', '~Takuya_Ikeda1', '~Hiroya_Sato1', '~Koichi_Nishiwaki1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Grasp Detection', 'Rotation Representation', 'Parallel Gripper']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel SO(3) representation that can parametrize a pair of planar-symmetric poses with a single parameter set by leveraging the 2D Bingham distribution.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Planar-symmetric hands, such as parallel grippers, are widely adopted in both research and industrial fields.
Their symmetry, however, introduces ambiguity and discontinuity in the SO(3) representation, which hinders both the training and inference of neural network-based grasp detectors.
We propose a novel SO(3) representation that can parametrize a pair of planar-symmetric poses with a single parameter set by leveraging the 2D Bingham distribution.
We also detail a grasp detector based on our representation, which provides a more consistent rotation output.
An intensive evaluation with multiple grippers and objects in both the simulation and the real world quantitatively shows our approach's contribution.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/905378c4e74a139f2e906debc6bad516f23bdf56.pdf" target="_blank">/pdf/905378c4e74a139f2e906debc6bad516f23bdf56.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0d80385f7583e3ae52cea6e9f849892c96188587.zip" target="_blank">/attachment/0d80385f7583e3ae52cea6e9f849892c96188587.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ko2024a,
title={A Planar-Symmetric {SO}(3) Representation for Learning Grasp Detection},
author={Tianyi Ko and Takuya Ikeda and Hiroya Sato and Koichi Nishiwaki},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=LmOF7UAOZ7}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d77d92bc6f59eb9023a554c98cf906c2a960da4f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=LmOF7UAOZ7&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ko|a_planarsymmetric_so3_representation_for_learning_grasp_detection</div>
</div>
<div class='paper-counter'>160/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Lixj7WEGEy</div>
<div class="field-name">title:</div>
<div class="field-value">MBC: Multi-Brain Collaborative Control for Quadruped Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hang Liu', 'Yi Cheng', 'Rankun Li', 'Xiaowen Hu', 'Linqi Ye', 'Houde Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hang_Liu9', '~Yi_Cheng7', '~Rankun_Li1', '~Xiaowen_Hu1', '~Linqi_Ye1', '~Houde_Liu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Quadruped Robots', 'Perception Fails', 'Multi-Brain Collaborative']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose the concept of Multi-Brain Collaborative Control based on Multi-Agent systems, establishing a quadruped robots training framework that achieves both perceptive motion and robust obstacle traversal in the event of perception failure .</div>
<div class="field-name">abstract:</div>
<div class="field-value">In the field of locomotion task of quadruped robots, Blind Policy and Perceptive Policy each have their own advantages and limitations. The Blind Policy relies on preset sensor information and algorithms, suitable for known and structured environments, but it lacks adaptability in complex or unknown environments. The Perceptive Policy uses visual sensors to obtain detailed environmental information, allowing it to adapt to complex terrains, but its effectiveness is limited under occluded conditions, especially when perception fails. Unlike the Blind Policy, the Perceptive Policy is not as robust under these conditions. To address these challenges, we propose a MBC:Multi-Brain collaborative system that incorporates the concepts of Multi-Agent Reinforcement Learning and introduces collaboration between the Blind Policy and the Perceptive Policy. By applying this multi-policy collaborative model to a quadruped robot, the robot can maintain stable locomotion even when the perceptual system is impaired or observational data is incomplete. Our simulations and real-world experiments demonstrate that this system significantly improves the robot's passability and robustness against perception failures in complex environments, validating the effectiveness of multi-policy collaboration in enhancing robotic motion performance.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/786134c4ac7f4da902ad9a43cc3465a913373b21.pdf" target="_blank">/pdf/786134c4ac7f4da902ad9a43cc3465a913373b21.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ff4af201a5def7f6c89f683192983da335a87638.zip" target="_blank">/attachment/ff4af201a5def7f6c89f683192983da335a87638.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024mbc,
title={{MBC}: Multi-Brain Collaborative Control for Quadruped Robots},
author={Hang Liu and Yi Cheng and Rankun Li and Xiaowen Hu and Linqi Ye and Houde Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Lixj7WEGEy}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://quad-mbc.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/905c02dad2d33248c548b1b6f6188e5e47c9377c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Lixj7WEGEy&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|mbc_multibrain_collaborative_control_for_quadruped_robots</div>
</div>
<div class='paper-counter'>161/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">LiwdXkMsDv</div>
<div class="field-name">title:</div>
<div class="field-value">Uncertainty-Aware Decision Transformer for Stochastic Driving Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zenan Li', 'Fan Nie', 'Qiao Sun', 'Fang Da', 'Hang Zhao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zenan_Li4', '~Fan_Nie1', '~Qiao_Sun1', '~Fang_Da2', '~Hang_Zhao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Self-Driving', 'Decision Transformer', 'Uncertainty-Aware Planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present UNREST, an uncertainty-aware decision transformer to apply offline RL in stochastic driving environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline Reinforcement Learning (RL) enables policy learning without active interactions, making it especially appealing for self-driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which, however, fails in stochastic environments with incorrect assumptions that identical actions can consistently achieve the same goal. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates uncertainties by conditional mutual information between transitions and returns. Discovering 'uncertainty accumulation' and 'temporal locality' properties of driving environments, we replace the global returns in decision transformers with truncated returns less affected by environments to learn from actual outcomes of actions rather than environment transitions. We also dynamically evaluate uncertainty at inference for cautious planning. Extensive experiments demonstrate UNREST's superior performance in various driving scenarios and the power of our uncertainty estimation strategy.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0c8ac3a5c5ddeb1bda9d8ca904f4f42af307a459.pdf" target="_blank">/pdf/0c8ac3a5c5ddeb1bda9d8ca904f4f42af307a459.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f6cf28b1e4448ce389d2afba5dad53cf28a6f38a.zip" target="_blank">/attachment/f6cf28b1e4448ce389d2afba5dad53cf28a6f38a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
li2024uncertaintyaware,
title={Uncertainty-Aware Decision Transformer for Stochastic Driving Environments},
author={Zenan Li and Fan Nie and Qiao Sun and Fang Da and Hang Zhao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=LiwdXkMsDv}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e2b1d9ed67922dc356364c39d502dd74123d682c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=LiwdXkMsDv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">li|uncertaintyaware_decision_transformer_for_stochastic_driving_environments</div>
</div>
<div class='paper-counter'>162/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">LZh48DTg71</div>
<div class="field-name">title:</div>
<div class="field-value">Evaluating Real-World Robot Manipulation Policies in Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xuanlin Li', 'Kyle Hsu', 'Jiayuan Gu', 'Oier Mees', 'Karl Pertsch', 'Homer Rich Walke', 'Chuyuan Fu', 'Ishikaa Lunawat', 'Isabel Sieh', 'Sean Kirmani', 'Sergey Levine', 'Jiajun Wu', 'Chelsea Finn', 'Hao Su', 'Quan Vuong', 'Ted Xiao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xuanlin_Li1', '~Kyle_Hsu1', '~Jiayuan_Gu1', '~Oier_Mees1', '~Karl_Pertsch1', '~Homer_Rich_Walke1', '~Chuyuan_Fu1', '~Ishikaa_Lunawat1', '~Isabel_Sieh1', '~Sean_Kirmani1', '~Sergey_Levine1', '~Jiajun_Wu1', '~Chelsea_Finn1', '~Hao_Su1', '~Quan_Vuong2', '~Ted_Xiao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['real-to-sim', 'policy evaluation', 'robot manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a suite of simulated environments for scalable evaluation of real-robot generalist policies, and show strong correlation to real robot evaluations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The field of robotics has made significant advances towards generalist robot manipulation policies. However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, issues that are likely to worsen as policies broaden the spectrum of tasks they can perform. In this work, we demonstrate that simulation-based evaluation can be a scalable, reproducible, and reliable proxy for real-world evaluation. We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to painstakingly craft full-fidelity digital twins. We then employ these techniques to create SIMPLER, a collection of simulated environments for policy evaluation on common real robot manipulation setups. Through over 1500 paired sim-and-real evaluations of manipulation policies across two embodiments and eight task families, we demonstrate strong correlation between policy performance in SIMPLER environments and that in the real world. Beyond aggregated trends, we find that SIMPLER evaluations effectively reflect the real-world behaviors of individual policies, such as sensitivity to various distribution shifts. We are committed to open-sourcing all SIMPLER environments along with our workflow for creating new environments to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks. Website: https://simpler-env.github.io/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/837c42918d6fa971e9576b51f2247c19c1fe7217.pdf" target="_blank">/pdf/837c42918d6fa971e9576b51f2247c19c1fe7217.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dd8cab45468d62e45732a3dac4967bb38219cf3d.zip" target="_blank">/attachment/dd8cab45468d62e45732a3dac4967bb38219cf3d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
li2024evaluating,
title={Evaluating Real-World Robot Manipulation Policies in Simulation},
author={Xuanlin Li and Kyle Hsu and Jiayuan Gu and Oier Mees and Karl Pertsch and Homer Rich Walke and Chuyuan Fu and Ishikaa Lunawat and Isabel Sieh and Sean Kirmani and Sergey Levine and Jiajun Wu and Chelsea Finn and Hao Su and Quan Vuong and Ted Xiao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=LZh48DTg71}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://simpler-env.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/cc2bda8bff5343db7145e4258039f036fd6c697b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=LZh48DTg71&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">li|evaluating_realworld_robot_manipulation_policies_in_simulation</div>
</div>
<div class='paper-counter'>163/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">L4p6zTlj6k</div>
<div class="field-name">title:</div>
<div class="field-value">TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jimmy Wu', 'William Chong', 'Robert Holmberg', 'Aaditya Prasad', 'Yihuai Gao', 'Oussama Khatib', 'Shuran Song', 'Szymon Rusinkiewicz', 'Jeannette Bohg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jimmy_Wu1', 'wmchong@stanford.edu', 'holmbergbob@gmail.com', '~Aaditya_Prasad2', '~Yihuai_Gao1', '~Oussama_Khatib1', '~Shuran_Song3', '~Szymon_Rusinkiewicz2', '~Jeannette_Bohg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['mobile manipulation', 'imitation learning', 'holonomic drive']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We design an open-source holonomic mobile manipulator and show that it is capable of performing a variety of household mobile manipulation tasks in a real apartment home.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Exploiting the promise of recent advances in imitation learning for mobile manipulation will require the collection of large numbers of human-guided demonstrations. This paper proposes an open-source design for an inexpensive, robust, and flexible mobile manipulator that can support arbitrary arms, enabling a wide range of real-world household mobile manipulation tasks. Crucially, our design uses powered casters to enable the mobile base to be fully holonomic, able to control all planar degrees of freedom independently and simultaneously. This feature makes the base more maneuverable and simplifies many mobile manipulation tasks, eliminating the kinematic constraints that create complex and time-consuming motions in nonholonomic bases. We equip our robot with an intuitive mobile phone teleoperation interface to enable easy data acquisition for imitation learning. In our experiments, we use this interface to collect data and show that the resulting learned policies can successfully perform a variety of common household mobile manipulation tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d13a516c5ddcd85843cde6f435afd16918b9b11d.pdf" target="_blank">/pdf/d13a516c5ddcd85843cde6f435afd16918b9b11d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/b38f10c809a116b09ddd9b9d62945e75f061ade4.zip" target="_blank">/attachment/b38f10c809a116b09ddd9b9d62945e75f061ade4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wu2024tidybot,
title={TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning},
author={Jimmy Wu and William Chong and Robert Holmberg and Aaditya Prasad and Yihuai Gao and Oussama Khatib and Shuran Song and Szymon Rusinkiewicz and Jeannette Bohg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=L4p6zTlj6k}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://tidybot2.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ad921bfd065bc016febbe23919ce22de55f79019.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=L4p6zTlj6k&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wu|tidybot_an_opensource_holonomic_mobile_manipulator_for_robot_learning</div>
</div>
<div class='paper-counter'>164/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Ke5xrnBFAR</div>
<div class="field-name">title:</div>
<div class="field-value">Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination</div>
<div class="field-name">authors:</div>
<div class="field-value">['Duy Phuong Nguyen', 'Kai-Chieh Hsu', 'Wenhao Yu', 'Jie Tan', 'Jaime Fernández Fisac']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Duy_Phuong_Nguyen2', '~Kai-Chieh_Hsu1', '~Wenhao_Yu1', '~Jie_Tan1', '~Jaime_Fernández_Fisac1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robust Safety', 'Adversarial Reinforcement Learning', 'Game Theory']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the impressive recent advances in learning-based robot control, ensuring robustness to out-of-distribution conditions remains an open challenge. Safety filters can, in principle, keep arbitrary control policies from incurring catastrophic failures by overriding unsafe actions, but existing solutions for complex (e.g., legged) robot dynamics do not span the full motion envelope and instead rely on local, reduced-order models. These filters tend to overly restrict agility and can still fail when perturbed away from nominal conditions. This paper presents the gameplay filter, a new class of predictive safety filter that continually plays out hypothetical matches between its simulation-trained safety strategy and a virtual adversary co-trained to invoke worst-case events and sim-to-real error, and precludes actions that would cause failures down the line. We demonstrate the scalability and robustness of the approach with a first-of-its-kind full-order safety filter for (36-D) quadrupedal dynamics. Physical experiments on two different quadruped platforms demonstrate the superior zero-shot effectiveness of the gameplay filter under large perturbations such as tugging and unmodeled terrain. Experiment videos and open-source software are available online: https://saferobotics.org/research/gameplay-filter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/42fa772763a7cdbe00b3d3173cdded60f3a29c8c.pdf" target="_blank">/pdf/42fa772763a7cdbe00b3d3173cdded60f3a29c8c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ec325a9bef6062658a6a63439d68e95c6736c58a.zip" target="_blank">/attachment/ec325a9bef6062658a6a63439d68e95c6736c58a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
nguyen2024gameplay,
title={Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination},
author={Duy Phuong Nguyen and Kai-Chieh Hsu and Wenhao Yu and Jie Tan and Jaime Fern{\'a}ndez Fisac},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Ke5xrnBFAR}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://saferobotics.org/research/gameplay-filter</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/54fd920252b5d9a8eab8f556d286956abe0ba3e9.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Ke5xrnBFAR&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">nguyen|gameplay_filters_robust_zeroshot_safety_through_adversarial_imagination</div>
</div>
<div class='paper-counter'>165/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KdVLK0Wo5z</div>
<div class="field-name">title:</div>
<div class="field-value">PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kuo-Hao Zeng', 'Zichen Zhang', 'Kiana Ehsani', 'Rose Hendrix', 'Jordi Salvador', 'Alvaro Herrasti', 'Ross Girshick', 'Aniruddha Kembhavi', 'Luca Weihs']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kuo-Hao_Zeng3', '~Zichen_Zhang2', '~Kiana_Ehsani1', '~Rose_Hendrix1', '~Jordi_Salvador3', '~Alvaro_Herrasti1', '~Ross_Girshick1', '~Aniruddha_Kembhavi1', '~Luca_Weihs1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Embodied Navigation', 'On-Policy RL', 'Transformer Policy']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present PoliFormer (Policy Transformer), an RGB-only indoor navigation agent trained end-to-end with reinforcement learning at scale that generalizes to the real-world without adaptation despite being trained purely in simulation. PoliFormer uses a foundational vision transformer encoder with a causal transformer decoder enabling long-term memory and reasoning. It is trained for hundreds of millions of interactions across diverse environments, leveraging parallelized, multi-machine rollouts for efficient training with high throughput. PoliFormer is a masterful navigator, producing state-of-the-art results across two distinct embodiments, the LoCoBot and Stretch RE-1 robots, and four navigation benchmarks. It breaks through the plateaus of previous work, achieving an unprecedented 85.5% success rate in object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement. PoliFormer can also be trivially extended to a variety of downstream applications such as object tracking, multi-object navigation, and open-vocabulary navigation with no finetuning.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/41cc0e05369ada8564105fcf87cb89ef2b0c67de.pdf" target="_blank">/pdf/41cc0e05369ada8564105fcf87cb89ef2b0c67de.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6d32fa27b6f91f01a53b225d0cf1274b50c9ea38.zip" target="_blank">/attachment/6d32fa27b6f91f01a53b225d0cf1274b50c9ea38.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zeng2024poliformer,
title={PoliFormer: Scaling On-Policy {RL} with Transformers Results in Masterful Navigators},
author={Kuo-Hao Zeng and Zichen Zhang and Kiana Ehsani and Rose Hendrix and Jordi Salvador and Alvaro Herrasti and Ross Girshick and Aniruddha Kembhavi and Luca Weihs},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KdVLK0Wo5z}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://poliformer.allen.ai</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2faeb729ea9adf35c3ee66cdea8f0bf201954ec5.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KdVLK0Wo5z&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zeng|poliformer_scaling_onpolicy_rl_with_transformers_results_in_masterful_navigators</div>
</div>
<div class='paper-counter'>166/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KcW31O0PtL</div>
<div class="field-name">title:</div>
<div class="field-value">Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kairui Ding', 'Boyuan Chen', 'Yuchen Su', 'Huan-ang Gao', 'Bu Jin', 'Chonghao Sima', 'Xiaohui Li', 'Wuqiang Zhang', 'Paul Barsch', 'Hongyang Li', 'Hao Zhao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kairui_Ding1', '~Boyuan_Chen5', '~Yuchen_Su4', '~Huan-ang_Gao1', '~Bu_Jin1', '~Chonghao_Sima1', '~Xiaohui_Li4', '~Wuqiang_Zhang1', '~Paul_Barsch1', '~Hongyang_Li1', '~Hao_Zhao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Interpretability', 'Language alignment', 'Autonomous driving']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Aligning natural language to intermediate states of autonomous driving model for aligned interpretability and state-of-the-art captioning accuracy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">End-to-end architectures in autonomous driving (AD) face a significant challenge in interpretability, impeding human-AI trust. Human-friendly natural language has been explored for tasks such as driving explanation and 3D captioning. However, previous works primarily focused on the paradigm of declarative interpretability, where the natural language interpretations are not grounded in the intermediate outputs of AD systems, making the interpretations only declarative. In contrast, aligned interpretability establishes a connection between language and the intermediate outputs of AD systems. Here we introduce Hint-AD, an integrated AD-language system that generates language aligned with the holistic perception-prediction-planning outputs of the AD model. By incorporating the intermediate outputs and a holistic token mixer sub-network for effective feature adaptation, Hint-AD achieves desirable accuracy, achieving state-of-the-art results in driving language tasks including driving explanation, 3D dense captioning, and command prediction. To facilitate further study on driving explanation task on nuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and models are publicly available at https://anonymous.4open.science/r/Hint-AD-1385/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f0ef48e8ad5794c10e6b7e7aa49a97245de2a6aa.pdf" target="_blank">/pdf/f0ef48e8ad5794c10e6b7e7aa49a97245de2a6aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/71f23fe2a4e89342f15378ddd076a1cc61c1705a.zip" target="_blank">/attachment/71f23fe2a4e89342f15378ddd076a1cc61c1705a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ding2024hintad,
title={Hint-{AD}: Holistically Aligned Interpretability in End-to-End Autonomous Driving},
author={Kairui Ding and Boyuan Chen and Yuchen Su and Huan-ang Gao and Bu Jin and Chonghao Sima and Xiaohui Li and Wuqiang Zhang and Paul Barsch and Hongyang Li and Hao Zhao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KcW31O0PtL}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://air-discover.github.io/Hint-AD/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1c0528f23042aa5938e7107d3698be5adddb8a80.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KcW31O0PtL&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ding|hintad_holistically_aligned_interpretability_in_endtoend_autonomous_driving</div>
</div>
<div class='paper-counter'>167/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KXsropnmNI</div>
<div class="field-name">title:</div>
<div class="field-value">Transferable Tactile Transformers for Representation Learning Across Diverse Sensors and Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jialiang Zhao', 'Yuxiang Ma', 'Lirui Wang', 'Edward Adelson']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jialiang_Zhao1', '~Yuxiang_Ma1', '~Lirui_Wang1', '~Edward_Adelson1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Tactile Sensing', 'Representation Learning', 'Heterogeneous Learning', 'Robot Manipulation', 'Robot Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A framework that learns a tactile representation that transfers between diverse sensors and tasks, and an aggregated tactile dataset that is the largest and most diverse to date.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents T3: Transferable Tactile Transformers, a framework for tactile representation learning that scales across multi-sensors and multi-tasks.T3 is designed to overcome the contemporary issue that camera-based tactile sensing is extremely heterogeneous, i.e. sensors are built into different form factors, and existing datasets were collected for disparate tasks. T3 captures the shared latent information across different sensor-task pairings by constructing a shared trunk transformer with sensor-specific encoders and task-specific decoders. The pre-training of T3utilizes a novel Foundation Tactile (FoTa) dataset, which is aggregated from several open-sourced datasets and it contains over 3 million data points gathered from 13 sensors and 11 tasks. FoTa is the largest and most diverse dataset in tactile sensing to date and it is made publicly available in a unified format. Across various sensors and tasks, experiments show that T3 pre-trained with FoTa achieved zero-shot transferability in certain sensor-task pairings, can be further fine-tuned with small amounts of domain-specific data, and its performance scales with bigger network sizes. T3 is also effective as a tactile encoder for long horizon contact-rich manipulation. Results from sub-millimeter multi-pin electronics insertion tasks show that T3 achieved a task success rate 25% higher than that of policies trained with tactile encoders trained from scratch, or 53% higher than without tactile sensing. Data, code, and model checkpoints are open-sourced at https://t3.alanz.info.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/cb667086993941a09ea1f2f0cac5a23852224983.pdf" target="_blank">/pdf/cb667086993941a09ea1f2f0cac5a23852224983.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/5f62a557eb107ca7de1751beb7282546a2b7f99c.zip" target="_blank">/attachment/5f62a557eb107ca7de1751beb7282546a2b7f99c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhao2024transferable,
title={Transferable Tactile Transformers for Representation Learning Across Diverse Sensors and Tasks},
author={Jialiang Zhao and Yuxiang Ma and Lirui Wang and Edward Adelson},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KXsropnmNI}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://t3.alanz.info/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/02a7d4485c97cf0c85dbbba0827ed01dcd47b11c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KXsropnmNI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhao|transferable_tactile_transformers_for_representation_learning_across_diverse_sensors_and_tasks</div>
</div>
<div class='paper-counter'>168/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KULBk5q24a</div>
<div class="field-name">title:</div>
<div class="field-value">CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jan Blumenkamp', 'Steven Morad', 'Jennifer Gielis', 'Amanda Prorok']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jan_Blumenkamp1', '~Steven_Morad1', 'jag233@cl.cam.ac.uk', '~Amanda_Prorok1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-Robot Systems', 'Robot Perception', 'Foundation Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a decentralized, platform-agnostic visual spatial foundation model for multi-robot systems, enhancing spatial understanding through data-derived priors for real-world, online deployment.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous robot operation in unstructured environments is often underpinned by spatial understanding through vision. Systems composed of multiple concurrently operating robots additionally require access to frequent, accurate and reliable pose estimates. Classical vision-based methods to regress relative pose are commonly computationally expensive (precluding real-time applications), and often lack data-derived priors for resolving ambiguities. In this work, we propose CoViS-Net, a cooperative, multi-robot visual spatial foundation model that learns spatial priors from data, enabling pose estimation as well as general spatial comprehension. Our model is fully decentralized, platform-agnostic, executable in real-time using onboard compute, and does not require existing networking infrastructure. CoViS-Net provides relative pose estimates and a local bird's-eye-view (BEV) representation, even without camera overlap between robots, and can predict BEV representations of unseen regions. We demonstrate its use in a multi-robot formation control task across various real-world settings. We provide supplementary material online and will open source our trained model in due course.
https://sites.google.com/view/covis-net</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c9d0e95ae275d094edd6743c3c72c293f878277d.pdf" target="_blank">/pdf/c9d0e95ae275d094edd6743c3c72c293f878277d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/7c08defc300ff8ecaee1f260c82fc05ea9508379.zip" target="_blank">/attachment/7c08defc300ff8ecaee1f260c82fc05ea9508379.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
blumenkamp2024covisnet,
title={CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications},
author={Jan Blumenkamp and Steven Morad and Jennifer Gielis and Amanda Prorok},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KULBk5q24a}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://proroklab.github.io/CoViS-Net/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ba5dfb9da61bd4a5f2ae3b91abfea2385c4f59ca.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KULBk5q24a&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">blumenkamp|covisnet_a_cooperative_visual_spatial_foundation_model_for_multirobot_applications</div>
</div>
<div class='paper-counter'>169/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KPcX4jetMw</div>
<div class="field-name">title:</div>
<div class="field-value">Reasoning Grasping via Multimodal Large Language Model</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shiyu Jin', 'JINXUAN XU', 'Yutian Lei', 'Liangjun Zhang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shiyu_Jin1', '~JINXUAN_XU2', '~Yutian_Lei1', '~Liangjun_Zhang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robotics Grasping', 'Multimodal Large Language Model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a grasping dataset and a Large Language Model to generate grasping poses based on implicit human instructions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite significant progress in robotic systems for operation within human-centric environments, existing models still heavily rely on explicit human commands to identify and manipulate specific objects. This limits their effectiveness in environments where understanding and acting on implicit human intentions are crucial. In this study, we introduce a novel task: reasoning grasping, where robots need to generate grasp poses based on indirect verbal instructions or intentions. To accomplish this, we propose an end-to-end reasoning grasping model that integrates a multimodal Large Language Model (LLM) with a vision-based robotic grasping framework. In addition, we present the first reasoning grasping benchmark dataset generated from the GraspNet-1 billion, incorporating implicit instructions for object-level and part-level grasping, and this dataset will soon be available for public access. Our results show that directly integrating CLIP or LLaVA with the grasp detection model performs poorly on the challenging reasoning grasping tasks, while our proposed model demonstrates significantly enhanced performance both in the reasoning grasping benchmark and real-world experiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f0b43ae58efcd8a8edf99ea42884598ddcf43995.pdf" target="_blank">/pdf/f0b43ae58efcd8a8edf99ea42884598ddcf43995.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dac505c3a8b0c884d13ada8cc4f6cbec84316a14.zip" target="_blank">/attachment/dac505c3a8b0c884d13ada8cc4f6cbec84316a14.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jin2024reasoning,
title={Reasoning Grasping via Multimodal Large Language Model},
author={Shiyu Jin and JINXUAN XU and Yutian Lei and Liangjun Zhang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KPcX4jetMw}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2eb10a851eea85d35ed99c2db1a8da61628ae11f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KPcX4jetMw&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jin|reasoning_grasping_via_multimodal_large_language_model</div>
</div>
<div class='paper-counter'>170/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">KAzku0Uyh1</div>
<div class="field-name">title:</div>
<div class="field-value">Object-Centric Dexterous Manipulation from Human Motion Data</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yuanpei Chen', 'Chen Wang', 'Yaodong Yang', 'Karen Liu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yuanpei_Chen2', '~Chen_Wang16', '~Yaodong_Yang1', '~Karen_Liu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous Manipulation', 'Reinforcement Learning', 'Learning from Human']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a hierarchical framework that uses human hand motion data and deep reinforcement learning to train dexterous robot hands for effective object-centric manipulation in both simulation and real world.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1f21b69d04be8f7711e083e6d6f95adad5068ab0.pdf" target="_blank">/pdf/1f21b69d04be8f7711e083e6d6f95adad5068ab0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/1d0507fff1c1b4678328df57a197e8dafc9304df.zip" target="_blank">/attachment/1d0507fff1c1b4678328df57a197e8dafc9304df.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024objectcentric,
title={Object-Centric Dexterous Manipulation from Human Motion Data},
author={Yuanpei Chen and Chen Wang and Yaodong Yang and Karen Liu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=KAzku0Uyh1}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://cypypccpy.github.io/obj-dex.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/db25f4b8116be62cc1d0cbf8507dba34eb932dae.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=KAzku0Uyh1&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|objectcentric_dexterous_manipulation_from_human_motion_data</div>
</div>
<div class='paper-counter'>171/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">JZzaRY8m8r</div>
<div class="field-name">title:</div>
<div class="field-value">KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jingxian Lu', 'Wenke Xia', 'Dong Wang', 'Zhigang Wang', 'Bin Zhao', 'Di Hu', 'Xuelong Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jingxian_Lu1', '~Wenke_Xia1', '~Dong_Wang1', '~Zhigang_Wang3', '~Bin_Zhao7', '~Di_Hu1', '~Xuelong_Li2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Online Imitation Learning; Robotic Manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation.
    Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of "what to do" and the mechanisms of "how to do".
    In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation.
    Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of "what to do". 
    Within the intervals between semantic key states, optical flow is employed to capture motion key states to understand the process of "how to do".
    By integrating a thorough grasp of both semantic and motion key states, we refine the trajectory-matching reward computation, encouraging task-aware exploration for efficient online imitation learning.
    Our experiment results prove that our method is more sample efficient than previous state-of-the-art approaches in the Meta-World and LIBERO environments. We also conduct real-world robotic manipulation experiments to validate the efficacy of our method, demonstrating the practical applicability of our KOI method.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/131ab1f1089f2a7777e22613873893486a418850.pdf" target="_blank">/pdf/131ab1f1089f2a7777e22613873893486a418850.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/338e0703da548da98aea7ccb4944fc140bfec3bd.zip" target="_blank">/attachment/338e0703da548da98aea7ccb4944fc140bfec3bd.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lu2024koi,
title={{KOI}: Accelerating Online Imitation Learning via Hybrid Key-state Guidance},
author={Jingxian Lu and Wenke Xia and Dong Wang and Zhigang Wang and Bin Zhao and Di Hu and Xuelong Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=JZzaRY8m8r}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gewu-lab.github.io/Keystate_Online_Imitation/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/cc53bc59e08df970c23a7f7bfe79ea21841e2230.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=JZzaRY8m8r&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lu|koi_accelerating_online_imitation_learning_via_hybrid_keystate_guidance</div>
</div>
<div class='paper-counter'>172/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">JScswMfEQ0</div>
<div class="field-name">title:</div>
<div class="field-value">Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhuo Xu', 'Hao-Tien Lewis Chiang', 'Zipeng Fu', 'Mithun George Jacob', 'Tingnan Zhang', 'Tsang-Wei Edward Lee', 'Wenhao Yu', 'Connor Schenck', 'David Rendleman', 'Dhruv Shah', 'Fei Xia', 'Jasmine Hsu', 'Jonathan Hoech', 'Pete Florence', 'Sean Kirmani', 'Sumeet Singh', 'Vikas Sindhwani', 'Carolina Parada', 'Chelsea Finn', 'Peng Xu', 'Sergey Levine', 'Jie Tan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhuo_Xu1', '~Hao-Tien_Lewis_Chiang1', '~Zipeng_Fu1', 'mithunjacob@google.com', '~Tingnan_Zhang1', '~Tsang-Wei_Edward_Lee1', '~Wenhao_Yu1', '~Connor_Schenck2', 'drendleman@google.com', '~Dhruv_Shah1', '~Fei_Xia1', '~Jasmine_Hsu1', 'jhoech@google.com', '~Pete_Florence1', '~Sean_Kirmani1', '~Sumeet_Singh3', '~Vikas_Sindhwani1', '~Carolina_Parada2', '~Chelsea_Finn1', '~Peng_Xu9', '~Sergey_Levine1', '~Jie_Tan1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['vision-language navigation', 'multimodal foundation models', 'long-context reasoning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">long context VLM unlocks unprecedented capability for multimodal instruction guided navigation</div>
<div class="field-name">abstract:</div>
<div class="field-value">An elusive goal in navigation research is to build an intelligent agent that can understand multimodal instructions including natural language and image, and perform useful navigation. To achieve this, we study a widely useful category of navigation tasks we call Multimodal Instruction Navigation with demonstration Tours (MINT), in which the environment prior is provided through a previously recorded demonstration video. Recent advances in Vision Language Models (VLMs) have shown a promising path in achieving this goal as it demonstrates capabilities in perceiving and reasoning about multimodal inputs. However, VLMs are typically trained to predict textual output and it is an open research question about how to best utilize them in navigation. To solve MINT, we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation policy that combines the environment understanding and common sense reasoning power of long-context VLMs and a robust low-level navigation policy based on topological graphs. The high-level policy consists of a long-context VLM that takes the demonstration tour video and the multimodal user instruction as input to find the goal frame in the tour video. Next, a low-level policy uses the goal frame and an offline constructed topological graph to generate robot actions at every timestep. We evaluated Mobility VLA in a 836$m^2$ real world environment and show that Mobility VLA has a high end-to-end success rates on previously unsolved multimodal instructions such as ``Where should I return this?'' while holding a plastic bin.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6d2b0d75745ba5715024ae4b93125d7cc4f0fbbb.pdf" target="_blank">/pdf/6d2b0d75745ba5715024ae4b93125d7cc4f0fbbb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/248bc3dc823f65b06741954366209c6b479d4632.zip" target="_blank">/attachment/248bc3dc823f65b06741954366209c6b479d4632.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xu2024mobility,
title={Mobility {VLA}: Multimodal Instruction Navigation with Long-Context {VLM}s and Topological Graphs},
author={Zhuo Xu and Hao-Tien Lewis Chiang and Zipeng Fu and Mithun George Jacob and Tingnan Zhang and Tsang-Wei Edward Lee and Wenhao Yu and Connor Schenck and David Rendleman and Dhruv Shah and Fei Xia and Jasmine Hsu and Jonathan Hoech and Pete Florence and Sean Kirmani and Sumeet Singh and Vikas Sindhwani and Carolina Parada and Chelsea Finn and Peng Xu and Sergey Levine and Jie Tan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=JScswMfEQ0}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/79fcf83c3ecd866da8fe306bdb68fd78180f23f7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=JScswMfEQ0&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xu|mobility_vla_multimodal_instruction_navigation_with_longcontext_vlms_and_topological_graphs</div>
</div>
<div class='paper-counter'>173/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">IssXUYvVTg</div>
<div class="field-name">title:</div>
<div class="field-value">MaIL: Improving Imitation Learning with Selective State Space Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xiaogang Jia', 'Qian Wang', 'Atalay Donat', 'Bowen Xing', 'Ge Li', 'Hongyi Zhou', 'Onur Celik', 'Denis Blessing', 'Rudolf Lioutikov', 'Gerhard Neumann']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xiaogang_Jia1', '~Qian_Wang34', '~Atalay_Donat1', '~Bowen_Xing2', '~Ge_Li3', '~Hongyi_Zhou1', '~Onur_Celik1', '~Denis_Blessing1', '~Rudolf_Lioutikov1', '~Gerhard_Neumann2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Sequence Models', 'Denoising Diffusion Policies']</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work introduces Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies. Transformer-based policies have achieved remarkable results due to their ability in handling human-recorded data with inherently non-Markovian behavior. However, their high performance comes with the drawback of large models that complicate effective training. While state space models (SSMs) have been known for their efficiency, they were not able to match the performance of Transformers. Mamba significantly improves the performance of SSMs and rivals against Transformers, positioning it as an appealing alternative for IL policies. MaIL leverages Mamba as a backbone and introduces a formalism that allows using Mamba in the encoder-decoder structure. This formalism makes it a versatile architecture that can be used as a standalone policy or as part of a more advanced architecture, such as a diffuser in the diffusion process. Extensive evaluations on the LIBERO IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good performance even with small datasets, iii) is able to effectively process multi-modal sensory inputs, iv) is more robust to input noise compared to Transformers.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c7cbc7192c69124836cc236d6905a3bb6643df0f.pdf" target="_blank">/pdf/c7cbc7192c69124836cc236d6905a3bb6643df0f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dcd2dfb8b2d78a28e5f0978a3d7dc1ec03c89a83.zip" target="_blank">/attachment/dcd2dfb8b2d78a28e5f0978a3d7dc1ec03c89a83.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jia2024mail,
title={Ma{IL}: Improving Imitation Learning with Selective State Space Models},
author={Xiaogang Jia and Qian Wang and Atalay Donat and Bowen Xing and Ge Li and Hongyi Zhou and Onur Celik and Denis Blessing and Rudolf Lioutikov and Gerhard Neumann},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=IssXUYvVTg}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1b46bc6a696c2cb50a503c038127b9ca74942597.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=IssXUYvVTg&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jia|mail_improving_imitation_learning_with_selective_state_space_models</div>
</div>
<div class='paper-counter'>174/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Isp19rFFV4</div>
<div class="field-name">title:</div>
<div class="field-value">Multi-Strategy Deployment-Time Learning and Adaptation for Navigation under Uncertainty</div>
<div class="field-name">authors:</div>
<div class="field-value">['Abhishek Paudel', 'Xuesu Xiao', 'Gregory J. Stein']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Abhishek_Paudel1', '~Xuesu_Xiao1', '~Gregory_J._Stein1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['policy selection', 'domain adaptation', 'navigation under uncertainty']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present an approach that enables a robot to deploy multiple learning/adaptation strategies during deployment and pick the best one.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present an approach for performant point-goal navigation in unfamiliar partially-mapped environments. When deployed, our robot runs multiple strategies for deployment-time learning and visual domain adaptation in parallel and quickly selects the best-performing among them. Choosing between policies as they are learned or adapted between navigation trials requires continually updating estimates of their performance as they evolve. Leveraging recent work in model-based learning-informed planning under uncertainty, we determine lower bounds on the would-be performance of newly-updated policies on old trials without needing to re-deploy them. This information constrains and accelerates bandit-like policy selection, affording quick selection of the best-performing strategy shortly after it would start to yield good performance. We validate the effectiveness of our approach in simulated maze-like environments, showing improved navigation cost and cumulative regret versus existing baselines.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d7056fcbfd7b8e2e3ec05b8a95f9b06cd3204764.pdf" target="_blank">/pdf/d7056fcbfd7b8e2e3ec05b8a95f9b06cd3204764.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cc93d8da0b89fd7a5983dd9c914af1412e67221e.zip" target="_blank">/attachment/cc93d8da0b89fd7a5983dd9c914af1412e67221e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
paudel2024multistrategy,
title={Multi-Strategy Deployment-Time Learning and Adaptation for Navigation under Uncertainty},
author={Abhishek Paudel and Xuesu Xiao and Gregory J. Stein},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Isp19rFFV4}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/9de6cf6dab77597f8d84d885803bb359242bad2c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Isp19rFFV4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">paudel|multistrategy_deploymenttime_learning_and_adaptation_for_navigation_under_uncertainty</div>
</div>
<div class='paper-counter'>175/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">IsZb0wT3Kw</div>
<div class="field-name">title:</div>
<div class="field-value">ANAVI: Audio Noise Awareness using Visual of Indoor environments for NAVIgation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Vidhi Jain', 'Rishi Veerapaneni', 'Yonatan Bisk']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Vidhi_Jain2', '~Rishi_Veerapaneni1', '~Yonatan_Bisk1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robots', 'Acoustic Noise', 'Vision', 'Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Acoustic Noise Predictor (ANP) that learns how "loud" the robot’s actions will be for a listener in a home or an office.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for quieter robot path planning.  While humans are naturally aware of the noise they make and its impact on those around them, robots currently lack this awareness. 
A key challenge in achieving audio awareness for robots is estimating how loud will the robot’s actions be at a listener’s location? Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments. To this end, we generate data on how loud an `impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Predictor (ANP). Next, we collect acoustic profiles corresponding to different actions for navigation. Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment.  All simulated and real-world data, code and model checkpoints is released at https://anavi-corl24.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c2372d11398e849843d541f74812a4fdef94c598.pdf" target="_blank">/pdf/c2372d11398e849843d541f74812a4fdef94c598.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e7651262222e374585c3632eee7b3d89343801c5.zip" target="_blank">/attachment/e7651262222e374585c3632eee7b3d89343801c5.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
jain2024anavi,
title={{ANAVI}: Audio Noise Awareness using Visual of Indoor environments for {NAVI}gation},
author={Vidhi Jain and Rishi Veerapaneni and Yonatan Bisk},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=IsZb0wT3Kw}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://anavi-corl24.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c994cd93662075421a73331a8793bd6bedc45d85.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=IsZb0wT3Kw&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">jain|anavi_audio_noise_awareness_using_visual_of_indoor_environments_for_navigation</div>
</div>
<div class='paper-counter'>176/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">InT87E5sr4</div>
<div class="field-name">title:</div>
<div class="field-value">Dreamitate: Real-World Visuomotor Policy Learning via Video Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Junbang Liang', 'Ruoshi Liu', 'Ege Ozguroglu', 'Sruthi Sudhakar', 'Achal Dave', 'Pavel Tokmakov', 'Shuran Song', 'Carl Vondrick']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Junbang_Liang2', '~Ruoshi_Liu2', '~Ege_Ozguroglu1', '~Sruthi_Sudhakar1', '~Achal_Dave1', '~Pavel_Tokmakov2', '~Shuran_Song3', '~Carl_Vondrick2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Visuomotor Policy', 'Video Generation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a visuomotor policy based on conditional video generation and 3D tracking which is much more generalizable in manipulation tasks than traditional behavior cloning methods.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A key challenge in manipulation is learning a policy that can robustly generalize to diverse visual environments. A promising mechanism for learning robust policies is to leverage video generative models, which are pretrained on large-scale datasets of internet videos. In this paper, we propose a visuomotor policy learning framework that fine-tunes a video diffusion model on human demonstrations of a given task. At test time, we generate an example of an execution of the task conditioned on images of a novel scene, and use this synthesized execution directly to control the robot. Our key insight is that using common tools allows us to effortlessly bridge the embodiment gap between the human hand and the robot manipulator. We evaluate our approach on 4 tasks of increasing complexity and demonstrate that capitalizing on internet-scale generative models allows the learned policy to achieve a significantly higher degree of generalization than existing behavior cloning approaches.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/43be8b9f504a3cffdc4e4323827c532d95b47146.pdf" target="_blank">/pdf/43be8b9f504a3cffdc4e4323827c532d95b47146.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0ee17f0c408f0946d8089691c6a6ed1b555d3cf2.zip" target="_blank">/attachment/0ee17f0c408f0946d8089691c6a6ed1b555d3cf2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liang2024dreamitate,
title={Dreamitate: Real-World Visuomotor Policy Learning via Video Generation},
author={Junbang Liang and Ruoshi Liu and Ege Ozguroglu and Sruthi Sudhakar and Achal Dave and Pavel Tokmakov and Shuran Song and Carl Vondrick},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=InT87E5sr4}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://dreamitate.cs.columbia.edu/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/58fb0e8995d32193b654e004cdd70606ed9e99dc.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=InT87E5sr4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liang|dreamitate_realworld_visuomotor_policy_learning_via_video_generation</div>
</div>
<div class='paper-counter'>177/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">IcOrwlXzMi</div>
<div class="field-name">title:</div>
<div class="field-value">VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding</div>
<div class="field-name">authors:</div>
<div class="field-value">['Runsen Xu', 'Zhiwei Huang', 'Tai Wang', 'Yilun Chen', 'Jiangmiao Pang', 'Dahua Lin']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Runsen_Xu1', '~Zhiwei_Huang4', '~Tai_Wang2', '~Yilun_Chen1', '~Jiangmiao_Pang1', '~Dahua_Lin1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D Visual Grounding', 'VLM Agent', 'Zero-Shot']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding.</div>
<div class="field-name">abstract:</div>
<div class="field-value">3D visual grounding is crucial for robots, requiring integration of natural language and 3D scene understanding. Traditional methods depend on supervised learning with 3D point clouds are limited by scarce datasets. Recently zero-shot methods leveraging LLMs have been proposed to address the data issue. While effective, these methods often miss detailed scene context, limiting their ability to handle complex queries. In this work, we present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder dynamically stitches image sequences, employs a grounding and feedback scheme to find the target object, and uses a multi-view ensemble projection to accurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D datasets show VLM-Grounder outperforms previous zero-shot methods, achieving 51.6\% Acc@0.25 on ScanRefer and 48.0\% Acc on Nr3D, without relying on 3D geometry or object priors.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/cc573049d705280f7ae570567b83354d6cb43e18.pdf" target="_blank">/pdf/cc573049d705280f7ae570567b83354d6cb43e18.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ea028a1f1b660b0558f048d7c9fe2745e0878ec1.zip" target="_blank">/attachment/ea028a1f1b660b0558f048d7c9fe2745e0878ec1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xu2024vlmgrounder,
title={{VLM}-Grounder: A {VLM} Agent for Zero-Shot 3D Visual Grounding},
author={Runsen Xu and Zhiwei Huang and Tai Wang and Yilun Chen and Jiangmiao Pang and Dahua Lin},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=IcOrwlXzMi}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://runsenxu.com/projects/VLM-Grounder</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8de10d365b54314f308225a09517a877c3dc8116.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=IcOrwlXzMi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xu|vlmgrounder_a_vlm_agent_for_zeroshot_3d_visual_grounding</div>
</div>
<div class='paper-counter'>178/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">HlxRd529nG</div>
<div class="field-name">title:</div>
<div class="field-value">Detect Everything with Few Examples</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xinyu Zhang', 'Yuhan Liu', 'Yuting Wang', 'Abdeslam Boularias']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xinyu_Zhang7', '~Yuhan_Liu2', '~Yuting_Wang2', '~Abdeslam_Boularias1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Vision', 'Object Detection and Recognition', 'Few-shot Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce DE-ViT, a few-shot object detector without the need for finetuning, which establishes new state-of-the-art on all few-shot detection benchmarks (Pascal VOC, COCO, LVIS), and we evaluate DE-ViT with a real robot in sorting novel objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Few-shot object detection aims at detecting novel categories given only a few example images. It is a basic skill for a robot to perform tasks in open environments. Recent methods focus on finetuning strategies, with complicated procedures that prohibit a wider application. In this paper, we introduce DE-ViT, a few-shot object detector without the need for finetuning. DE-ViT's novel architecture is based on a new region-propagation mechanism for localization. The propagated region masks are transformed into bounding boxes through a learnable spatial integral layer. Instead of training prototype classifiers, we propose to use prototypes to project ViT features into a subspace that is robust to overfitting on base classes. We evaluate DE-ViT on few-shot, and one-shot object detection benchmarks with Pascal VOC, COCO, and LVIS. DE-ViT establishes new state-of-the-art results on all benchmarks. Notably, for COCO, DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms few-shot SoTA by 17 box APr. Further, we evaluate DE-ViT with a real robot by building a pick-and-place system for sorting novel objects based on example images. The videos of our robot demonstrations, the source code and the models of DE-ViT can be found at https://mlzxy.github.io/devit.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4cda1054023c433e5213d71c68fc8f99512ca3e3.pdf" target="_blank">/pdf/4cda1054023c433e5213d71c68fc8f99512ca3e3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ed7deebd1406ef0bd4419b83702ff473bf85f0c1.zip" target="_blank">/attachment/ed7deebd1406ef0bd4419b83702ff473bf85f0c1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024detect,
title={Detect Everything with Few Examples},
author={Xinyu Zhang and Yuhan Liu and Yuting Wang and Abdeslam Boularias},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=HlxRd529nG}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://mlzxy.github.io/devit</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/993529f4d54dbc86f5640f8a5fada4a4c94e57ea.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=HlxRd529nG&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|detect_everything_with_few_examples</div>
</div>
<div class='paper-counter'>179/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">GVX6jpZOhU</div>
<div class="field-name">title:</div>
<div class="field-value">RoboPoint: A Vision-Language Model for Spatial Affordance Prediction in Robotics</div>
<div class="field-name">authors:</div>
<div class="field-value">['Wentao Yuan', 'Jiafei Duan', 'Valts Blukis', 'Wilbert Pumacay', 'Ranjay Krishna', 'Adithyavairavan Murali', 'Arsalan Mousavian', 'Dieter Fox']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Wentao_Yuan1', '~Jiafei_Duan1', '~Valts_Blukis1', '~Wilbert_Pumacay1', '~Ranjay_Krishna1', '~Adithyavairavan_Murali2', '~Arsalan_Mousavian1', '~Dieter_Fox1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Foundation Model', 'Affordance Prediction', 'Open-world Manipulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A Vision-Language Model that predicts image-based keypoint affordance conditioned on language instructions</div>
<div class="field-name">abstract:</div>
<div class="field-value">From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Anonymous project page: https://robopoint.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/bffec94c08e0bce84d5994fe5f5d4c80bb946362.pdf" target="_blank">/pdf/bffec94c08e0bce84d5994fe5f5d4c80bb946362.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/b3e1c4b237f0187ed29cd45e22bf0745d72775fb.zip" target="_blank">/attachment/b3e1c4b237f0187ed29cd45e22bf0745d72775fb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yuan2024robopoint,
title={RoboPoint: A Vision-Language Model for Spatial Affordance Prediction in Robotics},
author={Wentao Yuan and Jiafei Duan and Valts Blukis and Wilbert Pumacay and Ranjay Krishna and Adithyavairavan Murali and Arsalan Mousavian and Dieter Fox},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=GVX6jpZOhU}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robo-point.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/10f11eaa8d7d2b5087c13546bf93516585be2538.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=GVX6jpZOhU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yuan|robopoint_a_visionlanguage_model_for_spatial_affordance_prediction_in_robotics</div>
</div>
<div class='paper-counter'>180/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">GGuNkjQSrk</div>
<div class="field-name">title:</div>
<div class="field-value">Action Space Design in Reinforcement Learning for Robot Motor Skills</div>
<div class="field-name">authors:</div>
<div class="field-value">['Julian Eßer', 'Gabriel B. Margolis', 'Oliver Urbann', 'Sören Kerner', 'Pulkit Agrawal']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Julian_Eßer1', '~Gabriel_B._Margolis1', '~Oliver_Urbann1', 'soeren.kerner@iml.fraunhofer.de', '~Pulkit_Agrawal1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Action Spaces', 'Sim-to-Real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We examine action space selection for robots trough the lense of exploration, expressivity, and timing.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Practitioners often rely on intuition to select action spaces for learning. The choice can substantially impact final performance even when choosing among configuration-space representations such as joint position, velocity, and torque commands. We examine action space selection considering a wheeled-legged robot, a quadruped robot, and a simulated suite of locomotion, manipulation, and control tasks. 
We analyze the mechanisms by which action space can improve performance and conclude that the action space can influence learning performance substantially in a task-dependent way. Moreover, we find that much of the practical impact of action space selection on learning dynamics can be explained by improved policy initialization and behavior between timesteps.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8e5fa314c4c35280b1d3ef693d9ec880b415a44f.pdf" target="_blank">/pdf/8e5fa314c4c35280b1d3ef693d9ec880b415a44f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/372e91ffd517e3ea1305387ff9ef939f4265e773.zip" target="_blank">/attachment/372e91ffd517e3ea1305387ff9ef939f4265e773.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
e{\ss}er2024action,
title={Action Space Design in Reinforcement Learning for Robot Motor Skills},
author={Julian E{\ss}er and Gabriel B. Margolis and Oliver Urbann and S{\"o}ren Kerner and Pulkit Agrawal},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=GGuNkjQSrk}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/eb3d8b5459d8f74819801948a19a4a0b04ba87ed.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=GGuNkjQSrk&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">eßer|action_space_design_in_reinforcement_learning_for_robot_motor_skills</div>
</div>
<div class='paper-counter'>181/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">G8UcwxNAoD</div>
<div class="field-name">title:</div>
<div class="field-value">Teaching Robots with Show and Tell: Using Foundation Models to Synthesize Robot Policies from Language and Visual Demonstration</div>
<div class="field-name">authors:</div>
<div class="field-value">['Michael Murray', 'Abhishek Gupta', 'Maya Cakmak']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Michael_Murray2', '~Abhishek_Gupta1', '~Maya_Cakmak1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['learning from demonstration', 'language model planning', 'neuro-symbolic reasoning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce a modular, neuro-symbolic framework for teaching robots new skills through language and visual demonstration. Our approach, ShowTell, composes a mixture of foundation models to synthesize robot manipulation programs that are easy to interpret and generalize across a wide range of tasks and environments. ShowTell is designed to handle complex demonstrations involving high level logic such as loops and conditionals while being intuitive and natural for end-users. We validate this approach through a series of real-world robot experiments, showing that ShowTell out-performs a state-of-the-art baseline based on GPT4-V, on a variety of tasks, and that it is able to generalize to unseen environments and within category objects.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/407de39c11da32da946c49acd9c9bb854479a7d5.pdf" target="_blank">/pdf/407de39c11da32da946c49acd9c9bb854479a7d5.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
murray2024teaching,
title={Teaching Robots with Show and Tell: Using Foundation Models to Synthesize Robot Policies from Language and Visual Demonstration},
author={Michael Murray and Abhishek Gupta and Maya Cakmak},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=G8UcwxNAoD}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robo-showtell.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/aed2cfbd48f56b9d01f128efbdf38f470ee3103d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=G8UcwxNAoD&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">murray|teaching_robots_with_show_and_tell_using_foundation_models_to_synthesize_robot_policies_from_language_and_visual_demonstration</div>
</div>
<div class='paper-counter'>182/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">G0jqGG8Tta</div>
<div class="field-name">title:</div>
<div class="field-value">Not All Errors Are Made Equal: A Regret Metric for Detecting System-level Trajectory Prediction Failures</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kensuke Nakamura', 'Thomas Tian', 'Andrea Bajcsy']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kensuke_Nakamura1', '~Thomas_Tian1', '~Andrea_Bajcsy1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Human-Robot Interaction', 'Trajectory Prediction', 'Failure Detection']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We formalize system-level trajectory prediction failures via regret, and automatically mine for closed-loop human-robot interactions that state-of-the-art generative human predictors and robot planners struggle with.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot decision-making increasingly relies on data-driven human prediction models when operating around people. While these models are known to mispredict in out-of-distribution interactions, only a subset of prediction errors impact downstream robot performance.  
We propose characterizing such ``system-level'' prediction failures via the mathematical notion of regret: high-regret interactions are precisely those in which mispredictions degraded closed-loop robot performance. 
We further introduce a probabilistic generalization of regret that calibrates failure detection across disparate deployment contexts and renders regret compatible with reward-based and reward-free (e.g., generative) planners.  
In simulated autonomous driving interactions, we showcase that our system-level failure metric can automatically mine for closed-loop human-robot interactions that state-of-the-art generative human predictors and robot planners struggle with. 
We further find that the very presence of high-regret data during human predictor fine-tuning is highly predictive of robot re-deployment performance improvements. 
Furthermore, fine-tuning with the informative but significantly smaller high-regret data (23% of deployment data) is competitive with fine-tuning on the full deployment dataset, indicating a promising avenue for efficiently mitigating system-level human-robot interaction failures.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ea24ca019fe7bfb0e1a231d308897ca4761ba335.pdf" target="_blank">/pdf/ea24ca019fe7bfb0e1a231d308897ca4761ba335.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/40493bfe36615a0c66601547f9c9350623ff6802.zip" target="_blank">/attachment/40493bfe36615a0c66601547f9c9350623ff6802.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
nakamura2024not,
title={Not All Errors Are Made Equal: A Regret Metric for Detecting System-level Trajectory Prediction Failures},
author={Kensuke Nakamura and Thomas Tian and Andrea Bajcsy},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=G0jqGG8Tta}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://cmu-intentlab.github.io/not-all-errors/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/85f428d772e293fedfa4c7ce6c3f9917e34c18ad.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=G0jqGG8Tta&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">nakamura|not_all_errors_are_made_equal_a_regret_metric_for_detecting_systemlevel_trajectory_prediction_failures</div>
</div>
<div class='paper-counter'>183/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">FO6tePGRZj</div>
<div class="field-name">title:</div>
<div class="field-value">Mobile ALOHA: Learning Bimanual Mobile Manipulation using Low-Cost Whole-Body Teleoperation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zipeng Fu', 'Tony Z. Zhao', 'Chelsea Finn']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zipeng_Fu1', '~Tony_Z._Zhao1', '~Chelsea_Finn1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Mobile Manipulation', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Imitation learning for bimanual mobile manipulation using a customized robot and teleoperation system</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90\%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.  We will open-source all the hardware and software implementations upon publication.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/30f4379dada370ddd9bc8f2398b335408883cc21.pdf" target="_blank">/pdf/30f4379dada370ddd9bc8f2398b335408883cc21.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6c76dd385b64d19e161cc7a0f414f19554086e46.zip" target="_blank">/attachment/6c76dd385b64d19e161cc7a0f414f19554086e46.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
fu2024mobile,
title={Mobile {ALOHA}: Learning Bimanual Mobile Manipulation using Low-Cost Whole-Body Teleoperation},
author={Zipeng Fu and Tony Z. Zhao and Chelsea Finn},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=FO6tePGRZj}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://mobile-aloha.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/96b7f6c13bac4a2d0c74b9ced9db3ade92769f8e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=FO6tePGRZj&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">fu|mobile_aloha_learning_bimanual_mobile_manipulation_using_lowcost_wholebody_teleoperation</div>
</div>
<div class='paper-counter'>184/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">FHnVRmeqxf</div>
<div class="field-name">title:</div>
<div class="field-value">FlowRetrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Li-Heng Lin', 'Yuchen Cui', 'Amber Xie', 'Tianyu Hua', 'Dorsa Sadigh']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Li-Heng_Lin1', '~Yuchen_Cui1', '~Amber_Xie1', '~Tianyu_Hua1', '~Dorsa_Sadigh1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Data Retrieval', 'Few-shot Learning', 'Imitation Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a few-shot imitation learning method that uses optical flow to retrieve data with similar motion from prior dataset and augment downstream policy learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning policies in robotics tend to require an extensive amount of demonstrations. It is critical to develop few-shot adaptation strategies that rely only on a small amount of task-specific human demonstrations. Prior works focus on learning general policies from large scale dataset with diverse behaviors. Recent research has shown that directly retrieving relevant past experiences to augment policy learning has great promise in few-shot settings. However, existing data retrieval methods fall under two extremes: they either rely on the existence of exact same behaviors with visually similar scenes in the prior data, which is impractical to assume; or they retrieve based on semantic similarity of high-level language descriptions of the task, which might not be that informative about the shared behaviors or motions across tasks. In this work, we investigate how we can leverage motion similarity in the vast amount of cross-task data to improve few-shot imitation learning of the target task. Our key insight is that motion-similar data carry rich information about the effects of actions and object interactions that can be leveraged during few-shot adaptation. We propose FlowRetrieval, an approach that leverages optical flow representations for both extracting similar motions to target tasks from prior data, and for guiding learning of a policy that can maximally benefit from such data. Our results show FlowRetrieval significantly outperforms prior methods across simulated and real-world domains, achieving on average 27% higher success rate than the best retrieval-based prior method. In the Pen-in-Cup task with a real Franka Emika robot, FlowRetrieval achieves 3.7x the performance of the baseline learning from all prior and target data.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ce01635045f5e7d05e768aa8c127aee26c522384.pdf" target="_blank">/pdf/ce01635045f5e7d05e768aa8c127aee26c522384.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/05242c5d54d3fd9a1e9b5301afb8130136f1648f.zip" target="_blank">/attachment/05242c5d54d3fd9a1e9b5301afb8130136f1648f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lin2024flowretrieval,
title={FlowRetrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning},
author={Li-Heng Lin and Yuchen Cui and Amber Xie and Tianyu Hua and Dorsa Sadigh},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=FHnVRmeqxf}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://flow-retrieval.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6f20137df4d305cc74b63b8cc0abb46cd531b213.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=FHnVRmeqxf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lin|flowretrieval_flowguided_data_retrieval_for_fewshot_imitation_learning</div>
</div>
<div class='paper-counter'>185/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">F0rWEID2gb</div>
<div class="field-name">title:</div>
<div class="field-value">Environment Curriculum Generation via Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['William Liang', 'Sam Wang', 'Hung-Ju Wang', 'Osbert Bastani', 'Dinesh Jayaraman', 'Yecheng Jason Ma']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~William_Liang1', '~Sam_Wang1', '~Hung-Ju_Wang1', '~Osbert_Bastani1', '~Dinesh_Jayaraman2', '~Yecheng_Jason_Ma2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Large Language Models', 'Environment Curriculum', 'Quadrupeds', 'Sim-To-Real Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use LLMs to automatically design the sim-to-real RL environment curriculum for training a quadruped to acquire parkour skills.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent work has demonstrated that a promising strategy for teaching robots a wide range of complex skills is by training them on a curriculum of progressively more challenging environments. However, developing an effective curriculum of environment distributions currently requires significant expertise, which must be repeated for every new domain. Our key insight is that environments are often naturally represented as code. Thus, we probe whether effective environment curriculum design can be achieved and automated via code generation by large language models (LLM). In this paper, we introduce Eurekaverse, an unsupervised environment design algorithm that uses LLMs to sample progressively more challenging, diverse, and learnable environments for skill training. We validate Eurekaverse's effectiveness in the domain of quadrupedal parkour learning, in which a quadruped robot must traverse through a variety of obstacle courses. The automatic curriculum designed by Eurekaverse enables gradual learning of complex parkour skills in simulation and can successfully transfer to the real-world, outperforming manual training courses designed by humans.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/30545859617a88918fd31de9d788911b07723f28.pdf" target="_blank">/pdf/30545859617a88918fd31de9d788911b07723f28.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cb85ec72431fd55308a983d21177363303635c1a.zip" target="_blank">/attachment/cb85ec72431fd55308a983d21177363303635c1a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liang2024environment,
title={Environment Curriculum Generation via Large Language Models},
author={William Liang and Sam Wang and Hung-Ju Wang and Osbert Bastani and Dinesh Jayaraman and Yecheng Jason Ma},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=F0rWEID2gb}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://eureka-research.github.io/eurekaverse/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5538724a1de74386ffb58d81c2ad4e826b8e1e4d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=F0rWEID2gb&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liang|environment_curriculum_generation_via_large_language_models</div>
</div>
<div class='paper-counter'>186/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EyEE7547vy</div>
<div class="field-name">title:</div>
<div class="field-value">Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tianyi Xiong', 'Jiayi Wu', 'Botao He', 'Cornelia Fermuller', 'Yiannis Aloimonos', 'Heng Huang', 'Christopher Metzler']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tianyi_Xiong1', '~Jiayi_Wu9', '~Botao_He1', '~Cornelia_Fermuller3', '~Yiannis_Aloimonos1', '~Heng_Huang1', '~Christopher_Metzler1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Event-based 3D Reconstruction', 'Gaussian Splatting', 'High-speed Robot Egomotion']</div>
<div class="field-name">abstract:</div>
<div class="field-value">By combining differentiable rendering with explicit point-based scene representations, 3D Gaussian Splatting (3DGS) has demonstrated breakthrough 3D reconstruction capabilities. 
    However, to date 3DGS has had limited impact on robotics, where high-speed egomotion is pervasive: Egomotion introduces motion blur and leads to artifacts in existing frame-based 3DGS reconstruction methods. 
    To address this challenge, we introduce Event3DGS, an event-based 3DGS framework.
    By exploiting the exceptional temporal resolution of event cameras, Event3GDS can reconstruct high-fidelity 3D structure and appearance under high-speed egomotion. 
    Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of Event3DGS compared with existing event-based dense 3D scene reconstruction frameworks; Event3DGS substantially improves reconstruction quality (+3dB) while reducing computational costs by 95\%. 
    Our framework also allows one to incorporate a few motion-blurred frame-based measurements into the reconstruction process to further improve appearance fidelity without loss of structural accuracy.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/cddd8bbbedae60713f1018620c80e269a9bbd2d2.pdf" target="_blank">/pdf/cddd8bbbedae60713f1018620c80e269a9bbd2d2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2c1c3cee6c58ad804c1e8a0993cee6752515e4d0.zip" target="_blank">/attachment/2c1c3cee6c58ad804c1e8a0993cee6752515e4d0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xiong2024eventdgs,
title={Event3{DGS}: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion},
author={Tianyi Xiong and Jiayi Wu and Botao He and Cornelia Fermuller and Yiannis Aloimonos and Heng Huang and Christopher Metzler},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EyEE7547vy}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://tyxiong23.github.io/event3dgs</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6babd639ee48502da3536f40bc00d0c5c51999d8.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EyEE7547vy&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xiong|event3dgs_eventbased_3d_gaussian_splatting_for_highspeed_robot_egomotion</div>
</div>
<div class='paper-counter'>187/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EiqQEsOMZt</div>
<div class="field-name">title:</div>
<div class="field-value">TaMMa: Target-driven Multi-subscene Mobile Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jiawei Hou', 'Tianyu Wang', 'Tongying Pan', 'Shouyan Wang', 'Xiangyang Xue', 'Yanwei Fu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jiawei_Hou2', '~Tianyu_Wang10', '~Tongying_Pan1', '~Shouyan_Wang1', '~Xiangyang_Xue2', '~Yanwei_Fu2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-subscene', '3D Gaussians', 'Scene Inpainting', 'Target-driven Mobile Manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">For everyday service robotics, the ability to navigate back and forth based on tasks in multi-subscene environments and perform delicate manipulations is crucial and highly practical.
While existing robotics primarily focus on complex tasks within a single scene or simple tasks across scalable scenes individually, robots consisting of a mobile base with a robotic arm face the challenge of efficiently representing multiple subscenes, coordinating the collaboration between the mobile base and the robotic arm, and managing delicate tasks in scalable environments.
To address this issue, we propose Target-driven Multi-subscene Mobile Manipulation (\textit{TaMMa}), which efficiently handles mobile base movement and fine-grained manipulation across subscenes. Specifically, we obtain a reliable 3D Gaussian initialization of the whole scene using a sparse 3D point cloud with encoded semantics. Through querying the coarse Gaussians, we acquire the approximate pose of the target, navigate the mobile base to approach it, and reduce the scope of precise target pose estimation to the corresponding subscene. Optimizing while moving, we employ diffusion-based depth completion to optimize fine-grained Gaussians and estimate the target's refined pose. For target-driven manipulation, we adopt Gaussians inpainting to obtain precise poses for the origin and destination of the operation in a \textit{think before you do it} manner, enabling fine-grained manipulation. 
We conduct various experiments on a real robotic to demonstrate our method in effectively and efficiently achieving precise operation tasks across multiple tabletop subscenes.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9cc1789515c970af6bbbe74debc844fddddb516b.pdf" target="_blank">/pdf/9cc1789515c970af6bbbe74debc844fddddb516b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/60e7ed38f298bf0e450a843c6685db707b9fbdb8.zip" target="_blank">/attachment/60e7ed38f298bf0e450a843c6685db707b9fbdb8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hou2024tamma,
title={Ta{MM}a: Target-driven Multi-subscene Mobile Manipulation},
author={Jiawei Hou and Tianyu Wang and Tongying Pan and Shouyan Wang and Xiangyang Xue and Yanwei Fu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EiqQEsOMZt}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0ea4183384d90aee502f4ea2db2faca9b606f6c7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EiqQEsOMZt&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hou|tamma_targetdriven_multisubscene_mobile_manipulation</div>
</div>
<div class='paper-counter'>188/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EifoVoIyd5</div>
<div class="field-name">title:</div>
<div class="field-value">What Matters in Range View 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Benjamin Wilson', 'Nicholas Autio Mitchell', 'Jhony Kaesemodel Pontes', 'James Hays']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Benjamin_Wilson3', '~Nicholas_Autio_Mitchell1', '~Jhony_Kaesemodel_Pontes1', '~James_Hays1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D Object Detection', '3D Perception', 'Autonomous Driving']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Range view 3D detection shows competitive performance against other lidar representations while utilizing straightforward techniques.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Lidar-based perception pipelines rely on 3D object detection models to interpret complex scenes. While multiple representations for lidar exist, the range view is enticing since it losslessly encodes the entire lidar sensor output. In this work, we achieve state-of-the-art amongst range view 3D object detection models without using multiple techniques proposed in past range view literature. We explore range view 3D object detection across two modern datasets with substantially different properties: Argoverse 2 and Waymo Open. Our investigation reveals key insights: (1) input feature dimensionality significantly influences the overall performance, (2) surprisingly, employing a classification loss grounded in 3D spatial proximity works as well or better compared to more elaborate IoU-based losses, and (3) addressing non-uniform lidar density via a straightforward range subsampling technique outperforms existing multi-resolution, range-conditioned networks. Our experiments reveal that techniques proposed in recent range view literature are not needed to achieve state-of-the-art performance. Combining the above findings, we establish a new state-of-the-art model for range view 3D object detection — improving AP by 2.2% on the Waymo Open dataset while maintaining a runtime of 10 Hz. We are the first to benchmark a range view model on the Argoverse 2 dataset and outperform strong voxel-based baselines. All models are multi-class and open-source. Code is available at https://github.com/benjaminrwilson/range-view-3d-detection.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4e200002d55bdf669f18306475f78a967afd8246.pdf" target="_blank">/pdf/4e200002d55bdf669f18306475f78a967afd8246.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/568eaf5319ce1d2f8304bf2bcc3119b07b6e45c2.zip" target="_blank">/attachment/568eaf5319ce1d2f8304bf2bcc3119b07b6e45c2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wilson2024what,
title={What Matters in Range View 3D Object Detection},
author={Benjamin Wilson and Nicholas Autio Mitchell and Jhony Kaesemodel Pontes and James Hays},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EifoVoIyd5}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f7eea835de0d64300b177b2e0a38e7c1a46edce7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EifoVoIyd5&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wilson|what_matters_in_range_view_3d_object_detection</div>
</div>
<div class='paper-counter'>189/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EdVNB2kHv1</div>
<div class="field-name">title:</div>
<div class="field-value">Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Nils Blank', 'Moritz Reuss', 'Marcel Rühle', 'Ömer Erdinç Yağmurlu', 'Fabian Wenzel', 'Oier Mees', 'Rudolf Lioutikov']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Nils_Blank1', '~Moritz_Reuss1', '~Marcel_Rühle1', '~Ömer_Erdinç_Yağmurlu1', '~Fabian_Wenzel1', '~Oier_Mees1', '~Rudolf_Lioutikov1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Foundation Models', 'Language-conditioned Imitation Learning', 'Data Labeling']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel framework to label unucrated long-horizon robot demonstrations without any model training our human annotation for langauge-conditioned policy learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. 
To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention.
NILS combines pre-trained vision-language foundation models in a sophisticated, carefully considered manner in order to detect objects in a scene, detect object-centric changes, segment tasks from 
large datasets of unlabelled interaction data and ultimately label behavior datasets.
Evaluations on BridgeV2 and a kitchen play dataset show that NILS is able to autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets, while alleviating several shortcomings of crowdsourced human annotations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3215dbb1330272b2ef82ec65eed2d03b9dcafe49.pdf" target="_blank">/pdf/3215dbb1330272b2ef82ec65eed2d03b9dcafe49.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/4513a3d803d715c9e852fcfe0317963801b60391.zip" target="_blank">/attachment/4513a3d803d715c9e852fcfe0317963801b60391.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
blank2024scaling,
title={Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models},
author={Nils Blank and Moritz Reuss and Marcel R{\"u}hle and {\"O}mer Erdin{\c{c}} Ya{\u{g}}murlu and Fabian Wenzel and Oier Mees and Rudolf Lioutikov},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EdVNB2kHv1}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://robottasklabeling.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a5c1693f8e88819a37e52fff3b9e2ab717861304.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EdVNB2kHv1&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">blank|scaling_robot_policy_learning_via_zeroshot_labeling_with_foundation_models</div>
</div>
<div class='paper-counter'>190/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EPujQZWemk</div>
<div class="field-name">title:</div>
<div class="field-value">ViPER: Visibility-based Pursuit-Evasion via Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yizhuo Wang', 'Yuhong Cao', 'Jimmy Chiun', 'Subhadeep Koley', 'Mandy Pham', 'Guillaume Adrien Sartoretti']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yizhuo_Wang1', '~Yuhong_Cao1', '~Jimmy_Chiun1', '~Subhadeep_Koley2', '~Mandy_Pham1', '~Guillaume_Adrien_Sartoretti1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['MARL', 'pursuit-evasion', 'graph attention', 'path planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a neural framework for visibility-based pursuit-evasion to learn a coordinated yet distributed policy for multiple agents to effectively search for worst-case evaders, resulting in a significantly improved success rate across various maps.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In visibility-based pursuit-evasion tasks, a team of mobile pursuer robots with limited sensing capabilities is tasked with detecting all evaders in a multiply-connected planar environment, whose map may or may not be known to pursuers beforehand. This requires tight coordination among multiple agents to ensure that the omniscient and potentially arbitrarily fast evaders are guaranteed to be detected by the pursuers. Whereas existing methods typically rely on a relatively large team of agents to clear the environment, we propose ViPER, a neural solution that leverages a graph attention network to learn a coordinated yet distributed policy via multi-agent reinforcement learning (MARL). We experimentally demonstrate that ViPER significantly outperforms other state-of-the-art non-learning planners, showcasing its emergent coordinated behaviors and adaptability to more challenging scenarios and various team sizes, and finally deploy its learned policies on hardware in an aerial search task.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fc126d50c2ad5b98becedc6a46f956ec479cc001.pdf" target="_blank">/pdf/fc126d50c2ad5b98becedc6a46f956ec479cc001.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/99f55436f004f020a7bec156bd26ec9910e51c77.zip" target="_blank">/attachment/99f55436f004f020a7bec156bd26ec9910e51c77.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024viper,
title={Vi{PER}: Visibility-based Pursuit-Evasion via Reinforcement Learning},
author={Yizhuo Wang and Yuhong Cao and Jimmy Chiun and Subhadeep Koley and Mandy Pham and Guillaume Adrien Sartoretti},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EPujQZWemk}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d6210fe1d8db2fe2837d11bd385e51d797a9ac83.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EPujQZWemk&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|viper_visibilitybased_pursuitevasion_via_reinforcement_learning</div>
</div>
<div class='paper-counter'>191/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">EM0wndCeoD</div>
<div class="field-name">title:</div>
<div class="field-value">BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark</div>
<div class="field-name">authors:</div>
<div class="field-value">['Nikita Chernyadev', 'Nicholas Backshall', 'Xiao Ma', 'Yunfan Lu', 'Younggyo Seo', 'Stephen James']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Nikita_Chernyadev1', '~Nicholas_Backshall1', '~Xiao_Ma2', '~Yunfan_Lu2', '~Younggyo_Seo1', '~Stephen_James1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Bi-Manual Manipulation', 'Mobile Manipulation', 'Benchmark']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A Demo-Driven Mobile Bi-Manual Manipulation Benchmark & Learning Environment, for Reinforcement and Imitation Learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set in home environments, ranging from simple target reaching to complex kitchen cleaning. To capture the real-world performance accurately, we provide human-collected demonstrations for each task, reflecting the diverse modalities found in real-world robot trajectories. BiGym supports a variety of observations, including proprioceptive data and visual inputs such as RGB, and depth from 3 camera views. To validate the usability of BiGym, we thoroughly benchmark the state-of-the-art imitation learning algorithms and demo-driven reinforcement learning algorithms within the environment and discuss the future opportunities.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c4d7435745c764b7f227d6ce66bc75e56bd39c82.pdf" target="_blank">/pdf/c4d7435745c764b7f227d6ce66bc75e56bd39c82.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ef800c417e1cc31af464ffd97da90d232bbdde62.zip" target="_blank">/attachment/ef800c417e1cc31af464ffd97da90d232bbdde62.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chernyadev2024bigym,
title={BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark},
author={Nikita Chernyadev and Nicholas Backshall and Xiao Ma and Yunfan Lu and Younggyo Seo and Stephen James},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=EM0wndCeoD}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://chernyadev.github.io/bigym/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2eb94fff518c47ca0955fa48e245424d4809973a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=EM0wndCeoD&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chernyadev|bigym_a_demodriven_mobile_bimanual_manipulation_benchmark</div>
</div>
<div class='paper-counter'>192/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">E4K3yLQQ7s</div>
<div class="field-name">title:</div>
<div class="field-value">Visual Manipulation with Legs</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xialin He', 'Chengjing Yuan', 'Wenxuan Zhou', 'Ruihan Yang', 'David Held', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xialin_He1', '~Chengjing_Yuan1', '~Wenxuan_Zhou1', '~Ruihan_Yang2', '~David_Held1', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Legged robots', 'Non-prehensile manipulation', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work presents a system that empowers quadruped robot to perform object interactions with its legs</div>
<div class="field-name">abstract:</div>
<div class="field-value">Animals have the ability to use their arms and legs for both locomotion and manipulation. We envision quadruped robots to have the same versatility. This work presents a system that empowers a quadruped robot to perform object interactions with its legs, drawing inspiration from non-prehensile manipulation techniques. The proposed system has two main components: a visual manipulation policy module and a loco-manipulator module. The visual manipulation policy module decides how the leg should interact with the object, trained with reinforcement learning (RL) with point cloud observations and object-centric actions. The loco-manipulator controller controls the leg movements and body pose adjustments, implemented based on impedance control and Model Predictive Control (MPC). Besides manipulating objects with a single leg, the proposed system can also select from left or right legs based on the critic maps and move the object to distant goals through robot base adjustment. In the experiments, we evaluate the proposed system with the object pose alignment tasks both in simulation and in the real world, demonstrating object manipulation skills with legs more versatile than previous work.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/eeec90cc1687588b98ac9d745a655a267a7c9831.pdf" target="_blank">/pdf/eeec90cc1687588b98ac9d745a655a267a7c9831.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cfa0a8db38bc2292c06f82fbee78773b63e6af15.zip" target="_blank">/attachment/cfa0a8db38bc2292c06f82fbee78773b63e6af15.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
he2024visual,
title={Visual Manipulation with Legs},
author={Xialin He and Chengjing Yuan and Wenxuan Zhou and Ruihan Yang and David Held and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=E4K3yLQQ7s}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://legged-manipulation.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b206dbd4fc2e6888d589c6b923560164fee02f86.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=E4K3yLQQ7s&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">he|visual_manipulation_with_legs</div>
</div>
<div class='paper-counter'>193/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">DsFQg0G4Xu</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Long-Horizon Action Dependencies in Sampling-Based Bilevel Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Bartłomiej Cieślar', 'Leslie Pack Kaelbling', 'Tomás Lozano-Pérez', 'Jorge Mendez-Mendez']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Bartłomiej_Cieślar1', '~Leslie_Pack_Kaelbling1', '~Tomás_Lozano-Pérez1', '~Jorge_Mendez-Mendez1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['task and motion planning', 'long-horizon', 'learning for planning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We learn a heuristic to optimize backtracking in sampling-based approaches to Task and Motion Planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous robots will need the ability to make task and motion plans that involve long sequences of actions, e.g. to prepare a meal. One challenge is that the feasibility of actions late in the plan may depend on much earlier actions. This issue is exacerbated if these dependencies exist at a purely geometric level, making them difficult to express for a task planner. Backtracking is a common technique to resolve such geometric dependencies, but its time complexity limits its applicability to short-horizon dependencies. We propose an approach to account for these dependencies by learning a search heuristic for task and motion planning. We evaluate our approach on five quasi-static simulated domains and show a substantial improvement in success rate  over the baselines.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/56dd23324650d42e0d8675c36f79d182dd8dd0d8.pdf" target="_blank">/pdf/56dd23324650d42e0d8675c36f79d182dd8dd0d8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/81268d0dd8623d5564743231ce7299759ae3f6fc.zip" target="_blank">/attachment/81268d0dd8623d5564743231ce7299759ae3f6fc.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
cie{\'s}lar2024learning,
title={Learning Long-Horizon Action Dependencies in Sampling-Based Bilevel Planning},
author={Bart{\l}omiej Cie{\'s}lar and Leslie Pack Kaelbling and Tom{\'a}s Lozano-P{\'e}rez and Jorge Mendez-Mendez},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=DsFQg0G4Xu}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/fa4373a2cd3093861984d6b3eda8e2d92bf83602.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=DsFQg0G4Xu&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">cielar|learning_longhorizon_action_dependencies_in_samplingbased_bilevel_planning</div>
</div>
<div class='paper-counter'>194/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Dftu4r5jHe</div>
<div class="field-name">title:</div>
<div class="field-value">Context-Aware Replanning with Pre-Explored Semantic Map for Object Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Po-Chen Ko', 'Hung-Ting Su', 'CY Chen', 'Jia-Fong Yeh', 'Min Sun', 'Winston H. Hsu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Po-Chen_Ko1', '~Hung-Ting_Su1', '~CY_Chen1', '~Jia-Fong_Yeh1', '~Min_Sun1', '~Winston_H._Hsu2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['VLMs', 'map', 'navigation', 'uncertainty', 'multi-view consistency', 'robotics']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Pre-explored Semantic Map, constructed through prior exploration using visual language models (VLMs), has proven effective as a foundational element for training-free robotic applications. However, existing approaches assume the map's accuracy and do not provide effective mechanisms for revising decisions based on incorrect maps. This work introduces Context-Aware Replanning (CARe),, which estimates map uncertainty through confidence scores and multi-view consistency, enabling the agent to revise erroneous decisions stemming from inaccurate maps without additional labels. We demonstrate the effectiveness of our proposed method using two modern map backbones, VLMaps and OpenMask3D, and show significant improvements in performance on object navigation tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/c12b2159a4b09661caab2595904f6d7be1829377.pdf" target="_blank">/pdf/c12b2159a4b09661caab2595904f6d7be1829377.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/86229662b408ce96326c0d19f4087568b7242b0a.zip" target="_blank">/attachment/86229662b408ce96326c0d19f4087568b7242b0a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ko2024contextaware,
title={Context-Aware Replanning with Pre-Explored Semantic Map for Object Navigation},
author={Po-Chen Ko and Hung-Ting Su and CY Chen and Jia-Fong Yeh and Min Sun and Winston H. Hsu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Dftu4r5jHe}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://care-maps.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d07265da5f89576234e17806aa9450a101c0bfaa.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Dftu4r5jHe&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ko|contextaware_replanning_with_preexplored_semantic_map_for_object_navigation</div>
</div>
<div class='paper-counter'>195/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">DSdAEsEGhE</div>
<div class="field-name">title:</div>
<div class="field-value">SoloParkour: Constrained Reinforcement Learning for Visual Locomotion from Privileged Experience</div>
<div class="field-name">authors:</div>
<div class="field-value">['Elliot Chane-Sane', 'Joseph Amigo', 'Thomas Flayols', 'Ludovic Righetti', 'Nicolas Mansard']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Elliot_Chane-Sane1', '~Joseph_Amigo1', 'thomas.flayols@laas.fr', '~Ludovic_Righetti1', '~Nicolas_Mansard1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Agile Locomotion', 'Visuomotor Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel method to train visual locomotion policies end-to-end with constrained reinforcement learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Parkour poses a significant challenge for legged robots, requiring navigation through complex environments with agility and precision based on limited sensory inputs.
In this work,  we introduce a novel method for training end-to-end visual policies, from depth pixels to robot control commands, to achieve agile and safe quadruped locomotion.
We formulate robot parkour as a constrained reinforcement learning (RL) problem designed to maximize the emergence of agile skills within the robot's physical limits while ensuring safety. 
We first train a policy without vision using privileged information about the robot's surroundings. 
We then generate experience from this privileged policy to warm-start a sample efficient off-policy RL algorithm from depth images.
This allows the robot to adapt behaviors from this privileged experience to visual locomotion while circumventing the high computational costs of RL directly from pixels.
We demonstrate the effectiveness of our method on a real Solo-12 robot, showcasing its capability to perform a variety of parkour skills such as walking, climbing, leaping, and crawling.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a4f364620c73ae2decdbf5fd7c41ce590a32cd01.pdf" target="_blank">/pdf/a4f364620c73ae2decdbf5fd7c41ce590a32cd01.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/0269c46878e3b60f16499f0eb6593587c8314cbe.zip" target="_blank">/attachment/0269c46878e3b60f16499f0eb6593587c8314cbe.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chane-sane2024soloparkour,
title={SoloParkour: Constrained Reinforcement Learning for Visual Locomotion from Privileged Experience},
author={Elliot Chane-Sane and Joseph Amigo and Thomas Flayols and Ludovic Righetti and Nicolas Mansard},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=DSdAEsEGhE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gepetto.github.io/SoloParkour/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f819e30b00d9e350d9b86dbb9bda1a69b3714280.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=DSdAEsEGhE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chanesane|soloparkour_constrained_reinforcement_learning_for_visual_locomotion_from_privileged_experience</div>
</div>
<div class='paper-counter'>196/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">DDIoRSh8ID</div>
<div class="field-name">title:</div>
<div class="field-value">Multi-Task Interactive Robot Fleet Learning with Visual World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huihan Liu', 'Yu Zhang', 'Vaarij Betala', 'Evan Zhang', 'James Liu', 'Crystal Ding', 'Yuke Zhu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huihan_Liu1', '~Yu_Zhang77', 'vaarijbetala@gmail.com', 'evanczhang@utexas.edu', 'liujames2003@gmail.com', 'crystald@utexas.edu', '~Yuke_Zhu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Manipulation', 'Interactive Imitation Learning', 'Fleet Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We proposed a framework for multi-task interactive robot fleet learning with visual world models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a multi-task interactive robot fleet learning framework to address these challenges. Sirius-Fleet monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate Sirius-Fleet's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website: https://ut-austin-rpl.github.io/sirius-fleet</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/04f451a4c200c70f712ec90d06029b271cd5dd99.pdf" target="_blank">/pdf/04f451a4c200c70f712ec90d06029b271cd5dd99.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cb816467b676b7b6f52ffe44ab9cc088670ce3c8.zip" target="_blank">/attachment/cb816467b676b7b6f52ffe44ab9cc088670ce3c8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024multitask,
title={Multi-Task Interactive Robot Fleet Learning with Visual World Models},
author={Huihan Liu and Yu Zhang and Vaarij Betala and Evan Zhang and James Liu and Crystal Ding and Yuke Zhu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=DDIoRSh8ID}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://ut-austin-rpl.github.io/sirius-fleet/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/cc61a372e49311a81549a597270c98f1795d97c3.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=DDIoRSh8ID&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|multitask_interactive_robot_fleet_learning_with_visual_world_models</div>
</div>
<div class='paper-counter'>197/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Czs2xH9114</div>
<div class="field-name">title:</div>
<div class="field-value">WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chong Zhang', 'Wenli Xiao', 'Tairan He', 'Guanya Shi']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chong_Zhang6', '~Wenli_Xiao1', '~Tairan_He1', '~Guanya_Shi1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Whole-Body Humanoid Control', 'Multi-Contact Control', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a unified RL framework to learn whole-body humanoid control with sequential contacts.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models. 
Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, 
it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences. In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages. Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task. We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing. We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks. Website: lecar-lab.github.io/wococo/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/60b3135f71ecb14052e57c608f0cdc502ecab360.pdf" target="_blank">/pdf/60b3135f71ecb14052e57c608f0cdc502ecab360.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/26af738e231863416a08745358a94cd70f2f2688.zip" target="_blank">/attachment/26af738e231863416a08745358a94cd70f2f2688.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024wococo,
title={WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts},
author={Chong Zhang and Wenli Xiao and Tairan He and Guanya Shi},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Czs2xH9114}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://lecar-lab.github.io/wococo/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5bc919bcd0e7769e8afc2e9f3236905d35f02172.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Czs2xH9114&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|wococo_learning_wholebody_humanoid_control_with_sequential_contacts</div>
</div>
<div class='paper-counter'>198/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">CskuWHDBAr</div>
<div class="field-name">title:</div>
<div class="field-value">Enhancing Visual Domain Robustness in Behaviour Cloning via Saliency-Guided Augmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zheyu Zhuang', 'RUIYU WANG', 'Nils Ingelhag', 'Ville Kyrki', 'Danica Kragic']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zheyu_Zhuang1', '~RUIYU_WANG1', 'ingelhag@kth.se', '~Ville_Kyrki1', '~Danica_Kragic1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Behaviour Cloning', 'Visuomotor Policy', 'Data Augmentation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Enhance policies' robustness against visual domain shifts (distractors, background variations) by leveraging saliency maps generated from policy to perform data augmentation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In vision-based behaviour cloning (BC), traditional image-level augmentation methods such as pixel shifting enhance in-domain performance but often struggle with visual domain shifts, including distractors, occlusion, and changes in lighting and backgrounds. Conversely, superimposition-based augmentation, proven effective in computer vision, improves model generalisability by blending training images and out-of-domain images. Despite its potential, the applicability of these methods to vision-based BC remains unclear due to the unique challenges posed by BC demonstrations; specifically, preserving task-critical scene semantics, spatial-temporal relationships, and agent-target interactions is crucial. To address this, we introduce RoboSaGA, a context-aware approach that dynamically adjusts augmentation intensity per pixel based on input saliency derived from the policy. This method ensures aggressive augmentation within task-trivial areas without compromising task-critical information. Furthermore, RoboSaGA seamlessly integrates into existing network architectures without the need for structural changes or additional learning objectives. Our empirical evaluations across both simulated and real-world settings demonstrate that RoboSaGA not only maintains in-domain performance but significantly improves resilience to distractors and background variations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/50f4ed479bf8d8ac08fc3c267621aef41865a82b.pdf" target="_blank">/pdf/50f4ed479bf8d8ac08fc3c267621aef41865a82b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/56c99d32165c314ff713bbbdb143d6f305a78a7a.zip" target="_blank">/attachment/56c99d32165c314ff713bbbdb143d6f305a78a7a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhuang2024enhancing,
title={Enhancing Visual Domain Robustness in Behaviour Cloning via Saliency-Guided Augmentation},
author={Zheyu Zhuang and RUIYU WANG and Nils Ingelhag and Ville Kyrki and Danica Kragic},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=CskuWHDBAr}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0e69d2e942d19e3670f662cd76c23fa9c6701851.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=CskuWHDBAr&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhuang|enhancing_visual_domain_robustness_in_behaviour_cloning_via_saliencyguided_augmentation</div>
</div>
<div class='paper-counter'>199/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">CpXiqz6qf4</div>
<div class="field-name">title:</div>
<div class="field-value">SonicSense: Object Perception from In-Hand Acoustic Vibration</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jiaxun Liu', 'Boyuan Chen']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jiaxun_Liu1', '~Boyuan_Chen1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Tactile Perception', 'Object State Estimation', 'Audio', 'Acoustic Vibration Sensing']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">SonicSense, an integrated hardware and software solution to enable rich object perception capabilities with in-hand acoustic vibration for a multi-finger robot hand.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce SonicSense, a holistic design of hardware and software to enable rich robot object perception through in-hand acoustic vibration sensing. While previous studies have shown promising results with acoustic sensing for object perception, current solutions are constrained to a handful of objects with simple geometries and homogeneous materials, single-finger sensing, and mixing training and testing on the same objects. SonicSense enables container inventory status differentiation, heterogeneous material prediction, 3D shape reconstruction, and object re-identification from a diverse set of 83 real-world objects. Our system employs a simple but effective heuristic exploration policy to interact with the objects as well as end-to-end learning-based algorithms to fuse vibration signals to infer object properties. Our framework underscores the significance of in-hand acoustic vibration sensing in advancing robot tactile perception.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/854fa2e0634db55d1db7d3a695be32c3a7d33415.pdf" target="_blank">/pdf/854fa2e0634db55d1db7d3a695be32c3a7d33415.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6eeb9cdd25bc9b946697bdc758af1f2a33684a3b.zip" target="_blank">/attachment/6eeb9cdd25bc9b946697bdc758af1f2a33684a3b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024sonicsense,
title={SonicSense: Object Perception from In-Hand Acoustic Vibration},
author={Jiaxun Liu and Boyuan Chen},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=CpXiqz6qf4}
}</div>
<div class="field-name">website:</div>
<div class="field-value">http://www.generalroboticslab.com/blogs/blog/2024-06-26-sonicsense/index.html</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/597b87e553571e6599832c9f587c893f1bcc657b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=CpXiqz6qf4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|sonicsense_object_perception_from_inhand_acoustic_vibration</div>
</div>
<div class='paper-counter'>200/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">CPQW5kc0pe</div>
<div class="field-name">title:</div>
<div class="field-value">VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['I-Chun Arthur Liu', 'Sicheng He', 'Daniel Seita', 'Gaurav S. Sukhatme']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~I-Chun_Arthur_Liu1', 'sichengh@usc.edu', '~Daniel_Seita1', '~Gaurav_S._Sukhatme1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['bimanual manipulation', 'voxel representation', 'vision language models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose VoxAct-B for bimanual manipulation, a language-conditioned, voxel-based method that leverages Vision Language Models to prioritize key regions within the scene and subsequently reconstruct a voxel grid with increased voxel resolution.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world $\texttt{Open Drawer}$ and $\texttt{Open Jar}$ tasks using two UR5s. Code, data, and videos are available at https://voxact-b.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5041c74adb82fc0acceed560e53f1416503ecd4c.pdf" target="_blank">/pdf/5041c74adb82fc0acceed560e53f1416503ecd4c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/427d8fa501769ad4bc853f23e5b35d7c08446f54.zip" target="_blank">/attachment/427d8fa501769ad4bc853f23e5b35d7c08446f54.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
liu2024voxactb,
title={VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation},
author={I-Chun Arthur Liu and Sicheng He and Daniel Seita and Gaurav S. Sukhatme},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=CPQW5kc0pe}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://voxact-b.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/0b14f76535d7abd51761a6f34f3d7e944f63a853.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=CPQW5kc0pe&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">liu|voxactb_voxelbased_acting_and_stabilizing_policy_for_bimanual_manipulation</div>
</div>
<div class='paper-counter'>201/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">Bq4XOaU4sV</div>
<div class="field-name">title:</div>
<div class="field-value">Bridging the Sim-to-Real Gap from the Information Bottleneck Perspective</div>
<div class="field-name">authors:</div>
<div class="field-value">['Haoran He', 'Peilin Wu', 'Chenjia Bai', 'Hang Lai', 'Lingxiao Wang', 'Ling Pan', 'Xiaolin Hu', 'Weinan Zhang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Haoran_He1', '~Peilin_Wu3', '~Chenjia_Bai2', '~Hang_Lai1', '~Lingxiao_Wang6', '~Ling_Pan1', '~Xiaolin_Hu1', '~Weinan_Zhang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Sim-to-Real', 'Information Bottleneck', 'Reinforcement Learning', 'Locomotion']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel privileged knowledge distillation method called the Historical Information Bottleneck (HIB) to bridge the sim-to-real gap.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning (RL) has recently achieved remarkable success in robotic control. However, most works in RL operate in simulated environments where privileged knowledge (e.g., dynamics, surroundings, terrains) is readily available. Conversely, in real-world scenarios, robot agents usually rely solely on local states (e.g., proprioceptive feedback of robot joints) to select actions, leading to a significant sim-to-real gap. Existing methods address this gap by either gradually reducing the reliance on privileged knowledge or performing a two-stage policy imitation. However, we argue that these methods are limited in their ability to fully leverage the available privileged knowledge, resulting in suboptimal performance. In this paper, we formulate the sim-to-real gap as an information bottleneck problem and therefore propose a novel privileged knowledge distillation method called the Historical Information Bottleneck (HIB). In particular, HIB learns a privileged knowledge representation from historical trajectories by capturing the underlying changeable dynamic information. Theoretical analysis shows that the learned privileged knowledge representation helps reduce the value discrepancy between the oracle and learned policies. Empirical experiments on both simulated and real-world tasks demonstrate that HIB yields improved generalizability compared to previous methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f86b69a45ffceb93d8980fdcc87c5a9e7dd86e8f.pdf" target="_blank">/pdf/f86b69a45ffceb93d8980fdcc87c5a9e7dd86e8f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e6700ea4ef905dbb0720aed959936a540ec7ef4b.zip" target="_blank">/attachment/e6700ea4ef905dbb0720aed959936a540ec7ef4b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
he2024bridging,
title={Bridging the Sim-to-Real Gap from the Information Bottleneck Perspective},
author={Haoran He and Peilin Wu and Chenjia Bai and Hang Lai and Lingxiao Wang and Ling Pan and Xiaolin Hu and Weinan Zhang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=Bq4XOaU4sV}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/history-ib</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/ac22a6034ee767077d4751a0831b0113ea82364a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=Bq4XOaU4sV&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">he|bridging_the_simtoreal_gap_from_the_information_bottleneck_perspective</div>
</div>
<div class='paper-counter'>202/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">BmvUg1FIWC</div>
<div class="field-name">title:</div>
<div class="field-value">Neural Inverse Source Problem</div>
<div class="field-name">authors:</div>
<div class="field-value">['Youngsun Wi', 'Jayjun Lee', 'Miquel Oller', 'Nima Fazeli']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Youngsun_Wi1', 'jayjun@umich.edu', '~Miquel_Oller1', '~Nima_Fazeli1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Inverse source problem', 'Physics informed neural network']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reconstructing unknown external source functions is an important perception capability for a large range of robotics domains including manipulation, aerial, and underwater robotics. In this work, we propose a Physics-Informed Neural Network (PINN) based approach for solving the inverse source problems in robotics, jointly identifying unknown source functions and the complete state of a system given partial and noisy observations.  Our approach demonstrates several advantages over prior works (Finite Element Methods (FEM) and data-driven approaches): it offers flexibility in integrating diverse constraints and boundary conditions; eliminates the need for complex discretizations (e.g., meshing); easily accommodates gradients from real measurements; and does not limit performance based on the diversity and quality of training data. We validate our method across three simulation and real-world scenarios involving up to 4th order partial differential equations (PDEs), constraints such as Signorini and Dirichlet, and various regression losses including Chamfer distance and L2 norm.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f4763f321518e56240c74460d0496716eec57145.pdf" target="_blank">/pdf/f4763f321518e56240c74460d0496716eec57145.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wi2024neural,
title={Neural Inverse Source Problem},
author={Youngsun Wi and Jayjun Lee and Miquel Oller and Nima Fazeli},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=BmvUg1FIWC}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a8df9f84a87bc712d44c6335cf033c3ced3bc6db.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=BmvUg1FIWC&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wi|neural_inverse_source_problem</div>
</div>
<div class='paper-counter'>203/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">B7Lf6xEv7l</div>
<div class="field-name">title:</div>
<div class="field-value">DiffusionSeeder: Seeding Motion Optimization with Diffusion for Rapid Motion Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huang Huang', 'Balakumar Sundaralingam', 'Arsalan Mousavian', 'Adithyavairavan Murali', 'Ken Goldberg', 'Dieter Fox']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huang_Huang1', '~Balakumar_Sundaralingam1', '~Arsalan_Mousavian1', '~Adithyavairavan_Murali2', '~Ken_Goldberg1', '~Dieter_Fox1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Motion Planning', 'Diffusion Model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose DiffusionSeeder, a diffusion based approach that generates trajectories to seed motion optimization for rapid robot motion planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Running optimization across many parallel seeds leveraging GPU compute [2] have relaxed the need for a good initialization, but this can fail if the problem is highly non-convex as all seeds could get stuck in local minima. One such setting is collision-free motion optimization for robot manipulation, where optimization converges quickly on easy problems but struggle in obstacle dense environments (e.g., a cluttered cabinet or table). In these situations, graph based planning algorithms are called to obtain seeds, resulting significant slowdowns. We propose DiffusionSeeder, a diffusion based approach that generates trajectories to seed motion optimization for rapid robot motion planning. DiffusionSeeder takes the initial depth image observation of the scene and generates high quality, multi-modal trajectories that are then fine-tuned with few iterations of motion optimization. We integrated DiffusionSeeder with cuRobo, a GPU-accelerated motion optimization method, to generate the seed trajectories which results in 12x speed up on average, and 36x speed up for more complicated problems, while achieving 10% higher success rate in partially observed simulation environments. Our results prove the effectiveness of using diverse solutions from learned diffusion model. Physical experiments on a Franka robot demonstrate the sim2real transfer of DiffusionSeeder to the real robot, with an average success rate of 86% and planning time of 26ms, increasing on cuRobo by 51% higher success rate and 2.5x speed up. The code and the model weights will be available after publication.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3802a98d25b1e8d899328e22eaf693ad826592c5.pdf" target="_blank">/pdf/3802a98d25b1e8d899328e22eaf693ad826592c5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/cd3175849fc5d944ea34849aa92d156f795e8cdd.zip" target="_blank">/attachment/cd3175849fc5d944ea34849aa92d156f795e8cdd.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024diffusionseeder,
title={DiffusionSeeder: Seeding Motion Optimization with Diffusion for Rapid Motion Planning},
author={Huang Huang and Balakumar Sundaralingam and Arsalan Mousavian and Adithyavairavan Murali and Ken Goldberg and Dieter Fox},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=B7Lf6xEv7l}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://diffusion-seeder.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/358f568db7a6ecda734ed8d02d47855f727c174d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=B7Lf6xEv7l&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|diffusionseeder_seeding_motion_optimization_with_diffusion_for_rapid_motion_planning</div>
</div>
<div class='paper-counter'>204/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">B45HRM4Wb4</div>
<div class="field-name">title:</div>
<div class="field-value">ResPilot: Teleoperated Finger Gaiting via Gaussian Process Residual Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Patrick Naughton', 'Jinda Cui', 'Karankumar Patel', 'Soshi Iba']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Patrick_Naughton1', '~Jinda_Cui1', '~Karankumar_Patel1', '~Soshi_Iba1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Teleoperation', 'Dexterous Manipulation', 'Gaussian Process']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We achieve teleoperated finger gaiting using Gaussian Process residual learning to calibrate a human hand to robot hand motion retargeter.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Dexterous robot hand teleoperation allows for long-range transfer of human manipulation expertise, and could simultaneously provide a way for humans to teach these skills to robots. However, current methods struggle to reproduce the functional workspace of the human hand, often limiting them to simple grasping tasks. We present a novel method for finger-gaited manipulation with multi-fingered robot hands. Our method provides the operator enhanced flexibility in making contacts by expanding the reachable workspace of the robot hand through residual Gaussian Process learning. We also assist the operator in maintaining stable contacts with the object by allowing them to constrain fingertips of the hand to move in concert. Extensive quantitative evaluations show that our method significantly increases the reachable workspace of the robot hand and enables the completion of novel dexterous finger gaiting tasks.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/19e82803e1263635e53dca77866fe0a99d30db18.pdf" target="_blank">/pdf/19e82803e1263635e53dca77866fe0a99d30db18.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/885bfaf8308ea7374123678dc940db2b447b8224.zip" target="_blank">/attachment/885bfaf8308ea7374123678dc940db2b447b8224.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
naughton2024respilot,
title={ResPilot: Teleoperated Finger Gaiting via Gaussian Process Residual Learning},
author={Patrick Naughton and Jinda Cui and Karankumar Patel and Soshi Iba},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=B45HRM4Wb4}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://respilot-hri.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/48358248d8c50e6ac58e31b7224a3ab2ead6fec6.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=B45HRM4Wb4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">naughton|respilot_teleoperated_finger_gaiting_via_gaussian_process_residual_learning</div>
</div>
<div class='paper-counter'>205/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">B2X57y37kC</div>
<div class="field-name">title:</div>
<div class="field-value">Learning to Look: Seeking Information for Decision Making via Policy Factorization</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shivin Dass', 'Jiaheng Hu', 'Ben Abbatematteo', 'Peter Stone', 'Roberto Martín-Martín']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shivin_Dass2', '~Jiaheng_Hu1', '~Ben_Abbatematteo1', '~Peter_Stone1', '~Roberto_Martín-Martín1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Active Vision', 'Manipulation', 'Interactive Perception']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study the problem of active perception in the manipulation domain and identify a subset of tasks within it as factorized CMDPs. We then propose DISaM, a dual-policy solution to enable information-seeking behavior in learnt agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Many robot manipulation tasks require active or interactive exploration behavior in order to be performed successfully. Such tasks are ubiquitous in embodied domains, where agents must actively search for the information necessary for each stage of a task, e.g., moving the head of the robot to find information relevant to manipulation, or in multi-robot domains, where one scout robot may search for the information that another robot needs to make informed decisions. We identify these tasks with a new type of problem, factorized Contextual Markov Decision Processes, and propose DISaM, a dual-policy solution composed of an information-seeking policy that explores the environment to find the relevant contextual information and an information-receiving policy that exploits the context to achieve the manipulation goal. This factorization allows us to train both policies separately, using the information-receiving one to provide reward to train the information-seeking policy. At test time, the dual agent balances exploration and exploitation based on the uncertainty the manipulation policy has on what the next best action is. We demonstrate the capabilities of our dual policy solution in five manipulation tasks that require information-seeking behaviors, both in simulation and in the real-world, where DISaM significantly outperforms existing methods. More information at https://robin-lab.cs.utexas.edu/learning2look/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f37084db4139cf6739ce4f4d87e1f759ce93fd58.pdf" target="_blank">/pdf/f37084db4139cf6739ce4f4d87e1f759ce93fd58.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dead20101f90259e5e4e601a4793d9943951482b.zip" target="_blank">/attachment/dead20101f90259e5e4e601a4793d9943951482b.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
dass2024learning,
title={Learning to Look: Seeking Information for Decision Making via Policy Factorization},
author={Shivin Dass and Jiaheng Hu and Ben Abbatematteo and Peter Stone and Roberto Mart{\'\i}n-Mart{\'\i}n},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=B2X57y37kC}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robin-lab.cs.utexas.edu/learning2look/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d768866fe9a91f483282e7648bc6fa5efaa3e624.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=B2X57y37kC&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">dass|learning_to_look_seeking_information_for_decision_making_via_policy_factorization</div>
</div>
<div class='paper-counter'>206/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AzP6kSEffm</div>
<div class="field-name">title:</div>
<div class="field-value">Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xiaomeng Xu', 'Huy Ha', 'Shuran Song']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xiaomeng_Xu1', '~Huy_Ha1', '~Shuran_Song3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['manipulator design', 'hardware optimization', 'diffusion model']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Generating task-specific manipulator designs without task-specific training</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present  Dynamics-Guided Diffusion Model (DGDM), a data-driven framework for generating task-specific manipulator designs without task-specific training. Given object shapes and task specifications, DGDM generates sensor-less manipulator designs that can blindly manipulate objects towards desired motions and poses using an open-loop parallel motion. This framework 1) flexibly represents manipulation tasks as interaction profiles, 2) represents the design space using a geometric diffusion model, and 3) efficiently searches this design space using the gradients provided by a dynamics network trained without any task information. We evaluate DGDM on various manipulation tasks ranging from shifting/rotating objects to converging objects to a specific pose. Our generated designs outperform optimization-based and unguided diffusion baselines relatively by 31.5\% and 45.3\% on average success rate. With the ability to generate a new design within 0.8s, DGDM facilitates rapid design iteration and enhances the adoption of data-driven approaches for robot mechanism design. Qualitative results are best viewed on our project website https://dgdmcorl.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ca96392ccd542b94e892a7249f225eeaf1512c03.pdf" target="_blank">/pdf/ca96392ccd542b94e892a7249f225eeaf1512c03.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f8581099fbef79f675705005a41de8c3b6c3cb22.zip" target="_blank">/attachment/f8581099fbef79f675705005a41de8c3b6c3cb22.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
xu2024dynamicsguided,
title={Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design},
author={Xiaomeng Xu and Huy Ha and Shuran Song},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AzP6kSEffm}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://dgdm-robot.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6176fa6a2e1a3e6a5cf22e9fc1cc3bd74a077203.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AzP6kSEffm&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">xu|dynamicsguided_diffusion_model_for_sensorless_robot_manipulator_design</div>
</div>
<div class='paper-counter'>207/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AuJnXGq3AL</div>
<div class="field-name">title:</div>
<div class="field-value">Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ria Doshi', 'Homer Rich Walke', 'Oier Mees', 'Sudeep Dasari', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ria_Doshi1', '~Homer_Rich_Walke1', '~Oier_Mees1', '~Sudeep_Dasari2', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Cross-Embodiment']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train a transformer-based policy on 900K robot trajectories across 20 embodiments with varying observation and action spaces and find that it matches performance of robot-specific approaches.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Modern machine learning systems rely on large datasets to attain broad generalization, and this often poses a challenge in robotic learning, where each robotic platform and task might have only a small dataset. By training a single policy across many different kinds of robots, a robotic learning method can leverage much broader and more diverse datasets, which in turn can lead to better generalization and robustness. However, training a single policy on multi-robot data is challenging because robots can have widely varying sensors, actuators, and control frequencies. We propose CrossFormer, a scalable and flexible transformer-based policy that can consume data from any embodiment. We train CrossFormer on the largest and most diverse dataset to date, 900K trajectories across 20 different robot embodiments. We demonstrate that the same network weights can control vastly different robots, including single and dual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds. Unlike prior work, our model does not require manual alignment of the observation or action spaces. Extensive experiments in the real world show that our method matches the performance of specialist policies tailored for each embodiment, while also significantly outperforming the prior state of the art in cross-embodiment learning.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/56f7efa76ef273c743e8584304f1e0005118d2aa.pdf" target="_blank">/pdf/56f7efa76ef273c743e8584304f1e0005118d2aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d5423814975ff6dde6f1d7bb5d0199a460c9812c.zip" target="_blank">/attachment/d5423814975ff6dde6f1d7bb5d0199a460c9812c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
doshi2024scaling,
title={Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation},
author={Ria Doshi and Homer Rich Walke and Oier Mees and Sudeep Dasari and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AuJnXGq3AL}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://crossformer-model.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/81754e9e1401a8d9b91019129e0578c7971e35fb.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AuJnXGq3AL&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">doshi|scaling_crossembodied_learning_one_policy_for_manipulation_navigation_locomotion_and_aviation</div>
</div>
<div class='paper-counter'>208/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AsbyZRdqPv</div>
<div class="field-name">title:</div>
<div class="field-value">Simple Masked Training Strategies Yield Control Policies That Are Robust to Sensor Failure</div>
<div class="field-name">authors:</div>
<div class="field-value">['Skand Skand', 'Bikram Pandit', 'Chanho Kim', 'Li Fuxin', 'Stefan Lee']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Skand_Skand1', '~Bikram_Pandit1', '~Chanho_Kim2', '~Li_Fuxin1', '~Stefan_Lee1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Robustness', 'Sensorimotor Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use a multimodal encoder with modality dropout to train policies that are robust to sensory failures during deployment.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Sensor failure is common when robots are deployed in the real world, as sensors naturally wear out over time. Such failures can lead to catastrophic outcomes, including damage to the robot from unexpected robot behaviors such as falling during walking. Previous work has tried to address this problem by recovering missing sensor values from the history of states or by adapting learned control policies to handle corrupted sensors through fine-tuning during deployment.
In this work, we propose training reinforcement learning (RL) policies that are robust to sensory failures. We use a multimodal encoder designed to account for these failures and a training strategy that randomly drops a subset of sensor modalities, similar to missing observations caused by failed sensors. We conduct evaluations across multiple tasks (bipedal locomotion and robotic manipulation) with varying robot embodiments in both simulation and the real world to demonstrate the effectiveness of our approach. Our results show that the proposed method produces robust RL policies that handle failures in both low-dimensional proprioceptive and high-dimensional visual modalities without a significant increase in training time or decrease in sample efficiency, making it a promising solution for learning RL policies robust to sensory failures.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4b786b975c1774f6a9b3b22d2ed34cea9c606792.pdf" target="_blank">/pdf/4b786b975c1774f6a9b3b22d2ed34cea9c606792.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/53a37ce3bb7c10f95f5bfe1fe73dd23a4f8c94a4.zip" target="_blank">/attachment/53a37ce3bb7c10f95f5bfe1fe73dd23a4f8c94a4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
skand2024simple,
title={Simple Masked Training Strategies Yield Control Policies That Are Robust to Sensor Failure},
author={Skand Skand and Bikram Pandit and Chanho Kim and Li Fuxin and Stefan Lee},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AsbyZRdqPv}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://pvskand.github.io/projects/RME</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3b2b45a9ddaa0abc6ef8a46e95ab5569daf998c0.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AsbyZRdqPv&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">skand|simple_masked_training_strategies_yield_control_policies_that_are_robust_to_sensor_failure</div>
</div>
<div class='paper-counter'>209/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AhEE5wrcLU</div>
<div class="field-name">title:</div>
<div class="field-value">Velociraptor: Leveraging Visual Foundation Models for Label-Free, Risk-Aware Off-Road Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Samuel Triest', 'Matthew Sivaprakasam', 'Shubhra Aich', 'David Fan', 'Wenshan Wang', 'Sebastian Scherer']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Samuel_Triest1', '~Matthew_Sivaprakasam1', '~Shubhra_Aich1', '~David_Fan1', '~Wenshan_Wang2', '~Sebastian_Scherer1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Field Robotics', 'Self-Supervised Learning', 'Visual Foundation Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a fully self-supervised traversability learning method that leverages visual foundation model features and demonstrate it in high-speed off-road navigation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Traversability analysis in off-road regimes is a challenging task that requires understanding of multi-modal inputs such as camera and LiDAR. These measurements are often sparse, noisy, and difficult to interpret, particularly in the off-road setting. Existing systems are very engineering-intensive, often requiring hand-tuning of traversability rules and manual annotation of semantic labels. Furthermore, existing methods for analyzing traversability risk and uncertainty are computationally expensive or not well-calibrated. We propose Velociraptor, a traversability analysis system that performs [veloci]ty-informed, [r]isk-[a]ware [p]erception and [t]raversability for [o]ff-[r]oad driving without any human annotations. We achieve this via the use of visual foundation models (VFMs) and geometric mapping to produce a rich visual-geometric representation of the robot's local environment. We then leverage this representation to produce costmaps, speedmaps, and uncertainty maps using state-of-the-art fully self-supervised techniques. Our approach enables intelligent high-speed off-road navigation with zero human annotation, and with about forty minutes of expert data, outperforms several geometric and semantic traversability baselines, both in offline and real-world robot trials across multiple challenging off-road sites.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/e5c744ff77314c97c288f6a2a5300805ef3330f9.pdf" target="_blank">/pdf/e5c744ff77314c97c288f6a2a5300805ef3330f9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ab31fe2566e74c3ee0af29e0a42e7d5c9b4fbed2.zip" target="_blank">/attachment/ab31fe2566e74c3ee0af29e0a42e7d5c9b4fbed2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
triest2024velociraptor,
title={Velociraptor: Leveraging Visual Foundation Models for Label-Free, Risk-Aware Off-Road Navigation},
author={Samuel Triest and Matthew Sivaprakasam and Shubhra Aich and David Fan and Wenshan Wang and Sebastian Scherer},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AhEE5wrcLU}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/eb0f03c89efea819d5d5cb77b55471f16667d6ec.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AhEE5wrcLU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">triest|velociraptor_leveraging_visual_foundation_models_for_labelfree_riskaware_offroad_navigation</div>
</div>
<div class='paper-counter'>210/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AGG1zlrrMw</div>
<div class="field-name">title:</div>
<div class="field-value">Neural Attention Field: Emerging Point Relevance in 3D Scenes for One-Shot Dexterous Grasping</div>
<div class="field-name">authors:</div>
<div class="field-value">['Qianxu Wang', 'Congyue Deng', 'Tyler Ga Wei Lum', 'Yuanpei Chen', 'Yaodong Yang', 'Jeannette Bohg', 'Yixin Zhu', 'Leonidas Guibas']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Qianxu_Wang1', '~Congyue_Deng1', '~Tyler_Ga_Wei_Lum1', '~Yuanpei_Chen2', '~Yaodong_Yang1', '~Jeannette_Bohg1', '~Yixin_Zhu1', '~Leonidas_Guibas1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Desterous Grasping', 'One-Shot Manipulation', 'Distilled Feature Field', 'Neural Implicit Field', 'Self-Supervised Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">One-shot transfer of dexterous grasps to novel scenes with object and context variations has been a challenging problem. While distilled feature fields from large vision models have enabled semantic correspondences across 3D scenes, their features are point-based and restricted to object surfaces, limiting their capability of modeling complex semantic feature distributions for hand-object interactions. In this work, we propose the *neural attention field* for representing semantic-aware dense feature fields in the 3D space by modeling inter-point relevance instead of individual point features. Core to it is a transformer decoder that computes the cross-attention between any 3D query point with all the scene points, and provides the query point feature with an attention-based aggregation. We further propose a self-supervised framework for training the transformer decoder from only a few 3D pointclouds without hand demonstrations. Post-training, the attention field can be applied to novel scenes for semantics-aware dexterous grasping from one-shot demonstration. Experiments show that our method provides better optimization landscapes by encouraging the end-effector to focus on task-relevant scene regions, resulting in significant improvements in success rates on real robots compared with the feature-field-based methods.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/b4ec08201168e297998c6953df11a168619ee579.pdf" target="_blank">/pdf/b4ec08201168e297998c6953df11a168619ee579.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/4b5c004e23e9bb3b7f5794c0d82f478b4f42e9f8.zip" target="_blank">/attachment/4b5c004e23e9bb3b7f5794c0d82f478b4f42e9f8.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024neural,
title={Neural Attention Field: Emerging Point Relevance in 3D Scenes for One-Shot Dexterous Grasping},
author={Qianxu Wang and Congyue Deng and Tyler Ga Wei Lum and Yuanpei Chen and Yaodong Yang and Jeannette Bohg and Yixin Zhu and Leonidas Guibas},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AGG1zlrrMw}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/39b49b3a5088dfca918ffbdca61ea668228fd441.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AGG1zlrrMw&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|neural_attention_field_emerging_point_relevance_in_3d_scenes_for_oneshot_dexterous_grasping</div>
</div>
<div class='paper-counter'>211/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">AEq0onGrN2</div>
<div class="field-name">title:</div>
<div class="field-value">Physically Embodied Gaussian Splatting: A Visually Learnt and Physically Grounded 3D Representation for Robotics</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jad Abou-Chakra', 'Krishan Rana', 'Feras Dayoub', 'Niko Suenderhauf']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jad_Abou-Chakra1', '~Krishan_Rana1', '~Feras_Dayoub1', '~Niko_Suenderhauf1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['3D Representation', 'Gaussian Splatting', 'Robotics', 'Tracking', 'Physics']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use particles to represent the physical state of a robot's world and 3D Gaussians for its visual state, combining them to enable forward modeling and realtime visual-based correction from images coming from 3 cameras.</div>
<div class="field-name">abstract:</div>
<div class="field-value">For robots to robustly understand and interact with the physical world, it is highly beneficial to have a comprehensive representation  -- modelling geometry, physics, and visual observations -- that informs perception, planning, and control algorithms. We propose a novel dual "Gaussian-Particle" representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates "visual forces" that correct the particle positions while respecting known physical constraints. By integrating predictive physical modeling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality. We validate our approach on 2D and 3D tracking tasks as well as photometric reconstruction quality. Videos are found at https://embodied-gaussians.github.io/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fab06e004fa48c6b6d3fb84170fc8715e5a678e2.pdf" target="_blank">/pdf/fab06e004fa48c6b6d3fb84170fc8715e5a678e2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/97e886f5218e2ab43716005edaf75b31d6bf5956.zip" target="_blank">/attachment/97e886f5218e2ab43716005edaf75b31d6bf5956.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
abou-chakra2024physically,
title={Physically Embodied Gaussian Splatting: A Visually Learnt and Physically Grounded 3D Representation for Robotics},
author={Jad Abou-Chakra and Krishan Rana and Feras Dayoub and Niko Suenderhauf},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=AEq0onGrN2}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://embodied-gaussians.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/57ce1cfa7d12fa2933479495456a9652c537b033.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=AEq0onGrN2&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">abouchakra|physically_embodied_gaussian_splatting_a_visually_learnt_and_physically_grounded_3d_representation_for_robotics</div>
</div>
<div class='paper-counter'>212/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">A6ikGJRaKL</div>
<div class="field-name">title:</div>
<div class="field-value">KOROL: Learning Visualizable Object Feature with Koopman Operator Rollout for Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hongyi Chen', 'ABULIKEMU ABUDUWEILI', 'Aviral Agrawal', 'Yunhai Han', 'Harish Ravichandar', 'Changliu Liu', 'Jeffrey Ichnowski']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hongyi_Chen2', '~ABULIKEMU_ABUDUWEILI1', '~Aviral_Agrawal1', '~Yunhai_Han1', '~Harish_Ravichandar1', '~Changliu_Liu1', '~Jeffrey_Ichnowski1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Manipulation', 'Koopman Operator', 'Visual Representation Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning dexterous manipulation skills presents significant challenges due to complex nonlinear dynamics that underlie the interactions between objects and multi-fingered hands. Koopman operators have emerged as a robust method for modeling such nonlinear dynamics within a linear framework.
However, current methods rely on runtime access to ground-truth (GT) object states, making them unsuitable for vision-based practical applications.
Unlike image-to-action policies that implicitly learn visual features for control, we use a dynamics model, specifically the Koopman operator, to learn visually interpretable object features critical for robotic manipulation within a scene.
We construct a Koopman operator using object features predicted by a feature extractor and utilize it to auto-regressively advance system states. We train the feature extractor to embed scene information into object features, thereby enabling the accurate propagation of robot trajectories.
We evaluate our approach on simulated and real-world robot tasks, with results showing that it outperformed the model-based imitation learning NDP by 1.08$\times$ and the image-to-action Diffusion Policy by 1.16$\times$. The results suggest that our method maintains task success rates with learned features and extends applicability to real-world manipulation without GT object states. Project video and code are available at: https://github.com/hychen-naza/KOROL.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/23c46101bf3d6c866bdedc3d31acae87e2d502fb.pdf" target="_blank">/pdf/23c46101bf3d6c866bdedc3d31acae87e2d502fb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/49581c3230a9894bb2c111223d4ba59d204f6943.zip" target="_blank">/attachment/49581c3230a9894bb2c111223d4ba59d204f6943.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024korol,
title={{KOROL}: Learning Visualizable Object Feature with Koopman Operator Rollout for Manipulation},
author={Hongyi Chen and ABULIKEMU ABUDUWEILI and Aviral Agrawal and Yunhai Han and Harish Ravichandar and Changliu Liu and Jeffrey Ichnowski},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=A6ikGJRaKL}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c6c1e515d8e043a0395d87585a334429ebe873ef.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=A6ikGJRaKL&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|korol_learning_visualizable_object_feature_with_koopman_operator_rollout_for_manipulation</div>
</div>
<div class='paper-counter'>213/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">A1hpY5RNiH</div>
<div class="field-name">title:</div>
<div class="field-value">What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kaylee Burns', 'Zach Witzel', 'Jubayer Ibn Hamid', 'Tianhe Yu', 'Chelsea Finn', 'Karol Hausman']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kaylee_Burns2', '~Zach_Witzel1', '~Jubayer_Ibn_Hamid1', '~Tianhe_Yu1', '~Chelsea_Finn1', '~Karol_Hausman2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['representation learning', 'manipulation', 'visual features']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as a means to improve the learning efficiency and generalization ability of policies learned from pixels. To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing diverse tasks, in pursuit of manipulation-relevant features. Although this approach improves the efficiency of policy learning, it remains unclear how reliable these representations are in the presence of distribution shifts that arise commonly in robotic applications. Surprisingly, we find that visual representations designed for control tasks do not necessarily generalize under subtle changes in lighting and scene texture or the introduction of distractor objects. To understand what properties _do_ lead to robust representations, we compare the performance of 15 pre-trained vision models under different visual appearances. We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT models. The rank order induced by this metric is more predictive than metrics that have previously guided generalization research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by cue-conflict performance. We test this finding extensively on a suite of distribution shifts in ten tasks across two simulated manipulation environments. On the ALOHA setup, segmentation score predicts real-world performance after offline training with 50 demonstrations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d9671bcf6104cc5428d85bb96d1819f2a92748da.pdf" target="_blank">/pdf/d9671bcf6104cc5428d85bb96d1819f2a92748da.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/575e2da8ba3d7fd7c1a6ed8bca295f2216d30995.zip" target="_blank">/attachment/575e2da8ba3d7fd7c1a6ed8bca295f2216d30995.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
burns2024what,
title={What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?},
author={Kaylee Burns and Zach Witzel and Jubayer Ibn Hamid and Tianhe Yu and Chelsea Finn and Karol Hausman},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=A1hpY5RNiH}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://kayburns.github.io/segmentingfeatures/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/90934d0ad5fb1f9069da90df6422ab75111882ce.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=A1hpY5RNiH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">burns|what_makes_pretrained_visual_representations_successful_for_robust_manipulation</div>
</div>
<div class='paper-counter'>214/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9jJP2J1oBP</div>
<div class="field-name">title:</div>
<div class="field-value">Leveraging Mutual Information for Asymmetric Learning under Partial Observability</div>
<div class="field-name">authors:</div>
<div class="field-value">['Hai Huu Nguyen', 'Long Dinh Van The', 'Christopher Amato', 'Robert Platt']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Hai_Huu_Nguyen1', '~Long_Dinh_Van_The1', '~Christopher_Amato1', '~Robert_Platt1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Partial Observability', 'Mutual Information', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes to use the mutual information between the history and the state to improve learning under partial observability, assuming state availability during training.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Even though partial observability is prevalent in robotics, most reinforcement learning studies avoid it due to the difficulty of learning a policy that can efficiently memorize past events and seek information. Fortunately, in many cases, learning can be done in an asymmetric setting where states are available during training but not during execution. Prior studies often leverage the state to indirectly influence the training of a history-based actor (actor-critic methods) or a history-based critic (value-based methods). Instead, we propose using state-observation and state-history mutual information to improve the agent's architecture and ability to seek information and memorize efficiently through intrinsic rewards and an auxiliary task. Our method outperforms strong baselines through extensive experiments and achieves successful sim-to-real transfers to a real robot.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ae8563da9955c6378a01414cc0d5c9c71204adee.pdf" target="_blank">/pdf/ae8563da9955c6378a01414cc0d5c9c71204adee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/39344a49b05e52800ba528bf1df6d7295295085f.zip" target="_blank">/attachment/39344a49b05e52800ba528bf1df6d7295295085f.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
nguyen2024leveraging,
title={Leveraging Mutual Information for Asymmetric Learning under Partial Observability},
author={Hai Huu Nguyen and Long Dinh Van The and Christopher Amato and Robert Platt},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9jJP2J1oBP}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/mi-asym-pomdp</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/97174d1d03d475b99770bd77d0fc2e9156aac101.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9jJP2J1oBP&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">nguyen|leveraging_mutual_information_for_asymmetric_learning_under_partial_observability</div>
</div>
<div class='paper-counter'>215/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9iG3SEbMnL</div>
<div class="field-name">title:</div>
<div class="field-value">ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Wenlong Huang', 'Chen Wang', 'Yunzhu Li', 'Ruohan Zhang', 'Li Fei-Fei']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Wenlong_Huang1', '~Chen_Wang16', '~Yunzhu_Li1', '~Ruohan_Zhang1', '~Li_Fei-Fei1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Structural Representation', 'Model-Based Planning', 'Foundation Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Large vision models and vision-language models can generate a spatio-temporal series of constraints that map keypoints to a numerical cost, which are optimized to obtain actions for multi-stage, in-the-wild, bimanual, and reactive manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep are expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observation. We present system implementations on a mobile single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/1dd9dafa8ab5088141114af205652a5d948f2c67.pdf" target="_blank">/pdf/1dd9dafa8ab5088141114af205652a5d948f2c67.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/73867ed05093550cd8131231d2eece8f1ef0ec1c.zip" target="_blank">/attachment/73867ed05093550cd8131231d2eece8f1ef0ec1c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024rekep,
title={ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation},
author={Wenlong Huang and Chen Wang and Yunzhu Li and Ruohan Zhang and Li Fei-Fei},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9iG3SEbMnL}
}</div>
<div class="field-name">website:</div>
<div class="field-value">rekep-robot.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b65ed8286f84d56a78d30f7a54f37308bdaa8b25.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9iG3SEbMnL&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|rekep_spatiotemporal_reasoning_of_relational_keypoint_constraints_for_robotic_manipulation</div>
</div>
<div class='paper-counter'>216/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9dsBQhoqVr</div>
<div class="field-name">title:</div>
<div class="field-value">Fleet Supervisor Allocation: A Submodular Maximization Approach</div>
<div class="field-name">authors:</div>
<div class="field-value">['Oguzhan Akcin', 'Ahmet Ege Tanriverdi', 'Kaan Kale', 'Sandeep P. Chinchali']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Oguzhan_Akcin2', '~Ahmet_Ege_Tanriverdi1', '~Kaan_Kale1', '~Sandeep_P._Chinchali1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Imitation Learning', 'Submodular Maximization', 'Fleet Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">In real-world scenarios, the data collected by robots in diverse and unpredictable environments is crucial for enhancing their perception and decision-making models. This data is predominantly collected under human supervision, particularly through imitation learning (IL), where robots learn complex tasks by observing human supervisors. However, the deployment of multiple robots and supervisors to accelerate the learning process often leads to data redundancy and inefficiencies, especially as the scale of robot fleets increases. Moreover, the reliance on teleoperation for supervision introduces additional challenges due to potential network connectivity issues. 
To address these issues in data collection, we introduce an Adaptive Submodular Allocation policy, ASA, designed for efficient human supervision allocation within multi-robot systems under uncertain connectivity conditions. Our approach reduces data redundancy by balancing the informativeness and diversity of data collection, and is capable of accommodating connectivity variances. We evaluate the effectiveness of ASA in simulations with 100 robots across four different environments and various network settings, including a real-world teleoperation scenario over a 5G network. We train and test our policy, ASA, and state-of-the-art policies utilizing NVIDIA's Isaac Gym. Our results show that ASA enhances the return on human effort by up to $3.37\times$, outperforming current baselines in all simulated scenarios and providing robustness against connectivity disruptions.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/23053d55f68f8281a5a23351f8a3006912cce579.pdf" target="_blank">/pdf/23053d55f68f8281a5a23351f8a3006912cce579.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/951a39444173d20924a61c00f4353598eda9517d.zip" target="_blank">/attachment/951a39444173d20924a61c00f4353598eda9517d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
akcin2024fleet,
title={Fleet Supervisor Allocation: A Submodular Maximization Approach},
author={Oguzhan Akcin and Ahmet Ege Tanriverdi and Kaan Kale and Sandeep P. Chinchali},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9dsBQhoqVr}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7b65b5fdedf931a1cb9f4bd65c04521067e8d96e.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9dsBQhoqVr&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">akcin|fleet_supervisor_allocation_a_submodular_maximization_approach</div>
</div>
<div class='paper-counter'>217/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9aZ4ehSTRc</div>
<div class="field-name">title:</div>
<div class="field-value">Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jean Pierre Sleiman', 'Mayank Mittal', 'Marco Hutter']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jean_Pierre_Sleiman1', '~Mayank_Mittal1', '~Marco_Hutter1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Whole-body Loco-Manipulation', 'Reinforcement Learning', 'Legged Mobile Manipulators']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work proposes a task-agnostic RL formulation that leverages a single demonstration to train robust policies for complex multi-contact behaviors, such as traversing spring loaded doors.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) has shown remarkable proficiency in developing robust control policies for contact-rich applications. However, it typically requires meticulous Markov Decision Process (MDP) designing tailored to each task and robotic platform. This work addresses this challenge by creating a systematic approach to behavior synthesis and control for multi-contact loco-manipulation.
We define a task-independent MDP formulation to learn robust RL policies using a single demonstration (per task) generated from a fast model-based trajectory optimization method. Our framework is validated on diverse real-world tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. The learned behaviors can handle dynamic uncertainties and external disturbances, showcasing recovery maneuvers, such as re-grasping objects during execution. Finally, we successfully transfer the policies to a real robot, demonstrating the approach's practical viability.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/abea175a78f0202f9cdaea5a9b48878dc242cd13.pdf" target="_blank">/pdf/abea175a78f0202f9cdaea5a9b48878dc242cd13.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c65c3ebdae36cddc775c41a02304da8cf5af0ee0.zip" target="_blank">/attachment/c65c3ebdae36cddc775c41a02304da8cf5af0ee0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
sleiman2024guided,
title={Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation},
author={Jean Pierre Sleiman and Mayank Mittal and Marco Hutter},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9aZ4ehSTRc}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://leggedrobotics.github.io/guided-rl-locoma/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5c3ef304a5cb5378ee4685b2176961249e408a43.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9aZ4ehSTRc&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">sleiman|guided_reinforcement_learning_for_robust_multicontact_locomanipulation</div>
</div>
<div class='paper-counter'>218/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9XV3dBqcfe</div>
<div class="field-name">title:</div>
<div class="field-value">Generalized Animal Imitator: Agile Locomotion with Versatile Motion Prior</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ruihan Yang', 'Zhuoqun Chen', 'Jianhan Ma', 'Chongyi Zheng', 'Yiyu Chen', 'Quan Nguyen', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ruihan_Yang2', '~Zhuoqun_Chen1', '~Jianhan_Ma1', '~Chongyi_Zheng1', '~Yiyu_Chen4', '~Quan_Nguyen5', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Legged Robots', 'Imitation Learning', 'Agile Locomotion']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our system learns a multiple agile locomotion skills with a single instructable motion prior from a diverse reference motion dataset.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The agility of animals, particularly in complex activities such as running, turning, jumping, and backflipping, stands as an exemplar for robotic system design. Transferring this suite of behaviors to legged robotic systems introduces essential inquiries: How can a robot be trained to learn multiple locomotion behaviors simultaneously? How can the robot execute these tasks with a smooth transition? How to integrate these skills for wide-range applications? This paper introduces the Versatile Instructable Motion prior (VIM) – a Reinforcement Learning framework designed to incorporate a range of agile locomotion tasks suitable for advanced robotic applications. Our framework enables legged robots to learn diverse agile low-level skills by imitating animal motions and manually designed motions. Our Functionality reward guides the robot's ability to adopt varied skills, and our Stylization reward ensures that robot motions align with reference motions. Our evaluations of the VIM framework span both simulation environments and real-world deployment. To the best of our knowledge, this is the first work that allows a robot to concurrently learn diverse agile locomotion skills using a single learning-based controller in the real world.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/baa42585c78975cde3f082fbdfe63aeae26c9338.pdf" target="_blank">/pdf/baa42585c78975cde3f082fbdfe63aeae26c9338.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/38f42c37a4ffe3376ccbbaef5a83c7167cea7566.zip" target="_blank">/attachment/38f42c37a4ffe3376ccbbaef5a83c7167cea7566.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yang2024generalized,
title={Generalized Animal Imitator: Agile Locomotion with Versatile Motion Prior},
author={Ruihan Yang and Zhuoqun Chen and Jianhan Ma and Chongyi Zheng and Yiyu Chen and Quan Nguyen and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9XV3dBqcfe}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://rchalyang.github.io/VIM/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c355ce8c043593adda28241862358d84a0e9fe65.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9XV3dBqcfe&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yang|generalized_animal_imitator_agile_locomotion_with_versatile_motion_prior</div>
</div>
<div class='paper-counter'>219/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">9HkElMlPbU</div>
<div class="field-name">title:</div>
<div class="field-value">Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Teli Ma', 'Jiaming Zhou', 'Zifan Wang', 'Ronghe Qiu', 'Junwei Liang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Teli_Ma1', '~Jiaming_Zhou1', '~Zifan_Wang7', '~Ronghe_Qiu2', '~Junwei_Liang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Contrastive imitation learning', 'Multi-task learning', 'Robotic manipulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing robots capable of executing various manipulation tasks, guided by natural language instructions and visual observations of intricate real-world environments, remains a significant challenge in robotics. Such robot agents need to understand linguistic commands and  distinguish between the requirements of different tasks. In this work, we present $\mathtt{\Sigma\mbox{-}agent}$, an end-to-end imitation learning agent for multi-task robotic manipulation. $\mathtt{\Sigma\mbox{-}agent}$ incorporates contrastive Imitation Learning (contrastive IL) modules to strengthen vision-language and current-future representations. An effective and efficient multi-view querying Transformer (MVQ-Former) for aggregating representative semantic information is introduced. $\mathtt{\Sigma\mbox{-}agent}$ shows substantial improvement over state-of-the-art methods under diverse settings in 18 RLBench tasks, surpassing RVT by an average of 5.2% and 5.9% in 10 and 100 demonstration training, respectively. $\mathtt{\Sigma\mbox{-}agent}$ also achieves 62% success rate with a single policy in 5 real-world manipulation tasks. The code will be released upon acceptance.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/6982f1a52f3ba3cbc25906960896290ed61645bc.pdf" target="_blank">/pdf/6982f1a52f3ba3cbc25906960896290ed61645bc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/41e2e342a34331a80626426e9a2f5760692653d2.zip" target="_blank">/attachment/41e2e342a34331a80626426e9a2f5760692653d2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ma2024contrastive,
title={Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation},
author={Teli Ma and Jiaming Zhou and Zifan Wang and Ronghe Qiu and Junwei Liang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=9HkElMlPbU}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://teleema.github.io/projects/Sigma_Agent/index.html</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/6c64659637a6fb85ff28fb6c30990ebb0795118f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=9HkElMlPbU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ma|contrastive_imitation_learning_for_languageguided_multitask_robotic_manipulation</div>
</div>
<div class='paper-counter'>220/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">97QXO0uBEO</div>
<div class="field-name">title:</div>
<div class="field-value">Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jonas Günster', 'Puze Liu', 'Jan Peters', 'Davide Tateo']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jonas_Günster1', '~Puze_Liu1', '~Jan_Peters3', '~Davide_Tateo2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Safe Reinforcement Learning', 'Chance Constraint', 'Distributional RL']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Safety is one of the key issues preventing the deployment of reinforcement learning techniques in real-world robots. While most approaches in the Safe Reinforcement Learning area do not require prior knowledge of constraints and robot kinematics and rely solely on data, it is often difficult to deploy them in complex real-world settings. Instead, model-based approaches that incorporate prior knowledge of the constraints and dynamics into the learning framework have proven capable of deploying the learning algorithm directly on the real robot.
Unfortunately, while an approximated model of the robot dynamics is often available, the safety constraints are task-specific and hard to obtain: they may be too complicated to encode analytically, too expensive to compute, or it may be difficult to envision a priori the long-term safety requirements. In this paper, we bridge this gap by extending the safe exploration method, ATACOM, with learnable constraints, with a particular focus on ensuring long-term safety and handling of uncertainty. Our approach is competitive or superior to state-of-the-art methods in final performance while maintaining safer behavior during training.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4bd9f1f29b35b6d3f008f23acf15d52579c9b054.pdf" target="_blank">/pdf/4bd9f1f29b35b6d3f008f23acf15d52579c9b054.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d59b2cb21876f93d73d9069d2a8215d0922524d0.zip" target="_blank">/attachment/d59b2cb21876f93d73d9069d2a8215d0922524d0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
g{\"u}nster2024handling,
title={Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning},
author={Jonas G{\"u}nster and Puze Liu and Jan Peters and Davide Tateo},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=97QXO0uBEO}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3c1d93288742e131b08fc511ddebe2bfd352935a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=97QXO0uBEO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">günster|handling_longterm_safety_and_uncertainty_in_safe_reinforcement_learning</div>
</div>
<div class='paper-counter'>221/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">928V4Umlys</div>
<div class="field-name">title:</div>
<div class="field-value">DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xiaoyu Tian', 'Junru Gu', 'Bailin Li', 'Yicheng Liu', 'Yang Wang', 'Zhiyong Zhao', 'Kun Zhan', 'Peng Jia', 'XianPeng Lang', 'Hang Zhao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xiaoyu_Tian3', '~Junru_Gu1', '~Bailin_Li3', '~Yicheng_Liu2', '~Yang_Wang56', '~Zhiyong_Zhao2', '~Kun_Zhan3', '~Peng_Jia1', '~XianPeng_Lang1', '~Hang_Zhao1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Autonomous Driving', 'Vision Language Model', 'Dual System']</div>
<div class="field-name">abstract:</div>
<div class="field-value">A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and unpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a production vehicle, verifying it is effective in real-world autonomous driving environments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/df7c5b77cb5e68213b8b1ecaa0bf0b1ee3f5d16b.pdf" target="_blank">/pdf/df7c5b77cb5e68213b8b1ecaa0bf0b1ee3f5d16b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/1b81b0d29e93277361dc141ffd2f5af9f3d0f07e.zip" target="_blank">/attachment/1b81b0d29e93277361dc141ffd2f5af9f3d0f07e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tian2024drivevlm,
title={Drive{VLM}: The Convergence of Autonomous Driving and Large Vision-Language Models},
author={Xiaoyu Tian and Junru Gu and Bailin Li and Yicheng Liu and Yang Wang and Zhiyong Zhao and Kun Zhan and Peng Jia and XianPeng Lang and Hang Zhao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=928V4Umlys}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://tsinghua-mars-lab.github.io/DriveVLM/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/edc3943c7230a98289a08a560afbc169e7b3d2d9.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=928V4Umlys&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tian|drivevlm_the_convergence_of_autonomous_driving_and_large_visionlanguage_models</div>
</div>
<div class='paper-counter'>222/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8Yu0TNJNGK</div>
<div class="field-name">title:</div>
<div class="field-value">AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch</div>
<div class="field-name">authors:</div>
<div class="field-value">['Max Yang', 'chenghua lu', 'Alex Church', 'Yijiong Lin', 'Christopher J. Ford', 'Haoran Li', 'Efi Psomopoulou', 'David A.W. Barton', 'Nathan F. Lepora']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Max_Yang1', '~chenghua_lu1', 'alex.church@caint.io', '~Yijiong_Lin2', '~Christopher_J._Ford1', '~Haoran_Li19', '~Efi_Psomopoulou1', '~David_A.W._Barton1', '~Nathan_F._Lepora1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Tactile Sensing', 'In-hand Object Rotation', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present AnyRotate, a system that achieves multi-axis gravity-invariant in-hand object rotation with dense featured sim-to-real touch.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human hands are capable of in-hand manipulation in the presence of different hand motions. For a robot hand, harnessing rich tactile information to achieve this level of dexterity still remains a significant challenge. In this paper, we present AnyRotate, a system for gravity-invariant multi-axis in-hand object rotation using dense featured sim-to-real touch. We tackle this problem by training a dense tactile policy in simulation and present a sim-to-real method for rich tactile sensing to achieve zero-shot policy transfer. Our formulation allows the training of a unified policy to rotate unseen objects about arbitrary rotation axes in any hand direction. In our experiments, we highlight the benefit of capturing detailed contact information when handling objects of varying properties. Interestingly, we found rich multi-fingered tactile sensing can detect unstable grasps and provide a reactive behavior that improves the robustness of the policy.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d7e57c896e26a1ce748ade9404b583c0704ab849.pdf" target="_blank">/pdf/d7e57c896e26a1ce748ade9404b583c0704ab849.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/278c01253ca0e1933845391d5486180c5d7c4ac0.zip" target="_blank">/attachment/278c01253ca0e1933845391d5486180c5d7c4ac0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yang2024anyrotate,
title={AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch},
author={Max Yang and chenghua lu and Alex Church and Yijiong Lin and Christopher J. Ford and Haoran Li and Efi Psomopoulou and David A.W. Barton and Nathan F. Lepora},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8Yu0TNJNGK}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://maxyang27896.github.io/anyrotate/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/81817ad0eed1adb5c4f43ea4d87b2401cd9d2b39.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8Yu0TNJNGK&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yang|anyrotate_gravityinvariant_inhand_object_rotation_with_simtoreal_touch</div>
</div>
<div class='paper-counter'>223/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8XFT1PatHy</div>
<div class="field-name">title:</div>
<div class="field-value">Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting</div>
<div class="field-name">authors:</div>
<div class="field-value">['Olaolu Shorinwa', 'Johnathan Tucker', 'Aliyah Smith', 'Aiden Swann', 'Timothy Chen', 'Roya Firoozi', 'Monroe David Kennedy', 'Mac Schwager']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Olaolu_Shorinwa1', 'jatucker@stanford.edu', 'aliyah1@stanford.edu', '~Aiden_Swann1', '~Timothy_Chen1', 'rfiroozi@stanford.edu', '~Monroe_David_Kennedy1', '~Mac_Schwager1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Gaussian Splatting', 'Robotic Grasping', 'Robotic Manipulation', 'Scene Editing']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting scene representations to enable multi-stage manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present Splat-MOVER, a modular robotics stack for open-vocabulary
robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) ASK-Splat, a GSplat representation that distills semantic and grasp
affordance features into the 3D scene. ASK-Splat enables geometric, semantic,
and affordance understanding of 3D scenes, which is critical for many robotics
tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D semantic masking
and infilling to visualize the motions of objects that result from robot interactions
in the real-world. SEE-Splat creates a “digital twin” of the evolving environment
throughout the manipulation task; and (iii) Grasp-Splat, a grasp generation module
that uses ASK-Splat and SEE-Splat to propose affordance-aligned candidate grasps
for open-world objects. ASK-Splat is trained in real-time from RGB images
in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat
run in real-time during operation. We demonstrate the superior performance of
Splat-MOVER in hardware experiments on a Kinova robot compared to two recent
baselines in four single-stage, open-vocabulary manipulation tasks. In addition, we
demonstrate Splat-MOVER in four multi-stage manipulation tasks, using the edited
scene to reflect changes due to prior manipulation stages, which is not possible
with existing baselines. Video demonstrations and the code for the project are 
available at https://splatmover.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8246960dad685641143075980779dbcf7ca6fc47.pdf" target="_blank">/pdf/8246960dad685641143075980779dbcf7ca6fc47.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3aee93ae7c2dba5136fb1543122a328fe3a6c623.zip" target="_blank">/attachment/3aee93ae7c2dba5136fb1543122a328fe3a6c623.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
shorinwa2024splatmover,
title={Splat-{MOVER}: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting},
author={Olaolu Shorinwa and Johnathan Tucker and Aliyah Smith and Aiden Swann and Timothy Chen and Roya Firoozi and Monroe David Kennedy and Mac Schwager},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8XFT1PatHy}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://splatmover.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8c137f3ec42a2004bf0bb811c6727729d05d1708.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8XFT1PatHy&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">shorinwa|splatmover_multistage_openvocabulary_robotic_manipulation_via_editable_gaussian_splatting</div>
</div>
<div class='paper-counter'>224/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8PcRynpd1m</div>
<div class="field-name">title:</div>
<div class="field-value">Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yunyue Wei', 'Zeji Yi', 'Hongda Li', 'Saraswati Soedarmadji', 'Yanan Sui']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yunyue_Wei1', '~Zeji_Yi1', '~Hongda_Li2', '~Saraswati_Soedarmadji1', '~Yanan_Sui1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Safe Bayesian Optimization', 'High-dimensional Embodied System']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to move is a primary goal for animals and robots, where ensuring safety is often important when optimizing control policies on the embodied systems. For complex tasks such as the control of human or humanoid control, the high-dimensional parameter space adds complexity to the safe optimization effort. Current safe exploration algorithms exhibit inefficiency and may even become infeasible with large high-dimensional input spaces. Furthermore, existing high-dimensional constrained optimization methods neglect safety in the search process. In this paper, we propose High-dimensional Safe Bayesian Optimization with local optimistic exploration (HdSafeBO), a novel approach designed to handle high-dimensional sampling problems under probabilistic safety constraints. We introduce a local optimistic strategy to efficiently and safely optimize the objective function, providing a probabilistic safety guarantee and a cumulative safety violation bound. Through the use of isometric embedding, HdSafeBO addresses problems ranging from a few hundred to several thousand dimensions while maintaining safety guarantees. To our knowledge, HdSafeBO is the first algorithm capable of optimizing the control of high-dimensional musculoskeletal systems with high safety probability. We also demonstrate the real-world applicability of HdSafeBO through its use in the safe online optimization of neural stimulation-induced human motion control.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0da9bd551bae46b56a208a416a34d01c9ebd4e3f.pdf" target="_blank">/pdf/0da9bd551bae46b56a208a416a34d01c9ebd4e3f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f0161e67763789e40f87ed44cf636daf7ad5850d.zip" target="_blank">/attachment/f0161e67763789e40f87ed44cf636daf7ad5850d.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wei2024safe,
title={Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems},
author={Yunyue Wei and Zeji Yi and Hongda Li and Saraswati Soedarmadji and Yanan Sui},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8PcRynpd1m}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/183ee12133ed5c845aa75e4cee74a9f4125a663d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8PcRynpd1m&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wei|safe_bayesian_optimization_for_the_control_of_highdimensional_embodied_systems</div>
</div>
<div class='paper-counter'>225/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8LPXeGhhbH</div>
<div class="field-name">title:</div>
<div class="field-value">RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yuxuan Kuang', 'Junjie Ye', 'Haoran Geng', 'Jiageng Mao', 'Congyue Deng', 'Leonidas Guibas', 'He Wang', 'Yue Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yuxuan_Kuang1', '~Junjie_Ye3', '~Haoran_Geng1', '~Jiageng_Mao1', '~Congyue_Deng1', '~Leonidas_Guibas1', '~He_Wang5', '~Yue_Wang2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Hierarchical Retrieval', 'Affordance Transfer', 'Zero-Shot Robotic Manipulation', 'Visual Foundation Models']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. RAM first extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D actionable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/540ab9f468673f9fd4c2db88c26b2b58deaa2e44.pdf" target="_blank">/pdf/540ab9f468673f9fd4c2db88c26b2b58deaa2e44.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/54f23d46e3df993eca81e658a8b391b425dff4fb.zip" target="_blank">/attachment/54f23d46e3df993eca81e658a8b391b425dff4fb.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kuang2024ram,
title={{RAM}: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation},
author={Yuxuan Kuang and Junjie Ye and Haoran Geng and Jiageng Mao and Congyue Deng and Leonidas Guibas and He Wang and Yue Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8LPXeGhhbH}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://yuxuank.com/RAM/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d7d69e11833a6539a8b6718a0755d3cb9063f284.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8LPXeGhhbH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kuang|ram_retrievalbased_affordance_transfer_for_generalizable_zeroshot_robotic_manipulation</div>
</div>
<div class='paper-counter'>226/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8JLmTZsxGh</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Performance-oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Lakshmideepakreddy Manda', 'Shaoru Chen', 'Mahyar Fazlyab']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Lakshmideepakreddy_Manda2', '~Shaoru_Chen1', '~Mahyar_Fazlyab1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Control Barrier Functions', 'Safety', 'Hamilton-Jacobi Partial Differential Equation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use Hamilton-Jacobi PDEs to learn Control Barrier Functions for complex environments</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control Barrier Functions (CBFs) offer an elegant framework for constraining nonlinear control system dynamics to an invariant subset of a pre-specified safe set. However, finding a CBF that simultaneously promotes performance by maximizing the resulting control invariant set while accommodating complex safety constraints, especially in high relative degree systems with actuation constraints, remains a significant challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints defining the safe set, our approach begins by constructing a smooth function whose zero superlevel set provides an inner approximation of the safe set. This function is then used with a smooth neural network to parameterize the CBF candidate. Finally, we design a physics-informed training loss function based on a Hamilton-Jacobi Partial Differential Equation (PDE) to train the PINN-CBF and enlarge the volume of the induced control invariant set. We demonstrate the effectiveness of our approach on a 2D double integrator (DI) system and a 7D fixed-wing aircraft system (F16).</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ead6715dd7f70fd28d5ac5cf0d1810fde9d03082.pdf" target="_blank">/pdf/ead6715dd7f70fd28d5ac5cf0d1810fde9d03082.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/28b1bd8ea7a8ac0059a60620330c9a154b318569.zip" target="_blank">/attachment/28b1bd8ea7a8ac0059a60620330c9a154b318569.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
manda2024learning,
title={Learning Performance-oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation},
author={Lakshmideepakreddy Manda and Shaoru Chen and Mahyar Fazlyab},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8JLmTZsxGh}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/acd82b9fdcdacc1e9726c86d6696531dcca19e7b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8JLmTZsxGh&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">manda|learning_performanceoriented_control_barrier_functions_under_complex_safety_constraints_and_limited_actuation</div>
</div>
<div class='paper-counter'>227/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">8Ar8b00GJC</div>
<div class="field-name">title:</div>
<div class="field-value">Autonomous Improvement of Instruction Following Skills via Foundation Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhiyuan Zhou', 'Pranav Atreya', 'Abraham Lee', 'Homer Rich Walke', 'Oier Mees', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhiyuan_Zhou2', '~Pranav_Atreya1', '~Abraham_Lee2', '~Homer_Rich_Walke1', '~Oier_Mees1', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Autonomous Improvement', 'Instruction Following Skills', 'Scaled Data Collection']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a robotic system capable of fully autonomous large scale data collection in the real world, which can use that data to improve a multitask instruction-following policy with self-supervision</div>
<div class="field-name">abstract:</div>
<div class="field-value">Intelligent robots capable of improving from autonomously collected experience have the potential to transform robot learning: instead of collecting costly teleoperated demonstration data, large-scale deployment of fleets of robots can quickly collect larger quantities of autonomous data useful for training better robot policies. However, autonomous improvement requires solving two key problems: (i) fully automating a scalable data collection procedure that can collect diverse and semantically meaningful robot data and (ii) learning from non-optimal, autonomous data with no human annotations. To this end, we propose a novel approach that addresses these challenges, allowing instruction following policies to improve from autonomously collected data without human supervision. Our framework leverages vision-language models to collect and evaluate semantically meaningful experiences in new environments, and then utilizes a decomposition of instruction following tasks into (semantic) language-conditioned image generation and (non-semantic) goal reaching, which makes it significantly more practical to improve from this autonomously collected data without any human annotations. We carry out extensive experiments in the real world to demonstrate the effectiveness of our approach, and find that in a suite of unseen environments, the robot policy can be improved significantly with autonomously collected data. We open-source the code for our semantic autonomous improvement pipeline, as well as our autonomous dataset of 25K trajectories collected across five tabletop environments: https://soar-autonomous-improvement.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5163fdeccb9d7ca9b5935153927b451c048dd0e5.pdf" target="_blank">/pdf/5163fdeccb9d7ca9b5935153927b451c048dd0e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/12b42a82f850f6c9043c871aa437bf01b347cfc6.zip" target="_blank">/attachment/12b42a82f850f6c9043c871aa437bf01b347cfc6.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhou2024autonomous,
title={Autonomous Improvement of Instruction Following Skills via Foundation Models},
author={Zhiyuan Zhou and Pranav Atreya and Abraham Lee and Homer Rich Walke and Oier Mees and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=8Ar8b00GJC}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://auto-improvement.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/92a8b2a77b69f8f31b89cd9dc22cc9786d0efcaf.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=8Ar8b00GJC&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhou|autonomous_improvement_of_instruction_following_skills_via_foundation_models</div>
</div>
<div class='paper-counter'>228/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">82bpTugrMt</div>
<div class="field-name">title:</div>
<div class="field-value">Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor</div>
<div class="field-name">authors:</div>
<div class="field-value">['Anish Bhattacharya', 'Marco Cannici', 'Nishanth Rao', 'Yuezhan Tao', 'Vijay Kumar', 'Nikolai Matni', 'Davide Scaramuzza']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Anish_Bhattacharya1', '~Marco_Cannici1', '~Nishanth_Rao1', '~Yuezhan_Tao1', '~Vijay_Kumar2', '~Nikolai_Matni1', '~Davide_Scaramuzza1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['event-based vision', 'learning for control', 'simulation-to-real transfer', 'aerial robotics']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate the avoidance of trees with an event camera onboard a fast-flying quadrotor.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2f89afe25a20366a03f5804a58dbe3552f73bc84.pdf" target="_blank">/pdf/2f89afe25a20366a03f5804a58dbe3552f73bc84.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ace21a8a0881fe48df43cf90e3a69cb67d99abd9.zip" target="_blank">/attachment/ace21a8a0881fe48df43cf90e3a69cb67d99abd9.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
bhattacharya2024monocular,
title={Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor},
author={Anish Bhattacharya and Marco Cannici and Nishanth Rao and Yuezhan Tao and Vijay Kumar and Nikolai Matni and Davide Scaramuzza},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=82bpTugrMt}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://www.anishbhattacharya.com/research/evfly</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1c4d58f2b179eb64ab606d142772bca42a904484.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=82bpTugrMt&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">bhattacharya|monocular_eventbased_vision_for_obstacle_avoidance_with_a_quadrotor</div>
</div>
<div class='paper-counter'>229/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7yMZAUkXa4</div>
<div class="field-name">title:</div>
<div class="field-value">MimicTouch: Leveraging Multi-modal Human Tactile Demonstrations for Contact-rich Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kelin Yu', 'Yunhai Han', 'Qixian Wang', 'Vaibhav Saxena', 'Danfei Xu', 'Ye Zhao']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kelin_Yu1', '~Yunhai_Han1', 'qxwang_m@zju.edu.cn', '~Vaibhav_Saxena1', '~Danfei_Xu1', '~Ye_Zhao2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Tactile Sensing', 'Learning from Human', 'Data Collection', 'Imitation Learning', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">MimicTouch is a multi-modal imitation learning framework that efficiently collects human tactile demonstrations, learns human-like tactile-guided control strategies from them, and zero-shot generalizes to different task settings.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Tactile sensing is critical to fine-grained, contact-rich manipulation tasks, such as insertion and assembly. Prior research has shown the possibility of learning tactile-guided policy from teleoperated demonstration data. However, to provide the demonstration, human users often rely on visual feedback to control the robot. This creates a gap between the sensing modality used for controlling the robot (visual) and the modality of interest (tactile). To bridge this gap, we introduce "MimicTouch'', a novel framework for learning policies directly from demonstrations provided by human users with their hands. The key innovations are i) a human tactile data collection system which collects multi-modal tactile dataset for learning human's tactile-guided control strategy, ii) an imitation learning-based framework for learning human's tactile-guided control strategy through such data, and iii) an online residual RL framework to bridge the embodiment gap between the human hand and the robot gripper. Through comprehensive experiments, we highlight the efficacy of utilizing human's tactile-guided control strategy to resolve contact-rich manipulation tasks. The project website is at https://sites.google.com/view/MimicTouch.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/30434f637545e72670a57dc3923c7f97d1f938af.pdf" target="_blank">/pdf/30434f637545e72670a57dc3923c7f97d1f938af.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d8f5c3f27d856704eb4f2529bd7b8bc6ebec401a.zip" target="_blank">/attachment/d8f5c3f27d856704eb4f2529bd7b8bc6ebec401a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yu2024mimictouch,
title={MimicTouch: Leveraging Multi-modal Human Tactile Demonstrations for Contact-rich Manipulation},
author={Kelin Yu and Yunhai Han and Qixian Wang and Vaibhav Saxena and Danfei Xu and Ye Zhao},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7yMZAUkXa4}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/MimicTouch</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/60bb2c4abdc72e310aa616ace47fafd59018d11f.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7yMZAUkXa4&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yu|mimictouch_leveraging_multimodal_human_tactile_demonstrations_for_contactrich_manipulation</div>
</div>
<div class='paper-counter'>230/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7wMlwhCvjS</div>
<div class="field-name">title:</div>
<div class="field-value">GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yixuan Wang', 'Guang Yin', 'Binghao Huang', 'Tarik Kelestemur', 'Jiuguang Wang', 'Yunzhu Li']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yixuan_Wang2', '~Guang_Yin1', '~Binghao_Huang1', '~Tarik_Kelestemur1', '~Jiuguang_Wang1', '~Yunzhu_Li1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Semantic Fields', 'Category-Level Generalization', 'Imitation Learning', 'Diffusion Models']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion-based policies have shown remarkable capability in executing complex robotic manipulation tasks but lack explicit characterization of geometry and semantics, which often limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of Diffusion Policy, we introduce a novel framework that incorporates explicit spatial and semantic information via 3D semantic fields. We generate 3D descriptor fields from multi-view RGBD observations with large foundational vision models, then compare these descriptor fields against reference descriptors to obtain semantic fields. The proposed method explicitly considers geometry and semantics, enabling strong generalization capabilities in tasks requiring category-level generalization, resolving geometric ambiguities, and attention to subtle geometric details. We evaluate our method across eight tasks involving articulated objects and instances with varying shapes and textures from multiple object categories. Our method demonstrates its effectiveness by increasing Diffusion Policy's average success rate on \textit{unseen} instances from 20\% to 93\%. Additionally, we provide a detailed analysis and visualization to interpret the sources of performance gain and explain how our method can generalize to novel instances. Project page: https://robopil.github.io/GenDP/</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0a3137305ebb7b47c229e12ad1f6d02593091463.pdf" target="_blank">/pdf/0a3137305ebb7b47c229e12ad1f6d02593091463.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ee856a511a4fe4b2f0a9378a2d83b3d6a21c9865.zip" target="_blank">/attachment/ee856a511a4fe4b2f0a9378a2d83b3d6a21c9865.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024gendp,
title={Gen{DP}: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy},
author={Yixuan Wang and Guang Yin and Binghao Huang and Tarik Kelestemur and Jiuguang Wang and Yunzhu Li},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7wMlwhCvjS}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robopil.github.io/GenDP/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c761570c2e56250748ab241b4f0e92c23e4784dc.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7wMlwhCvjS&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|gendp_3d_semantic_fields_for_categorylevel_generalizable_diffusion_policy</div>
</div>
<div class='paper-counter'>231/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7vzDBvviRO</div>
<div class="field-name">title:</div>
<div class="field-value">UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">['Chunru Lin', 'Jugang Fan', 'Yian Wang', 'Zeyuan Yang', 'Zhehuan Chen', 'Lixing Fang', 'Tsun-Hsuan Wang', 'Zhou Xian', 'Chuang Gan']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Chunru_Lin1', '~Jugang_Fan1', '~Yian_Wang1', '~Zeyuan_Yang3', '~Zhehuan_Chen1', '~Lixing_Fang1', '~Tsun-Hsuan_Wang2', '~Zhou_Xian1', '~Chuang_Gan1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Soft-Body Manipulation', 'Locomotion', 'Physics Simulation']</div>
<div class="field-name">abstract:</div>
<div class="field-value">It is desired to equip robots with the capability of interacting with various soft materials as they are ubiquitous in the real world. While physics simulations are one of the predominant methods for data collection and robot training, simulating soft materials presents considerable challenges. Specifically, it is significantly more costly than simulating rigid objects in terms of simulation speed and storage requirements. These limitations typically restrict the scope of studies on soft materials to small and bounded areas, thereby hindering the learning of skills in broader spaces. To address this issue, we introduce UBSoft, a new simulation platform designed to support unbounded soft environments for robot skill acquisition. Our platform utilizes spatially adaptive resolution scales, where simulation resolution dynamically adjusts based on proximity to active robotic agents. Our framework markedly reduces the demand for extensive storage space and computation costs required for large-scale scenarios involving soft materials. We also establish a set of benchmark tasks in our platform, including both locomotion and manipulation tasks, and conduct experiments to evaluate the efficacy of various reinforcement learning algorithms and trajectory optimization techniques, both gradient-based and sampling-based. Preliminary results indicate that sampling-based trajectory optimization generally achieves better results for obtaining one trajectory to solve the task. Additionally, we conduct experiments in real-world environments to demonstrate that advancements made in our UBSoft simulator could translate to improved robot interactions with large-scale soft material. More videos can be found at https://ubsoft24.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/97004b79a2516b2515eb5b5118c11e1ce482c5aa.pdf" target="_blank">/pdf/97004b79a2516b2515eb5b5118c11e1ce482c5aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a47e08ede1fb53864532e5b3e10f0b23b4cf4ce2.zip" target="_blank">/attachment/a47e08ede1fb53864532e5b3e10f0b23b4cf4ce2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lin2024ubsoft,
title={{UBS}oft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments},
author={Chunru Lin and Jugang Fan and Yian Wang and Zeyuan Yang and Zhehuan Chen and Lixing Fang and Tsun-Hsuan Wang and Zhou Xian and Chuang Gan},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7vzDBvviRO}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://vis-www.cs.umass.edu/ubsoft/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a88eed249ccb023fc016e6c18b23dd30b37de5cf.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7vzDBvviRO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lin|ubsoft_a_simulation_platform_for_robotic_skill_learning_in_unbounded_soft_environments</div>
</div>
<div class='paper-counter'>232/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7ddT4eklmQ</div>
<div class="field-name">title:</div>
<div class="field-value">ACE: A Cross-platform and visual-Exoskeletons System for Low-Cost Dexterous Teleoperation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shiqi Yang', 'Minghuan Liu', 'Yuzhe Qin', 'Runyu Ding', 'Jialong Li', 'Xuxin Cheng', 'Ruihan Yang', 'Sha Yi', 'Xiaolong Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shiqi_Yang2', '~Minghuan_Liu1', '~Yuzhe_Qin1', '~Runyu_Ding1', '~Jialong_Li3', '~Xuxin_Cheng2', '~Ruihan_Yang2', '~Sha_Yi1', '~Xiaolong_Wang3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Teleopration System; Hardware; Imitation Learning; Robot Learning; Exoskeletons']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel and powerful teleoperation system for dexterous tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Bimanual robotic manipulation with dexterous hands has a large potential workability and a wide workspace as it follows the most natural human workflow.
Learning from human demonstrations has proven highly effective for learning a dexterous manipulation policy. To collect such data, teleoperation serves as a straightforward and efficient way to do so.
However, a cost-effective and easy-to-use teleoperation system is lacking for anthropomorphic robot hands.
To fill the deficiency, we developed \our, a cross-platform visual-exoskeleton system for low-cost dexterous teleoperation. 
Our system employs a hand-facing camera to capture 3D hand poses and an exoskeleton mounted on a base that can be easily carried on users' backs. ACE captures both the hand root end-effector and hand pose in real-time and enables cross-platform operations. 
We evaluate the key system parameters compared with previous teleoperation systems and show clear advantages of \our.
We then showcase the desktop and mobile versions of our system on six different robot platforms (including humanoid-hands, arm-hands, arm-gripper, and quadruped-gripper systems), and demonstrate the effectiveness of learning three difficult real-world tasks through the collected demonstration on two of them.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d86755721141f81f5826a7165d59249c84fd0133.pdf" target="_blank">/pdf/d86755721141f81f5826a7165d59249c84fd0133.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f8a80d05a3e6c9b401f3dd54691beb2a8764fb42.zip" target="_blank">/attachment/f8a80d05a3e6c9b401f3dd54691beb2a8764fb42.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yang2024ace,
title={{ACE}: A Cross-platform and visual-Exoskeletons System for Low-Cost Dexterous Teleoperation},
author={Shiqi Yang and Minghuan Liu and Yuzhe Qin and Runyu Ding and Jialong Li and Xuxin Cheng and Ruihan Yang and Sha Yi and Xiaolong Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7ddT4eklmQ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://ace-teleop.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/83b65d1d519eeb568313e2aa195ec6aefda28737.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7ddT4eklmQ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yang|ace_a_crossplatform_and_visualexoskeletons_system_for_lowcost_dexterous_teleoperation</div>
</div>
<div class='paper-counter'>233/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7c5rAY8oU3</div>
<div class="field-name">title:</div>
<div class="field-value">Automated Creation of Digital Cousins for Robust Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tianyuan Dai', 'Josiah Wong', 'Yunfan Jiang', 'Chen Wang', 'Cem Gokmen', 'Ruohan Zhang', 'Jiajun Wu', 'Li Fei-Fei']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['tydai@stanford.edu', '~Josiah_Wong1', '~Yunfan_Jiang1', '~Chen_Wang16', '~Cem_Gokmen1', '~Ruohan_Zhang1', '~Jiajun_Wu1', '~Li_Fei-Fei1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Real-to-Sim; Digital Twin; Sim-to-Real Transfer']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">An end-to-end framework for automatically generating fully interactive simulated scenes ("digital cousins") from a single real-world RGB image enabling training of robot policies that can be deployed zero-shot in the real world.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in *digital twins*, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of ***digital cousins***, a virtual asset or scene that, unlike a *digital twin*, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, *digital cousins* simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a609094053229b596be0cc69bdcf3c3997c02873.pdf" target="_blank">/pdf/a609094053229b596be0cc69bdcf3c3997c02873.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/14f10d5e363aa736b73dd19e9c6dc1ce568bb8a4.zip" target="_blank">/attachment/14f10d5e363aa736b73dd19e9c6dc1ce568bb8a4.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
dai2024automated,
title={Automated Creation of Digital Cousins for Robust Policy Learning},
author={Tianyuan Dai and Josiah Wong and Yunfan Jiang and Chen Wang and Cem Gokmen and Ruohan Zhang and Jiajun Wu and Li Fei-Fei},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7c5rAY8oU3}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://digital-cousins.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/13fea6d9b9788f170fbbc1ea43595f4229e7700a.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7c5rAY8oU3&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">dai|automated_creation_of_digital_cousins_for_robust_policy_learning</div>
</div>
<div class='paper-counter'>234/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">7E3JAys1xO</div>
<div class="field-name">title:</div>
<div class="field-value">D$^3$RoMa: Disparity Diffusion-based Depth Sensing for Material-Agnostic Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Songlin Wei', 'Haoran Geng', 'Jiayi Chen', 'Congyue Deng', 'Cui Wenbo', 'Chengyang Zhao', 'Xiaomeng Fang', 'Leonidas Guibas', 'He Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Songlin_Wei1', '~Haoran_Geng1', '~Jiayi_Chen5', '~Congyue_Deng1', '~Cui_Wenbo1', '~Chengyang_Zhao1', '~Xiaomeng_Fang1', '~Leonidas_Guibas1', '~He_Wang5']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Depth Estimation', 'Diffusion Model', 'Stereo Vision']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose D3RoMa, a diffusion model-based depth estimation framework on stereo image pairs for robotic manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Depth sensing is an important problem for 3D vision-based robotics. Yet, a real-world active stereo or ToF depth camera often produces noisy and incomplete depth which bottlenecks robot performances. In this work, we propose D3RoMa, a learning-based depth estimation framework on stereo image pairs that predicts clean and accurate depth in diverse indoor scenes, even in the most challenging scenarios with translucent or specular surfaces where classical depth sensing completely fails. Key to our method is that we unify depth estimation and restoration into an image-to-image translation problem by predicting the disparity map with a denoising diffusion probabilistic model.  At inference time, we further incorporated a left-right consistency constraint as classifier guidance to the diffusion process. Our framework combines recently advanced learning-based approaches and geometric constraints from traditional stereo vision. For model training, we create a large scene-level synthetic dataset with diverse transparent and specular objects to compensate for existing tabletop datasets. The trained model can be directly applied to real-world in-the-wild scenes and achieve state-of-the-art performance in multiple public depth estimation benchmarks. Further experiments in both simulated and real environments show that accurate depth prediction significantly improves robotic manipulation in various scenarios.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/233ac1bd9d8996580a01931f121662b8b68fbfc2.pdf" target="_blank">/pdf/233ac1bd9d8996580a01931f121662b8b68fbfc2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/123ec883a3bd1018e1ee7efc7d7e55a34bbfd280.zip" target="_blank">/attachment/123ec883a3bd1018e1ee7efc7d7e55a34bbfd280.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wei2024droma,
title={D\${\textasciicircum}3\$RoMa: Disparity Diffusion-based Depth Sensing for Material-Agnostic Robotic Manipulation},
author={Songlin Wei and Haoran Geng and Jiayi Chen and Congyue Deng and Cui Wenbo and Chengyang Zhao and Xiaomeng Fang and Leonidas Guibas and He Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=7E3JAys1xO}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://pku-epic.github.io/D3RoMa/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/2b1f650e814b87b26711a347e5e33691ca6796de.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=7E3JAys1xO&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wei|d^3roma_disparity_diffusionbased_depth_sensing_for_materialagnostic_robotic_manipulation</div>
</div>
<div class='paper-counter'>235/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">6oESa4g05O</div>
<div class="field-name">title:</div>
<div class="field-value">Distribution Discrepancy and Feature Heterogeneity for Active 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huang-Yu Chen', 'Jia-Fong Yeh', 'Jiawei', 'Pin-Hsuan Peng', 'Winston H. Hsu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huang-Yu_Chen1', '~Jia-Fong_Yeh1', '~Jiawei1', '~Pin-Hsuan_Peng1', '~Winston_H._Hsu2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Active Learning', 'LiDAR 3D Object Detection', 'Autonomous Driving']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">DDFH leverages a dual-component approach for LiDAR-based 3D object detection, combining Instance-level Distribution Discrepancy with Frame-level Feature Heterogeneity to enhance data informativeness and reduce annotation costs effectively.</div>
<div class="field-name">abstract:</div>
<div class="field-value">LiDAR-based 3D object detection is a critical technology for the development of autonomous driving and robotics. However, the high cost of data annotation limits its advancement. We propose a novel and effective active learning (AL) method called Distribution Discrepancy and Feature Heterogeneity (DDFH), which simultaneously considers geometric features and model embeddings, assessing information from both the instance-level and frame-level perspectives. Distribution Discrepancy evaluates the difference and novelty of instances within the unlabeled and labeled distributions, enabling the model to learn efficiently with limited data. Feature Heterogeneity ensures the heterogeneity of intra-frame instance features, maintaining feature diversity while avoiding redundant or similar instances, thus minimizing annotation costs. Finally, multiple indicators are efficiently aggregated using Quantile Transform, providing a unified measure of informativeness. Extensive experiments demonstrate that DDFH outperforms the current state-of-the-art (SOTA) methods on the KITTI and Waymo datasets, effectively reducing the bounding box annotation cost by 56.3% and showing robustness when working with both one-stage and two-stage models.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9e2cc58cce4bd695c13da3122196199a8c51c015.pdf" target="_blank">/pdf/9e2cc58cce4bd695c13da3122196199a8c51c015.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3826dcc6152f44835194290954c447d38e0deee0.zip" target="_blank">/attachment/3826dcc6152f44835194290954c447d38e0deee0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024distribution,
title={Distribution Discrepancy and Feature Heterogeneity for Active 3D Object Detection},
author={Huang-Yu Chen and Jia-Fong Yeh and Jiawei and Pin-Hsuan Peng and Winston H. Hsu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=6oESa4g05O}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/7282f17af7db6ef770d4a5fa3ea8c21fe1289f04.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=6oESa4g05O&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|distribution_discrepancy_and_feature_heterogeneity_for_active_3d_object_detection</div>
</div>
<div class='paper-counter'>236/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">6X3ybeVpDi</div>
<div class="field-name">title:</div>
<div class="field-value">Online Transfer and Adaptation of Tactile Skill: A Teleoperation Framework</div>
<div class="field-name">authors:</div>
<div class="field-value">['Xiao Chen', 'Tianle Ni', 'Kübra Karacan', 'Hamid Sadeghian', 'Sami Haddadin']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Xiao_Chen14', '~Tianle_Ni1', '~Kübra_Karacan1', 'hamid.sadeghian@tum.de', '~Sami_Haddadin1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Learning from Demonstration', 'Online Adaptation', 'Tactile Skill', 'Teleoperation', 'Autonomy Allocation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A tele-teaching framework enables robot online learning and adapting tactile skill from a remote demonstration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents a teleoperation framework designed for online learning and adaptation of tactile skills, which provides an intuitive interface without need for physical access to execution robot. The proposed tele-teaching approach utilizes periodical Dynamical Movement Primitives (DMP) and Recursive Least Square (RLS) for generating tactile skills. An autonomy allocation strategy, guided by the learning confidence and operator intention, ensures a smooth transition between human demonstration to autonomous robot operation. Our experimental results with two 7 Degree of Freedom (DoF) Franka Panda robot demonstrates that the tele-teaching framework facilitates online motion and force learning and adaptation within a few iterations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2e6a3b445f1bda2c4942f9d0a78b21559c172eaa.pdf" target="_blank">/pdf/2e6a3b445f1bda2c4942f9d0a78b21559c172eaa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/66d790f4e06d2b6947747476b7858e57d91ed087.zip" target="_blank">/attachment/66d790f4e06d2b6947747476b7858e57d91ed087.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024online,
title={Online Transfer and Adaptation of Tactile Skill: A Teleoperation Framework},
author={Xiao Chen and Tianle Ni and K{\"u}bra Karacan and Hamid Sadeghian and Sami Haddadin},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=6X3ybeVpDi}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e34601ce478776e7455df0a30ff17030878325c6.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=6X3ybeVpDi&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|online_transfer_and_adaptation_of_tactile_skill_a_teleoperation_framework</div>
</div>
<div class='paper-counter'>237/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">6FGlpzC9Po</div>
<div class="field-name">title:</div>
<div class="field-value">Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">['Mitsuhiko Nakamoto', 'Oier Mees', 'Aviral Kumar', 'Sergey Levine']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Mitsuhiko_Nakamoto1', '~Oier_Mees1', '~Aviral_Kumar2', '~Sergey_Levine1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['generalist policies', 'value functions', 'robot reinforcement learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Value-Guided Policy Steering (V-GPS), a novel approach that improves performance of pre-trained generalist robotic policies by re-ranking their actions according to a value function learned via offline RL at deployment time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4c8e74f714242d258c2592502393c160e7bbe18d.pdf" target="_blank">/pdf/4c8e74f714242d258c2592502393c160e7bbe18d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ca91a83d299762a4c35f72768603b794f5a1888e.zip" target="_blank">/attachment/ca91a83d299762a4c35f72768603b794f5a1888e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
nakamoto2024steering,
title={Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance},
author={Mitsuhiko Nakamoto and Oier Mees and Aviral Kumar and Sergey Levine},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=6FGlpzC9Po}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://nakamotoo.github.io/V-GPS</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8ba9f4ded04ab546b33218873b1a0ff81c39a8d9.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=6FGlpzC9Po&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">nakamoto|steering_your_generalists_improving_robotic_foundation_models_via_value_guidance</div>
</div>
<div class='paper-counter'>238/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">67tTQeO4HQ</div>
<div class="field-name">title:</div>
<div class="field-value">In-Flight Attitude Control of a Quadruped using Deep Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tarek El-Agroudi', 'Finn Gross Maurer', 'Jørgen Anker Olsen', 'Kostas Alexis']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tarek_El-Agroudi2', '~Finn_Gross_Maurer1', '~Jørgen_Anker_Olsen1', '~Kostas_Alexis1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Deep Reinforcement Learning', 'Legged Robotics']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use DRL to train and demonstrate an in-flight attitude control law for a small low-cost quadruped with a five-bar-linkage leg design using only its legs as reaction masses.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present the development and real world demonstration of an in-flight attitude control law for a small low-cost quadruped with a five-bar-linkage leg design using only its legs as reaction masses. The control law is trained using deep reinforcement learning (DRL) and specifically through Proximal Policy Optimization (PPO) in the NVIDIA Omniverse Isaac Sim simulator with a GPU-accelerated DRL pipeline. To demonstrate the policy, a small quadruped is designed, constructed, and evaluated both on a rotating pole test setup and in free fall. During a free fall of 0.7 seconds, the quadruped follows commanded attitude steps of 45 degrees in all principal axes, and achieves an average base angular velocity of 110 degrees per second during large attitude reference steps.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/2b86c391a99faf3c1e303207f129a262ebeac7a4.pdf" target="_blank">/pdf/2b86c391a99faf3c1e303207f129a262ebeac7a4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f126c9ddcbbd1c51ad34e3ffa8c7ffb616d19de1.zip" target="_blank">/attachment/f126c9ddcbbd1c51ad34e3ffa8c7ffb616d19de1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
el-agroudi2024inflight,
title={In-Flight Attitude Control of a Quadruped using Deep Reinforcement Learning},
author={Tarek El-Agroudi and Finn Gross Maurer and J{\o}rgen Anker Olsen and Kostas Alexis},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=67tTQeO4HQ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://finnfi.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e1cab1d5de9ef2755d737cc7a7ac3d4a4a029903.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=67tTQeO4HQ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">elagroudi|inflight_attitude_control_of_a_quadruped_using_deep_reinforcement_learning</div>
</div>
<div class='paper-counter'>239/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">5u9l6U61S7</div>
<div class="field-name">title:</div>
<div class="field-value">GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">['Pu Hua', 'Minghuan Liu', 'Annabella Macaluso', 'Yunfeng Lin', 'Weinan Zhang', 'Huazhe Xu', 'Lirui Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Pu_Hua1', '~Minghuan_Liu1', '~Annabella_Macaluso1', '~Yunfeng_Lin1', '~Weinan_Zhang1', '~Huazhe_Xu1', '~Lirui_Wang1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Generative Simulation; Robotics; Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose GenSim2, a scalable framework that uses foundation models for diverse, complex, realistic simulation tasks and data generation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated objects. To automatically generate demonstration data for these tasks at scale, we propose planning and RL solvers that generalize within object categories. The pipeline can generate data for up to 100 articulated tasks with 200 objects and reduce the required human efforts. To utilize such data, we propose an effective multi-task language-conditioned policy architecture, dubbed proprioceptive point-cloud transformer (PPT), that learns from the generated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the policy architecture, we show a promising usage of GenSim2 that the generated data can be used for zero-shot transfer or co-train with real-world collected data, which enhances the policy performance by 20% compared with training exclusively on limited real data.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/809d275233901666eba033b66d0f91d761e7dfdf.pdf" target="_blank">/pdf/809d275233901666eba033b66d0f91d761e7dfdf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/a1a1de0aa3d62bc93c8b1b79831b04688abfce03.zip" target="_blank">/attachment/a1a1de0aa3d62bc93c8b1b79831b04688abfce03.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
hua2024gensim,
title={GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning {LLM}s},
author={Pu Hua and Minghuan Liu and Annabella Macaluso and Yunfeng Lin and Weinan Zhang and Huazhe Xu and Lirui Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=5u9l6U61S7}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://gensim2.github.io</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e1b819fd46c28e38898da6d31a9348ac825128df.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=5u9l6U61S7&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">hua|gensim2_scaling_robot_data_generation_with_multimodal_and_reasoning_llms</div>
</div>
<div class='paper-counter'>240/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">5lSkn5v4LK</div>
<div class="field-name">title:</div>
<div class="field-value">EquiGraspFlow: SE(3)-Equivariant 6-DoF Grasp Pose Generative Flows</div>
<div class="field-name">authors:</div>
<div class="field-value">['Byeongdo Lim', 'Jongmin Kim', 'Jihwan Kim', 'Yonghyeon Lee', 'Frank C. Park']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Byeongdo_Lim1', '~Jongmin_Kim7', '~Jihwan_Kim2', '~Yonghyeon_Lee2', '~Frank_C._Park1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['6-DoF grasp pose generation', 'equivariance', 'generative models', 'continuous normalizing flows']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an SE(3)-equivariant grasp pose generative model by constructing a framework to learn SE(3)-invariant conditional distributions with Continuous Normalizing Flows</div>
<div class="field-name">abstract:</div>
<div class="field-value">Traditional methods for synthesizing 6-DoF grasp poses from 3D observations often rely on geometric heuristics, resulting in poor generalizability, limited grasp options, and higher failure rates. Recently, data-driven methods have been proposed that use generative models to learn the distribution of grasp poses and generate diverse candidate poses. The main drawback of these methods is that they fail to achieve SE(3)-equivariance, meaning that the generated grasp poses do not transform correctly with object rotations and translations. In this paper, we propose \textit{EquiGraspFlow}, a flow-based SE(3)-equivariant 6-DoF grasp pose generative model that can learn complex conditional distributions on the SE(3) manifold while guaranteeing SE(3)-equivariance. Our model achieves the equivariance without relying on data augmentation, by using network architectures that guarantee the equivariance by construction. Extensive experiments show that \textit{EquiGraspFlow} accurately learns grasp pose distribution, achieves the SE(3)-equivariance, and significantly outperforms existing grasp pose generative models. Code is available at https://github.com/bdlim99/EquiGraspFlow.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0cf6620dc9bab0efd016f183a48fddc6d3ada955.pdf" target="_blank">/pdf/0cf6620dc9bab0efd016f183a48fddc6d3ada955.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/9cdfab291b2cb9eafc55863f27dd67c9c91f772a.zip" target="_blank">/attachment/9cdfab291b2cb9eafc55863f27dd67c9c91f772a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lim2024equigraspflow,
title={EquiGraspFlow: {SE}(3)-Equivariant 6-DoF Grasp Pose Generative Flows},
author={Byeongdo Lim and Jongmin Kim and Jihwan Kim and Yonghyeon Lee and Frank C. Park},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=5lSkn5v4LK}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://equigraspflow.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e6a04721eabe312bb1f2c0001d68cb13a6975965.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=5lSkn5v4LK&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lim|equigraspflow_se3equivariant_6dof_grasp_pose_generative_flows</div>
</div>
<div class='paper-counter'>241/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">5iXG6EgByK</div>
<div class="field-name">title:</div>
<div class="field-value">Promptable Closed-loop Traffic Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Shuhan Tan', 'Boris Ivanovic', 'Yuxiao Chen', 'Boyi Li', 'Xinshuo Weng', 'Yulong Cao', 'Philipp Kraehenbuehl', 'Marco Pavone']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Shuhan_Tan2', '~Boris_Ivanovic1', '~Yuxiao_Chen3', '~Boyi_Li1', '~Xinshuo_Weng3', '~Yulong_Cao1', '~Philipp_Kraehenbuehl1', '~Marco_Pavone1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Autonomous Driving', 'Scenario Generation', 'Traffic Simulation']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">1) We propose ProSim, a multimodal promptable closed-loop traffic simulation framework. 2) We create ProSim-Instruct-520k, a large-scale multimodal prompt-scenario paired driving dataset.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Simulation stands as a cornerstone for safe and efficient autonomous driving development. At its core a simulation system ought to produce realistic, reactive, and controllable traffic patterns. In this paper, we propose ProSim, a multimodal promptable closed-loop traffic simulation framework. ProSim allows the user to give a complex set of numerical, categorical or textual prompts to instruct each agent’s behavior and intention. ProSim then rolls out a traffic scenario in a closed-loop manner, modeling each agent’s interaction with other traffic participants. Our experiments show that ProSim achieves high prompt controllability given different user prompts, while reaching competitive performance on the Waymo Sim Agents Challenge when no prompt is given. To support research on promptable traffic simulation, we create ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with over 10M text prompts for over 520k real-world driving scenarios. We will release data, benchmark, and labeling tools of ProSim-Instruct-520k upon publication.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/152ed4cd9af8ee71b596ca2e5608957e32872cc1.pdf" target="_blank">/pdf/152ed4cd9af8ee71b596ca2e5608957e32872cc1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/421bf5a701bcf78e3f3886c30653cca835dc6011.zip" target="_blank">/attachment/421bf5a701bcf78e3f3886c30653cca835dc6011.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
tan2024promptable,
title={Promptable Closed-loop Traffic Simulation},
author={Shuhan Tan and Boris Ivanovic and Yuxiao Chen and Boyi Li and Xinshuo Weng and Yulong Cao and Philipp Kraehenbuehl and Marco Pavone},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=5iXG6EgByK}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://ariostgx.github.io/ProSim/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/3724459b5b31053479a3ddffefae9843e1a07270.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=5iXG6EgByK&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">tan|promptable_closedloop_traffic_simulation</div>
</div>
<div class='paper-counter'>242/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">5W0iZR9J7h</div>
<div class="field-name">title:</div>
<div class="field-value">DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jialiang Zhang', 'Haoran Liu', 'Danshi Li', 'XinQiang Yu', 'Haoran Geng', 'Yufei Ding', 'Jiayi Chen', 'He Wang']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jialiang_Zhang2', '~Haoran_Liu4', '~Danshi_Li1', '~XinQiang_Yu1', '~Haoran_Geng1', '~Yufei_Ding4', '~Jiayi_Chen5', '~He_Wang5']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous Grasping', 'Synthetic Data', 'Generative Models']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Grasping in cluttered scenes remains highly challenging for dexterous hands due to the scarcity of data. To address this problem, we present a large-scale synthetic dataset, encompassing 1319 objects, 8270 scenes, and 426 million grasps. Beyond benchmarking, we also explore data-efficient learning strategies from grasping data. We reveal that the combination of a conditional generative model that focuses on local geometry and a grasp dataset that emphasizes complex scene variations is key to achieving effective generalization. Our proposed generative method outperforms all baselines in simulation experiments. Furthermore, it demonstrates zero-shot sim-to-real transfer through test-time depth restoration, attaining 91% real-world success rate, showcasing the robust potential of utilizing fully synthetic training data.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/fbb6c2345c14ff26561d6b8f558c4aae254a6df5.pdf" target="_blank">/pdf/fbb6c2345c14ff26561d6b8f558c4aae254a6df5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/c407edfba237de0891b457c270efdc906d33d610.zip" target="_blank">/attachment/c407edfba237de0891b457c270efdc906d33d610.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhang2024dexgraspnet,
title={DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes},
author={Jialiang Zhang and Haoran Liu and Danshi Li and XinQiang Yu and Haoran Geng and Yufei Ding and Jiayi Chen and He Wang},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=5W0iZR9J7h}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/841d948ef9312d919a65466faeb89b4fa9f50eba.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=5W0iZR9J7h&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhang|dexgraspnet_20_learning_generative_dexterous_grasping_in_largescale_synthetic_cluttered_scenes</div>
</div>
<div class='paper-counter'>243/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">5Awumz1VKU</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Differentiable Tensegrity Dynamics using Graph Neural Networks</div>
<div class="field-name">authors:</div>
<div class="field-value">['Nelson Chen', 'Kun Wang', 'William R. Johson III', 'Rebecca Kramer-Bottiglio', 'Kostas Bekris', 'Mridul Aanjaneya']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Nelson_Chen1', '~Kun_Wang10', 'will.johnson@yale.edu', '~Rebecca_Kramer-Bottiglio1', '~Kostas_Bekris1', '~Mridul_Aanjaneya3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['graph neural networks', 'differentiable simulation', 'tensegrity robots']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper describes a graph neural network method to learn Tensegrity robot dynamics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Tensegrity robots are composed of rigid struts and flexible cables. They constitute an emerging class of hybrid rigid-soft robotic systems and are promising systems for a wide array of applications, ranging from locomotion to assembly. They are difficult to control and model accurately, however, due to their compliance and high number of degrees of freedom. To address this issue, prior work has introduced a differentiable physics engine designed for tensegrity robots based on first principles. In contrast, this work proposes the use of graph neural networks to model contact dynamics over a graph representation of tensegrity robots, which leverages their natural graph-like cable connec- tivity between end caps of rigid rods. This learned simulator can accurately model 3-bar and 6-bar tensegrity robot dynamics in simulation-to-simulation experiments where MuJoCo is used as the ground truth. It can also achieve higher accuracy than the previous differentiable engine for a real 3-bar tensegrity robot, for which the robot state is only partially observable. When compared against direct applications of recent mesh-based graph neural network simulators, the proposed approach is computationally more efficient, both for training and inference, while achieving higher accuracy. Code and data are available at https://github.com/nchen9191/tensegrity_gnn_simulator_public</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d9b0bac6dd159b196acb0aa7778ef369523946ab.pdf" target="_blank">/pdf/d9b0bac6dd159b196acb0aa7778ef369523946ab.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/4ad817067b11ab08286dc60d31e33afe4a1c10a5.zip" target="_blank">/attachment/4ad817067b11ab08286dc60d31e33afe4a1c10a5.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024learning,
title={Learning Differentiable Tensegrity Dynamics using Graph Neural Networks},
author={Nelson Chen and Kun Wang and William R. Johson III and Rebecca Kramer-Bottiglio and Kostas Bekris and Mridul Aanjaneya},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=5Awumz1VKU}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/e984f4b331e33259a36b1181155f37efa915280d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=5Awumz1VKU&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|learning_differentiable_tensegrity_dynamics_using_graph_neural_networks</div>
</div>
<div class='paper-counter'>244/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">56IzghzjfZ</div>
<div class="field-name">title:</div>
<div class="field-value">IMAGINATION POLICY: Using Generative Point Cloud Models for Learning Manipulation Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">['Haojie Huang', 'Karl Schmeckpeper', 'Dian Wang', 'Ondrej Biza', 'Yaoyao Qian', 'Haotian Liu', 'Mingxi Jia', 'Robert Platt', 'Robin Walters']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Haojie_Huang1', '~Karl_Schmeckpeper1', '~Dian_Wang1', '~Ondrej_Biza1', '~Yaoyao_Qian1', '~Haotian_Liu6', '~Mingxi_Jia1', '~Robert_Platt1', '~Robin_Walters1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Manipulation policy learning', 'Generative model', 'Geometric learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Using generative point cloud models for learning manipulation policies</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans can imagine goal states during planning and perform actions to match those goals. In this work, we propose IMAGINATION POLICY, a novel multi-task key-frame policy network for solving high-precision pick and place tasks. Instead of learning actions directly, IMAGINATION POLICY generates point clouds to imagine desired states which are then translated to actions using rigid action estimation. This transforms action inference into a local generative task. We leverage pick and place symmetries underlying the tasks in the generation process and achieve extremely high sample efficiency and generalizability to unseen configurations. Finally, we demonstrate state-of-the-art performance across various tasks on the RLbench benchmark compared with several strong baselines and validate our approach on a real robot.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9a9b76ae8952b32061b0b18f52ea62232bf2e664.pdf" target="_blank">/pdf/9a9b76ae8952b32061b0b18f52ea62232bf2e664.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/dcc041c26760108bf00fa06b749b7505de3362cc.zip" target="_blank">/attachment/dcc041c26760108bf00fa06b749b7505de3362cc.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
huang2024imagination,
title={{IMAGINATION} {POLICY}: Using Generative Point Cloud Models for Learning Manipulation Policies},
author={Haojie Huang and Karl Schmeckpeper and Dian Wang and Ondrej Biza and Yaoyao Qian and Haotian Liu and Mingxi Jia and Robert Platt and Robin Walters},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=56IzghzjfZ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://haojhuang.github.io/imagine_page/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/05dda6a24c52f11c1b7ac8887f1fdd4e53dda309.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=56IzghzjfZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">huang|imagination_policy_using_generative_point_cloud_models_for_learning_manipulation_policies</div>
</div>
<div class='paper-counter'>245/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">55tYfHvanf</div>
<div class="field-name">title:</div>
<div class="field-value">Bimanual Dexterity for Complex Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">['Kenneth Shaw', 'Yulong Li', 'Jiahui Yang', 'Mohan Kumar Srirama', 'Ray Liu', 'Haoyu Xiong', 'Russell Mendonca', 'Deepak Pathak']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Kenneth_Shaw1', '~Yulong_Li1', 'jiahuiya@andrew.cmu.edu', '~Mohan_Kumar_Srirama1', 'muxinl@andrew.cmu.edu', '~Haoyu_Xiong3', '~Russell_Mendonca1', '~Deepak_Pathak1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Dexterous Manipulation', 'Bimanual', 'Behavior Cloning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Bimanual Robot Hand Teleoperation System</div>
<div class="field-name">abstract:</div>
<div class="field-value">To train generalist robot policies, machine learning methods often require a substantial amount of expert human teleoperation data. An ideal robot for humans collecting data is one that closely mimics them: bimanual arms and dexterous hands. However, creating such a bimanual teleoperation system with over 50 DoF is a significant challenge.  To address this, we introduce Bidex, an extremely dexterous, low-cost, low-latency and portable bimanual dexterous teleoperation system which relies on motion capture gloves and teacher arms.   We compare Bidex to a Vision Pro teleoperation system and a SteamVR system and find Bidex to produce better quality data for more complex tasks at a faster rate. Additionally, we show Bidex operating a mobile bimanual robot for in the wild tasks. Please refer to https://bidex-teleop.github.io for video results and instructions to recreate Bidex.  The robot hands (5k USD) and teleoperation system (7k USD) is readily reproducible and can be used on many robot arms including two xArms ($16k USD).</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/f27ad6595630dc595c83ccc355b2a75c75088ab5.pdf" target="_blank">/pdf/f27ad6595630dc595c83ccc355b2a75c75088ab5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/af087a100a66586612357fa1354351355862b729.zip" target="_blank">/attachment/af087a100a66586612357fa1354351355862b729.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
shaw2024bimanual,
title={Bimanual Dexterity for Complex Tasks},
author={Kenneth Shaw and Yulong Li and Jiahui Yang and Mohan Kumar Srirama and Ray Liu and Haoyu Xiong and Russell Mendonca and Deepak Pathak},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=55tYfHvanf}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://bidex-teleop.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/c6b73c02fb9e2ba460626ef2b5a6bee26ecb9ff7.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=55tYfHvanf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">shaw|bimanual_dexterity_for_complex_tasks</div>
</div>
<div class='paper-counter'>246/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">4Of4UWyBXE</div>
<div class="field-name">title:</div>
<div class="field-value">RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yi Zhao', 'Le Chen', 'Jan Schneider', 'Quankai Gao', 'Juho Kannala', 'Bernhard Schölkopf', 'Joni Pajarinen', 'Dieter Büchler']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yi_Zhao6', '~Le_Chen3', '~Jan_Schneider1', '~Quankai_Gao1', '~Juho_Kannala5', '~Bernhard_Schölkopf1', '~Joni_Pajarinen2', '~Dieter_Büchler1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Bi-manual dexterous robot hands', 'dataset for robot piano playing', 'imitation learning', 'robot learning at scale']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Endowing robot hands with human-level dexterity is a long-lasting research objective. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/3d7c7932b06b3d3964ae2736894962927f75b2f9.pdf" target="_blank">/pdf/3d7c7932b06b3d3964ae2736894962927f75b2f9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/e5b5cd7f0fc1a21be0d7d008c2eb99558b951dc6.zip" target="_blank">/attachment/e5b5cd7f0fc1a21be0d7d008c2eb99558b951dc6.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhao2024rpm,
title={{RP}1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands},
author={Yi Zhao and Le Chen and Jan Schneider and Quankai Gao and Juho Kannala and Bernhard Sch{\"o}lkopf and Joni Pajarinen and Dieter B{\"u}chler},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=4Of4UWyBXE}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://rp1m.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/fed96cd405c2d0728bc29b2624f6edcf85c43ba2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=4Of4UWyBXE&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhao|rp1m_a_largescale_motion_dataset_for_piano_playing_with_bimanual_dexterous_robot_hands</div>
</div>
<div class='paper-counter'>247/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">46SluHKoE9</div>
<div class="field-name">title:</div>
<div class="field-value">Continuously Improving Mobile Manipulation with Autonomous Real-World RL</div>
<div class="field-name">authors:</div>
<div class="field-value">['Russell Mendonca', 'Emmanuel Panov', 'Bernadette Bucher', 'Jiuguang Wang', 'Deepak Pathak']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Russell_Mendonca1', 'epanov@theaiinstitute.com', '~Bernadette_Bucher1', '~Jiuguang_Wang1', '~Deepak_Pathak1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Continual Learning', 'Mobile Manipulation', 'Reinforcement Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a fully autonomous real-world RL framework for mobile manipulation that can learn policies without extensive instrumentation or human supervision. This is enabled by 1) task-relevant autonomy, which guides exploration towards object interactions and prevents stagnation near goal states, 2) efficient policy learning by leveraging basic task knowledge in behavior priors, and 3) formulating generic rewards that combine human-interpretable semantic information with low-level, fine-grained observations. We demonstrate that our approach allows Spot robots to continually improve their performance on a set of four challenging mobile manipulation tasks, obtaining an average success rate of 80% across tasks, a 3-4 times improvement over existing approaches. Videos can be found at https://continual-mobile-manip.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/622f7d44b7bbd5ce9d3b8e25bb3acb369238d988.pdf" target="_blank">/pdf/622f7d44b7bbd5ce9d3b8e25bb3acb369238d988.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2db38218795e642c7d6d39741976f20a0edaf37a.zip" target="_blank">/attachment/2db38218795e642c7d6d39741976f20a0edaf37a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
mendonca2024continuously,
title={Continuously Improving Mobile Manipulation with Autonomous Real-World {RL}},
author={Russell Mendonca and Emmanuel Panov and Bernadette Bucher and Jiuguang Wang and Deepak Pathak},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=46SluHKoE9}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://continual-mobile-manip.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/189f1a54eef4f9fc4f87a3fc1a39b467cc6a5e22.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=46SluHKoE9&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">mendonca|continuously_improving_mobile_manipulation_with_autonomous_realworld_rl</div>
</div>
<div class='paper-counter'>248/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3wBqoPfoeJ</div>
<div class="field-name">title:</div>
<div class="field-value">Twisting Lids Off with Two Hands</div>
<div class="field-name">authors:</div>
<div class="field-value">['Toru Lin', 'Zhao-Heng Yin', 'Haozhi Qi', 'Pieter Abbeel', 'Jitendra Malik']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Toru_Lin1', '~Zhao-Heng_Yin1', '~Haozhi_Qi1', '~Pieter_Abbeel2', '~Jitendra_Malik2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Bimanual Manipulation', 'Sim-to-Real', 'Reinforcement Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, due to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we share novel insights into physical modeling, real-time perception, and reward design that enable policies trained in simulation using deep reinforcement learning (RL) to be effectively and efficiently transferred to the real world. Specifically, we consider the problem of twisting lids of various bottle-like objects with two hands, demonstrating policies with generalization capabilities across a diverse set of unseen objects as well as dynamic and dexterous behaviors. To the best of our knowledge, this is the first sim-to-real RL system that enables such capabilities on bimanual multi-fingered hands.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/ecd3c030b5d094e1725f7695b83465738089b417.pdf" target="_blank">/pdf/ecd3c030b5d094e1725f7695b83465738089b417.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/545221b5d6ea058b57c6ccbdbead43544cbebe1a.zip" target="_blank">/attachment/545221b5d6ea058b57c6ccbdbead43544cbebe1a.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lin2024twisting,
title={Twisting Lids Off with Two Hands},
author={Toru Lin and Zhao-Heng Yin and Haozhi Qi and Pieter Abbeel and Jitendra Malik},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3wBqoPfoeJ}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://toruowo.github.io/bimanual-twist/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d91138b7293605ebfd291c027b89bc69f3b06176.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3wBqoPfoeJ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lin|twisting_lids_off_with_two_hands</div>
</div>
<div class='paper-counter'>249/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3jNEz3kUSl</div>
<div class="field-name">title:</div>
<div class="field-value">PointPatchRL - Masked Reconstruction Improves Reinforcement Learning on Point Clouds</div>
<div class="field-name">authors:</div>
<div class="field-value">['Balazs Gyenes', 'Nikolai Franke', 'Philipp Becker', 'Gerhard Neumann']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Balazs_Gyenes1', '~Nikolai_Franke1', '~Philipp_Becker1', '~Gerhard_Neumann2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Point Clouds', 'Self-Supervised Learning', 'Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Recent methods for representation learning on point clouds significantly improve RL on point clouds for challenging simulated robotic tasks</div>
<div class="field-name">abstract:</div>
<div class="field-value">Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics. While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects. In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views. However, while point-cloud processing with deep learning has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature. We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing
the tokens with transformers. PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL. We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9303906c2c11fc9cb4f93e9802b8671a85d10fa8.pdf" target="_blank">/pdf/9303906c2c11fc9cb4f93e9802b8671a85d10fa8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/44302b70e61f4aed693ec23656aef478724d0b57.zip" target="_blank">/attachment/44302b70e61f4aed693ec23656aef478724d0b57.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
gyenes2024pointpatchrl,
title={PointPatch{RL} - Masked Reconstruction Improves Reinforcement Learning on Point Clouds},
author={Balazs Gyenes and Nikolai Franke and Philipp Becker and Gerhard Neumann},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3jNEz3kUSl}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://alrhub.github.io/pprl-website</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/8dece94301be1dd1ea5848885a91278abaf7e997.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3jNEz3kUSl&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">gyenes|pointpatchrl_masked_reconstruction_improves_reinforcement_learning_on_point_clouds</div>
</div>
<div class='paper-counter'>250/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3i7j8ZPnbm</div>
<div class="field-name">title:</div>
<div class="field-value">UMI-on-Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huy Ha', 'Yihuai Gao', 'Zipeng Fu', 'Jie Tan', 'Shuran Song']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huy_Ha1', '~Yihuai_Gao1', '~Zipeng_Fu1', '~Jie_Tan1', '~Shuran_Song3']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Manipulation', 'Visuo-motor Policy', 'Whole-body Controller']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A framework for scalable quadruped manipulation, combining manipulation policies trained from real-world demonstrations with whole-body controllers trained from simulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce UMI-on-Legs, a new framework that combines real-world and simulation data for quadruped manipulation systems.  We scale task-centric data collection in the real world using a handheld gripper (UMI), providing a cheap way to demonstrate task-relevant manipulation skills without a robot. Simultaneously, we scale robot-centric data in simulation by training a whole-body controller.  The interface between these two policies are end-effector trajectories in the task-frame, which are inferred by the manipulation policy and passed to the whole-body controller for tracking. We evaluate UMI-on-Legs on prehensile, non-prehensile, and dynamic manipulation tasks, and report over 70% success rate for all tasks. Lastly, we also demonstrate the zero-shot cross-embodiment deployment of a pre-trained manipulation policy checkpoint from a prior work, originally intended for a fixed-base robot arm, on our quadruped system. We believe this framework provides a scalable path towards learning expressive manipulation skills on dynamic robot embodiments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/eb7c154330e977747915c265c296a9c412ce8694.pdf" target="_blank">/pdf/eb7c154330e977747915c265c296a9c412ce8694.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/6481d8b227d2365719a25d9f3ca5370a69ab1482.zip" target="_blank">/attachment/6481d8b227d2365719a25d9f3ca5370a69ab1482.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
ha2024umionlegs,
title={{UMI}-on-Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers},
author={Huy Ha and Yihuai Gao and Zipeng Fu and Jie Tan and Shuran Song},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3i7j8ZPnbm}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://umi-on-legs.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/f5267c80da2d920befa604588571a5385be8a91d.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3i7j8ZPnbm&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">ha|umionlegs_making_manipulation_policies_mobile_with_manipulationcentric_wholebody_controllers</div>
</div>
<div class='paper-counter'>251/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3bcujpPikC</div>
<div class="field-name">title:</div>
<div class="field-value">FREA: Feasibility-Guided Generation of Safety-Critical Scenarios with Reasonable Adversariality</div>
<div class="field-name">authors:</div>
<div class="field-value">['Keyu Chen', 'Yuheng Lei', 'Hao Cheng', 'Haoran Wu', 'Wenchao Sun', 'Sifa Zheng']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Keyu_Chen5', '~Yuheng_Lei1', '~Hao_Cheng21', '~Haoran_Wu9', '~Wenchao_Sun1', '~Sifa_Zheng1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Feasibility', 'Scenario Generation', 'Autonomous Driving']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">FREA incorporates feasibility as guidance to generate adversarial yet AV-feasible, safety-critical scenarios for autonomous driving.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generating safety-critical scenarios, which are essential yet difficult to collect at scale, offers an effective method to evaluate the robustness of autonomous vehicles (AVs). Existing methods focus on optimizing adversariality while preserving the naturalness of scenarios, aiming to achieve a balance through data-driven approaches. However, without an appropriate upper bound for adversariality, the scenarios might exhibit excessive adversariality, potentially leading to unavoidable collisions. In this paper, we introduce FREA, a novel safety-critical scenarios generation method that incorporates the Largest Feasible Region (LFR) of AV as guidance to ensure the reasonableness of the adversarial scenarios. Concretely, FREA initially pre-calculates the LFR of AV from offline datasets. Subsequently, it learns a reasonable adversarial policy that controls critical background vehicles (CBVs) in the scene to generate adversarial yet AV-feasible scenarios by maximizing a novel feasibility-dependent objective function. Extensive experiments illustrate that FREA can effectively generate safety-critical scenarios, yielding considerable near-miss events while ensuring AV's feasibility. Generalization analysis also confirms the robustness of FREA in AV testing across various surrogate AV methods and traffic environments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/a5ace76e24bfd945af3888584d75ecccd519e07a.pdf" target="_blank">/pdf/a5ace76e24bfd945af3888584d75ecccd519e07a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/3c0021e3c9f3fe3c4cd6196df2ec396baeca02a0.zip" target="_blank">/attachment/3c0021e3c9f3fe3c4cd6196df2ec396baeca02a0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
chen2024frea,
title={{FREA}: Feasibility-Guided Generation of Safety-Critical Scenarios with Reasonable Adversariality},
author={Keyu Chen and Yuheng Lei and Hao Cheng and Haoran Wu and Wenchao Sun and Sifa Zheng},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3bcujpPikC}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://currychen77.github.io/FREA/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/1b4c688b246346992a4b495a667a28506ea22b46.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3bcujpPikC&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">chen|frea_feasibilityguided_generation_of_safetycritical_scenarios_with_reasonable_adversariality</div>
</div>
<div class='paper-counter'>252/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3ZAgXBRvla</div>
<div class="field-name">title:</div>
<div class="field-value">FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yishu Li', 'Wen Hui Leng', 'Yiming Fang', 'Ben Eisner', 'David Held']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yishu_Li1', '~Wen_Hui_Leng1', '~Yiming_Fang1', '~Ben_Eisner1', '~David_Held1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Ambiguity', 'Multi-modality', 'Occlusion', 'Articulated Objects', 'Diffusion']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present FlowBotHD, a history-aware diffuser that handles multi-modality and occlusions in articulated object manipulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce a novel approach to manipulate articulated objects with ambiguities, such as opening a door, in which multi-modality and occlusions create ambiguities about the opening side and direction. Multi-modality occurs when the method to open a fully closed door (push, pull, slide) is uncertain, or the side from which it should be opened is uncertain.  Occlusions further obscure the door’s shape from certain angles, creating further ambiguities during the occlusion. To tackle these challenges, we propose a history-aware diffusion network that models the multi-modal distribution of the articulated object and uses history to disambiguate actions and make stable predictions under occlusions. Experiments and analysis demonstrate the state-of-art performance of our method and specifically improvements in ambiguity-caused failure modes. Our project website is available at https://flowbothd.github.io/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/0c84c02051c8931f3eb0f9cd5b3ad1069d8136bf.pdf" target="_blank">/pdf/0c84c02051c8931f3eb0f9cd5b3ad1069d8136bf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/2bb523f3afa03f2bf298fa1508ce5f3e58c195dc.zip" target="_blank">/attachment/2bb523f3afa03f2bf298fa1508ce5f3e58c195dc.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
li2024flowbothd,
title={FlowBot{HD}: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation},
author={Yishu Li and Wen Hui Leng and Yiming Fang and Ben Eisner and David Held},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3ZAgXBRvla}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://flowbothd.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/19483628abc13f70dffbc560aa8391d5d78c959b.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3ZAgXBRvla&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">li|flowbothd_historyaware_diffuser_handling_ambiguities_in_articulated_objects_manipulation</div>
</div>
<div class='paper-counter'>253/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">3NI5SxsJqf</div>
<div class="field-name">title:</div>
<div class="field-value">Accelerating Visual Sparse-Reward Learning with Latent Nearest-Demonstration-Guided Explorations</div>
<div class="field-name">authors:</div>
<div class="field-value">['Ruihan Zhao', 'ufuk topcu', 'Sandeep P. Chinchali', 'Mariano Phielipp']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Ruihan_Zhao1', '~ufuk_topcu1', '~Sandeep_P._Chinchali1', '~Mariano_Phielipp2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Computer Vision', 'Sparse Reward', 'RL from Demonstrations']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">Under-an-hour RL training of real robot manipulation is possible with demonstration-guided reward augmentation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in deep reinforcement learning (RL) and computer vision enables artificial agents to solve complex tasks, including locomotion, manipulation, and video games from high-dimensional pixel observations. However, RL usually relies on domain-specific reward functions for sufficient learning signals, requiring expert knowledge. While vision-based agents could learn skills from only sparse rewards, exploration challenges arise. We present Latent Nearest-demonstration-guided Exploration (LaNE), a novel and efficient method to solve sparse-reward robot manipulation tasks from image observations and a few demonstrations. First, LaNE builds on the pre-trained DINOv2 feature extractor to learn an embedding space for forward prediction. Next, it rewards the agent for exploring near the demos, quantified by quadratic control costs in the embedding space. Finally, LaNE optimizes the policy for the augmented rewards with RL. Experiments demonstrate that our method achieves state-of-the-art sample efficiency in Robosuite simulation and enables under-an-hour RL training from scratch on a Franka Panda robot, using only a few demonstrations.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/dc84be679c3393b009b0c984da22fe165065c644.pdf" target="_blank">/pdf/dc84be679c3393b009b0c984da22fe165065c644.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/ec7e7bbc79505c3368ffdcc9228b23303a131063.zip" target="_blank">/attachment/ec7e7bbc79505c3368ffdcc9228b23303a131063.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
zhao2024accelerating,
title={Accelerating Visual Sparse-Reward Learning with Latent Nearest-Demonstration-Guided Explorations},
author={Ruihan Zhao and ufuk topcu and Sandeep P. Chinchali and Mariano Phielipp},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=3NI5SxsJqf}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/55d2281fa8e7710c72abdbe02974bdbb5e9b2706.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=3NI5SxsJqf&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">zhao|accelerating_visual_sparsereward_learning_with_latent_nearestdemonstrationguided_explorations</div>
</div>
<div class='paper-counter'>254/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">2sg4PY1W9d</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Transparent Reward Models via Unsupervised Feature Selection</div>
<div class="field-name">authors:</div>
<div class="field-value">['Daulet Baimukashev', 'Gokhan Alcan', 'Kevin Sebastian Luck', 'Ville Kyrki']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Daulet_Baimukashev1', '~Gokhan_Alcan1', '~Kevin_Sebastian_Luck1', '~Ville_Kyrki1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Inverse reinforcement learning', 'Reinforcement learning', 'Imitation learning', 'Robots', 'Reward learning', 'Robot learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We construct interpretable and compact reward model from observational data. This learned reward model can be directly used using standart reinforcement learning framework for solving complex tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and interpretable reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8e9422f874c841f9dc22f9a3e0d073aa52a4e760.pdf" target="_blank">/pdf/8e9422f874c841f9dc22f9a3e0d073aa52a4e760.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/12632391c43ee33427cf74813a3411073b2b99b0.zip" target="_blank">/attachment/12632391c43ee33427cf74813a3411073b2b99b0.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
baimukashev2024learning,
title={Learning Transparent Reward Models via Unsupervised Feature Selection},
author={Daulet Baimukashev and Gokhan Alcan and Kevin Sebastian Luck and Ville Kyrki},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=2sg4PY1W9d}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/transparent-reward</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/adc3e326dd1f7c1ab457367a34b9cbade2a5f8b2.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=2sg4PY1W9d&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">baimukashev|learning_transparent_reward_models_via_unsupervised_feature_selection</div>
</div>
<div class='paper-counter'>255/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">2SYFDG4WRA</div>
<div class="field-name">title:</div>
<div class="field-value">Manipulate-Anything: Automating Real-World Robots using Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">['Jiafei Duan', 'Wentao Yuan', 'Wilbert Pumacay', 'Yi Ru Wang', 'Kiana Ehsani', 'Dieter Fox', 'Ranjay Krishna']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Jiafei_Duan1', '~Wentao_Yuan1', '~Wilbert_Pumacay1', '~Yi_Ru_Wang1', '~Kiana_Ehsani1', '~Dieter_Fox1', '~Ranjay_Krishna1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Robot Learning; Multimodal Large Language Model; Data Generation; Imitation Learning; Behavior Cloning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A scalable automated demonstration generation method leveraging on Vision-Language Models for real-world robotics manipulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large-scale endeavors like RT-1 and widespread community efforts such as Open-X-Embodiment have contributed to growing the scale of robot demonstration data. However, there is still an opportunity to improve the quality, quantity, and diversity of robot demonstration data. Although vision-language models have been shown to automatically generate demonstration data, their utility has been limited to environments with privileged state information, they require hand-designed skills, and are limited to interactions with few object instances. We propose Manipulate-Anything, a scalable automated generation method for real-world robotic manipulation.
Unlike prior work, our method can operate in real-world environments without any privileged state information, hand-designed skills, and can manipulate any static object. We evaluate our method using two setups. First, Manipulate-Anything successfully generates trajectories for all 5 real-world and 12 simulation tasks, significantly outperforming existing methods like VoxPoser. 
Second, Manipulate-Anything's demonstrations can train more robust behavior cloning policies than training with human demonstrations, or from data generated by VoxPoser and Code-As-Policies.
We believe Manipulate-Anything can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting.  Anonymous project page: manipulate-anything.github.io.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/5dc842658e943e9e7bb5ed7fe2fe5bb489d000e4.pdf" target="_blank">/pdf/5dc842658e943e9e7bb5ed7fe2fe5bb489d000e4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/12563b4e01eee0d581683b9bfa185910d242b1da.zip" target="_blank">/attachment/12563b4e01eee0d581683b9bfa185910d242b1da.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
duan2024manipulateanything,
title={Manipulate-Anything: Automating Real-World Robots using Vision-Language Models},
author={Jiafei Duan and Wentao Yuan and Wilbert Pumacay and Yi Ru Wang and Kiana Ehsani and Dieter Fox and Ranjay Krishna},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=2SYFDG4WRA}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robot-ma.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/913606a9b3770e3be165c30ed9eae6d81d1288d4.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=2SYFDG4WRA&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">duan|manipulateanything_automating_realworld_robots_using_visionlanguage_models</div>
</div>
<div class='paper-counter'>256/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">2LLu3gavF1</div>
<div class="field-name">title:</div>
<div class="field-value">Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</div>
<div class="field-name">authors:</div>
<div class="field-value">['Justin Kerr', 'Chung Min Kim', 'Mingxuan Wu', 'Brent Yi', 'Qianqian Wang', 'Ken Goldberg', 'Angjoo Kanazawa']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Justin_Kerr1', '~Chung_Min_Kim1', '~Mingxuan_Wu1', '~Brent_Yi1', '~Qianqian_Wang2', '~Ken_Goldberg1', '~Angjoo_Kanazawa1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Feature Fields', 'Visual Imitation', 'Grasping', 'Articulated Objects']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that part-centric feature fields can be used for articulated object manipulation from a single human video.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi- view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feature fields in an iterative optimization which enables the use of geometric regularizers to re- cover 3D motions from only a single video. Given this 4D reconstruction, the robot replicates object trajectories by planning bimanual arm motions that induce the demonstrated object part motion. By representing demonstrations as part- centric trajectories, RSRD focuses on replicating the demonstration’s intended behavior while considering the robot’s own morphological limits, rather than at- tempting to reproduce the hand’s motion. We evaluate 4D-DPM’s 3D tracking accuracy on ground truth annotated 3D part trajectories and RSRD’s physical ex- ecution performance on 9 objects across 10 trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of 87% success rate, for a total end- to-end success rate of 60% across 90 trials. Notably, this is accomplished using only feature fields distilled from large pretrained vision models — without any task-specific training, fine-tuning, dataset collection, or annotation. Project page: https://robot-see-robot-do.github.io</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d5ceb55e50c82345f43522c64c83e8c4ff33903b.pdf" target="_blank">/pdf/d5ceb55e50c82345f43522c64c83e8c4ff33903b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/eff56165158d01ceca747ddfa925d7059e0c96a2.zip" target="_blank">/attachment/eff56165158d01ceca747ddfa925d7059e0c96a2.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
kerr2024robot,
title={Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction},
author={Justin Kerr and Chung Min Kim and Mingxuan Wu and Brent Yi and Qianqian Wang and Ken Goldberg and Angjoo Kanazawa},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=2LLu3gavF1}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://robot-see-robot-do.github.io/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/eb39cf3922c23963b9e2ea886db4f355de24d8ed.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=2LLu3gavF1&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">kerr|robot_see_robot_do_imitating_articulated_object_manipulation_with_monocular_4d_reconstruction</div>
</div>
<div class='paper-counter'>257/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">2CScZqkUPZ</div>
<div class="field-name">title:</div>
<div class="field-value">Genetic Algorithm for Curriculum Design in Multi-Agent Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">['Yeeho Song', 'Jeff Schneider']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Yeeho_Song1', '~Jeff_Schneider1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Multiagent Learning', 'Curricular Learning']</div>
<div class="field-name">abstract:</div>
<div class="field-value">As the deployment of autonomous agents in real-world scenarios grows, so does the interest in their application to competitive environments with other robots. Self-play in Reinforcement Learning (RL) enables agents to develop competitive strategies. However, the complexity arising from multi-agent interactions and the tendency for RL agents to disrupt competitors' training introduce instability and a risk of overfitting. While traditional methods depend on costly Nash equilibrium approximations or random exploration for training scenario optimization, this can be inefficient in large search spaces often prevalent in multi-agent problems. However, related works in single-agent setups show that genetic algorithms perform better in large scenario spaces. Therefore, we propose using genetic algorithms to adaptively adjust environment parameters and opponent policies in a multi-agent context to find and synthesize coherent scenarios efficiently. We also introduce GenOpt Agent—a genetically optimized, open-loop agent executing scheduled actions. The open-loop aspect of GenOpt prevents RL agents from winning through adversarial perturbations, thereby fostering generalizable strategies. Also, GenOpt is genetically optimized without expert supervision, negating the need for expensive expert supervision to have meaningful opponents at the start of training. Our empirical studies indicate that this method surpasses several established baselines in two-player competitive settings with continuous action spaces, validating its effectiveness and stability in training.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/af1747347181c359ddc54f90fc6a415bad0b5c18.pdf" target="_blank">/pdf/af1747347181c359ddc54f90fc6a415bad0b5c18.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d163caeab2d1892e26d1bd7fbde7bc6ae0595a39.zip" target="_blank">/attachment/d163caeab2d1892e26d1bd7fbde7bc6ae0595a39.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
song2024genetic,
title={Genetic Algorithm for Curriculum Design in Multi-Agent Reinforcement Learning},
author={Yeeho Song and Jeff Schneider},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=2CScZqkUPZ}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/61ee00182ae1396f66d9aaa6ac00c149e330a16c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=2CScZqkUPZ&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">song|genetic_algorithm_for_curriculum_design_in_multiagent_reinforcement_learning</div>
</div>
<div class='paper-counter'>258/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">2AZfKk9tRI</div>
<div class="field-name">title:</div>
<div class="field-value">Multi-agent Reinforcement Learning with Hybrid Action Space for Free Gait Motion Planning of Hexapod Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">['Huiqiao Fu', 'Kaiqiang Tang', 'Peng Li', 'Guizhou Deng', 'Chunlin Chen']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Huiqiao_Fu1', '~Kaiqiang_Tang1', '~Peng_Li10', '~Guizhou_Deng1', '~Chunlin_Chen1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Free Gait', 'Hexapod Robot', 'Hybrid Action Space', 'Multi-agent Reinforcement Learning']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We tackle the problem of free gait motion planning for hexapod robots walking in randomly generated plum blossom pile environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Legged robots are able to overcome challenging terrains through diverse gaits formed by contact sequences. However, environments characterized by discrete footholds present significant challenges. In this paper, we tackle the problem of free gait motion planning for hexapod robots walking in randomly generated plum blossom pile environments. Specifically, we first address the complexity of multi-leg coordination in discrete environments by treating each leg of the hexapod robot as an individual agent. Then, we propose the Hybrid action space Multi-Agent Soft Actor Critic (Hybrid-MASAC) algorithm capable of handling both discrete and continuous actions. Finally, we present an integrated free gait motion planning method based on Hybrid-MASAC, streamlining gait, Center of Mass (COM), and foothold sequences planning into a single model. Comparative and ablation experiments in both of the simulated and real plum blossom pile environments demonstrate the feasibility and efficiency of our method.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/d52167976d4728e36271dd0be4de2af4b975d08b.pdf" target="_blank">/pdf/d52167976d4728e36271dd0be4de2af4b975d08b.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
fu2024multiagent,
title={Multi-agent Reinforcement Learning with Hybrid Action Space for Free Gait Motion Planning of Hexapod Robots},
author={Huiqiao Fu and Kaiqiang Tang and Peng Li and Guizhou Deng and Chunlin Chen},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=2AZfKk9tRI}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/5d7be1981b138348b3041d4bd8aa11e3e03940fc.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=2AZfKk9tRI&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">fu|multiagent_reinforcement_learning_with_hybrid_action_space_for_free_gait_motion_planning_of_hexapod_robots</div>
</div>
<div class='paper-counter'>259/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">1tCteNSbFH</div>
<div class="field-name">title:</div>
<div class="field-value">Trajectory Improvement and Reward Learning from Comparative Language Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhaojing Yang', 'Miru Jun', 'Jeremy Tien', 'Stuart Russell', 'Anca Dragan', 'Erdem Biyik']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhaojing_Yang1', '~Miru_Jun1', '~Jeremy_Tien1', '~Stuart_Russell1', '~Anca_Dragan1', '~Erdem_Biyik1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Learning from Human Language Feedback', 'Reward Learning', 'Human-Robot Interaction']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an approach to learn reward function from comparative language feedback, which outperforms traditional preference-based learning method</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning from human feedback has gained traction in fields like robotics and natural language processing in recent years. While prior works mostly rely on human feedback in the form of comparisons, language is a preferable modality that provides more informative insights into user preferences. In this work, we aim to incorporate comparative language feedback to iteratively improve robot trajectories and to learn reward functions that encode human preferences. To achieve this goal, we learn a shared latent space that integrates trajectory data and language feedback, and subsequently leverage the learned latent space to improve trajectories and learn human preferences. To the best of our knowledge, we are the first to incorporate comparative language feedback into reward learning. Our simulation experiments demonstrate the effectiveness of the learned latent space and the success of our learning algorithms. We also conduct human subject studies that show our reward learning algorithm achieves a 23.9% higher subjective score on average and is 11.3% more time-efficient compared to preference-based reward learning, underscoring the superior performance of our method. Our website is at https://liralab.usc.edu/comparative-language-feedback/.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/4ce343befe875d84f16213a4dbb55a1bb4ddaf77.pdf" target="_blank">/pdf/4ce343befe875d84f16213a4dbb55a1bb4ddaf77.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/993fe7f9ebd8f0878d8a1f80c391eb4abd807e2c.zip" target="_blank">/attachment/993fe7f9ebd8f0878d8a1f80c391eb4abd807e2c.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
yang2024trajectory,
title={Trajectory Improvement and Reward Learning from Comparative Language Feedback},
author={Zhaojing Yang and Miru Jun and Jeremy Tien and Stuart Russell and Anca Dragan and Erdem Biyik},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=1tCteNSbFH}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://liralab.usc.edu/comparative-language-feedback/</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/74d2425db6e34d39fa0adb82970c3fa5677b42ee.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=1tCteNSbFH&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">yang|trajectory_improvement_and_reward_learning_from_comparative_language_feedback</div>
</div>
<div class='paper-counter'>260/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">1jc2zA5Z6J</div>
<div class="field-name">title:</div>
<div class="field-value">Get a Grip: Multi-Finger Grasp Evaluation at Scale Enables Robust Sim-to-Real Transfer</div>
<div class="field-name">authors:</div>
<div class="field-value">['Tyler Ga Wei Lum', 'Albert H. Li', 'Preston Culbertson', 'Krishnan Srinivasan', 'Aaron Ames', 'Mac Schwager', 'Jeannette Bohg']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Tyler_Ga_Wei_Lum1', '~Albert_H._Li1', '~Preston_Culbertson1', '~Krishnan_Srinivasan1', '~Aaron_Ames2', '~Mac_Schwager1', '~Jeannette_Bohg1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Multi-Fingered Grasping', 'Large-Scale Grasp Dataset', 'Sim-to-Real']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that learned grasp evaluators enable robust real-world dexterous grasping when trained at sufficient scale with perceptual data. We release a dataset of 3.5M labeled grasps and show evaluators trained on it achieve SOTA hardware performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work explores conditions under which multi-finger grasping algorithms can attain robust sim-to-real transfer. While numerous large datasets facilitate learning *generative* models for multi-finger grasping at scale, reliable real-world dexterous grasping remains challenging, with most methods degrading when deployed on hardware. An alternate strategy is to use *discriminative* grasp evaluation models for grasp selection and refinement, conditioned on real-world sensor measurements. This paradigm has produced state-of-the-art results for vision-based parallel-jaw grasping, but remains unproven in the multi-finger setting. In this work, we find that existing datasets and methods have been insufficient for training discriminitive models for multi-finger grasping. To train grasp evaluators at scale, datasets must provide on the order of millions of grasps, including both positive *and negative examples*, with corresponding visual data resembling measurements at inference time. To that end, we release a new, open-source dataset of 3.5M grasps on 4.3K objects annotated with RGB images, point clouds, and trained NeRFs. Leveraging this dataset, we train vision-based grasp evaluators that outperform both analytic and generative modeling-based baselines on extensive simulated and real-world trials across a diverse range of objects. We show via numerous ablations that the key factor for performance is indeed the evaluator, and that its quality degrades as the dataset shrinks, demonstrating the importance of our new dataset. Project website at: https://sites.google.com/view/get-a-grip-dataset.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/039971039ffe39f0e7bd5b8f3ed2333ebb25aa1f.pdf" target="_blank">/pdf/039971039ffe39f0e7bd5b8f3ed2333ebb25aa1f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/f57e56b1137832862b892162a2d7454e9efeda67.zip" target="_blank">/attachment/f57e56b1137832862b892162a2d7454e9efeda67.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
lum2024get,
title={Get a Grip: Multi-Finger Grasp Evaluation at Scale Enables Robust Sim-to-Real Transfer},
author={Tyler Ga Wei Lum and Albert H. Li and Preston Culbertson and Krishnan Srinivasan and Aaron Ames and Mac Schwager and Jeannette Bohg},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=1jc2zA5Z6J}
}</div>
<div class="field-name">website:</div>
<div class="field-value">https://sites.google.com/view/get-a-grip-dataset</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/b6088808fa411d0ec622c9d609dee24137141b59.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=1jc2zA5Z6J&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">lum|get_a_grip_multifinger_grasp_evaluation_at_scale_enables_robust_simtoreal_transfer</div>
</div>
<div class='paper-counter'>261/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">1TEZ1hiY5m</div>
<div class="field-name">title:</div>
<div class="field-value">Learning Robotic Locomotion Affordances and Photorealistic Simulators from Human-Captured Data</div>
<div class="field-name">authors:</div>
<div class="field-value">['Alejandro Escontrela', 'Justin Kerr', 'Kyle Stachowicz', 'Pieter Abbeel']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Alejandro_Escontrela1', '~Justin_Kerr1', '~Kyle_Stachowicz1', '~Pieter_Abbeel2']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Navigation', 'Dataset', 'Real2Sim']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A data capture system and dataset that enable the training of locomotion affordance models and fast, photorealistic simulators.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning reliable affordance models which satisfy human preferences is often hindered by a lack of high-quality training data. Similarly, learning visuomotor policies in simulation can be challenging due to the high cost of photo-realistic rendering. We present PAWS: a comprehensive robot learning framework that uses a novel portable data capture rig and processing pipeline to collect long-horizon trajectories that include camera poses, foot poses, terrain meshes, and 3D radiance fields. We also contribute PAWS-Data: an extensive dataset gathered with PAWS containing over 10 hours of indoor and outdoor trajectories spanning a variety of scenes. With PAWS-Data we leverage radiance fields' photo-realistic rendering to generate tens of thousands of viewpoint-augmented images, then produce pixel affordance labels by identifying semantically similar regions to those traversed by the user. On this data we finetune a navigation affordance model from a pretrained backbone, and perform detailed ablations. Additionally, We open source PAWS-Sim, a high-speed photo-realistic simulator which integrates PAWS-Data with IsaacSim, enabling research for visuomotor policy learning. We evaluate the utility of the affordance model on a quadrupedal robot, which plans through affordances to follow pathways and sidewalks, and avoid human collisions. Project resources are available on the [website](https://pawslocomotion.com).</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/9a62348eb89fe9205145b7ff7f21ad5e0b5ac551.pdf" target="_blank">/pdf/9a62348eb89fe9205145b7ff7f21ad5e0b5ac551.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/abdd8309b2555462a6e48dba872e27d4256670b1.zip" target="_blank">/attachment/abdd8309b2555462a6e48dba872e27d4256670b1.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
escontrela2024learning,
title={Learning Robotic Locomotion Affordances and Photorealistic Simulators from Human-Captured Data},
author={Alejandro Escontrela and Justin Kerr and Kyle Stachowicz and Pieter Abbeel},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=1TEZ1hiY5m}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/325a7bee63c9f65de5a90325103344286eceffc9.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=1TEZ1hiY5m&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">escontrela|learning_robotic_locomotion_affordances_and_photorealistic_simulators_from_humancaptured_data</div>
</div>
<div class='paper-counter'>262/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">1IzW0aniyg</div>
<div class="field-name">title:</div>
<div class="field-value">EscIRL: Evolving Self-Contrastive IRL for Trajectory Prediction in Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">['Siyue Wang', 'Zhaorun Chen', 'Zhuokai Zhao', 'Chaoli Mao', 'Yiyang Zhou', 'Jiayu He', 'Albert Sibo Hu']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Siyue_Wang4', '~Zhaorun_Chen1', '~Zhuokai_Zhao1', '~Chaoli_Mao1', '~Yiyang_Zhou1', '~Jiayu_He1', '~Albert_Sibo_Hu1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Reinforcement Learning', 'Trajectory Prediction', 'Autonomous Driving']</div>
<div class="field-name">abstract:</div>
<div class="field-value">While deep neural networks (DNN) and inverse reinforcement learning (IRL) have both been commonly used in autonomous driving to predict trajectories through learning from expert demonstrations, DNN-based methods suffer from data-scarcity, while IRL-based approaches often struggle with generalizability, making both hard to apply to new driving scenarios. To address these issues, we introduce EscIRL, a novel decoupled bi-level training framework that iteratively learns robust reward models from only a few mixed-scenario demonstrations. At the inner level, EscIRL introduces a self-contrastive IRL module that learns a spectrum of specialized reward functions by contrasting demonstrations across different scenarios. At the outer level, ESCIRL employs an evolving loop that iteratively refines the contrastive sets, ensuring global convergence. Experiments on two multi-scenario datasets, CitySim and INTERACTION, demonstrate the effectiveness of EscIRL, outperforming state-of-the-art DNN and IRL-based methods by 41.3% on average. Notably, we show that EscIRL achieves superior generalizability compared to DNN-based approaches while requiring only a small fraction of the data, effectively addressing data-scarcity constraints. All code and data are available at https://github.com/SiyueWang-CiDi/EscIRL.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/50c2c437e5305e46db7ed693796a21afe5633303.pdf" target="_blank">/pdf/50c2c437e5305e46db7ed693796a21afe5633303.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
wang2024escirl,
title={Esc{IRL}: Evolving Self-Contrastive {IRL} for Trajectory Prediction in Autonomous Driving},
author={Siyue Wang and Zhaorun Chen and Zhuokai Zhao and Chaoli Mao and Yiyang Zhou and Jiayu He and Albert Sibo Hu},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=1IzW0aniyg}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/d4037541cb82e5547317afa732590027ab9bd9d3.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=1IzW0aniyg&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">wang|escirl_evolving_selfcontrastive_irl_for_trajectory_prediction_in_autonomous_driving</div>
</div>
<div class='paper-counter'>263/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">0gDbaEtVrd</div>
<div class="field-name">title:</div>
<div class="field-value">One Model to Drift Them All: Physics-Informed Conditional Diffusion Model for Driving at the Limits</div>
<div class="field-name">authors:</div>
<div class="field-value">['Franck Djeumou', 'Thomas Jonathan Lew', 'NAN DING', 'Michael Thompson', 'Makoto Suminaka', 'Marcus Greiff', 'John Subosits']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Franck_Djeumou1', '~Thomas_Jonathan_Lew1', '~NAN_DING5', '~Michael_Thompson5', '~Makoto_Suminaka1', '~Marcus_Greiff1', '~John_Subosits1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Diffusion Models', 'Learning for Control', 'Autonomous Drifting', 'Model Predictive Control']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">A single diffusion model can adapt online to reliably control a Toyota Supra and a Lexus LC 500 on challenging autonomous drifting tasks</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling autonomous vehicles to reliably operate at the limits of handling— where tire forces are saturated — would improve their safety, particularly in scenarios like emergency obstacle avoidance or adverse weather conditions.
However, unlocking this capability is challenging due to the task's dynamic nature and the high sensitivity to uncertain multimodal properties of the road, vehicle, and their dynamic interactions.
Motivated by these challenges, we propose a framework to learn a conditional diffusion model for high-performance vehicle control using an unlabelled multimodal trajectory dataset.
We design the diffusion model to capture the distribution of parameters of a physics-informed data-driven dynamics model.
By conditioning the generation process on online measurements, we integrate the diffusion model into a real-time model predictive control framework for driving at the limits, and show that it can adapt on the fly to a given vehicle and environment.
Extensive experiments on a Toyota Supra and a Lexus LC 500 show that a single diffusion model enables reliable autonomous drifting on both vehicles when operating with different tires in varying road conditions.
The model matches the performance of task-specific expert models while outperforming them in generalization to unseen conditions, paving the way towards a general, reliable method for autonomous driving at the limits of handling.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/dff0c4223355b993224186719c1e9c795277fde1.pdf" target="_blank">/pdf/dff0c4223355b993224186719c1e9c795277fde1.pdf</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
djeumou2024one,
title={One Model to Drift Them All: Physics-Informed Conditional Diffusion Model for Driving at the Limits},
author={Franck Djeumou and Thomas Jonathan Lew and NAN DING and Michael Thompson and Makoto Suminaka and Marcus Greiff and John Subosits},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=0gDbaEtVrd}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/90f1cc549a73921121b2dca94989d899e75df7e1.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">2</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=0gDbaEtVrd&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">djeumou|one_model_to_drift_them_all_physicsinformed_conditional_diffusion_model_for_driving_at_the_limits</div>
</div>
<div class='paper-counter'>264/265</div>
<div class="paper">
<div class="field-name">id:</div>
<div class="field-value">0M7JiV1GFN</div>
<div class="field-name">title:</div>
<div class="field-value">Provably Safe Online Multi-Agent Navigation in Unknown Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">['Zhan Gao', 'Guang Yang', 'Jasmine Bayrooti', 'Amanda Prorok']</div>
<div class="field-name">authorids:</div>
<div class="field-value">['~Zhan_Gao1', '~Guang_Yang16', '~Jasmine_Bayrooti1', '~Amanda_Prorok1']</div>
<div class="field-name">keywords:</div>
<div class="field-value">['Decentralized Multi-Agent Navigation', 'Unknown Environment', 'Support Vector Machine', 'Graph Attention Learning', 'Control Barrier Function']</div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes an online learning-based approach that allows for decentralized multi-agent navigation in unknown environments with provable safety guarantees.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control Barrier Functions (CBFs) provide safety guarantees for multi-agent navigation. However, traditional approaches require full knowledge of the environment (e.g., obstacle positions and shapes) to formulate CBFs and hence, are not applicable in unknown environments. This paper overcomes this issue by proposing an Online Exploration-based Control Lyapunov Barrier Function (OE-CLBF) controller. It estimates the unknown environment by learning its corresponding CBF with a Support Vector Machine (SVM) in an online manner, using local neighborhood information, and leverages the latter to generate actions for safe navigation. To reduce the computation incurred by the online SVM training, we use an Imitation Learning (IL) framework to predict the importance of neighboring agents with Graph Attention Networks (GATs), and train the SVM only with information received from neighbors of high `value'. The OE-CLBF allows for decentralized deployment, and importantly, provides provable safety guarantees that we derive in this paper. Experiments corroborate theoretical findings and demonstrate superior performance w.r.t. state-of-the-art baselines in a variety of unknown environments.</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="/pdf/8a97437356772d76ea701fe63b4c6f81d9382415.pdf" target="_blank">/pdf/8a97437356772d76ea701fe63b4c6f81d9382415.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="/attachment/d984985e22ac8b5dbd70289ce301727caa08fd7e.zip" target="_blank">/attachment/d984985e22ac8b5dbd70289ce301727caa08fd7e.zip</a></div>
<div class="field-name">venue:</div>
<div class="field-value">CoRL 2024</div>
<div class="field-name">venueid:</div>
<div class="field-value">robot-learning.org/CoRL/2024/Conference</div>
<div class="field-name">_bibtex:</div>
<div class="field-value">@inproceedings{
gao2024provably,
title={Provably Safe Online Multi-Agent Navigation in Unknown Environments},
author={Zhan Gao and Guang Yang and Jasmine Bayrooti and Amanda Prorok},
booktitle={8th Annual Conference on Robot Learning},
year={2024},
url={https://openreview.net/forum?id=0M7JiV1GFN}
}</div>
<div class="field-name">publication_agreement:</div>
<div class="field-value">/attachment/a7cd753b4a7c34a60e60507ca391a8d85629c27c.pdf</div>
<div class="field-name">student_paper:</div>
<div class="field-value">1</div>
<div class="field-name">spotlight_video:</div>
<div class="field-value">https://openreview.net/attachment?id=0M7JiV1GFN&name=spotlight_video</div>
<div class="field-name">paperhash:</div>
<div class="field-value">gao|provably_safe_online_multiagent_navigation_in_unknown_environments</div>
</div>
<div class='paper-counter'>265/265</div>

    </body>
    </html>
    