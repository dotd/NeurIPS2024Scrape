
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .paper { 
                margin-bottom: 30px;
                padding: 20px;
                border: 1px solid #ccc;
                border-radius: 5px;
            }
            .conference { 
                font-size: 24px;
                color: #2c3e50;
                margin: 20px 0;
                padding-bottom: 10px;
                border-bottom: 2px solid #3498db;
            }
            .field-name {
                font-weight: bold;
                color: #2980b9;
            }
            .field-value {
                margin-bottom: 10px;
                line-break: anywhere;
            }
            a { color: #3498db; }
            a:hover { color: #2980b9; }
            .paper-counter {
                font-size: 18px;
                color: #2c3e50;
                margin: 20px 0;
                padding-bottom: 10px;
                border-bottom: 2px solid #3498db;
            }
        </style>
    </head>
    <body>
    <div class="conference">ICML_cc_2024_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis</div>
<div class="field-name">authors:</div>
<div class="field-value">Yao Mu;Junting Chen;Qing-Long Zhang;Shoufa Chen;Qiaojun Yu;Chongjian GE;Runjian Chen;Zhixuan Liang;Mengkang Hu;Chaofan Tao;Peize Sun;Haibao Yu;Chao Yang;Wenqi Shao;Wenhai Wang;Jifeng Dai;Yu Qiao;Mingyu Ding;Ping Luo</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ced23f9c13c7128e0f550576b1df8df5cbc3717b.pdf" target="_blank">https://openreview.net/pdf/ced23f9c13c7128e0f550576b1df8df5cbc3717b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one embodied navigation task.</div>
</div>
<div class='paper-counter'>1/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LLM-Empowered State Representation for Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Boyuan Wang;Yun Qu;Yuhang Jiang;Jianzhun Shao;Chang Liu;Wenming Yang;Xiangyang Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2180b41c14831553480ebcb8051ea1034a8ca93a.pdf" target="_blank">https://openreview.net/pdf/2180b41c14831553480ebcb8051ea1034a8ca93a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Conventional state representations in reinforcement learning often omit critical task-related details, presenting a significant challenge for value networks in establishing accurate mappings from states to task rewards. Traditional methods typically depend on extensive sample learning to enrich state representations with task-specific information, which leads to low sample efficiency and high time costs. Recently, surging knowledgeable large language models (LLM) have provided promising substitutes for prior injection with minimal human intervention. Motivated by this, we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training. Experimental results demonstrate LESR exhibits high sample efficiency and outperforms state-of-the-art baselines by an average of **29%** in accumulated reward in Mujoco tasks and **30%** in success rates in Gym-Robotics tasks. Codes of LESR are accessible at https://github.com/thu-rllab/LESR.</div>
</div>
<div class='paper-counter'>2/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiachen Li;Qiaozi Gao;Michael Johnston;Xiaofeng Gao;Xuehai He;Hangjie Shi;Suhaila Shakiah;Reza Ghanadan;William Yang Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d68be525ddacdc8f5272ea75b89c4ac268115934.pdf" target="_blank">https://openreview.net/pdf/d68be525ddacdc8f5272ea75b89c4ac268115934.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability.</div>
</div>
<div class='paper-counter'>3/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">HarmonyDream: Task Harmonization Inside World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoyu Ma;Jialong Wu;Ningya Feng;Chenjun Xiao;Dong Li;Jianye HAO;Jianmin Wang;Mingsheng Long</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e6836b38ddecdef092d92c42d027c8687bf8c7a3.pdf" target="_blank">https://openreview.net/pdf/e6836b38ddecdef092d92c42d027c8687bf8c7a3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a simple yet effective approach, HarmonyDream, which automatically adjusts loss coefficients to maintain task harmonization, i.e. a dynamic equilibrium between the two tasks in world model learning. Our experiments show that the base MBRL method equipped with HarmonyDream gains 10%-69% absolute performance boosts on visual robotic tasks and sets a new state-of-the-art result on the Atari 100K benchmark. Code is available at https://github.com/thuml/HarmonyDream.</div>
</div>
<div class='paper-counter'>4/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Joint Composite Latent Space Bayesian Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Natalie Maus;Zhiyuan Jerry Lin;Maximilian Balandat;Eytan Bakshy</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/42c2a05e3791c8397f88f9f65eb4cc5e9ed94d0d.pdf" target="_blank">https://openreview.net/pdf/42c2a05e3791c8397f88f9f65eb4cc5e9ed94d0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Bayesian Optimization (BO) is a technique for sample-efficient black-box optimization that employs probabilistic models to identify promising input for evaluation. When dealing with composite-structured functions, such as $f=g \circ h$, evaluating a specific location $x$ yields observations of both the final outcome $f(x) = g(h(x))$ as well as the intermediate output(s) $h(x)$. Previous research has shown that integrating information from these intermediate outputs can enhance BO performance substantially. However, existing methods struggle if the outputs $h(x)$ are high-dimensional. Many relevant problems fall into this setting, including in the context of generative AI, molecular design, or robotics. To effectively tackle these challenges, we introduce Joint Composite Latent Space Bayesian Optimization (JoCo), a novel framework that jointly trains neural network encoders and probabilistic models to adaptively compress high-dimensional input and output spaces into manageable latent representations. This enables effective BO on these compressed representations, allowing JoCo to outperform other state-of-the-art methods in high-dimensional BO on a wide variety of simulated and real-world problems.</div>
</div>
<div class='paper-counter'>5/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Offline Actor-Critic Reinforcement Learning Scales to Large Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Jost Tobias Springenberg;Abbas Abdolmaleki;Jingwei Zhang;Oliver Groth;Michael Bloesch;Thomas Lampe;Philemon Brakel;Sarah Maria Elisabeth Bechtle;Steven Kapturowski;Roland Hafner;Nicolas Heess;Martin Riedmiller</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/59d9a12d5b29fbffdc5f456cd51121d962fdd92c.pdf" target="_blank">https://openreview.net/pdf/59d9a12d5b29fbffdc5f456cd51121d962fdd92c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset; containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.</div>
</div>
<div class='paper-counter'>6/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Junjie Zhang;Chenjia Bai;Haoran He;Zhigang Wang;Bin Zhao;Xiu Li;Xuelong Li</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4ac6d60d3105195905e914acc87498c655f84b33.pdf" target="_blank">https://openreview.net/pdf/4ac6d60d3105195905e914acc87498c655f84b33.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot’s end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose **SAM-E**, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.</div>
</div>
<div class='paper-counter'>7/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Visual Representation Learning with Stochastic Frame Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Huiwon Jang;Dongyoung Kim;Junsu Kim;Jinwoo Shin;Pieter Abbeel;Younggyo Seo</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e7de5b30553097a9c41eaef5d788d0146d0c94d5.pdf" target="_blank">https://openreview.net/pdf/e7de5b30553097a9c41eaef5d788d0146d0c94d5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Self-supervised learning of image representations by predicting future frames is a promising direction but still remains a challenge. This is because of the under-determined nature of frame prediction; multiple potential futures can arise from a single current frame. To tackle this challenge, in this paper, we revisit the idea of stochastic video generation that learns to capture uncertainty in frame prediction and explore its effectiveness for representation learning. Specifically, we design a framework that trains a stochastic frame prediction model to learn temporal information between frames. Moreover, to learn dense information within each frame, we introduce an auxiliary masked image modeling objective along with a shared decoder architecture. We find this architecture allows for combining both objectives in a synergistic and compute-efficient manner. We demonstrate the effectiveness of our framework on a variety of tasks from video label propagation and vision-based robot learning domains, such as video segmentation, pose tracking, vision-based robotic locomotion, and manipulation tasks. Code is available on the project webpage: https://sites.google.com/view/2024rsp.</div>
</div>
<div class='paper-counter'>8/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Sparse-to-dense Multimodal Image Registration via Multi-Task Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaining Zhang;Jiayi Ma</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7434907792ee7ddf98c3a087ed716e660d544c63.pdf" target="_blank">https://openreview.net/pdf/7434907792ee7ddf98c3a087ed716e660d544c63.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Aligning image pairs captured by different sensors or those undergoing significant appearance changes is crucial for various computer vision and robotics applications. Existing approaches cope with this problem via either Sparse feature Matching (SM) or Dense direct Alignment (DA) paradigms. Sparse methods are efficient but lack accuracy in textureless scenes, while dense ones are more accurate in all scenes but demand for good initialization. In this paper, we propose SDME, a Sparse-to-Dense Multimodal feature Extractor based on a novel multi-task network that simultaneously predicts SM and DA features for robust multimodal image registration. We propose the sparse-to-dense registration paradigm: we first perform initial registration via SM and then refine the result via DA. By using the well-designed SDME, the sparse-to-dense approach combines the merits from both SM and DA. Extensive experiments on MSCOCO, GoogleEarth, VIS-NIR and VIS-IR-drone datasets demonstrate that our method achieves remarkable performance on multimodal cases. Furthermore, our approach exhibits robust generalization capabilities, enabling the fine-tuning of models initially trained on single-modal datasets for use with smaller multimodal datasets. Our code is available at https://github.com/KN-Zhang/SDME.</div>
</div>
<div class='paper-counter'>9/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Latent Logic Tree Extraction for Event Sequence Explanation from LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">Zitao Song;Chao Yang;Chaojie Wang;Bo An;Shuang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/58327570e93a52393e30ed8c17e0839ba62379f3.pdf" target="_blank">https://openreview.net/pdf/58327570e93a52393e30ed8c17e0839ba62379f3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.</div>
</div>
<div class='paper-counter'>10/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Subgoal-based Demonstration Learning for Formal Theorem Proving</div>
<div class="field-name">authors:</div>
<div class="field-value">Xueliang Zhao;Wenda Li;Lingpeng Kong</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2cd3b24496ff2feac2f6fb917eaedd04e4ed71ed.pdf" target="_blank">https://openreview.net/pdf/2cd3b24496ff2feac2f6fb917eaedd04e4ed71ed.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) present a promising pathway for advancing the domain of formal theorem proving. In this paper, we aim to improve the performance of LLMs in formal theorem proving by thoroughly examining the structure and organization of demonstrative in-context examples. We introduce a subgoal-based demonstration learning framework, specifically designed to enhance the efficiency of proof search in LLMs. First, drawing upon the insights of subgoal learning from reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Second, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Our integration of subgoal-based learning has notably increased proof accuracy from 38.9% to 44.1% on the miniF2F benchmark. Furthermore, the adoption of diffusion models for demonstration organization can lead to an additional enhancement in accuracy to 45.5%, or a $5\times$ improvement in sampling efficiency compared to previously established methods.</div>
</div>
<div class='paper-counter'>11/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruijie Zheng;Ching-An Cheng;Hal Daumé III;Furong Huang;Andrey Kolobov</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9ef85f99ba7352a053f093996a9463cb24e34d1e.pdf" target="_blank">https://openreview.net/pdf/9ef85f99ba7352a053f093996a9463cb24e34d1e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to bear on the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the learning performance of behavior cloning on downstream tasks.</div>
</div>
<div class='paper-counter'>12/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Longxin Kou;Fei Ni;YAN ZHENG;Jinyi Liu;Yifu Yuan;Zibin Dong;Jianye HAO</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bfa8c6d597da384a0a23f067471391339f2b99e8.pdf" target="_blank">https://openreview.net/pdf/bfa8c6d597da384a0a23f067471391339f2b99e8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic manipulation tasks often span over long horizons and encapsulate multiple subtasks with different skills. Learning policies directly from long-horizon demonstrations is challenging without intermediate keyframes guidance and corresponding skill annotations. Existing approaches for keyframe identification often struggle to offer reliable decomposition for low accuracy and fail to provide semantic relevance between keyframes and skills. For this, we propose a unified **K**eyframe **I**dentifier and **S**kill **A**notator (**KISA**) that utilizes pretrained visual-language representations for precise and interpretable decomposition of unlabeled demonstrations. Specifically, we develop a simple yet effective temporal enhancement module that enriches frame-level representations with expanded receptive fields to capture semantic dynamics at the video level. We further propose coarse contrastive learning and fine-grained monotonic encouragement to enhance the alignment between visual representations from keyframes and language representations from skills. The experimental results across three benchmarks demonstrate that KISA outperforms competitive baselines in terms of accuracy and interpretability of keyframe identification. Moreover, KISA exhibits robust generalization capabilities and the flexibility to incorporate various pretrained representations.</div>
</div>
<div class='paper-counter'>13/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Bringing Motion Taxonomies to Continuous Domains via GPLVM on Hyperbolic manifolds</div>
<div class="field-name">authors:</div>
<div class="field-value">Noémie Jaquier;Leonel Rozo;Miguel González-Duque;Viacheslav Borovitskiy;Tamim Asfour</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/10b66d5926fadc95555e27ef77c4ee12dbac31aa.pdf" target="_blank">https://openreview.net/pdf/10b66d5926fadc95555e27ef77c4ee12dbac31aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Human motion taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different human motion taxonomies to learn hyperbolic embeddings that faithfully preserve the original graph structure. We show that our model properly encodes unseen data from existing or new taxonomy categories, and outperforms its Euclidean and VAE-based counterparts. Finally, through proof-of-concept experiments, we show that our model may be used to generate realistic trajectories between the learned embeddings.</div>
</div>
<div class='paper-counter'>14/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Hengkai Tan;Songming Liu;Kai Ma;Chengyang Ying;Xingxing Zhang;Hang Su;Jun Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c4b23ea3b2a5c8b61db19feaa7ee3d7ec093de57.pdf" target="_blank">https://openreview.net/pdf/c4b23ea3b2a5c8b61db19feaa7ee3d7ec093de57.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Transformer has shown promise in reinforcement learning to model time-varying features for obtaining generalized low-level robot policies on diverse robotics datasets in embodied learning. However, it still suffers from the issues of low data efficiency and high inference latency. In this paper, we propose to investigate the task from a new perspective of the frequency domain. We first observe that the energy density in the frequency domain of a robot's trajectory is mainly concentrated in the low-frequency part. Then, we present the Fourier Controller Network (FCNet), a new network that uses Short-Time Fourier Transform (STFT) to extract and encode time-varying features through frequency domain interpolation. In order to do real-time decision-making, we further adopt FFT and Sliding DFT methods in the model architecture to achieve parallel training and efficient recurrent inference. Extensive results in both simulated (e.g., D4RL) and real-world environments (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental robotics datasets of all types of sizes (from 1.9M to 120M). The project page and code can be found https://thkkk.github.io/fcnet.</div>
</div>
<div class='paper-counter'>15/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Coverage Paths in Unknown Environments with Deep Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Arvi Jonnarth;Jie Zhao;Michael Felsberg</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0ad8faee1dd698b97b7510066d1aa5d50be947b3.pdf" target="_blank">https://openreview.net/pdf/0ad8faee1dd698b97b7510066d1aa5d50be947b3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Coverage path planning (CPP) is the problem of finding a path that covers the entire free space of a confined area, with applications ranging from robotic lawn mowing to search-and-rescue. When the environment is unknown, the path needs to be planned online while mapping the environment, which cannot be addressed by offline planning methods that do not allow for a flexible path space. We investigate how suitable reinforcement learning is for this challenging problem, and analyze the involved components required to efficiently learn coverage paths, such as action space, input feature representation, neural network architecture, and reward function. We propose a computationally feasible egocentric map representation based on frontiers, and a novel reward term based on total variation to promote complete coverage. Through extensive experiments, we show that our approach surpasses the performance of both previous RL-based approaches and highly specialized methods across multiple CPP variations.</div>
</div>
<div class='paper-counter'>16/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs</div>
<div class="field-name">authors:</div>
<div class="field-value">Shenzhi Yang;Bin Liang;An Liu;Lin Gui;Xingkai Yao;Xiaofang Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/751be5bcbae5205807593ef28db4ba06d7ae1a37.pdf" target="_blank">https://openreview.net/pdf/751be5bcbae5205807593ef28db4ba06d7ae1a37.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Given the critical role of graphs in real-world applications and their high-security requirements, improving the ability of graph neural networks (GNNs) to detect out-of-distribution (OOD) data is an urgent research problem. The recent work GNNSAFE proposes a framework based on the aggregation of negative energy scores that significantly improves the performance of GNNs to detect node-level OOD data. However, our study finds that score aggregation among nodes is susceptible to extreme values due to the unboundedness of the negative energy scores and logit shifts, which severely limits the accuracy of GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE: reducing the generation of extreme scores of nodes by adding two optimization terms that make the negative energy scores bounded and mitigate the logit shift. Experimental results show that our approach dramatically improves the ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD data induced by Structure Manipulation, the metric of FPR95 (lower is better) in scenarios without (with) OOD data exposure are reduced from the current SOTA by 28.4% ( 22.7% ). The code is available via https://github.com/ShenzhiYang2000/NODESAFE-Bounded-and-Uniform-Energy-based-Out-of-distribution-Detection-for-Graphs.</div>
</div>
<div class='paper-counter'>17/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling</div>
<div class="field-name">authors:</div>
<div class="field-value">Utsav Singh;Wesley A Suttle;Brian M. Sadler;Vinay P. Namboodiri;Amrit Bedi</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/486d2e3f8b1d6cd84cc050e257c70fda5c2f6980.pdf" target="_blank">https://openreview.net/pdf/486d2e3f8b1d6cd84cc050e257c70fda5c2f6980.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we introduce PIPER: Primitive-Informed Preference-based Hierarchical reinforcement learning via Hindsight Relabeling, a novel approach that leverages preference-based learning to learn a reward model, and subsequently uses this reward model to relabel higher-level replay buffers. Since this reward is unaffected by lower primitive behavior, our relabeling-based approach is able to mitigate non-stationarity, which is common in existing hierarchical approaches, and demonstrates impressive performance across a range of challenging sparse-reward tasks. Since obtaining human feedback is typically impractical, we propose to replace the human-in-the-loop approach with our primitive-in-the-loop approach, which generates feedback using sparse rewards provided by the environment. Moreover, in order to prevent infeasible subgoal prediction and avoid degenerate solutions, we propose primitive-informed regularization that conditions higher-level policies to generate feasible subgoals for lower-level policies. We perform extensive experiments to show that PIPER mitigates non-stationarity in hierarchical reinforcement learning and achieves greater than 50$\\%$ success rates in challenging, sparse-reward robotic environments, where most other baselines fail to achieve any significant progress.</div>
</div>
<div class='paper-counter'>18/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Feasible Reachable Policy Iteration</div>
<div class="field-name">authors:</div>
<div class="field-value">Shentao Qin;Yujie Yang;Yao Mu;JIE LI;Wenjun Zou;Jingliang Duan;Shengbo Eben Li</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/092551028ea8df6430d3080ced2ae5440997a754.pdf" target="_blank">https://openreview.net/pdf/092551028ea8df6430d3080ced2ae5440997a754.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The goal-reaching tasks with safety constraints are common control problems in real world, such as intelligent driving and robot manipulation. The difficulty of this kind of problem comes from the exploration termination caused by safety constraints and the sparse rewards caused by goals. The existing safe RL avoids unsafe exploration by restricting the search space to a feasible region, the essence of which is the pruning of the search space. However, there are still many ineffective explorations in the feasible region because of the ignorance of the goals. Our approach considers both safety and goals; the policy space pruning is achieved by a function called feasible reachable function, which describes whether there is a policy to make the agent safely reach the goals in the finite time domain. This function naturally satisfies the self-consistent condition and the risky Bellman equation, which can be solved by the fixed point iteration method. On this basis, we propose feasible reachable policy iteration (FRPI), which is divided into three steps: policy evaluation, region expansion, and policy improvement. In the region expansion step, by using the information of agent to reach the goals, the convergence of the feasible region is accelerated, and simultaneously a smaller feasible reachable region is identified. The experimental results verify the effectiveness of the proposed FR function in both improving the convergence speed of better or comparable performance without sacrificing safety and identifying a smaller policy space with higher sample efficiency.</div>
</div>
<div class='paper-counter'>19/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Prompt-guided Precise Audio Editing with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Manjie Xu;Chenxing Li;Duzhen Zhang;Dan Su;Wei Liang;Dong Yu</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e44c72ec686db3e586ca90bb16f4a704b886fbb5.pdf" target="_blank">https://openreview.net/pdf/e44c72ec686db3e586ca90bb16f4a704b886fbb5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Audio editing involves the arbitrary manipulation of audio content through precise control. Although text-guided diffusion models have made significant advancements in text-to-audio generation, they still face challenges in finding a flexible and precise way to modify target events within an audio track. We present a novel approach, referred to as **PPAE**, which serves as a general module for diffusion models and enables precise audio editing. The editing is based on the input textual prompt only and is entirely training-free. We exploit the cross-attention maps of diffusion models to facilitate accurate local editing and employ a hierarchical local-global pipeline to ensure a smoother editing process. Experimental results highlight the effectiveness of our method in various editing tasks.</div>
</div>
<div class='paper-counter'>20/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboDreamer: Learning Compositional World Models for Robot Imagination</div>
<div class="field-name">authors:</div>
<div class="field-value">Siyuan Zhou;Yilun Du;Jiaben Chen;YANDONG LI;Dit-Yan Yeung;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8f2443802f986f929538987ce0055681f150dea3.pdf" target="_blank">https://openreview.net/pdf/8f2443802f986f929538987ce0055681f150dea3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.</div>
</div>
<div class='paper-counter'>21/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Executable Code Actions Elicit Better LLM Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Xingyao Wang;Yangyi Chen;Lifan Yuan;Yizhe Zhang;Yunzhu Li;Hao Peng;Heng Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/77201ef17d3ec7f9b7079d50149857895613a953.pdf" target="_blank">https://openreview.net/pdf/77201ef17d3ec7f9b7079d50149857895613a953.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python **code** to consolidate LLM agents' **act**ions into a unified action space (**CodeAct**). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.</div>
</div>
<div class='paper-counter'>22/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Behavior Generation with Latent Actions</div>
<div class="field-name">authors:</div>
<div class="field-value">Seungjae Lee;Yibin Wang;Haritheja Etukuru;H. Jin Kim;Nur Muhammad Mahi Shafiullah;Lerrel Pinto</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b71d5f77310d333dc283e2dce3c7b7d90f17dc30.pdf" target="_blank">https://openreview.net/pdf/b71d5f77310d333dc283e2dce3c7b7d90f17dc30.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision-making. Unlike language or image generation, decision-making requires modeling actions – continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT’s improved ability to capture behavior modes while accelerating inference speed 5× over Diffusion Policies. Videos can be found https://sjlee.cc/vq-bet/</div>
</div>
<div class='paper-counter'>23/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Chia-Cheng Chiang;Li-Cheng Lan;Wei-Fang Sun;Chien Feng;Cho-Jui Hsieh;Chun-Yi Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a35f2b4966588d6fdc54ee81b86852cf3666b083.pdf" target="_blank">https://openreview.net/pdf/a35f2b4966588d6fdc54ee81b86852cf3666b083.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where acquiring multiple expert demonstrations is costly or infeasible and the ground truth reward function is not available. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existing IL approaches and achieves expert-level performance in the single-demonstration IL setting across five widely adopted MuJoCo benchmarks as well as the "Adroit Door" robotic environment.</div>
</div>
<div class='paper-counter'>24/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Randall Balestriero;Romain Cosentino;Sarath Shekkizhar</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/71aad921ec144a189aa4015835cc54f5d3097a1e.pdf" target="_blank">https://openreview.net/pdf/71aad921ec144a189aa4015835cc54f5d3097a1e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: https://github.com/RandallBalestriero/SplineLLM</div>
</div>
<div class='paper-counter'>25/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity</div>
<div class="field-name">authors:</div>
<div class="field-value">Chang He;Zhaoye Pan;Xiao Wang;Bo Jiang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a615fdc9761e2ec1449788231d868dddccfd2d61.pdf" target="_blank">https://openreview.net/pdf/a615fdc9761e2ec1449788231d868dddccfd2d61.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning. While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds. The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of (Li et al., 2023a). However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice. To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness. Regarding efficiency, our accelerated algorithm has the function query complexity of $\mathcal{O}(\epsilon^{-7/4}d)$ for finding an $\epsilon$-approximate first-order stationary point. By introducing a small perturbation, it exhibits a function query complexity of $\tilde{\mathcal{O}}(\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space. Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem. Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\tilde{\mathcal{O}}(\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\tilde{\mathcal{O}}(\epsilon^{3/4})$.</div>
</div>
<div class='paper-counter'>26/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scale-Free Image Keypoints Using Differentiable Persistent Homology</div>
<div class="field-name">authors:</div>
<div class="field-value">Giovanni Barbarani;Francesco Vaccarino;Gabriele Trivigno;Marco Guerra;Gabriele Berton;Carlo Masone</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/abd51db577c18aeaaa405973233f07dc1df668a6.pdf" target="_blank">https://openreview.net/pdf/abd51db577c18aeaaa405973233f07dc1df668a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In computer vision, keypoint detection is a fundamental task, with applications spanning from robotics to image retrieval; however, existing learning-based methods suffer from scale dependency, and lack flexibility. This paper introduces a novel approach that leverages Morse theory and persistent homology, powerful tools rooted in algebraic topology. We propose a novel loss function based on the recent introduction of a notion of subgradient in persistent homology, paving the way towards topological learning. Our detector, MorseDet, is the first topology-based learning model for feature detection, which achieves competitive performance in keypoint repeatability and introduces a principled and theoretically robust approach to the problem.</div>
</div>
<div class='paper-counter'>27/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Data Poisoning Attacks against Conformal Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Yangyi Li;Aobo Chen;Wei Qian;Chenxu Zhao;Divya Lidder;Mengdi Huai</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d632f21cf04c12157d5a74df3154786bd351ae92.pdf" target="_blank">https://openreview.net/pdf/d632f21cf04c12157d5a74df3154786bd351ae92.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The efficient and theoretically sound uncertainty quantification is crucial for building trust in deep learning models. This has spurred a growing interest in conformal prediction (CP), a powerful technique that provides a model-agnostic and distribution-free method for obtaining conformal prediction sets with theoretical guarantees. However, the vulnerabilities of such CP methods with regard to dedicated data poisoning attacks have not been studied previously. To bridge this gap, for the first time, we in this paper propose a new class of black-box data poisoning attacks against CP, where the adversary aims to cause the desired manipulations of some specific examples' prediction uncertainty results (instead of misclassifications). Additionally, we design novel optimization frameworks for our proposed attacks. Further, we conduct extensive experiments to validate the effectiveness of our attacks on various settings (e.g., the full and split CP settings). Notably, our extensive experiments show that our attacks are more effective in manipulating uncertainty results than traditional poisoning attacks that aim at inducing misclassifications, and existing defenses against conventional attacks are ineffective against our proposed attacks.</div>
</div>
<div class='paper-counter'>28/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: Leverage Foundational Models for Black-Box Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Xingyou Song;Yingtao Tian;Robert Tjarko Lange;Chansoo Lee;Yujin Tang;Yutian Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/87c951c63cc9772ab939bd3ff0a49981df243b58.pdf" target="_blank">https://openreview.net/pdf/87c951c63cc9772ab939bd3ff0a49981df243b58.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research. However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.</div>
</div>
<div class='paper-counter'>29/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Qi Lv;Hao Li;Xiang Deng;Rui Shao;Michael Y Wang;Liqiang Nie</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c28b6e15acaae2a0445aee5fe01553ae01a97911.pdf" target="_blank">https://openreview.net/pdf/c28b6e15acaae2a0445aee5fe01553ae01a97911.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel **Robo**tic **M**ultimodal **P**erception-**P**lanning (**RoboMP$^2$**) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines.</div>
</div>
<div class='paper-counter'>30/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: A Call for Embodied AI</div>
<div class="field-name">authors:</div>
<div class="field-value">Giuseppe Paolo;Jonas Gonzalez-Billandon;Balázs Kégl</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b959413bdbf7dbcf659c37642b24f6b7de36acd2.pdf" target="_blank">https://openreview.net/pdf/b959413bdbf7dbcf659c37642b24f6b7de36acd2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose Embodied AI (E-AI) as the next fundamental step in the pursuit of Artificial General Intelligence (AGI), juxtaposing it against current AI advancements, particularly Large Language Models (LLMs). We traverse the evolution of the embodiment concept across diverse fields (philosophy, psychology, neuroscience, and robotics) to highlight how E-AI distinguishes itself from the classical paradigm of static learning. By broadening the scope of E-AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston’s active inference principle, offering a comprehensive approach to E-AI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future E-AI research. Highlighting the importance of creating E-AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.</div>
</div>
<div class='paper-counter'>31/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: Automatic Environment Shaping is the Next Frontier in RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Younghyo Park;Gabriel B. Margolis;Pulkit Agrawal</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ab986a7984eac1598beea86386428fa53f5694f3.pdf" target="_blank">https://openreview.net/pdf/ab986a7984eac1598beea86386428fa53f5694f3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures.</div>
</div>
<div class='paper-counter'>32/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Stop Regressing: Training Value Functions via Classification for Scalable Deep RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Jesse Farebrother;Jordi Orbay;Quan Vuong;Adrien Ali Taiga;Yevgen Chebotar;Ted Xiao;Alex Irpan;Sergey Levine;Pablo Samuel Castro;Aleksandra Faust;Aviral Kumar;Rishabh Agarwal</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d7be2bfd406a29a0a8152605d8f88ab08afd5f3d.pdf" target="_blank">https://openreview.net/pdf/d7be2bfd406a29a0a8152605d8f88ab08afd5f3d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost.</div>
</div>
<div class='paper-counter'>33/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity</div>
<div class="field-name">authors:</div>
<div class="field-value">Hyunki Seong;Hyunchul Shim</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/38651f9e4dfaf7d4ff333fe05f5d4e63134b353b.pdf" target="_blank">https://openreview.net/pdf/38651f9e4dfaf7d4ff333fe05f5d4e63134b353b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce MoNet, a novel functionally modular network for self-supervised and interpretable end-to-end learning. By leveraging its functional modularity with a latent-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space without requiring task-level supervision. Moreover, our method incorporates an online, post-hoc explainability approach that enhances the interpretability of end-to-end inferences without compromising sensorimotor control performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, outperforming baseline models by 7% to 28% in task specificity analysis. We further explore the interpretability of our network through post-hoc analysis of perceptual saliency maps and latent decision vectors. This provides valuable insights into the incorporation of explainable artificial intelligence into robotic learning, encompassing both perceptual and behavioral perspectives. Supplementary materials are available at https://sites.google.com/view/monet-lgc.</div>
</div>
<div class='paper-counter'>34/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">State-Constrained Zero-Sum Differential Games with One-Sided Information</div>
<div class="field-name">authors:</div>
<div class="field-value">Mukesh Ghimire;Lei Zhang;Zhe Xu;Yi Ren</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5e9fe3d4d6aa479322cbd558ebb4f1af501c87d2.pdf" target="_blank">https://openreview.net/pdf/5e9fe3d4d6aa479322cbd558ebb4f1af501c87d2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to violate the state constraints if possible, or to maximize the payoff otherwise. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing behavioral strategies. Different from existing works that are concerned about the scalability of no-regret learning in games with discrete dynamics, our study reveals the underlying structure of strategies for belief manipulation resulting from information asymmetry and state constraints. This structure will be necessary for scalable learning on games with continuous actions and long time windows. We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random deceptive moves to take advantage of information asymmetry, and compute how the defender should respond.</div>
</div>
<div class='paper-counter'>35/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings</div>
<div class="field-name">authors:</div>
<div class="field-value">Kevin Frans;Seohong Park;Pieter Abbeel;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d3b3b9aac1b31718d454799ac770e55071f1bd77.pdf" target="_blank">https://openreview.net/pdf/d3b3b9aac1b31718d454799ac770e55071f1bd77.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a *functional* reward encoding (FRE) as a general, scalable solution to this *zero-shot RL* problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods.</div>
</div>
<div class='paper-counter'>36/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Reward for Robot Skills Using Large Language Models via Self-Alignment</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuwei Zeng;Yao Mu;Lin Shao</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9cc8228179c02fcb22d1ecc387b2cd74221c4731.pdf" target="_blank">https://openreview.net/pdf/9cc8228179c02fcb22d1ecc387b2cd74221c4731.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement in training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method.</div>
</div>
<div class='paper-counter'>37/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Yufei Wang;Zhanyi Sun;Jesse Zhang;Zhou Xian;Erdem Biyik;David Held;Zackory Erickson</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/32a72f0f408c93a651571488d34ca52443f4e0c3.pdf" target="_blank">https://openreview.net/pdf/32a72f0f408c93a651571488d34ca52443f4e0c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains — including classic control, as well as manipulation of rigid, articulated, and deformable objects — without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/</div>
</div>
<div class='paper-counter'>38/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Zheng Xiong;Risto Vuorio;Jacob Beck;Matthieu Zimmer;Kun Shao;Shimon Whiteson</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/11f951c35380ce0bc3ebedfa3ef7fabac97266f9.pdf" target="_blank">https://openreview.net/pdf/11f951c35380ce0bc3ebedfa3ef7fabac97266f9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency advantage of HyperDistill at inference time to knowledge decoupling, i.e., the ability to decouple inter-task and intra-task knowledge, a general principle that could also be applied to improve inference efficiency in other domains. The code is publicly available at https://github.com/MasterXiong/Universal-Morphology-Control.</div>
</div>
<div class='paper-counter'>39/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Pengying Wu;Yao Mu;Bingxian Wu;Yi Hou;Ji Ma;Shanghang Zhang;Chang Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3eb46b8f8e758d102d94fa9f5671872122acd59a.pdf" target="_blank">https://openreview.net/pdf/3eb46b8f8e758d102d94fa9f5671872122acd59a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io</div>
</div>
<div class='paper-counter'>40/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Provably Efficient Long-Horizon Exploration in Monte Carlo Tree Search through State Occupancy Regularization</div>
<div class="field-name">authors:</div>
<div class="field-value">Liam Schramm;Abdeslam Boularias</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0da7c15e882392d81223a66653f387f65f9c4b27.pdf" target="_blank">https://openreview.net/pdf/0da7c15e882392d81223a66653f387f65f9c4b27.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Monte Carlo tree search (MCTS) has been successful in a variety of domains, but faces challenges with long-horizon exploration when compared to sampling-based motion planning algorithms like Rapidly-Exploring Random Trees. To address these limitations of MCTS, we derive a tree search algorithm based on policy optimization with state-occupancy measure regularization, which we call *Volume-MCTS*. We show that count-based exploration and sampling-based motion planning can be derived as approximate solutions to this state-occupancy measure regularized objective. We test our method on several robot navigation problems, and find that Volume-MCTS outperforms AlphaZero and displays significantly better long-horizon exploration properties.</div>
</div>
<div class='paper-counter'>41/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling</div>
<div class="field-name">authors:</div>
<div class="field-value">Raunaq Bhirangi;Chenyu Wang;Venkatesh Pattabiraman;Carmel Majidi;Abhinav Gupta;Tess Hellebrekers;Lerrel Pinto</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc3783db035c7ac10d98199bc33f47c536bd3bfa.pdf" target="_blank">https://openreview.net/pdf/dc3783db035c7ac10d98199bc33f47c536bd3bfa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.</div>
</div>
<div class='paper-counter'>42/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yufei Wang;Zhou Xian;Feng Chen;Tsun-Hsuan Wang;Yian Wang;Katerina Fragkiadaki;Zackory Erickson;David Held;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1475e7232cd7c484bed60587c83c2040626987e5.pdf" target="_blank">https://openreview.net/pdf/1475e7232cd7c484bed60587c83c2040626987e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates simulation environments by populating pertinent assets with proper spatial configurations. Afterwards, the agent decomposes the proposed task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.</div>
</div>
<div class='paper-counter'>43/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OMPO: A Unified Framework for RL under Policy and Dynamics Shifts</div>
<div class="field-name">authors:</div>
<div class="field-value">Yu Luo;Tianying Ji;Fuchun Sun;Jianwei Zhang;Huazhe Xu;Xianyuan Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/40eb269e30c44a2dc2678f17792ba08fd225dcb0.pdf" target="_blank">https://openreview.net/pdf/40eb269e30c44a2dc2678f17792ba08fd225dcb0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and non-stationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications.</div>
</div>
<div class='paper-counter'>44/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Potential Based Diffusion Motion Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunhao Luo;Chen Sun;Joshua B. Tenenbaum;Yilun Du</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7f1d1139c2ebb244e396529871ea7385e9f43899.pdf" target="_blank">https://openreview.net/pdf/7f1d1139c2ebb244e396529871ea7385e9f43899.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Effective motion planning in high dimensional spaces is a long-standing open problem in robotics. One class of traditional motion planning algorithms corresponds to potential-based motion planning. An advantage of potential based motion planning is composability -- different motion constraints can easily combined by adding corresponding potentials. However, constructing motion paths from potentials requires solving a global optimization across configuration space potential landscape, which is often prone to local minima. We propose a new approach towards learning potential based motion planning, where we train a neural network to capture and learn an easily optimizable potentials over motion planning trajectories. We illustrate the effectiveness of such approach, significantly outperforming both classical and recent learned motion planning approaches and avoiding issues with local minima. We further illustrate its inherent composability, enabling us to generalize to a multitude of different motion constraints. Project website at https://energy-based-model.github.io/potential-motion-plan.</div>
</div>
<div class='paper-counter'>45/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Think Before You Act: Decision Transformers with Working Memory</div>
<div class="field-name">authors:</div>
<div class="field-value">Jikun Kang;Romain Laroche;Xingdi Yuan;Adam Trischler;Xue Liu;Jie Fu</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/94d90465c730dddba70ec12400b0afd4b6f1a4fe.pdf" target="_blank">https://openreview.net/pdf/94d90465c730dddba70ec12400b0afd4b6f1a4fe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.</div>
</div>
<div class='paper-counter'>46/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion Model-Augmented Behavioral Cloning</div>
<div class="field-name">authors:</div>
<div class="field-value">Shang-Fu Chen;Hsiang-Chun Wang;Ming-Hao Hsu;Chun-Mao Lai;Shao-Hua Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e8d7a79267c4d78f4da2db3ec44e0c6c777723f7.pdf" target="_blank">https://openreview.net/pdf/e8d7a79267c4d78f4da2db3ec44e0c6c777723f7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning addresses the challenge of learning by observing an expert’s demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a). Despite the simplicity of modeling the conditional probability with BC, it usually struggles with generalization. While modeling the joint probability can improve generalization performance, the inference procedure is often time-consuming, and the model can suffer from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed Diffusion Model-Augmented Behavioral Cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to optimize both the BC loss (conditional) and our proposed diffusion model loss (joint). DBC outperforms baselines in various continuous control tasks in navigation, robot arm manipulation, dexterous manipulation, and locomotion. We design additional experiments to verify the limitations of modeling either the conditional probability or the joint probability of the expert distribution, as well as compare different generative models. Ablation studies justify the effectiveness of our design choices.</div>
</div>
<div class='paper-counter'>47/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">One-Shot Strategic Classification Under Unknown Costs</div>
<div class="field-name">authors:</div>
<div class="field-value">Elan Rosenfeld;Nir Rosenfeld</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4742ae4a70213b3a7ba557cb4c986e2a8ad3ea05.pdf" target="_blank">https://openreview.net/pdf/4742ae4a70213b3a7ba557cb4c986e2a8ad3ea05.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains – particularly in public policy, a common motivating use case – where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of *one-shot* strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, aiming to minimize worst-case risk over an uncertainty set of costs. We design efficient algorithms for both the full-batch and stochastic settings, which we prove converge (offline) to the minimax solution at the rate of $\tilde{\mathcal{O}}(T^{-\frac{1}{2}})$. Our analysis reveals important structure stemming from strategic responses, particularly the value of *dual norm regularization* with respect to the cost function.</div>
</div>
<div class='paper-counter'>48/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Foundation Policies with Hilbert Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Seohong Park;Tobias Kreiman;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8c6483b8317a104bd2fc9d9edce1b18c10498799.pdf" target="_blank">https://openreview.net/pdf/8c6483b8317a104bd2fc9d9edce1b18c10498799.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy “prompting” schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/</div>
</div>
<div class='paper-counter'>49/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">How to Explore with Belief: State Entropy Maximization in POMDPs</div>
<div class="field-name">authors:</div>
<div class="field-value">Riccardo Zamboni;Duilio Cirino;Marcello Restelli;Mirco Mutti</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fffa46a5651fc7d1c6252b8eed82eb450a780573.pdf" target="_blank">https://openreview.net/pdf/fffa46a5651fc7d1c6252b8eed82eb450a780573.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent works have studied *state entropy maximization* in reinforcement learning, in which the agent's objective is to learn a policy inducing high entropy over states visitation (Hazan et al., 2019). They typically assume full observability of the state of the system, so that the entropy of the observations is maximized. In practice, the agent may only get *partial* observations, e.g., a robot perceiving the state of a physical space through proximity sensors and cameras. A significant mismatch between the entropy over observations and true states of the system can arise in those settings. In this paper, we address the problem of entropy maximization over the *true states* with a decision policy conditioned on partial observations *only*. The latter is a generalization of POMDPs, which is intractable in general. We develop a memory and computationally efficient *policy gradient* method to address a first-order relaxation of the objective defined on *belief* states, providing various formal characterizations of approximation gaps, the optimization landscape, and the *hallucination* problem. This paper aims to generalize state entropy maximization to more realistic domains that meet the challenges of applications.</div>
</div>
<div class='paper-counter'>50/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fair Resource Allocation in Multi-Task Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Hao Ban;Kaiyi Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/68b0cb30bb426b70ec2f9df0b73d39c5f8e812a6.pdf" target="_blank">https://openreview.net/pdf/68b0cb30bb426b70ec2f9df0b73d39c5f8e812a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To address the problem, we propose FairGrad, a novel optimization objective. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of multi-task benchmarks in supervised learning and reinforcement learning. Furthermore, we incorporate the idea of $\alpha$-fairness into the loss functions of various MTL methods. Extensive empirical studies demonstrate that their performance can be significantly enhanced. Code is available at https://github.com/OptMN-Lab/fairgrad.</div>
</div>
<div class='paper-counter'>51/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: Scaling Simulation is Neither Necessary Nor Sufficient for In-the-Wild Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Homanga Bharadhwaj</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ac400395750767622295357a5a9dfe8fa47370bd.pdf" target="_blank">https://openreview.net/pdf/ac400395750767622295357a5a9dfe8fa47370bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we develop a structured critique of robotic simulations for real-world manipulation, by arguing that scaling simulators is neither necessary nor sufficient for making progress in general-purpose real-world robotic manipulation agents that are compliant with human preferences. With the ubiquity of robotic simulators, and recent efforts to scale them for diverse tasks, and at the same time the interest in generally capable real-world manipulation systems, we believe it is important to address the limitations of using simulation for real-world manipulation, so that as a community, we can focus our collective resources, energy, and time on approaches that have more principled odds of success. We further demonstrate the unique challenges that real-world manipulation presents, and show through examples and arguments why scaling simulation doesn't get us closer to solving these challenges required for diverse real-world deployment.</div>
</div>
<div class='paper-counter'>52/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Byzantine-Robust Federated Learning: Impact of Client Subsampling and Local Updates</div>
<div class="field-name">authors:</div>
<div class="field-value">Youssef Allouah;Sadegh Farhadkhani;Rachid Guerraoui;Nirupam Gupta;Rafael Pinot;Geovani Rizk;Sasha Voitovych</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/884f8f6093dfc579bf2e9ba340b43b3d566d74bd.pdf" target="_blank">https://openreview.net/pdf/884f8f6093dfc579bf2e9ba340b43b3d566d74bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The possibility of adversarial (a.k.a., Byzantine) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a robust averaging rule. While a significant amount of work has been devoted to studying the convergence of federated robust averaging (which we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of client subsampling and local steps, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy diminishes with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks.</div>
</div>
<div class='paper-counter'>53/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics</div>
<div class="field-name">authors:</div>
<div class="field-value">Luca Grillotti;Maxence Faldor;Borja G. León;Antoine Cully</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2642c7d321494d8c48c30a9b0767c4ddabc25709.pdf" target="_blank">https://openreview.net/pdf/2642c7d321494d8c48c30a9b0767c4ddabc25709.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks. We also demonstrate that we can harness the learned skills to adapt better than other baselines to five perturbed environments. Finally, qualitative analyses showcase a range of remarkable behaviors: [adaptive-intelligent-robotics.github.io/QDAC](https://adaptive-intelligent-robotics.github.io/QDAC/).</div>
</div>
<div class='paper-counter'>54/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction</div>
<div class="field-name">authors:</div>
<div class="field-value">Diwen Wan;Ruijie Lu;Gang Zeng</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/47bab0f40727681ca5adec9bafa96d20fb427e94.pdf" target="_blank">https://openreview.net/pdf/47bab0f40727681ca5adec9bafa96d20fb427e94.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Rendering novel view images in dynamic scenes is a crucial yet challenging task. Current methods mainly utilize NeRF-based methods to represent the static scene and an additional time-variant MLP to model scene deformations, resulting in relatively low rendering quality as well as slow inference speed. To tackle these challenges, we propose a novel framework named Superpoint Gaussian Splatting (SP-GS). Specifically, our framework first employs explicit 3D Gaussians to reconstruct the scene and then clusters Gaussians with similar properties (e.g., rotation, translation, and location) into superpoints. Empowered by these superpoints, our method manages to extend 3D Gaussian splatting to dynamic scenes with only a slight increase in computational expense. Apart from achieving state-of-the-art visual quality and real-time rendering under high resolutions, the superpoint representation provides a stronger manipulation capability. Extensive experiments demonstrate the practicality and effectiveness of our approach on both synthetic and real-world datasets. Please see our project page at https://dnvtmf.github.io/SP_GS.github.io.</div>
</div>
<div class='paper-counter'>55/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Mechanistic Design and Scaling of Hybrid Architectures</div>
<div class="field-name">authors:</div>
<div class="field-value">Michael Poli;Armin W Thomas;Eric Nguyen;Pragaash Ponnusamy;Björn Deiseroth;Kristian Kersting;Taiji Suzuki;Brian Hie;Stefano Ermon;Christopher Re;Ce Zhang;Stefano Massaroli</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1f23a20e6e98232c95c75607749544c9df15185c.pdf" target="_blank">https://openreview.net/pdf/1f23a20e6e98232c95c75607749544c9df15185c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.</div>
</div>
<div class='paper-counter'>56/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3D-VLA: A 3D Vision-Language-Action Generative World Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoyu Zhen;Xiaowen Qiu;Peihao Chen;Jincheng Yang;Xin Yan;Yilun Du;Yining Hong;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8c311215d4b6672af18218dcbe87e1b212561849.pdf" target="_blank">https://openreview.net/pdf/8c311215d4b6672af18218dcbe87e1b212561849.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan action accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM) and a set of action tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train the embodied diffusion models and align them into the LLM for predicting the goal image and point cloud. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodality generation and planning capabilities in embodied environments, showcasing its potential in real-world applications.</div>
</div>
<div class='paper-counter'>57/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: Video as the New Language for Real-World Decision Making</div>
<div class="field-name">authors:</div>
<div class="field-value">Sherry Yang;Jacob C Walker;Jack Parker-Holder;Yilun Du;Jake Bruce;Andre Barreto;Pieter Abbeel;Dale Schuurmans</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b52d5e7a41f05f7e4de8fca452aa865a1ebcf5c5.pdf" target="_blank">https://openreview.net/pdf/b52d5e7a41f05f7e4de8fca452aa865a1ebcf5c5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.</div>
</div>
<div class='paper-counter'>58/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiong-Hui Chen;Junyin Ye;Hang Zhao;Yi-Chen Li;Xu-Hui Liu;Haoran Shi;Yu-Yan Xu;Zhihao Ye;Si-Hang Yang;Yang Yu;Anqi Huang;Kai Xu;Zongzhang Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c5587a7db500bde7c4dd8b1e0377593bf06d7cbb.pdf" target="_blank">https://openreview.net/pdf/c5587a7db500bde7c4dd8b1e0377593bf06d7cbb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">One-shot imitation learning (OSIL) is to learn an imitator agent that can execute multiple tasks with only a single demonstration. In real-world scenario, the environment is dynamic, e.g., unexpected changes can occur after demonstration. Thus, achieving generalization of the imitator agent is crucial as agents would inevitably face situations unseen in the provided demonstrations. While traditional OSIL methods excel in relatively stationary settings, their adaptability to such unforeseen changes, which asking for a higher level of generalization ability for the imitator agents, is limited and rarely discussed. In this work, we present a new algorithm called Deep Demonstration Tracing (DDT). In DDT, we propose a demonstration transformer architecture to encourage agents to adaptively trace suitable states in demonstrations. Besides, it integrates OSIL into a meta-reinforcement-learning training paradigm, providing regularization for policies in unexpected situations. We evaluate DDT on a new navigation task suite and robotics tasks, demonstrating its superior performance over existing OSIL methods across all evaluated tasks in dynamic environments with unforeseen changes. The project page is in https://osil-ddt.github.io.</div>
</div>
<div class='paper-counter'>59/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences</div>
<div class="field-name">authors:</div>
<div class="field-value">Jie Cheng;Gang Xiong;Xingyuan Dai;Qinghai Miao;Yisheng Lv;Fei-Yue Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/178c98a6aecc8ff37de53654c10eccd017b79914.pdf" target="_blank">https://openreview.net/pdf/178c98a6aecc8ff37de53654c10eccd017b79914.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Preference-based Reinforcement Learning (PbRL) circumvents the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL methods excessively depend on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method utilizes a sample selection-based discriminator to dynamically filter out noise and ensure robust training. To counteract the cumulative error stemming from incorrect selection, we suggest a warm start for the reward model, which additionally bridges the performance gap during the transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the state-of-the-art PbRL method. Code is available at https://github.com/CJReinforce/RIME_ICML2024.</div>
</div>
<div class='paper-counter'>60/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Proactive Detection of Voice Cloning with Localized Watermarking</div>
<div class="field-name">authors:</div>
<div class="field-value">Robin San Roman;Pierre Fernandez;Hady Elsahar;Alexandre Défossez;Teddy Furon;Tuan Tran</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8753fe22b324eb5c20d653a8c2ae6a4243f002b3.pdf" target="_blank">https://openreview.net/pdf/8753fe22b324eb5c20d653a8c2ae6a4243f002b3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator / detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed, achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.Code is available at https://github.com/facebookresearch/audioseal</div>
</div>
<div class='paper-counter'>61/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Li Ding;Jenny Zhang;Jeff Clune;Lee Spector;Joel Lehman</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a6b4da0b7f13de189bf289ba4ec76ffd371aee0b.pdf" target="_blank">https://openreview.net/pdf/a6b4da0b7f13de189bf289ba4ec76ffd371aee0b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning from Human Feedback (RLHF) has shown potential in qualitative tasks where easily defined performance measures are lacking. However, there are drawbacks when RLHF is commonly used to optimize for average human preferences, especially in generative tasks that demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying diverse and high-quality solutions but often rely on manually crafted diversity metrics. This paper introduces Quality Diversity through Human Feedback (QDHF), a novel approach that progressively infers diversity metrics from human judgments of similarity among solutions, thereby enhancing the applicability and effectiveness of QD algorithms in complex and open-ended domains. Empirical studies show that QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery and matches the efficacy of QD with manually crafted diversity metrics on standard benchmarks in robotics and reinforcement learning. Notably, in open-ended generative tasks, QDHF substantially enhances the diversity of text-to-image generation from a diffusion model and is more favorably received in user studies. We conclude by analyzing QDHF's scalability, robustness, and quality of derived diversity metrics, emphasizing its strength in open-ended optimization tasks. Code and tutorials are available at https://liding.info/qdhf.</div>
</div>
<div class='paper-counter'>62/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Chain-of-Thought Predictive Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhiwei Jia;Vineet Thumuluri;Fangchen Liu;Linghao Chen;Zhiao Huang;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/36291353613f34f7dfaae8532cf48373ce9cd197.pdf" target="_blank">https://openreview.net/pdf/36291353613f34f7dfaae8532cf48373ce9cd197.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We study generalizable policy learning from demonstrations for complex low-level control (e.g., contact-rich object manipulations). We propose a novel hierarchical imitation learning method that utilizes sub-optimal demos. Firstly, we propose an observation space-agnostic approach that efficiently discovers the multi-step subskill decomposition of the demos in an unsupervised manner. By grouping temporarily close and functionally similar actions into subskill-level demo segments, the observations at the segment boundaries constitute a chain of planning steps for the task, which we refer to as the chain-of-thought (CoT). Next, we propose a Transformer-based design that effectively learns to predict the CoT as the subskill-level guidance. We couple action and subskill predictions via learnable prompt tokens and a hybrid masking strategy, which enable dynamically updated guidance at test time and improve feature representation of the trajectory for generalizable policy learning. Our method, Chain-of-Thought Predictive Control (CoTPC), consistently surpasses existing strong baselines on various challenging low-level manipulation tasks with sub-optimal demos. See project page at https://sites.google.com/view/cotpc.</div>
</div>
<div class='paper-counter'>63/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts</div>
<div class="field-name">authors:</div>
<div class="field-value">Onur Celik;Aleksandar Taranovic;Gerhard Neumann</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9eab51ffd9ded49f98c8072baadbba0c43763cd4.pdf" target="_blank">https://openreview.net/pdf/9eab51ffd9ded49f98c8072baadbba0c43763cd4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose Diverse Skill Learning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills.</div>
</div>
<div class='paper-counter'>64/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianying Ji;Yu Luo;Fuchun Sun;Xianyuan Zhan;Jianwei Zhang;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8ae24c4cb660d3ec4cf3df2602d7dee1f356480a.pdf" target="_blank">https://openreview.net/pdf/8ae24c4cb660d3ec4cf3df2602d7dee1f356480a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works primarily focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that $Q$-values are often underestimated in the latter stage of the RL training process, potentially hindering policy learning and reducing sample efficiency. We find that such a long-neglected phenomenon is often related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. Based on BEE, the resulting practical algorithm BAC outperforms state-of-the-art methods in **over 50** continuous control tasks and achieves strong performance in failure-prone scenarios and **real-world robot** tasks. Benchmark results and videos are available at https://jity16.github.io/BEE/.</div>
</div>
<div class='paper-counter'>65/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Position: Standardization of Behavioral Use Clauses is Necessary for the Adoption of Responsible Licensing of AI</div>
<div class="field-name">authors:</div>
<div class="field-value">Daniel McDuff;Tim Korjakow;Scott Cambo;Jesse Josua Benjamin;Jenny Lee;Yacine Jernite;Carlos Muñoz Ferrandis;Aaron Gokaslan;Alek Tarkowski;Joseph Lindley;A. Feder Cooper;Danish Contractor</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c3b75e52e8fdb793e0098d9724a41537f0c16b76.pdf" target="_blank">https://openreview.net/pdf/c3b75e52e8fdb793e0098d9724a41537f0c16b76.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Growing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standardization to avoid confusing users or diluting their impact. At the same time, customization of behavioral restrictions is also appropriate in some contexts (e.g., medical domains). We advocate for “standardized customization” that can meet users’ needs and can be supported via tooling.</div>
</div>
<div class='paper-counter'>66/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Causal Action Influence Aware Counterfactual Data Augmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">Núria Armengol Urpí;Marco Bagatella;Marin Vlastelica;Georg Martius</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/957b6ba3a663e0817dcfd30eff78129efb7cc953.pdf" target="_blank">https://openreview.net/pdf/957b6ba3a663e0817dcfd30eff78129efb7cc953.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\textit{action}$-unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift.</div>
</div>
<div class='paper-counter'>67/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Siddharth Karamcheti;Suraj Nair;Ashwin Balakrishna;Percy Liang;Thomas Kollar;Dorsa Sadigh</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2c4372ab4b1dfa0d623b755486b8ba021bcc8bac.pdf" target="_blank">https://openreview.net/pdf/2c4372ab4b1dfa0d623b755486b8ba021bcc8bac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance – a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs.</div>
</div>
<div class='paper-counter'>68/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinlu Zhang;Yiyi Zhou;Qiancheng Zheng;Xiaoxiong Du;Gen Luo;Jun Peng;Xiaoshuai Sun;Rongrong Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2c6504d23d75b1a1efccf62efa66138b56997a4c.pdf" target="_blank">https://openreview.net/pdf/2c6504d23d75b1a1efccf62efa66138b56997a4c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality. In this paper, we propose an ***E**nd-to-End **E**fficient and **E**ffective* network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet. Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space. We introduce a novel *Style Code Enhancer* to enhance cross-modal semantic alignment, alongside an innovative *Geometric Regularization* objective to maintain consistency across multi-view generations. Extensive experiments on three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes. For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality. Our code is released at <https://github.com/Aria-Zhangjl/E3-FaceNet>.</div>
</div>
<div class='paper-counter'>69/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">To the Max: Reinventing Reward in Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Grigorii Veviurko;Wendelin Boehmer;Mathijs de Weerdt</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/682d624bc8b2c7efedbad50310e0aa1d11cbfaa2.pdf" target="_blank">https://openreview.net/pdf/682d624bc8b2c7efedbad50310e0aa1d11cbfaa2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In reinforcement learning (RL), different reward functions can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach for using rewards for learning. We introduce *max-reward RL*, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is available at https://github.com/veviurko/To-the-Max.</div>
</div>
<div class='paper-counter'>70/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhaoliang Wan;Yonggen Ling;Senlin Yi;Lu Qi;Wang Wei Lee;Minglei Lu;Sicheng Yang;Xiao Teng;Peng Lu;Xu Yang;Ming-Hsuan Yang;Hui Cheng</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/11b5699ccb3e8bcd0b7559de88515fe570cf8967.pdf" target="_blank">https://openreview.net/pdf/11b5699ccb3e8bcd0b7559de88515fe570cf8967.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper addresses the scarcity of large-scale datasets for accurate object-in-hand pose estimation, which is crucial for robotic in-hand manipulation within the "Perception-Planning-Control" paradigm. Specifically, we introduce VinT-6D, the first extensive multi-modal dataset integrating vision, touch, and proprioception, to enhance robotic manipulation. VinT-6D comprises 2 million VinT-Sim and 0.1 million VinT-Real entries, collected via simulations in Mujoco and Blender and a custom-designed real-world platform. This dataset is tailored for robotic hands, offering models with whole-hand tactile perception and high-quality, well-aligned data. To the best of our knowledge, the VinT-Real is the largest considering the collection difficulties in the real-world environment so it can bridge the gap of simulation to real compared to the previous works. Built upon VinT-6D, we present a benchmark method that shows significant improvements in performance by fusing multi-modal information. The project is available at https://VinT-6D.github.io/.</div>
</div>
<div class='paper-counter'>71/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Lujie Yang;Hongkai Dai;Zhouxing Shi;Cho-Jui Hsieh;Russ Tedrake;Huan Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/871eedaeed0a21c898b9eb63cdafe42277392f0c.pdf" target="_blank">https://openreview.net/pdf/871eedaeed0a21c898b9eb63cdafe42277392f0c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning-based neural-network (NN) control policies have shown impressive empirical performance in a wide range of tasks in robotics and control. However, formal (Lyapunov) stability guarantees over the region-of-attraction (ROA) for NN controllers with nonlinear dynamical systems are challenging to obtain, and most existing approaches rely on expensive solvers for sums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability modulo theories (SMT). In this paper, we demonstrate a new framework for learning NN controllers together with Lyapunov certificates using fast empirical falsification and strategic regularizations. We propose a novel formulation that defines a larger verifiable region-of-attraction (ROA) than shown in the literature, and refines the conventional restrictive constraints on Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov condition is rigorously verified post-hoc using branch-and-bound with scalable linear bound propagation-based NN verification techniques. The approach is efficient and flexible, and the full training and verification procedure is accelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT. The flexibility and efficiency of our framework allow us to demonstrate Lyapunov-stable output feedback control with synthesized NN-based controllers and NN-based observers with formal stability guarantees, for the first time in literature.</div>
</div>
<div class='paper-counter'>72/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianxiong Li;Jinliang Zheng;Yinan Zheng;Liyuan Mao;Xiao Hu;Sijie Cheng;Haoyi Niu;Jihao Liu;Yu Liu;Jingjing Liu;Ya-Qin Zhang;Xianyuan Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/74e25fbbf5a730ba3ad2107241d4b9a5c249565b.pdf" target="_blank">https://openreview.net/pdf/74e25fbbf5a730ba3ad2107241d4b9a5c249565b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multimodal pretraining is an effective strategy for the trinity of goals of representation learning in autonomous robots: $1)$ extracting both local and global task progressions; $2)$ enforcing temporal consistency of visual representation; $3)$ capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning. Project Page: https://2toinf.github.io/DecisionNCE/</div>
</div>
<div class='paper-counter'>73/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Runze Liu;Yali Du;Fengshuo Bai;Jiafei Lyu;Xiu Li</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/60b2cedc588ffc8bd1ffb26f4189fffa4f2e343b.pdf" target="_blank">https://openreview.net/pdf/60b2cedc588ffc8bd1ffb26f4189fffa4f2e343b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In preference-based Reinforcement Learning (RL), obtaining a large number of preference labels are both time-consuming and costly. Furthermore, the queried human preferences cannot be utilized for the new tasks. In this paper, we propose Zero-shot Cross-task Preference Alignment and Robust Reward Learning (PEARL), which learns policies from cross-task preference transfer without any human labels of the target task. Our contributions include two novel components that facilitate the transfer and learning process. The first is Cross-task Preference Alignment (CPA), which transfers the preferences between tasks via optimal transport. The key idea of CPA is to use Gromov-Wasserstein distance to align the trajectories between tasks, and the solved optimal transport matrix serves as the correspondence between trajectories. The target task preferences are computed as the weighted sum of source task preference labels with the correspondence as weights. Moreover, to ensure robust learning from these transferred labels, we introduce Robust Reward Learning (RRL), which considers both reward mean and uncertainty by modeling rewards as Gaussian distributions. Empirical results on robotic manipulation tasks from Meta-World and Robomimic demonstrate that our method is capable of transferring preference labels across tasks accurately and then learns well-behaved policies. Notably, our approach significantly exceeds existing methods when there are few human preferences. The code and videos of our method are available at: https://sites.google.com/view/pearl-preference.</div>
</div>
<div class='paper-counter'>74/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">Soroush Nasiriany;Fei Xia;Wenhao Yu;Ted Xiao;Jacky Liang;Ishita Dasgupta;Annie Xie;Danny Driess;Ayzaan Wahid;Zhuo Xu;Quan Vuong;Tingnan Zhang;Tsang-Wei Edward Lee;Kuang-Huei Lee;Peng Xu;Sean Kirmani;Yuke Zhu;Andy Zeng;Karol Hausman;Nicolas Heess;Chelsea Finn;Sergey Levine;brian ichter</div>
<div class="field-name">keywords:</div>
<div class="field-value"></div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2024 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/82706003983f0b6a2b65d49549013751951097c1.pdf" target="_blank">https://openreview.net/pdf/82706003983f0b6a2b65d49549013751951097c1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains.</div>
</div>
<div class='paper-counter'>75/828</div>
<div class="conference">ICLR_cc_2024_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game</div>
<div class="field-name">authors:</div>
<div class="field-value">Simin Li;Jun Guo;Jingqiao Xiu;Ruixiao Xu;Xin Yu;Jiakai Wang;Aishan Liu;Yaodong Yang;Xianglong Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-agent reinforcement learning;Robustness;Game Theory;Adversarial Attack</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf" target="_blank">https://openreview.net/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c51bf7cfc88e08b5f921f5e3c705191cf1a381f5.zip" target="_blank">https://openreview.net/attachment/c51bf7cfc88e08b5f921f5e3c705191cf1a381f5.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study robust cooperative MARL against Byzantine adversary using a Bayesian game approach</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.</div>
</div>
<div class='paper-counter'>76/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Image Translation as Diffusion Visual Programmers</div>
<div class="field-name">authors:</div>
<div class="field-value">Cheng Han;James Chenhao Liang;Qifan Wang;MAJID RABBANI;Sohail Dianat;Raghuveer Rao;Ying Nian Wu;Dongfang Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image translation;Diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc4fa789446c6f2c8a588bc9e92aa4e49783f261.pdf" target="_blank">https://openreview.net/pdf/dc4fa789446c6f2c8a588bc9e92aa4e49783f261.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs ($i.e.$, computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP’s remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the frame work enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols ($e.g.$, [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.</div>
</div>
<div class='paper-counter'>77/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Conversational Drug Editing Using Retrieval and Domain Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Shengchao Liu;Jiongxiao Wang;Yijin Yang;Chengpeng Wang;Ling Liu;Hongyu Guo;Chaowei Xiao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;prompt;retrieval;domain feedback;conversation;drug editing;drug optimization;controllable generation;small molecule;peptide;protein</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to physical sciences (physics, chemistry, biology, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/857cda25f605f8054f538d39823dd8328fc64da4.pdf" target="_blank">https://openreview.net/pdf/857cda25f605f8054f538d39823dd8328fc64da4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose ChatDrug, a framework that utilizing Large Language Models for conversation drug editing with a retrieval and domain feedback module.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.</div>
</div>
<div class='paper-counter'>78/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Dual RL: Unification and New Methods for Reinforcement and Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Harshit Sikchi;Qinqing Zheng;Amy Zhang;Scott Niekum</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Offline Imitation Learning;Offline Reinforcement Learning;Deep Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/981af9a85be76008d6063c6ddd1477450c3bf463.pdf" target="_blank">https://openreview.net/pdf/981af9a85be76008d6063c6ddd1477450c3bf463.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a980e449192699b610c6e3ee69309be5fbbd5161.zip" target="_blank">https://openreview.net/attachment/a980e449192699b610c6e3ee69309be5fbbd5161.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A unification of RL and IL methods through the lens of duality that allows us to propose new methods for discriminator-free imitation learning and stable offline reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as *dual RL*, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL method XQL in the dual framework, and we further propose a new method $f$-DVL that provides alternative choices to the Gumbel regression loss that fixes the known training instability issue of XQL. The performance improvements by both of our proposed methods, ReCOIL and $f$-DVL, in IL and RL are validated on an extensive suite of simulated robot locomotion and manipulation tasks.</div>
</div>
<div class='paper-counter'>79/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency</div>
<div class="field-name">authors:</div>
<div class="field-value">Stone Tao;Arth Shukla;Tse-kai Chan;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;learning from demonstrations;curriculum learning;reverse curriculum learning;robot learning;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a4e50c83e30246b73792b78ff16bbc0d7da5225a.pdf" target="_blank">https://openreview.net/pdf/a4e50c83e30246b73792b78ff16bbc0d7da5225a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Combining a novel reverse curriculum approach with a forward curriculum to accelerate RL and massively reduce the demonstrations needed to solve complex tasks from sparse rewards.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics. Our approach consists of a reverse curriculum followed by a forward curriculum. Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets. The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems. A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency. We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control. Website with code and visualizations are here: https://reverseforward-cl.github.io/</div>
</div>
<div class='paper-counter'>80/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING</div>
<div class="field-name">authors:</div>
<div class="field-value">Dingyuan Shi;Yongxin Tong;Zimu Zhou;Ke Xu;Zheng Wang;Jieping Ye</div>
<div class="field-name">keywords:</div>
<div class="field-value">path planning;diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9e4c9fa690770bd2184573bbec75c797f7c9648f.pdf" target="_blank">https://openreview.net/pdf/9e4c9fa690770bd2184573bbec75c797f7c9648f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We integrate graph structure into diffusion process hence bypassing search framework for data-driven path planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Path planning underpins various applications such as transportation, logistics, and robotics.Conventionally, path planning is formulated with explicit optimization objectives such as distance or time.However, real-world data reveals that user intentions are hard-to-model, suggesting a need for data-driven path planning that implicitly incorporates the complex user intentions.In this paper, we propose GDP, a diffusion-based model for end-to-end data-driven path planning.It effectively learns path patterns via a novel diffusion process that incorporates constraints from road networks, and plans paths as conditional path generation given the origin and destination as prior evidence.GDP is the first solution that bypasses the traditional search-based frameworks, a long-standing performance bottleneck in path planning.We validate the efficacy of GDP on two real-world datasets.Our GDP beats strong baselines by 14.2% ~ 43.5% and achieves state-of-the-art performances.</div>
</div>
<div class='paper-counter'>81/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Entity-Centric Reinforcement Learning for Object Manipulation from Pixels</div>
<div class="field-name">authors:</div>
<div class="field-value">Dan Haramati;Tal Daniel;Aviv Tamar</div>
<div class="field-name">keywords:</div>
<div class="field-value">deep reinforcement learning;visual reinforcement learning;object-centric;robotic object manipulation;compositional generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2627c85192603f1981d419b5964256fa755a98c8.pdf" target="_blank">https://openreview.net/pdf/2627c85192603f1981d419b5964256fa755a98c8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bb5b60c4035a3b17038427116d310f44d1759371.zip" target="_blank">https://openreview.net/attachment/bb5b60c4035a3b17038427116d310f44d1759371.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a structured visual goal-conditioned RL approach for multi-object manipulation, showcasing agents that generalize from 3 to over 10 objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl</div>
</div>
<div class='paper-counter'>82/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Large Language Models as Generalizable Policies for Embodied Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Andrew Szot;Max Schwarzer;Harsh Agrawal;Bogdan Mazoure;Rin Metcalf;Walter Talbott;Natalie Mackraz;R Devon Hjelm;Alexander T Toshev</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Reinforcement Learning;Large Language Models;Foundational Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1ff0314a1b06bd6ce9cddb6dd452e4cf2e8f4ee7.pdf" target="_blank">https://openreview.net/pdf/1ff0314a1b06bd6ce9cddb6dd452e4cf2e8f4ee7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Reinforcement Learned Policies for multi-task embodied AI problems, when initialized from LLMs, demonstrate strong generalization properties across novel task and novel ways of describing tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement.</div>
</div>
<div class='paper-counter'>83/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Kun LEI;Zhengmao He;Chenhao Lu;Kaizhe Hu;Yang Gao;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline-to-Online Fine-tuning;On-policy Learning;Robot Learning;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7af8ce8e01ce2bab2390957f79b4e02c3f6cdf30.pdf" target="_blank">https://openreview.net/pdf/7af8ce8e01ce2bab2390957f79b4e02c3f6cdf30.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We seamlessly integrate offline and online learning through an on-policy RL algorithm, attaining SOTA performance in simulated and real-world environments across both phases, all without the need for additional regularization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: *Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?* In this study, we propose Uni-O4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-O4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE) approach, Uni-O4 can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments. Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. [Our website](uni-o4.github.io)</div>
</div>
<div class='paper-counter'>84/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Text2Reward: Reward Shaping with Language Models for Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianbao Xie;Siheng Zhao;Chen Henry Wu;Yitao Liu;Qian Luo;Victor Zhong;Yanchao Yang;Tao Yu</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning; large language models; robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a52e7202163a42116fae8ada42123e37f2aef287.pdf" target="_blank">https://openreview.net/pdf/a52e7202163a42116fae8ada42123e37f2aef287.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Reward shaping for reinforcement learning based on large language models</div>
<div class="field-name">abstract:</div>
<div class="field-value">Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/</div>
</div>
<div class='paper-counter'>85/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fiber Monte Carlo</div>
<div class="field-name">authors:</div>
<div class="field-value">Nick Richardson;Deniz Oktay;Yaniv Ovadia;James C Bowden;Ryan P Adams</div>
<div class="field-name">keywords:</div>
<div class="field-value">Stochastic Methods</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7d5a83d7a94183d1d9225cc666d97a8d77188d5b.pdf" target="_blank">https://openreview.net/pdf/7d5a83d7a94183d1d9225cc666d97a8d77188d5b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/415dba228924d664a7b810fa4eea8b83f59867d6.pdf" target="_blank">https://openreview.net/attachment/415dba228924d664a7b810fa4eea8b83f59867d6.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Integrals with discontinuous integrands are ubiquitous, arising from discrete structure in applications like topology optimization, graphics, and computational geometry.    These integrals are often part of a forward model in an inverse problem where it is necessary to reason backwards about the parameters, ideally using gradient-based optimization.     Monte Carlo methods are widely used to estimate the value of integrals, but this results in a non-differentiable approximation that is amenable to neither conventional automatic differentiation nor reparameterization-based gradient methods.     This significantly disrupts efforts to integrate machine learning methods in areas that exhibit these discontinuities: physical simulation and robotics, design, graphics, and computational geometry.      Although bespoke domain-specific techniques can handle special cases, a general methodology to wield automatic differentiation in these discrete contexts is wanting.     We introduce a differentiable variant of the simple Monte Carlo estimator which samples line segments rather than points from the domain.     We justify our estimator analytically as conditional Monte Carlo and demonstrate the diverse functionality of the method as applied to image stylization, topology optimization, and computational geometry.</div>
</div>
<div class='paper-counter'>86/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Interactive Real-World Simulators</div>
<div class="field-name">authors:</div>
<div class="field-value">Sherry Yang;Yilun Du;Seyed Kamyar Seyed Ghasemipour;Jonathan Tompson;Leslie Pack Kaelbling;Dale Schuurmans;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Generative simulator;simulating real-world interactions;planning;reinforcement learning;vision language models;video generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ebbd0d77e65c2e2ffb1eef300c8c55e4f2f27c86.pdf" target="_blank">https://openreview.net/pdf/ebbd0d77e65c2e2ffb1eef300c8c55e4f2f27c86.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We learn an interactive real-world simulator from broad data rich in different axes that enables long-horizon interactions with humans, vision language models, and reinforcement learning agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by x,y” from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.</div>
</div>
<div class='paper-counter'>87/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Grounding Language Plans in Demonstrations Through Counterfactual Perturbations</div>
<div class="field-name">authors:</div>
<div class="field-value">Yanwei Wang;Tsun-Hsuan Wang;Jiayuan Mao;Michael Hagenow;Julie Shah</div>
<div class="field-name">keywords:</div>
<div class="field-value">Grounding LLM;Learning Mode Abstractions for Manipulation;Learning from Demonstration;Robotics;Task and Motion Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/095205c11ca0cd7a485da10923a605bbfd899160.pdf" target="_blank">https://openreview.net/pdf/095205c11ca0cd7a485da10923a605bbfd899160.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/</div>
</div>
<div class='paper-counter'>88/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning with Language-Guided State Abstractions</div>
<div class="field-name">authors:</div>
<div class="field-value">Andi Peng;Ilia Sucholutsky;Belinda Z. Li;Theodore Sumers;Thomas L. Griffiths;Jacob Andreas;Julie Shah</div>
<div class="field-name">keywords:</div>
<div class="field-value">human-ai interaction;state abstractions</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/82b7030ec2e4c0eb0c075da87a0fc3ad7741fdeb.pdf" target="_blank">https://openreview.net/pdf/82b7030ec2e4c0eb0c075da87a0fc3ad7741fdeb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use language models to help construct human-aligned state abstractions for robot learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We describe a framework for using natural language to design state abstractions for imitation learning.Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones.These state representations are typically manually specified, or derived from other labor-intensive labeling procedures.Our method, LGA (\textit{language-guided abstraction}), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications.We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.</div>
</div>
<div class='paper-counter'>89/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robustifying and Boosting Training-Free Neural Architecture Search</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhenfeng He;Yao Shu;Zhongxiang Dai;Bryan Kian Hsiang Low</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neural Architecture Search;Training-free NAS;Bayesian Optimization;Precision@K;Partial Monitoring</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general machine learning (i.e., none of the above)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f97ed936f6ee4c7934b224a81e86983a64344534.pdf" target="_blank">https://openreview.net/pdf/f97ed936f6ee4c7934b224a81e86983a64344534.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a1f2471e62d08b40b4ac63dcc78572dc7b83de88.zip" target="_blank">https://openreview.net/attachment/a1f2471e62d08b40b4ac63dcc78572dc7b83de88.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper proposes a new NAS algorithm RoBoT to robustify existing training-free NAS and further boost them.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results.</div>
</div>
<div class='paper-counter'>90/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Seer: Language Instructed Video Prediction with Latent Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Xianfan Gu;Chuan Wen;Weirui Ye;Jiaming Song;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">video prediction;multi-modality generation;diffusion model;language-vision model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b4ad5effde78b04f3efda6933cc9586927c29004.pdf" target="_blank">https://openreview.net/pdf/b4ad5effde78b04f3efda6933cc9586927c29004.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7b2b06c78e724075063ff8bfc32cf699cb57f1cb.zip" target="_blank">https://openreview.net/attachment/7b2b06c78e724075063ff8bfc32cf699cb57f1cb.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning.To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We enhance the U-Net and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence's global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained T2I models across the frames. With the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-GPU hours versus CogVideo with over 12,480-GPU hours: achieving the 31\% FVD improvement compared to the current SOTA model on SSv2 and 83.7\% average preference in the human evaluation. Our project is available at https://seervideodiffusion.github.io/</div>
</div>
<div class='paper-counter'>91/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design</div>
<div class="field-name">authors:</div>
<div class="field-value">Heng Dong;Junyu Zhang;Chongjie Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Design;Hyperbolic Space;Coarse-to-Fine;Multi-Cellular</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9f01d3b965f2c102a1d6332805027091b7f2a6c5.pdf" target="_blank">https://openreview.net/pdf/9f01d3b965f2c102a1d6332805027091b7f2a6c5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose to design multi-cellular robots in a coarse-to-fine manner and leverage hyperbolic embeddings for realization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and concentrate on regions demonstrating promise.  Finally, the extensive empirical studies on various challenging tasks sourced from EvoGym show our approach's superior efficiency and generalization capability.</div>
</div>
<div class='paper-counter'>92/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Probabilistic Adaptation of Black-Box Text-to-Video Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Sherry Yang;Yilun Du;Bo Dai;Dale Schuurmans;Joshua B. Tenenbaum;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Controllable video generation;probabilistic adaptation;video diffusion;black box adaptation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/21d62c151b9cf1a16a5b4a6498d2df9d99541ae8.pdf" target="_blank">https://openreview.net/pdf/21d62c151b9cf1a16a5b4a6498d2df9d99541ae8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b256095867262269082dc1a70b1921b732293fff.zip" target="_blank">https://openreview.net/attachment/b256095867262269082dc1a70b1921b732293fff.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Video Adapter, a framework for generating personalized, stylized, and domain-specific videos by leveraging large pretrained text-to-video diffusion models as a probabilistic prior without requiring access to pretrained model weights.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large text-to-video models trained on internet-scale data have demonstrated exceptional capabilities in generating high-fidelity videos from arbitrary textual descriptions. However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are not publicly available, posing a significant challenge to adapting these models to specific domains such as robotics, animation, and personalized stylization. Inspired by how a large language model can be prompted to perform new tasks without access to the model weights, we investigate how to adapt a black-box pretrained text-to-video model to a variety of downstream domains without weight access to the pretrained model. In answering this question, we propose \emph{\methodname}, which leverages the score function of a large pretrained video diffusion model as a probabilistic prior to guide the generation of a task-specific small video model. Our experiments show that, by incorporating broad knowledge and fidelity of the pretrained model probabilistically, a small model with as few as 1.25% parameters of the pretrained model can generate high-quality yet domain-specific videos for a variety of downstream domains such as animation, egocentric modeling, and modeling of simulated and real-world robotics data. As large text-to-video models starting to become available as a service similar to large language models, we advocate for private institutions to expose scores of video diffusion models as outputs in addition to generated videos to allow flexible adaptation of large pretrained text-to-video models by the general public.</div>
</div>
<div class='paper-counter'>93/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Imitation Learning from Observation with Automatic Discount Scheduling</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuyang Liu;Weijun Dong;Yingdong Hu;Chuan Wen;Zhao-Heng Yin;Chongjie Zhang;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;inverse reinforcement learning;reinforcement learning;currilulum learning;optimal transport</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f73926c464d89bfaaa3eea9f126267d2979abe23.pdf" target="_blank">https://openreview.net/pdf/f73926c464d89bfaaa3eea9f126267d2979abe23.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3f034ae367425b6bd84789233dde5883b663574f.zip" target="_blank">https://openreview.net/attachment/3f034ae367425b6bd84789233dde5883b663574f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observation (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earlier behaviors before advancing to later ones. We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively alters the discount factor in reinforcement learning during the training phase, prioritizing earlier rewards initially and gradually engaging later rewards only when the earlier behaviors have been mastered. Our experiments, conducted on nine Meta-World tasks, demonstrate that our method significantly outperforms state-of-the-art methods across all tasks, including those that are unsolvable by them. Our code is available at https://il-ads.github.io.</div>
</div>
<div class='paper-counter'>94/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiangyu Liu;Souradip Chakraborty;Yanchao Sun;Furong Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;adversarial policies</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5135816d419bcef3370d30cab554a82466b5c74b.pdf" target="_blank">https://openreview.net/pdf/5135816d419bcef3370d30cab554a82466b5c74b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this paper, we reassess the threats posed to RL agents by adversarial policies and introduce a generalized attack formulation with the first provably efficient defense algorithm.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Most existing works focus on direct perturbations to the victim's state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks. However, such direct manipulations may not be always realizable.In this paper, we consider a multi-agent setting where a well-trained victim agent $\nu$ is exploited by an attacker controlling another agent $\alpha$ with an \textit{adversarial policy}. Previous models do not account for the possibility that the attacker may only have partial control over $\alpha$ or that the attack may produce easily detectable ``abnormal'' behaviors. Furthermore, there is a lack of provably efficient defenses against these adversarial policies. To address these limitations, we introduce a generalized attack framework that has the flexibility to model to what extent the adversary is able to control the agent, and allows the attacker to regulate the state distribution shift and produce stealthier adversarial policies. Moreover, we offer a provably efficient defense with polynomial convergence to the most robust victim policy through adversarial training with timescale separation. This stands in sharp contrast to supervised learning, where adversarial training typically provides only \textit{empirical} defenses.Using the Robosumo competition experiments, we show that our generalized attack formulation results in much stealthier adversarial policies when maintaining the same winning rate as baselines. Additionally, our adversarial training approach yields stable learning dynamics and less exploitable victim policies.</div>
</div>
<div class='paper-counter'>95/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Score Models for Offline Goal-Conditioned Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Harshit Sikchi;Rohan Chitnis;Ahmed Touati;Alborz Geramifard;Amy Zhang;Scott Niekum</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Goal-Conditioned Reinforcement Learning;Deep Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e910f7909d83805095dc5214228a3ec894a22038.pdf" target="_blank">https://openreview.net/pdf/e910f7909d83805095dc5214228a3ec894a22038.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2811914e2c6f0f2ce7daf54f16c0f209ca91b8ba.pdf" target="_blank">https://openreview.net/attachment/2811914e2c6f0f2ce7daf54f16c0f209ca91b8ba.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A discriminator-free occupancy matching approach for performant offline goal-conditioned reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns *scores* or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.</div>
</div>
<div class='paper-counter'>96/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RLIF: Interactive Imitation Learning as Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianlan Luo;Perry Dong;Yuexiang Zhai;Yi Ma;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;imitation learning;online learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b45c120af5895fdbc18e17dcbcbd8d2d55ac2a33.pdf" target="_blank">https://openreview.net/pdf/b45c120af5895fdbc18e17dcbcbd8d2d55ac2a33.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An RL approach runs under DAgger setting without making optimal expert or task reward assumption.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Although reinforcement learning methods offer a powerful framework for auto-matic skill acquisition, for practical learning-based control problems in domainssuch as robotics, imitation learning often provides a more convenient and accessiblealternative. In particular, an interactive imitation learning method such as DAgger,which queries a near-optimal expert to intervene online to collect correction data foraddressing the distributional shift challenges that afflict naïve behavioral cloning,can enjoy good performance both in theory and practice without requiring manuallyspecified reward functions and other components of full reinforcement learningmethods. In this paper, we explore how off-policy reinforcement learning canenable improved performance under assumptions that are similar but potentiallyeven more practical than those of interactive imitation learning. Our proposedmethod uses reinforcement learning with user intervention signals themselves asrewards. This relaxes the assumption that intervening experts in interactive imita-tion learning should be near-optimal and enables the algorithm to learn behaviorsthat improve over the potential suboptimal human expert. We also provide a uni-fied framework to analyze our RL method and DAgger; for which we present theasymptotic analysis of the suboptimal gap for both methods as well as the non-asymptotic sample complexity bound of our method. We then evaluate our methodon challenging high-dimensional continuous control simulation benchmarks aswell as real-world robotic vision-based manipulation tasks. The results show that itstrongly outperforms DAgger-like approaches across the different tasks, especiallywhen the intervening experts are suboptimal. Additional ablations also empiricallyverify the proposed theoretical justification that the performance of our method isassociated with the choice of intervention model and suboptimality of the expert.Code and videos can be found on the project website: https://rlif-page.github.io</div>
</div>
<div class='paper-counter'>97/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">$\pi$2vec: Policy Representation with Successor Features</div>
<div class="field-name">authors:</div>
<div class="field-value">Gianluca Scarpellini;Ksenia Konyushkova;Claudio Fantacci;Thomas Paine;Yutian Chen;Misha Denil</div>
<div class="field-name">keywords:</div>
<div class="field-value">Policy representation;offline policy selection;robotics;evaluation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/35eacee9c8fd3a84eae896684e2e43482f5f8ddc.pdf" target="_blank">https://openreview.net/pdf/35eacee9c8fd3a84eae896684e2e43482f5f8ddc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces `pi2vec`, a method that represents black box policies as feature vectors by leveraging a pretrained foundation model and successor feature framework, enabling efficient policy evaluation in resource constrained environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper introduces $\pi$2vec, a method for representing black box policies as comparable feature vectors.Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy.$\pi$2vec represents the behavior of policies by capturing the statistics of the features from a pretrained model with the help of successor feature framework. We focus on the offline setting where policies and their representations are trained on a fixed dataset of trajectories.Finally, we employ linear regression on $\pi$2vec vector representations to predict the performance of held out policies.The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.</div>
</div>
<div class='paper-counter'>98/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoqi Yuan;Zhancun Mu;Feiyang Xie;Zongqing Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;pre-training;goal-conditioned RL;open-world environments</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/97ae12300fd1715ec484f1be154d49a619911fff.pdf" target="_blank">https://openreview.net/pdf/97ae12300fd1715ec484f1be154d49a619911fff.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM’s superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills.</div>
</div>
<div class='paper-counter'>99/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Interpretable Meta-Learning of Physical Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Matthieu Blanke;Marc Lelarge</div>
<div class="field-name">keywords:</div>
<div class="field-value">meta-learning;physical systems;multi-task learning;interpretable deep learning;identifiability;electrostatics;robotics;control;reinforcement learning;scientific discovery</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to physical sciences (physics, chemistry, biology, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6aa5ecddbb4062e010f6e87f67a3b7222f689dbe.pdf" target="_blank">https://openreview.net/pdf/6aa5ecddbb4062e010f6e87f67a3b7222f689dbe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new multi-environment meta-learning architecture for physical systems called CAMEL, that learns and generalizes at minimal cost and with interpretable weights.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. We introduce CAMEL, a new meta-learning architecture capable of learning efficiently from multiple environments, with an affine structure with respect to the learning task. We prove that CAMEL can identify the physical parameters of the system, enabling interpreable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpretability of our method is illustrated with original applications to parameter identification and to adaptive control and system identification.</div>
</div>
<div class='paper-counter'>100/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Ge Li;Hongyi Zhou;Dominik Roth;Serge Thilges;Fabian Otto;Rudolf Lioutikov;Gerhard Neumann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Trajectory Correlation Modeling;Robot Manipulation;Movement Primitives</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0cc127d1ae1c9418c3765818578fba6a538fe366.pdf" target="_blank">https://openreview.net/pdf/0cc127d1ae1c9418c3765818578fba6a538fe366.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ce524f8996bd138af0e8437b330e3ba8d2b0adac.zip" target="_blank">https://openreview.net/attachment/ce524f8996bd138af0e8437b330e3ba8d2b0adac.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce an efficient Reinforcement Learning (RL) method for robot trajectory generation that effectively captures temporal movement correlations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque black boxes. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.</div>
</div>
<div class='paper-counter'>101/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Time-Varying Propensity Score to Bridge the Gap between the Past and Present</div>
<div class="field-name">authors:</div>
<div class="field-value">Rasool Fakoor;Jonas Mueller;Zachary Chase Lipton;Pratik Chaudhari;Alex Smola</div>
<div class="field-name">keywords:</div>
<div class="field-value">model adaptation to changing data;distribution shift</div>
<div class="field-name">primary_area:</div>
<div class="field-value">transfer learning, meta learning, and lifelong learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5b11f3eed8b0d7474148defc87b1c6b07a950bdf.pdf" target="_blank">https://openreview.net/pdf/5b11f3eed8b0d7474148defc87b1c6b07a950bdf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">To keep models accurate in the real world, we must regularly update them with relevant data. This paper proposes a new method for doing so.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data evolves gradually. We introduce a time-varying propensity score that can detect gradual shifts in the distribution of data which allows us to selectively sample past data to update the model---not just similar data from the past like that of a standard propensity score but also data that evolved in a similar fashion in the past. The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.</div>
</div>
<div class='paper-counter'>102/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Vision-Language Foundation Models as Effective Robot Imitators</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinghang Li;Minghuan Liu;Hanbo Zhang;Cunjun Yu;Jie Xu;Hongtao Wu;Chilam Cheang;Ya Jing;Weinan Zhang;Huaping Liu;Hang Li;Tao Kong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Visual Language Model;Robotics;Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">representation learning for computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a1af30de31af35eb9ee853f1020a7e7041aa3e9a.pdf" target="_blank">https://openreview.net/pdf/a1af30de31af35eb9ee853f1020a7e7041aa3e9a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data.To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control.Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Our code will be made public upon acceptance.</div>
</div>
<div class='paper-counter'>103/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yueru Luo;Shuguang Cui;Zhen Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Lane Detection;Multi-modal</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/523a554f95e7455c265d3f7eff3e065b4898d0b7.pdf" target="_blank">https://openreview.net/pdf/523a554f95e7455c265d3f7eff3e065b4898d0b7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0e8383d29b798e28149ed15a41b3dac6e4b11346.pdf" target="_blank">https://openreview.net/attachment/0e8383d29b798e28149ed15a41b3dac6e4b11346.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurate 3D lane estimation is crucial for ensuring safety in autonomous driving. However, prevailing monocular techniques suffer from depth loss and lighting variations, hampering accurate 3D lane detection. In contrast, LiDAR points offer geometric cues and enable precise localization. In this paper, we present DV-3DLane, a novel end-to-end **D**ual-**V**iew multi-modal **3D Lane** detection framework that synergizes the strengths of both images and LiDAR points. We propose to learn multi-modal features in dual-view spaces, *i.e.*, *perspective view* (PV) and *bird's-eye-view* (BEV), effectively leveraging the modal-specific information. To achieve this, we introduce three designs: 1) A bidirectional feature fusion strategy that integrates multi-modal features into each view space, exploiting their unique strengths. 2) A unified query generation approach that leverages lane-aware knowledge from both PV and BEV spaces to generate queries. 3) A 3D dual-view deformable attention mechanism, which aggregates discriminative features from both PV and BEV spaces into queries for accurate 3D lane detection. Extensive experiments on the public benchmark, OpenLane, demonstrate the efficacy and efficiency of DV-3DLane. It achieves state-of-the-art performance, with a remarkable 11.2 gain in F1 score and a substantial 53.5% reduction in errors. Code is available on [github](https://github.com/JMoonr/dv-3dlane).</div>
</div>
<div class='paper-counter'>104/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Selective Visual Representations Improve Convergence and Generalization for Embodied AI</div>
<div class="field-name">authors:</div>
<div class="field-value">Ainaz Eftekhar;Kuo-Hao Zeng;Jiafei Duan;Ali Farhadi;Aniruddha Kembhavi;Ranjay Krishna</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied-AI;Task-conditioned Representations;Visual Navigation;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2fbfaec8070dacca6ff1916307768a1f7ce97be6.pdf" target="_blank">https://openreview.net/pdf/2fbfaec8070dacca6ff1916307768a1f7ce97be6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues.Inspired by selective attention in humans—the process through which people filter their perception based on their experiences, knowledge, and the task at hand—we introduce a parameter-efficient approach to filter visual stimuli for embodied AI.Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation.Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across $5$ benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects.</div>
</div>
<div class='paper-counter'>105/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ASID: Active Exploration for System Identification in Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Marius Memmel;Andrew Wagenmaker;Chuning Zhu;Dieter Fox;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">sim2real;system identification;exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f456ef2115275fac2aa0977b3c7db68ed00add89.pdf" target="_blank">https://openreview.net/pdf/f456ef2115275fac2aa0977b3c7db68ed00add89.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Model-free control strategies such as reinforcement learning have shown the ability to learn control strategies without requiring an accurate model or simulator of the world. While this is appealing due to the lack of modeling requirements, such methods can be sample inefficient, making them impractical in many real-world domains. On the other hand, model-based control techniques leveraging accurate simulators can circumvent these challenges and use a large amount of cheap simulation data to learn controllers that can effectively transfer to the real world. The challenge with such model-based techniques is the requirement for an extremely accurate simulation, requiring both the specification of appropriate simulation assets and physical parameters. This requires considerable human effort to design for every environment being considered. In this work, we propose a learning system that can leverage a small amount of real-world data to autonomously refine a simulation model and then plan an accurate control strategy that can be deployed in the real world. Our approach critically relies on utilizing an initial (possibly inaccurate) simulator to design effective exploration policies that, when deployed in the real world, collect high-quality data. We demonstrate the efficacy of this paradigm in identifying articulation, mass, and other physical parameters in several challenging robotic manipulation tasks, and illustrate that only a small amount of real-world data can allow for effective sim-to-real transfer.</div>
</div>
<div class='paper-counter'>106/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Task Planning for Visual Room Rearrangement under Partial Observability</div>
<div class="field-name">authors:</div>
<div class="field-value">Karan Mirakhor;Sourav Ghosh;Dipanjan Das;Brojeshwar Bhowmick</div>
<div class="field-name">keywords:</div>
<div class="field-value">Task Planning;Object Search;Deep-RL;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/12382b91f39a182002d5412bce5637aec72fe8c3.pdf" target="_blank">https://openreview.net/pdf/12382b91f39a182002d5412bce5637aec72fe8c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c45fb2afcba8e0b00d5fa5c2ef29149850a03232.zip" target="_blank">https://openreview.net/attachment/c45fb2afcba8e0b00d5fa5c2ef29149850a03232.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents a novel hierarchical task planner under partial observabilitythat empowers an embodied agent to use visual input to efficiently plan a sequenceof actions for simultaneous object search and rearrangement in an untidy room,to achieve a desired tidy state. The paper introduces (i) a novel Search Networkthat utilizes commonsense knowledge from large language models to find unseenobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novelgraph-based state representation to produce a scalable and effective planner thatinterleaves object search and rearrangement to minimize the number of steps takenand overall traversal of the agent, as well as to resolve blocked goal and swapcases, and (iv) a sample-efficient cluster-biased sampling for simultaneous trainingof the proxy reward network along with the Deep RL network. Furthermore,the paper presents new metrics and a benchmark dataset - RoPOR, to measurethe effectiveness of rearrangement planning. Experimental results show that ourmethod significantly outperforms the state-of-the-art rearrangement methods Weihset al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).</div>
</div>
<div class='paper-counter'>107/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Joey Hejna;Rafael Rafailov;Harshit Sikchi;Chelsea Finn;Scott Niekum;W. Bradley Knox;Dorsa Sadigh</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning from human feedback;preference-based RL;human-in-the-loop RL;preference learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/45058c90136046596530d7ce4145d2948f976ad0.pdf" target="_blank">https://openreview.net/pdf/45058c90136046596530d7ce4145d2948f976ad0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/efa06c4501dbc2e21a0c40ca772daba508b73299.zip" target="_blank">https://openreview.net/attachment/efa06c4501dbc2e21a0c40ca772daba508b73299.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Constrastive Preference Learning, a new supervised algorithm for learning optimal policies from regret-based preferences in general MDPs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the \emph{regret} under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the \textit{regret}-based model of human preferences. Using the principle of maximum entropy, we derive \fullname (\abv), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. \abv is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables \abv to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.</div>
</div>
<div class='paper-counter'>108/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Deceptive Fairness Attacks on Graphs via Meta Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jian Kang;Yinglong Xia;Ross Maciejewski;Jiebo Luo;Hanghang Tong</div>
<div class="field-name">keywords:</div>
<div class="field-value">graph learning;fairness;adversarial attacks</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning on graphs and other geometries & topologies</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec8c9c4fa87c072aa9815fe7601e5b37a8b938b2.pdf" target="_blank">https://openreview.net/pdf/ec8c9c4fa87c072aa9815fe7601e5b37a8b938b2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d9919c5a8f3e8af21303186e780c0261eeaa9a08.pdf" target="_blank">https://openreview.net/attachment/d9919c5a8f3e8af21303186e780c0261eeaa9a08.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a meta learning-based poisoning attack strategy to exacerbate unfairness of graph learning models, while preserving the utility in downstream tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity or individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.</div>
</div>
<div class='paper-counter'>109/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Neural Contractive Dynamical Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Hadi Beik Mohammadi;Søren Hauberg;Georgios Arvanitidis;Nadia Figueroa;Gerhard Neumann;Leonel Rozo</div>
<div class="field-name">keywords:</div>
<div class="field-value">learning from demonstration;dynamical systems;contraction theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a89591eec311a0efbd01f7135555a21d2d682c1c.pdf" target="_blank">https://openreview.net/pdf/a89591eec311a0efbd01f7135555a21d2d682c1c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/58947be005497ff04d158bb31e64b18f6186222b.zip" target="_blank">https://openreview.net/attachment/58947be005497ff04d158bb31e64b18f6186222b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper introduces the Neural Contractive Dynamical System (NCDS), providing a flexible and stable approach for learning contractive dynamics, and extends it to high-dimensional systems with obstacle avoidance capabilities.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. We propose a novel methodology to learn \emph{neural contractive dynamical systems}, where our neural architecture ensures contraction, and hence, global stability. To efficiently scale the method to high-dimensional dynamical systems, we develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. We further extend our approach to learning contractive systems on the Lie group of rotations to account for full-pose end-effector dynamic motions. The result is the first highly flexible learning architecture that provides contractive stability guarantees with capability to perform obstacle avoidance. Empirically, we demonstrate that our approach encodes the desired dynamics more accurately than the current state-of-the-art, which provides less strong stability guarantees.</div>
</div>
<div class='paper-counter'>110/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Murtaza Dalal;Tarun Chiruvolu;Devendra Singh Chaplot;Ruslan Salakhutdinov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Long-horizon robot learning;reinforcement learning;LLMs</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fcb3f95ba27cb8127a589ed992c1eec75347895a.pdf" target="_blank">https://openreview.net/pdf/fcb3f95ba27cb8127a589ed992c1eec75347895a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/96b6f340abfb8457f436df0c9d379b93308bfa13.pdf" target="_blank">https://openreview.net/attachment/96b6f340abfb8457f436df0c9d379b93308bfa13.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a method that enables Language Model guided RL for long-horizon robotics tasks by appropriately integrating vision-based motion planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) are highly capable of performing planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (*e.g.* picking, placing, pulling, pushing, navigating). However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings. Furthermore, for many tasks of interest, the robot needs to be able to adjust its behavior in a fine-grained manner, requiring the agent to be capable of modifying *low-level* control actions. Can we instead use the internet-scale knowledge from LLMs for high-level policies, guiding reinforcement learning (RL) policies to efficiently solve robotic control tasks online without requiring a pre-determined set of skills? In this paper, we propose **Plan-Seq-Learn** (PSL): a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks from scratch. We demonstrate that PSL is capable of solving 20+ challenging single and multi-stage robotics tasks on four benchmarks at success rates of over 80% from raw visual input, out-performing language-based, classical, and end-to-end approaches. Video results and code at https://planseqlearn.github.io/</div>
</div>
<div class='paper-counter'>111/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Path Choice Matters for Clear Attributions in Path Methods</div>
<div class="field-name">authors:</div>
<div class="field-value">Borui Zhang;Wenzhao Zheng;Jie Zhou;Jiwen Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">explainable AI;integrated gradient;attribution;path method</div>
<div class="field-name">primary_area:</div>
<div class="field-value">visualization or interpretation of learned representations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fdfec76299aea6f4172a06958754d19d20b2be55.pdf" target="_blank">https://openreview.net/pdf/fdfec76299aea6f4172a06958754d19d20b2be55.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ce6e73870d5f3174edb39c12fdc52ad823c23b08.zip" target="_blank">https://openreview.net/attachment/ce6e73870d5f3174edb39c12fdc52ad823c23b08.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices. To address the ambiguity, we introduce Concentration Principle, which centrally allocates high attributions to indispensable features, thereby endowing aesthetic and sparsity. We then present SAMP, a model-agnostic interpreter, which efficiently searches the near-optimal path from a pre-defined set of manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and momentum strategy (MS) to improve the rigorousness and optimality. Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient image pixels.We also perform quantitative experiments and observe that our method significantly outperforms the counterparts.</div>
</div>
<div class='paper-counter'>112/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">InfoCon: Concept Discovery with Generative and Discriminative Informativeness</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruizhe Liu;Qian Luo;Yanchao Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">concept discovery;robotic manipulation;self-supervised learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a4b3a7d01c5dc3f4d140f8da45983050a3301975.pdf" target="_blank">https://openreview.net/pdf/a4b3a7d01c5dc3f4d140f8da45983050a3301975.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We focus on the self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. We propose that the decision to conceptualize a physical procedure should not depend on how we name it (semantics) but rather on the significance of the informativeness in its representation regarding the low-level physical state and state changes. We model manipulation concepts -- discrete symbols -- as generative and discriminative goals and derive metrics that can autonomously link them to meaningful sub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ a trainable codebook containing encodings --symbols -- capable of synthesizing the end-state of a sub-trajectory given the current state (generative informativeness). Moreover, the encoding corresponding to a particular sub-trajectory should differentiate the state within and outside it and confidently predict the subsequent action based on the gradient of its discriminative score (discriminative informativeness). These metrics, which do not rely on human annotation, can be seamlessly integrated into a VQ-VAE framework, enabling the partitioning of demonstrations into semantically consistent sub-trajectories, fulfilling the purpose of discovering manipulation concepts and the corresponding (sub)-goal states. We evaluate the effectiveness of the learned concepts by training policies that utilize them as guidance, demonstrating superior performance compared to other baselines. Additionally, our discovered manipulation concepts compare favorably to human-annotated ones, while saving much manual effort. The code and trained models will be made public.</div>
</div>
<div class='paper-counter'>113/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SEPT: Towards Efficient Scene Representation Learning for Motion Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhiqian Lan;Yuxuan Jiang;Yao Mu;Chen Chen;Shengbo Eben Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">motion prediction;autonomous driving;self-supervised learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8087e5328150c9164f4ed4ea5d61cb9eadb1c690.pdf" target="_blank">https://openreview.net/pdf/8087e5328150c9164f4ed4ea5d61cb9eadb1c690.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin.</div>
</div>
<div class='paper-counter'>114/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Multimodal Web Navigation with Instruction-Finetuned Foundation Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Hiroki Furuta;Kuang-Huei Lee;Ofir Nachum;Yutaka Matsuo;Aleksandra Faust;Shixiang Shane Gu;Izzeddin Gur</div>
<div class="field-name">keywords:</div>
<div class="field-value">Web Navigation;Foundation Models;Large Language Models;Instruction Finetuning;Decision Making;Multimodal Document Understanding</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fabfa32c6fc766d096c8a789e2bd35887e182190.pdf" target="_blank">https://openreview.net/pdf/fabfa32c6fc766d096c8a789e2bd35887e182190.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an offline multimodal agent for autonomous web navigation based on instruction-finetuned large language models, that achieves comparable performance to humans and RL-finetuned SoTA agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data.In this work, we study data-driven offline training for web agents with vision-language foundation models.We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type.WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations.We empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B.Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web.We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.</div>
</div>
<div class='paper-counter'>115/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On Trajectory Augmentations for Off-Policy Evaluation</div>
<div class="field-name">authors:</div>
<div class="field-value">Ge Gao;Qitong Gao;Xi Yang;Song Ju;Miroslav Pajic;Min Chi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Trajectory augmentation;Off-policy evaluation;Sub-trajectory mining from offline dataset</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec9484bf2975dae1326d6ba008edb1cc61c0556c.pdf" target="_blank">https://openreview.net/pdf/ec9484bf2975dae1326d6ba008edb1cc61c0556c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5bc5195910214ff51cb07371bc708cfb92d74c46.zip" target="_blank">https://openreview.net/attachment/5bc5195910214ff51cb07371bc708cfb92d74c46.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In the realm of reinforcement learning (RL), off-policy evaluation (OPE) holds a pivotal position, especially in high-stake human-involved scenarios such as e-learning and healthcare. Applying OPE to these domains is often challenging with scarce and underrepresentative offline training trajectories. Data augmentation has been a successful technique to enrich training data. However, directly employing existing data augmentation methods to OPE may not be feasible, due to the Markovian nature within the offline trajectories and the desire for generalizability across diverse target policies. In this work, we propose an offline trajectory augmentation approach to specifically facilitate OPE in human-involved scenarios. We propose sub-trajectory mining to extract potentially valuable sub-trajectories from offline data, and diversify the behaviors within those sub-trajectories by varying coverage of the state-action space. Our work was empirically evaluated in a wide array of environments, encompassing both simulated scenarios and real-world domains like robotic control, healthcare, and e-learning, where the training trajectories include varying levels of coverage of the state-action space. By enhancing the performance of a variety of OPE methods, our work offers a promising path forward for tackling OPE challenges in situations where data may be limited or underrepresentative.</div>
</div>
<div class='paper-counter'>116/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Zilin Si;Gu Zhang;Qingwei Ben;Branden Romero;Zhou Xian;Chao Liu;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Tactile sensing;Simulation;Robotic manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ca114f69e2dc9d526e44fe9161eacd32eca35c8b.pdf" target="_blank">https://openreview.net/pdf/ca114f69e2dc9d526e44fe9161eacd32eca35c8b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website https://difftactile.github.io/.</div>
</div>
<div class='paper-counter'>117/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generalized Policy Iteration using Tensor Approximation for Hybrid Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Suhan Shetty;Teng Xue;Sylvain Calinon</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimal Control;Hybrid Actions;Robotics;Approximate Dynamic Programming;Tensor Approximation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a40c77cc6b21af1cc748441a1c8107b6a7896cc5.pdf" target="_blank">https://openreview.net/pdf/a40c77cc6b21af1cc748441a1c8107b6a7896cc5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bf64c633c8b95188350edcfdeb9d8cce26f66b20.zip" target="_blank">https://openreview.net/attachment/bf64c633c8b95188350edcfdeb9d8cce26f66b20.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper proposes a novel approximate dynamic programming algorithm that can handle hybrid action space</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control of dynamic systems involving hybrid actions is a challenging task in robotics.  To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment involving a non-prehensile manipulation task which is considered to be a highly challenging control problem.</div>
</div>
<div class='paper-counter'>118/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon</div>
<div class="field-name">authors:</div>
<div class="field-value">Guillaume Bono;Leonid Antsfeld;Boris Chidlovskii;Philippe Weinzaepfel;Christian Wolf</div>
<div class="field-name">keywords:</div>
<div class="field-value">Navigation;Embodied AI;Perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/34278c5e203491d903e4dc9abbcc9f691231f461.pdf" target="_blank">https://openreview.net/pdf/34278c5e203491d903e4dc9abbcc9f691231f461.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b5f146e5ef34d0e3f0fa3fe9dbe687056413a388.zip" target="_blank">https://openreview.net/attachment/b5f146e5ef34d0e3f0fa3fe9dbe687056413a388.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In an ImageGoal navigation context, we propose two pre-text tasks which let correspondence emerge as a solution and train a dual visual encoder based on a binocular transforme</div>
<div class="field-name">abstract:</div>
<div class="field-value">Most recent work in goal oriented visual navigation resorts to large-scale machine learning in simulated environments. The main challenge lies in learning compact representations generalizable to unseen environments and in learning high-capacity perception modules capable of reasoning on high-dimensional input. The latter is particularly difficult when the goal is not given as a category ("ObjectNav") but as an exemplar image ("ImageNav"), as the perception module needs to learn a comparison strategy requiring to solve an underlying visual correspondence problem. This has been shown to be difficult from reward alone or with standard auxiliary tasks. We address this problem through a sequence of two pretext tasks, which serve as a prior for what we argue is one of the main bottleneck in perception, extremely wide-baseline relative pose estimation and visibility prediction in complex scenes. The first pretext task, cross-view completion is a proxy for the underlying visual correspondence problem, while the second task addresses goal detection and finding directly. We propose a new dual encoder with a large-capacity binocular ViT  model and show that correspondence solutions naturally emerge from the training signals. Experiments show significant improvements and SOTA performance on the two benchmarks, ImageNav and the Instance-ImageNav variant, where camera intrinsics and height differ between observation and goal.</div>
</div>
<div class='paper-counter'>119/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuan Gao;WEIZHONG ZHANG;Wenhan Luo;Lin Ma;Jin-Gang Yu;Gui-Song Xia;Jiayi Ma</div>
<div class="field-name">keywords:</div>
<div class="field-value">Auxiliary Learning; Neural Architecture Search; Soft Parameter Sharing; Multi-Task Learning; Single Task Inference Cost</div>
<div class="field-name">primary_area:</div>
<div class="field-value">representation learning for computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5f009b99c187d83bc92692bc20b98869293ccbaf.pdf" target="_blank">https://openreview.net/pdf/5f009b99c187d83bc92692bc20b98869293ccbaf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel soft-parameter sharing architecture-based method optimized by Neural Architecture Search, which exploits auxiliary task labels to boost the primary task performance without increasing the inference cost for the primary task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference. Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence. These connections can be removed during the primary task inference, resulting in a single-task inference cost. We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections. Moreover, our method can be incorporated with optimization-based auxiliary learning approaches. Extensive experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG, ResNet, and ViT backbones validate the promising performance. The codes are available at https://github.com/ethanygao/Aux-NAS.</div>
</div>
<div class='paper-counter'>120/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">METRA: Scalable Unsupervised RL with Metric-Aware Abstraction</div>
<div class="field-name">authors:</div>
<div class="field-value">Seohong Park;Oleh Rybkin;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/957e22f4e911e7ad35fff291d142a0a622982c0a.pdf" target="_blank">https://openreview.net/pdf/957e22f4e911e7ad35fff291d142a0a622982c0a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Our main idea is, instead of directly covering the entire state space, to only cover a compact latent space $\mathcal{Z}$ that is metrically connected to the state space $\mathcal{S}$ by temporal distances. By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments. Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments, being the first unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid. Our code and videos are available at https://seohong.me/projects/metra/</div>
</div>
<div class='paper-counter'>121/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Kevin Black;Mitsuhiko Nakamoto;Pranav Atreya;Homer Rich Walke;Chelsea Finn;Aviral Kumar;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">robot learning;diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c0ae16a0a57aa4ec4f933f90e44a5e9f250f076e.pdf" target="_blank">https://openreview.net/pdf/c0ae16a0a57aa4ec4f933f90e44a5e9f250f076e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1674c6d00b0e3e294c545ab2398d5ab5382e0713.zip" target="_blank">https://openreview.net/attachment/1674c6d00b0e3e294c545ab2398d5ab5382e0713.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot’s own training data. We propose SuSIE, a method that leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. Specifically, we finetune InstructPix2Pix on video data, consisting of both human videos and robot rollouts, such that it outputs hypothetical future “subgoal” observations given the robot’s current observation and a language command. We also use the robot data to train a low-level goal-conditioned policy to act as the aforementioned low-level controller. We find that the high-level subgoal predictions can utilize Internet scale pretraining and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization and precision than conventional language-conditioned policies. We achieve state-of-the-art results on the CALVIN benchmark, and also demonstrate robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data. The project website can be found at http://rail-berkeley.github.io/susie.</div>
</div>
<div class='paper-counter'>122/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Finn Rietz;Erik Schaffernicht;Stefan Heinrich;Johannes A. Stork</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-Objective Reinforcement Learning;Lexicographic Task Priorities;Constrained RL;Transfer RL</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f1ffd49ec59c37900afa292aae71b2488d52baf4.pdf" target="_blank">https://openreview.net/pdf/f1ffd49ec59c37900afa292aae71b2488d52baf4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a decomposed algorithm to solve lexicographic multi-objective reinforcement learning problems with continuous action spaces that satisfies constraints by construction.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask training data for offline learning eliminates the need for new environment interaction during adaptation. We demonstrate the efficacy of our approach by presenting successful learning, reuse, and adaptation results for both low- and high-dimensional simulated robot control tasks, as well as offline learning results. In contrast to baseline approaches, PSQD does not trade off between conflicting subtasks or priority constraints and satisfies subtask priorities during learning. PSQD provides an intuitive framework for tackling complex RL problems, offering insights into the inner workings of the subtask composition.</div>
</div>
<div class='paper-counter'>123/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">UC-NERF: Neural Radiance Field for Under-Calibrated Multi-View Cameras in Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Kai Cheng;Xiaoxiao Long;Wei Yin;Jin Wang;Zhiqiang Wu;Yuexin Ma;Kaixuan Wang;Xiaozhi Chen;Xuejin Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neural radiance field;Multi-cameras;Under-Calibration;Color Inconsistency</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/daf90636fc5f113bc9a6cf634e72fb87a7138d72.pdf" target="_blank">https://openreview.net/pdf/daf90636fc5f113bc9a6cf634e72fb87a7138d72.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/55b68486b03f50116c83713c950f715b16a439a3.zip" target="_blank">https://openreview.net/attachment/55b68486b03f50116c83713c950f715b16a439a3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-camera setups find widespread use across various applications, such as autonomous driving, as they greatly expand sensing capabilities. Despite the fast development of Neural radiance field (NeRF) techniques and their wide applications in both indoor and outdoor scenes, applying NeRF to multi-camera systems remains very challenging. This is primarily due to the inherent under-calibration issues in multi-camera setup, including inconsistent imaging effects stemming from separately calibrated image signal processing units in diverse cameras, and system errors arising from mechanical vibrations during driving that affect relative camera poses.In this paper, we present UC-NeRF, a novel method tailored for novel view synthesis in under-calibrated multi-view camera systems.Firstly, we propose a layer-based color correction to rectify the color inconsistency in different image regions. Second, we propose virtual warping to generate more viewpoint-diverse but color-consistent virtual views for color correction and 3D recovery. Finally, a spatiotemporally constrained pose refinement is designed for more robust and accurate pose calibration in multi-camera systems.Our method not only achieves state-of-the-art performance of novel view synthesis in multi-camera setups, but also effectively facilitates depth estimation in large-scale outdoor scenes with the synthesized novel views.</div>
</div>
<div class='paper-counter'>124/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Unveiling and Manipulating Prompt Influence in Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zijian Feng;Hanzhang Zhou;ZIXIAO ZHU;Junlang Qian;Kezhi Mao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large language models;prompt interpretability;prompt manipulation;controlled LLM generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cf138ec1f70dae4c268d811f644aef08568a99d9.pdf" target="_blank">https://openreview.net/pdf/cf138ec1f70dae4c268d811f644aef08568a99d9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c0f3e61e0fdd4864b3950921fcbdacc73d34dad0.zip" target="_blank">https://openreview.net/attachment/c0f3e61e0fdd4864b3950921fcbdacc73d34dad0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Unveiling and Manipulating Prompt Influence in Large Language Models through Token Distribution Dynamics</div>
<div class="field-name">abstract:</div>
<div class="field-value">Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), an elegantly simple yet remarkably effective approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.</div>
</div>
<div class='paper-counter'>125/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Addressing Signal Delay in Deep Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Wang;Dongqi Han;Xufang Luo;Dongsheng Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforcement Learning;Signal Delay;Robotic Control;Continuous Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/56d8f487fd9b7b18c6c5bbea5d5d311be7d0eb5e.pdf" target="_blank">https://openreview.net/pdf/56d8f487fd9b7b18c6c5bbea5d5d311be7d0eb5e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8176ec6896868769880f6dc46a25389b3ce88a1d.zip" target="_blank">https://openreview.net/attachment/8176ec6896868769880f6dc46a25389b3ce88a1d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper formalizes and addresses signal delay in deep reinforcement learning, introducing effective strategies that maintain high performance in robotic control tasks despite substantial delays.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the notable advancements in deep reinforcement learning (DRL) in recent years, a prevalent issue that is often overlooked is the impact of signal delay. Signal delay occurs when there is a lag between an agent's perception of the environment and its corresponding actions. In this paper, we first formalize delayed-observation Markov decision processes (DOMDP) by extending the standard MDP framework to incorporate signal delays. Next, we elucidate the challenges posed by the presence of signal delay in DRL, showing that trivial DRL algorithms and generic methods for partially observable tasks suffer greatly from delays. Lastly, we propose effective strategies to overcome these challenges. Our methods achieve remarkable performance in continuous robotic control tasks with large delays, yielding results comparable to those in non-delayed cases. Overall, our work contributes to a deeper understanding of DRL in the presence of signal delays and introduces novel approaches to address the associated challenges.</div>
</div>
<div class='paper-counter'>126/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Trajeglish: Traffic Modeling as Next-Token Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Jonah Philion;Xue Bin Peng;Sanja Fidler</div>
<div class="field-name">keywords:</div>
<div class="field-value">self-driving;traffic modeling;autonomous vehicles;simulation;motion prediction;transformer</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf" target="_blank">https://openreview.net/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/df8250037d3a59d504c1d44d06db181075446911.zip" target="_blank">https://openreview.net/attachment/df8250037d3a59d504c1d44d06db181075446911.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An LLM-like architecture for modeling motion of vehicles, pedestrians, and cyclists for AV simulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.</div>
</div>
<div class='paper-counter'>127/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziping Xu;Zifan Xu;Runxuan Jiang;Peter Stone;Ambuj Tewari</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning; Multitask Learning; Exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5c003b03ad1f26d11e28a80a2ac2718e4b0660a5.pdf" target="_blank">https://openreview.net/pdf/5c003b03ad1f26d11e28a80a2ac2718e4b0660a5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/647243be6ecd446e68f366bd09a951057aac0af6.pdf" target="_blank">https://openreview.net/attachment/647243be6ecd446e68f366bd09a951057aac0af6.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We rigorously show that algorithm running myopic exploration with policy-sharing across tasks can be sample-efficient when the task set is diverse.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic  policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency.</div>
</div>
<div class='paper-counter'>128/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Antoine Bambade;Fabian Schramm;Adrien Taylor;Justin Carpentier</div>
<div class="field-name">keywords:</div>
<div class="field-value">Machine Learning;Optimization;Differentiable Optimization;Optimization layers</div>
<div class="field-name">primary_area:</div>
<div class="field-value">optimization</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/731de6c558997b87a5381292530348a26e31fb5f.pdf" target="_blank">https://openreview.net/pdf/731de6c558997b87a5381292530348a26e31fb5f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6e97e586201f446bc5cdd3c415e69219fddef15f.zip" target="_blank">https://openreview.net/attachment/6e97e586201f446bc5cdd3c415e69219fddef15f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a unified approach to differentiate over the closest feasible quadratic programming (QP) solutions. We show it enables to learn a wider range of QP layers with better performance for some classic learning tasks</div>
<div class="field-name">abstract:</div>
<div class="field-value">Optimization layers within neural network architectures have become increasingly popular for their ability to solve a wide range of machine learning tasks and to model domain-specific knowledge. However, designing optimization layers requires careful consideration as the underlying optimization problems might be infeasible during training. Motivated by applications in learning, control and robotics, this work focuses on convex quadratic programming (QP) layers. The specific structure of this type of optimization layer can be efficiently exploited for faster computations while still allowing rich modeling capabilities. We leverage primal-dual augmented Lagrangian techniques for computing derivatives of both feasible and infeasible QP solutions. More precisely, we propose a unified approach which tackles the differentiability of the closest feasible QP solutions in a classical $\ell_2$ sense. We then harness this approach to enrich the expressive capabilities of existing QP layers. More precisely, we show how differentiating through infeasible QPs during training enables to drive towards feasibility at test time a new range of QP layers. These layers notably demonstrate superior predictive performance in some conventional learning tasks. Additionally, we present alternative formulations that enhance numerical robustness, speed, and accuracy for training such layers. Along with these contributions, we provide an open-source C++ software package called QPLayer for differentiating feasible and infeasible convex QPs and which can be interfaced with modern learning frameworks.</div>
</div>
<div class='paper-counter'>129/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data</div>
<div class="field-name">authors:</div>
<div class="field-value">Chongyi Zheng;Benjamin Eysenbach;Homer Rich Walke;Patrick Yin;Kuan Fang;Ruslan Salakhutdinov;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;self-supervised learning;contrastive learning;goal-conditioned RL;offline RL;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d61ac5544c95c3239516e7c46f315bce6b00ce8b.pdf" target="_blank">https://openreview.net/pdf/d61ac5544c95c3239516e7c46f315bce6b00ce8b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/dee6fa973ab448942dd839bf64eff1ac100991dc.zip" target="_blank">https://openreview.net/attachment/dee6fa973ab448942dd839bf64eff1ac100991dc.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by $2 \times$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training.</div>
</div>
<div class='paper-counter'>130/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Visual Data-Type Understanding does not emerge from scaling Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Vishaal Udandarao;Max F Burg;Samuel Albanie;Matthias Bethge</div>
<div class="field-name">keywords:</div>
<div class="field-value">Data-Type Understanding;Vision-Language Models;Scaling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">representation learning for computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3d6f28056a72139d6a3d3b820e88727454b8139b.pdf" target="_blank">https://openreview.net/pdf/3d6f28056a72139d6a3d3b820e88727454b8139b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0ddd4ec602e887cceb7f2bbd26e682e277001631.pdf" target="_blank">https://openreview.net/attachment/0ddd4ec602e887cceb7f2bbd26e682e277001631.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce "Visual Data-Type Identification", a novel task for identifying visual distortions or perceptual changes to images, we find poor performance of VLMs across the board, and scaling models up does not improve performance significantly.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domains pecific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire anunderstanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding.</div>
</div>
<div class='paper-counter'>131/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</div>
<div class="field-name">authors:</div>
<div class="field-value">Sirui Hong;Mingchen Zhuge;Jonathan Chen;Xiawu Zheng;Yuheng Cheng;Jinlin Wang;Ceyao Zhang;Zili Wang;Steven Ka Shing Yau;Zijuan Lin;Liyang Zhou;Chenyu Ran;Lingfeng Xiao;Chenglin Wu;Jürgen Schmidhuber</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Agent;Meta Programming;Multi-Agent Society;Group Intelligence</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf" target="_blank">https://openreview.net/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces MetaGPT, an innovative meta-programming framework for LLM-based multi-agent collaborations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.</div>
</div>
<div class='paper-counter'>132/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D</div>
<div class="field-name">authors:</div>
<div class="field-value">Haojie Huang;Owen Lewis Howell;Dian Wang;Xupeng Zhu;Robert Platt;Robin Walters</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Geometric Deep Learning;Robotic Manipulation;Equivariant deep learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f56c749096b09d978aa51c7d16cb12dc4a128f65.pdf" target="_blank">https://openreview.net/pdf/f56c749096b09d978aa51c7d16cb12dc4a128f65.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Sample efficient policy learning for manipulation in 3D</div>
<div class="field-name">abstract:</div>
<div class="field-value">Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter ($\text{FourTran}$), which leverages the two-fold $\mathrm{SE}(d)\times\mathrm{SE}(d)$  symmetry in the pick-place problem to achieve much higher sample efficiency. $\text{FourTran}$ is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new configurations. $\text{FourTran}$ is constrained by the symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient computation. Tests on the RLbench benchmark achieve state-of-the-art results across various tasks.</div>
</div>
<div class='paper-counter'>133/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianyu Li;Hyunyoung Jung;Matthew Gombolay;Yong Cho;Sehoon Ha</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human Motion Driven Control;Legged Locomotion;Unsupervised Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5dc2d26064e3720009a09cd274c6ff48e2c64fd2.pdf" target="_blank">https://openreview.net/pdf/5dc2d26064e3720009a09cd274c6ff48e2c64fd2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/546b2b1b9bb3fe36455316bb3849713452600fce.zip" target="_blank">https://openreview.net/attachment/546b2b1b9bb3fe36455316bb3849713452600fce.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.</div>
</div>
<div class='paper-counter'>134/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On the Foundations of Shortcut Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Katherine Hermann;Hossein Mobahi;Thomas FEL;Michael Curtis Mozer</div>
<div class="field-name">keywords:</div>
<div class="field-value">shortcut learning;spurious correlations;architectural inductive bias</div>
<div class="field-name">primary_area:</div>
<div class="field-value">visualization or interpretation of learned representations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3f47b29f0e35691e7047d9fbfa0e4c47ea966e49.pdf" target="_blank">https://openreview.net/pdf/3f47b29f0e35691e7047d9fbfa0e4c47ea966e49.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on *predictivity*---how reliably a feature indicates training-set labels---but also on *availability*---how easily the feature can be extracted from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and we quantify a model's shortcut bias---its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less available, more predictive) feature. We find that linear models are relatively unbiased, but introducing a single hidden layer with ReLU or Tanh units yields a bias. Our empirical findings are consistent with a theoretical account based on Neural Tangent Kernels. Finally, we study how models used in practice trade off predictivity and availability in naturalistic datasets, discovering availability manipulations which increase models' degree of shortcut bias. Taken together, these findings suggest that the propensity to learn shortcut features is a fundamental characteristic of deep nonlinear architectures warranting systematic study given its role in shaping how models solve tasks.</div>
</div>
<div class='paper-counter'>135/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Enhancing Human-AI Collaboration Through Logic-Guided Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">Chengzhi Cao;Yinghao Fu;Sheng Xu;Ruimao Zhang;Shuang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Logic rule;human-robot interaction and collaboration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7b8552a57028c2e6eef4786a49e15a94ad70758b.pdf" target="_blank">https://openreview.net/pdf/7b8552a57028c2e6eef4786a49e15a94ad70758b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8c32dd12bc51d1cc1a771f1639dcaffde8c7beb0.pdf" target="_blank">https://openreview.net/attachment/8c32dd12bc51d1cc1a771f1639dcaffde8c7beb0.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a systematic framework designed to enhance human-robot perception and collaboration through the integration of logical rules and Theory of Mind (ToM). Logical rules provide interpretable predictions and generalize well across diverse tasks, making them valuable for learning and decision-making. Leveraging the ToM for understanding others' mental states, our approach facilitates effective collaboration. In this paper, we employ logic rules derived from observational data to infer human goals and guide human-like agents. These rules are treated as latent variables, and a rule encoder is trained alongside a multi-agent system in the robot's mind. We assess the posterior distribution of latent rules using learned embeddings, representing entities and relations. Confidence scores for each rule indicate their consistency with observed data. Then, we employ a hierarchical reinforcement learning model with ToM to plan robot actions for assisting humans. Extensive experiments validate each component of our framework, and results on multiple benchmarks demonstrate that our model outperforms the majority of existing approaches.</div>
</div>
<div class='paper-counter'>136/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Sumeet Batra;Bryon Tjanaka;Matthew Christopher Fontaine;Aleksei Petrenko;Stefanos Nikolaidis;Gaurav S. Sukhatme</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Quality Diversity;Robotics;Machine Learning;Evolution Strategies</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f3442a5e81e1db8cb186b12c6fe204d3de0490dc.pdf" target="_blank">https://openreview.net/pdf/f3442a5e81e1db8cb186b12c6fe204d3de0490dc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c86dc0f0cb27215dd68fdf05ef563b1b7f50bed9.zip" target="_blank">https://openreview.net/attachment/c86dc0f0cb27215dd68fdf05ef563b1b7f50bed9.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a novel QD-RL method that leverages on-policy RL and Differentiable Quality Diversity to discover a variety of high performing locomtion gaits on the challenging mujoco environment, including, for the first time in QD-RL, humanoid.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Training generally capable agents that thoroughly explore their environment andlearn new and diverse skills is a long-term goal of robot learning. Quality DiversityReinforcement Learning (QD-RL) is an emerging research area that blends thebest aspects of both fields – Quality Diversity (QD) provides a principled formof exploration and produces collections of behaviorally diverse agents, whileReinforcement Learning (RL) provides a powerful performance improvementoperator enabling generalization across tasks and dynamic environments. ExistingQD-RL approaches have been constrained to sample efficient, deterministic off-policy RL algorithms and/or evolution strategies and struggle with highly stochasticenvironments. In this work, we, for the first time, adapt on-policy RL, specificallyProximal Policy Optimization (PPO), to the Differentiable Quality Diversity (DQD)framework and propose several changes that enable efficient optimization anddiscovery of novel skills on high-dimensional, stochastic robotics tasks. Our newalgorithm, Proximal Policy Gradient Arborescence (PPGA), achieves state-of-the-art results, including a 4x improvement in best reward over baselines on thechallenging humanoid domain.</div>
</div>
<div class='paper-counter'>137/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</div>
<div class="field-name">authors:</div>
<div class="field-value">Francis Engelmann;Fabian Manhardt;Michael Niemeyer;Keisuke Tateno;Federico Tombari</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D;open set segmentation;neural radiance fields;VLM features</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8cfcd086bb55abcd08c8ecc773d988224fc45922.pdf" target="_blank">https://openreview.net/pdf/8cfcd086bb55abcd08c8ecc773d988224fc45922.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">OpenNeRF outperforms recent open-vocabulary methods for open set segmentation by using a neural representation that allows for incorporating pixel-wise VLM features and rendering novel views.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF’s ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.</div>
</div>
<div class='paper-counter'>138/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Social-Transmotion: Promptable Human Trajectory Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Saeed Saadatnejad;Yang Gao;Kaouther Messaoud;Alexandre Alahi</div>
<div class="field-name">keywords:</div>
<div class="field-value">human trajectory prediction;robot navigation;autonomous driving;attention mechanism</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5dcf47a66eb9a88f6cda4a59dfb2dd17df12bc68.pdf" target="_blank">https://openreview.net/pdf/5dcf47a66eb9a88f6cda4a59dfb2dd17df12bc68.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ecb46a9e701c832c79affccf0380473888a8cf97.zip" target="_blank">https://openreview.net/attachment/ecb46a9e701c832c79affccf0380473888a8cf97.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a generic Transformer-based model that integrates diverse visual cues as prompts, powered by masking technique to enhance human trajectory prediction.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.To address this, we introduce *Social-Transmotion*, a generic Transformer-based model that exploits diverse and numerous visual cues to predict human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes in the image plane, or body pose keypoints in either 2D or 3D.  This, in turn, augments trajectory data, leading to enhanced human trajectory prediction.Using masking technique, our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between agents based on the available visual cues.We delve into the merits of using 2D versus 3D poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and time-steps in the sequence are vital for optimizing human trajectory prediction.Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.The code is publicly available: [https://github.com/vita-epfl/social-transmotion](https://github.com/vita-epfl/social-transmotion).</div>
</div>
<div class='paper-counter'>139/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS</div>
<div class="field-name">authors:</div>
<div class="field-value">Junchi Yu;Ran He;Zhitao Ying</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM Complex Reasoning;Language Model Reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf" target="_blank">https://openreview.net/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}.To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.These analogous problems are related to the input one, with reusable solutions and problem-solving strategies.Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent Planning.</div>
</div>
<div class='paper-counter'>140/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Guiding Instruction-based Image Editing via Multimodal Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Tsu-Jui Fu;Wenze Hu;Xianzhi Du;William Yang Wang;Yinfei Yang;Zhe Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">image editing;multimodal large language model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/859bb6596377ab0632e753f95d8701a768c8a514.pdf" target="_blank">https://openreview.net/pdf/859bb6596377ab0632e753f95d8701a768c8a514.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0294fd04bb989d844f3463edf644165df9783a95.pdf" target="_blank">https://openreview.net/attachment/0294fd04bb989d844f3463edf644165df9783a95.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">MLLM-Guided image editing (MGIE) jointly learns the MLLM and editing model with visual-aware expressive instructions to provide explicit guidance for instruction-based image editing.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.</div>
</div>
<div class='paper-counter'>141/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer</div>
<div class="field-name">authors:</div>
<div class="field-value">Xingyu Liu;Deepak Pathak;Ding Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">policy transfer;transfer learning;imitation learning;reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5ef4fdbe7aea08a39e2674c53de2348273e64350.pdf" target="_blank">https://openreview.net/pdf/5ef4fdbe7aea08a39e2674c53de2348273e64350.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7d992d58ecc7b3f07e28cec44a1985f9fbd50744.zip" target="_blank">https://openreview.net/attachment/7d992d58ecc7b3f07e28cec44a1985f9fbd50744.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A method for efficiently transferring an expert policy from one robot to multiple different robots</div>
<div class="field-name">abstract:</div>
<div class="field-value">We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named *Meta-Evolve* that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\times$ and one-to-six transfer of agile locomotion policy by 2.4$\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.</div>
</div>
<div class='paper-counter'>142/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Tree Search-Based Policy Optimization under Stochastic Execution Delay</div>
<div class="field-name">authors:</div>
<div class="field-value">David Valensi;Esther Derman;Shie Mannor;Gal Dalal</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Delay;EfficientZero;Tree-search;Sample efficiency</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8830f6b3cafc9913288b14d81260c6f589144619.pdf" target="_blank">https://openreview.net/pdf/8830f6b3cafc9913288b14d81260c6f589144619.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f629821f2c338399309992cc79dc12d8436ea6df.pdf" target="_blank">https://openreview.net/attachment/f629821f2c338399309992cc79dc12d8436ea6df.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We prove results for Reinforcement Learning with stochastic delay, devise a corresponding tree-search based algorithm, and successfully test it on such environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately.However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through empirical analysis, we stress that none of the prior benchmarks consistently outperforms others across different delays. We demonstrate that our algorithm surpasses all benchmark methods in Atari games when dealing with constant or stochastic delays. The code is available at \url{https://github.com/davidva1/Delayed-EZ}.</div>
</div>
<div class='paper-counter'>143/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zuxin Liu;Jesse Zhang;Kavosh Asadi;Yao Liu;Ding Zhao;Shoham Sabach;Rasool Fakoor</div>
<div class="field-name">keywords:</div>
<div class="field-value">Efficient Adaptation;Continual Learning;Robot Learning;Language-Conditioned Visuomotor Control;Few-shot Adaptation;Large Pretrained Models;Imitation Learning;Transfer Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/671407dfbfe635fda06ddb5e51f46c96894073a3.pdf" target="_blank">https://openreview.net/pdf/671407dfbfe635fda06ddb5e51f46c96894073a3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a method of efficient adaptation for large pretrained decision-making models. Low-Rank Adaptation with our framework outperforms traditional fine-tuning with only 1% new parameters while avoiding issues like catastrophic forgetting.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective \emph{pretraining} of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, \emph{continual adaptation} for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques---e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)---in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.</div>
</div>
<div class='paper-counter'>144/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Memory-Consistent Neural Networks for Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaustubh Sridhar;Souradeep Dutta;Dinesh Jayaraman;James Weimer;Insup Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Behavior Cloning;Deep Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9c3fcbe612425300e9bde0103bd43d108b9655c6.pdf" target="_blank">https://openreview.net/pdf/9c3fcbe612425300e9bde0103bd43d108b9655c6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a method to interpolate between nearest neighbours and neural networks for controlling the sub-optimality gap and improving performance in imitation learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised "behavior cloning" for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our "memory-consistent neural network" (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical "memory" training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 10 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications. Website: https://sites.google.com/view/mcnn-imitation</div>
</div>
<div class='paper-counter'>145/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TorchRL: A data-driven decision-making library for PyTorch</div>
<div class="field-name">authors:</div>
<div class="field-value">Albert Bou;Matteo Bettini;Sebastian Dittert;Vikash Kumar;Shagun Sodhani;Xiaomeng Yang;Gianni De Fabritiis;Vincent Moens</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;pytorch;control;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/74ac7ade66fbb7c4ade0e5391457625d3599377c.pdf" target="_blank">https://openreview.net/pdf/74ac7ade66fbb7c4ade0e5391457625d3599377c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present TorchRL, a new generalistic RL and control library for PyTorch. TorchRL offers a modular, lightweight, and agnostic tool for training reinforcement learning agents and other decision-making paradigms.</div>
<div class="field-name">abstract:</div>
<div class="field-value">PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility, and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.</div>
</div>
<div class='paper-counter'>146/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Lunjun Zhang;Yuwen Xiong;Ze Yang;Sergio Casas;Rui Hu;Raquel Urtasun</div>
<div class="field-name">keywords:</div>
<div class="field-value">discrete diffusion; world model; autonomous driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/050b9f131ed2c13af3fa8c734f819689ae777809.pdf" target="_blank">https://openreview.net/pdf/050b9f131ed2c13af3fa8c734f819689ae777809.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose Copilot4D, a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer as discrete diffusion and enhance it with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, Copilot4D reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotics.</div>
</div>
<div class='paper-counter'>147/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Licheng Wen;Daocheng Fu;Xin Li;Xinyu Cai;Tao MA;Pinlong Cai;Min Dou;Botian Shi;Liang He;Yu Qiao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonmous Driving;Large Language Model;Embodied AI;Knowledge-driven</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/16ea0353444a7f3df64fcd9b4e95d34bfeed6502.pdf" target="_blank">https://openreview.net/pdf/16ea0353444a7f3df64fcd9b4e95d34bfeed6502.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b0f0e69c05614943b33cf3a1dcf9d7c1db9ca6a9.zip" target="_blank">https://openreview.net/attachment/b0f0e69c05614943b33cf3a1dcf9d7c1db9ca6a9.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework that can instill human knowledge into autonomous driving systems with the help of large language models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods.Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems.To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain.Project page: https://pjlab-adg.github.io/DiLu/</div>
</div>
<div class='paper-counter'>148/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GenSim: Generating Robotic Simulation Tasks via Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Lirui Wang;Yiyang Ling;Zhecheng Yuan;Mohit Shridhar;Chen Bao;Yuzhe Qin;Bailin Wang;Huazhe Xu;Xiaolong Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM Code Generation;Robotic Simulation;Multi-task Policy Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d84b32393144549665a7888268a368b1eb84b7c3.pdf" target="_blank">https://openreview.net/pdf/d84b32393144549665a7888268a368b1eb84b7c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We investigated LLM's capability to generate over 100 simulation tasks for training language-conditioned multitask robotic manipulation policy, which demonstrates task-level generalization in both simulation and the real world.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM  bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. See our project website (https://gen-sim.github.io) and demo (https://huggingface.co/spaces/Gen-Sim/Gen-Sim) for visualizations and open-source models and datasets.</div>
</div>
<div class='paper-counter'>149/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Chong Mou;Xintao Wang;Jiechong Song;Ying Shan;Jian Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion model;Image editing;Image generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d1d88ddeb5844da8d6140ed188e076244748489e.pdf" target="_blank">https://openreview.net/pdf/d1d88ddeb5844da8d6140ed188e076244748489e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6137b5c7b4050ec3ee40d347cdf8567872b2a40f.pdf" target="_blank">https://openreview.net/attachment/6137b5c7b4050ec3ee40d347cdf8567872b2a40f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A tuning-free diffusion method for general and drag-style image editing.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the ability of text-to-image (T2I) diffusion models to generate high-quality images, transferring this ability to accurate image editing remains a challenge. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we treat image editing as the change of feature correspondence in a pre-trained diffusion model. By leveraging feature correspondence, we develop energy functions that align with the editing target, transforming image editing operations into gradient guidance. Based on this guidance approach, we also construct multi-scale guidance that considers both semantic and geometric alignment. Furthermore, we incorporate a visual cross-attention strategy based on a memory bank design to ensure consistency between the edited result and original image. Benefiting from these efficient designs, all content editing and consistency operations come from the feature correspondence without extra model fine-tuning. Extensive experiments demonstrate that our method has promising performance on various image editing tasks, including within a single image (e.g., object moving, resizing, and content dragging) or across images (e.g., appearance replacing and object pasting). Code is available at https://github.com/MC-E/DragonDiffusion.</div>
</div>
<div class='paper-counter'>150/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongtao Wu;Ya Jing;Chilam Cheang;Guangzeng Chen;Jiafeng Xu;Xinghang Li;Minghuan Liu;Hang Li;Tao Kong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Robot Manipulation;Video Generative Pre-Training;Causal Transformer</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/aaad6a41f63d34aa186ee9635d74c908044d2aec.pdf" target="_blank">https://openreview.net/pdf/aaad6a41f63d34aa186ee9635d74c908044d2aec.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io</div>
</div>
<div class='paper-counter'>151/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Jointly Understand Visual and Tactile Signals</div>
<div class="field-name">authors:</div>
<div class="field-value">Yichen Li;Yilun Du;Chao Liu;Chao Liu;Francis Williams;Michael Foshey;Benjamin Eckart;Jan Kautz;Joshua B. Tenenbaum;Antonio Torralba;Wojciech Matusik</div>
<div class="field-name">keywords:</div>
<div class="field-value">multi-modal perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">visualization or interpretation of learned representations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c135df862286581108dd47d811ee802f77b44aad.pdf" target="_blank">https://openreview.net/pdf/c135df862286581108dd47d811ee802f77b44aad.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7cc44733138b8f1f8a3904dacf33d217fdbcc6df.pdf" target="_blank">https://openreview.net/attachment/7cc44733138b8f1f8a3904dacf33d217fdbcc6df.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Modeling and analyzing object and shape has been well studied in the past. However, manipulation of these complex tools and articulated objects remains difficult for autonomous agents. Our human hands, however, are dexterous and adaptive. We can easily adapt a manipulation skill on one object to all objects in the class and to other similar classes. Our intuition comes from that there is a close connection between manipulations and topology and articulation of objects. The possible articulation of objects indicates the types of manipulation necessary to operate the object. In this work, we aim to take a manipulation perspective to understand everyday objects and tools. We collect a multi-modal visual-tactile dataset that contains paired full-hand force pressure maps and manipulation videos. We also propose a novel method to learn a cross-modal latent manifold that allow for cross-modal prediction and discovery of latent structure in different data modalities. We conduct extensive experiments to demonstrate the effectiveness of our method.</div>
</div>
<div class='paper-counter'>152/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</div>
<div class="field-name">authors:</div>
<div class="field-value">Sipeng Zheng;jiazheng liu;Yicheng Feng;Zongqing Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">large multimodal pre-training;open-world embodied agent;large language model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf" target="_blank">https://openreview.net/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.</div>
</div>
<div class='paper-counter'>153/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Replay across Experiments: A Natural Extension of Off-Policy RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Dhruva Tirumala;Thomas Lampe;Jose Enrique Chen;Tuomas Haarnoja;Sandy Huang;Guy Lever;Ben Moran;Tim Hertweck;Leonard Hasenclever;Martin Riedmiller;Nicolas Heess;Markus Wulfmeier</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;robotics;data transfer</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/92dfcf81b3ef440f1e519a9a7b48c3bdcd93046f.pdf" target="_blank">https://openreview.net/pdf/92dfcf81b3ef440f1e519a9a7b48c3bdcd93046f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL).We present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times.At its core, Replay across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work. We empirically show benefits across a number of RL algorithms and challenging control domains spanning both locomotion and manipulation, including hard exploration tasks from egocentric vision. Through comprehensive ablations, we demonstrate  robustness to the quality and amount of data available and various hyperparameter choices. Finally, we discuss how our approach can be applied more broadly across research life cycles and can increase resilience by reloading data across random seeds or hyperparameter variations.</div>
</div>
<div class='paper-counter'>154/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GIM: Learning Generalizable Image Matcher From Internet Videos</div>
<div class="field-name">authors:</div>
<div class="field-value">Xuelun Shen;zhipeng cai;Wei Yin;Matthias Müller;Zijun Li;Kaixuan Wang;Xiaozhi Chen;Cheng Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Matching;Pose Estimation;3D Reconstruction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c51ae05af771bb0017771606dc8f8cc514003720.pdf" target="_blank">https://openreview.net/pdf/c51ae05af771bb0017771606dc8f8cc514003720.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose the first framework to learn a single generalizable image matching model from internet videos.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-the-art image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 6.9% − 18.1%. GIM also enables generalization to extreme cross-domain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.</div>
</div>
<div class='paper-counter'>155/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hindsight PRIORs for Reward Learning from Human Preferences</div>
<div class="field-name">authors:</div>
<div class="field-value">Mudit Verma;Katherine Metcalf</div>
<div class="field-name">keywords:</div>
<div class="field-value">preference based reinforcement learning;world models;return redistribution</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/83a2844bd7349c71ea3533dc2bea9b7fa6d0018f.pdf" target="_blank">https://openreview.net/pdf/83a2844bd7349c71ea3533dc2bea9b7fa6d0018f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/17208f20711a518faba9e198a3f5c6e5d2bcac0c.pdf" target="_blank">https://openreview.net/attachment/17208f20711a518faba9e198a3f5c6e5d2bcac0c.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Presents a method to address credit assignment problem in preference-based reinforcement learning by guiding rewards to key states according to relative state importance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference resulting in data intensive approaches and subpar reward models. We address such limitations by introducing a credit assignment strategy (PRIOR) that uses a forward dynamics world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, PRIOR achieves 80% success rate with half the amount of data compared to baselines. The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision.</div>
</div>
<div class='paper-counter'>156/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DittoGym: Learning to Control Soft Shape-Shifting Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">Suning Huang;Boyuan Chen;Huazhe Xu;Vincent Sitzmann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforcement Learning;Soft Robot</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b93409a86b32391e7fbd38e4a0628ad80d597cbb.pdf" target="_blank">https://openreview.net/pdf/b93409a86b32391e7fbd38e4a0628ad80d597cbb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/18886a8879866926e1eb7dfe4771dc70e1541e26.zip" target="_blank">https://openreview.net/attachment/18886a8879866926e1eb7dfe4771dc70e1541e26.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop algorithm and benchmark for reconfigurable robots, who can dynamically change their fine-grained morphology to accomplish a task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine algorithm on DittoGym,  and demonstrate robots that learn to change their morphology several times within a sequence, uniquely enabled by our RL algorithm. More results are available at https://dittogym.github.io.</div>
</div>
<div class='paper-counter'>157/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Act from Actionless Videos through Dense Correspondences</div>
<div class="field-name">authors:</div>
<div class="field-value">Po-Chen Ko;Jiayuan Mao;Yilun Du;Shao-Hua Sun;Joshua B. Tenenbaum</div>
<div class="field-name">keywords:</div>
<div class="field-value">Video-Based Policy;Video Dense Correspondence</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fff0f1d4e51a3d9d660e98a91daabc1b70364dbd.pdf" target="_blank">https://openreview.net/pdf/fff0f1d4e51a3d9d660e98a91daabc1b70364dbd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7116e5a638e853835dbcc742ebb57b3d31a4aa97.zip" target="_blank">https://openreview.net/attachment/7116e5a638e853835dbcc742ebb57b3d31a4aa97.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We developed a robot policy using images, eliminating action annotations. It's trained on RGB videos, effective for table-top tasks and navigation. Our framework allows rapid modeling with just 4 GPUs in a day.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that "hallucinate" robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.</div>
</div>
<div class='paper-counter'>158/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Guowei Xu;Ruijie Zheng;Yongyuan Liang;Xiyao Wang;Zhecheng Yuan;Tianying Ji;Yu Luo;Xiaoyu Liu;Jiaxin Yuan;Pu Hua;Shuzhen Li;Yanjie Ze;Hal Daumé III;Furong Huang;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual RL; Dormant Ratio</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1c921e84904e3fe06d5ed433201fa0fb2d619046.pdf" target="_blank">https://openreview.net/pdf/1c921e84904e3fe06d5ed433201fa0fb2d619046.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">DrM, a visual RL algorithm, minimizes the dormant ratio to guide exploration-exploitation trade-offs, achieving significant improvements in sample efficiency and asymptotic performance across diverse domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual reinforcement learning (RL) has shown promise in continuous control tasks.Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds.In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks.To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network.Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals.Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that  DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit.Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.</div>
</div>
<div class='paper-counter'>159/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianyu Li;Peijin Jia;Bangjun Wang;Li Chen;KUN JIANG;Junchi Yan;Hongyang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Laneline detection;autonomous driving;topology reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c017406a4e6d34007982fc886c82d80eb20d9ac1.pdf" target="_blank">https://openreview.net/pdf/c017406a4e6d34007982fc886c82d80eb20d9ac1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, i.e., map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS. Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.</div>
</div>
<div class='paper-counter'>160/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Liyiming Ke;Yunchu Zhang;Abhay Deshpande;Siddhartha Srinivasa;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;data augmentation;robotics;model-based method</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3046b5613b6d1f752c67bf106be8c190923d066c.pdf" target="_blank">https://openreview.net/pdf/3046b5613b6d1f752c67bf106be8c190923d066c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9cd7e0095416c3afe1c5bc732cd7337ca18915e8.pdf" target="_blank">https://openreview.net/attachment/9cd7e0095416c3afe1c5bc732cd7337ca18915e8.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We generate corrective labels for imitation learning by leveraging the presence of local continuity in the dynamics function.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding error and disturbances. While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond expert data. The key insight is to leverage local continuity in the environment dynamics. Our method first constructs a dynamics model from the expert demonstration, enforcing local Lipschitz continuity while skipping the discontinuous regions. In the locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset. Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding error. We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains that have distinct forms of continuity and discontinuity, including classic control, drone flying, high-dimensional navigation, locomotion, and tabletop manipulation.</div>
</div>
<div class='paper-counter'>161/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Thin-Shell Object Manipulations With Differentiable Physics Simulations</div>
<div class="field-name">authors:</div>
<div class="field-value">Yian Wang;Juntian Zheng;Zhehuan Chen;Zhou Xian;Gu Zhang;Chao Liu;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">differentiable physics simulation;thin-shell object manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/edbd3be2c1ca4369cdf41d2d892af284a0c21cc3.pdf" target="_blank">https://openreview.net/pdf/edbd3be2c1ca4369cdf41d2d892af284a0c21cc3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we aim to teach robots to manipulate various thin-shell materials. Prior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks.On the other hand, while virtual simulations are shown to be effective in diverse robot skill learning and evaluation, prior thin-shell simulation environments only support a subset of thin-shell materials, which also limits their supported range of tasks. To fill in this gap, we introduce ThinShellLab - a fully differentiable simulation platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Building on top of our developed simulation engine, we design a diverse set of manipulation tasks centered around different thin-shell objects. Our experiments suggest that manipulating thin-shell objects presents several unique challenges: 1) thin-shell manipulation relies heavily on frictional forces due to the objects' co-dimensional nature, 2) the materials being manipulated are highly sensitive to minimal variations in interaction actions, and 3) the constant and frequent alteration in contact pairs makes trajectory optimization methods susceptible to local optima, and neither standard reinforcement learning algorithms nor trajectory optimization methods (either gradient-based or gradient-free) are able to solve the tasks alone. To overcome these challenges, we present an optimization scheme that couples sampling-based trajectory optimization and gradient-based optimization, boosting both learning efficiency and converged performance across various proposed tasks. In addition, the differentiable nature of our platform facilitates a smooth sim-to-real transition. By tuning simulation parameters with a minimal set of real-world data, we demonstrate successful deployment of the learned skills to real-robot settings.  ThinShellLab will be publicly available. Video demonstration and more information can be found on the project website https://vis-www.cs.umass.edu/ThinShellLab/.</div>
</div>
<div class='paper-counter'>162/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">Yoonyoung Cho;Junhyek Han;Yoontae Cho;Beomjoon Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">pretraining;robotics;manipulation;object representation;representation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/be6d29e6e7d18c8ea0250289f353011374d395b1.pdf" target="_blank">https://openreview.net/pdf/be6d29e6e7d18c8ea0250289f353011374d395b1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7b9d6b575b4cf1f40ee33003886bcdc8106880b7.zip" target="_blank">https://openreview.net/attachment/7b9d6b575b4cf1f40ee33003886bcdc8106880b7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">contact-based representation learning for nonprehensile robotic manipulation on objects with general geometry, with zero-shot real-world transfer.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object geometry is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based transformer architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by zero-shot transferring the trained policy to novel real-world objects. We highly recommend the video attached in the supplementary material. Code and videos are available at \url{https://sites.google.com/view/contact-non-prehensile}.</div>
</div>
<div class='paper-counter'>163/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Weiran Yao;Shelby Heinecke;Juan Carlos Niebles;Zhiwei Liu;Yihao Feng;Le Xue;Rithesh R N;Zeyuan Chen;Jianguo Zhang;Devansh Arpit;Ran Xu;Phil L Mui;Huan Wang;Caiming Xiong;Silvio Savarese</div>
<div class="field-name">keywords:</div>
<div class="field-value">Language Agent;AI Agent;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf" target="_blank">https://openreview.net/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.</div>
</div>
<div class='paper-counter'>164/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">Fuxiao Liu;Kevin Lin;Linjie Li;Jianfeng Wang;Yaser Yacoob;Lijuan Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">instruction tuning;multimodal large language model;hallucination;datasets</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a7dc45dbafdb5c8be70b9ae7f987f44a08b62885.pdf" target="_blank">https://openreview.net/pdf/a7dc45dbafdb5c8be70b9ae7f987f44a08b62885.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visualinstructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several publicdatasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data will be released upon publication.</div>
</div>
<div class='paper-counter'>165/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robot Fleet Learning via Policy Merging</div>
<div class="field-name">authors:</div>
<div class="field-value">Lirui Wang;Kaiqing Zhang;Allan Zhou;Max Simchowitz;Russ Tedrake</div>
<div class="field-name">keywords:</div>
<div class="field-value">Fleet Learning;Weight Merging;Multi-task Policy Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4753844551f66dbe5c093bd4d98a2905c14da138.pdf" target="_blank">https://openreview.net/pdf/4753844551f66dbe5c093bd4d98a2905c14da138.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We investigated a "bottom-up" approach to learn robotic policies from a fleet of robots, by leveraging weight merging, and the method demonstrates strong results on settings such as linear control, Meta-World, and a novel robotic tool-use benchmark.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate policy merging (PoMe) from such distributed heterogeneous datasets as a potential solution. To efficiently merge policies in the fleet setting, we propose FLEET-MERGE, an instantiation of distributed learning that accounts for the permutation invariance that arises when parameterizing the control policies with recurrent neural networks. We show that FLEET-MERGE consolidates the behavior of policies trained on 50 tasks in the Meta-World environment, with good performance on nearly all training tasks at test time. Moreover, we introduce a novel robotic tool-use benchmark, FLEET-TOOLS, for fleet policy learning in compositional and contact-rich robot manipulation tasks, to validate the efficacy of FLEET-MERGE on the benchmark.</div>
</div>
<div class='paper-counter'>166/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Eureka: Human-Level Reward Design via Coding Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Yecheng Jason Ma;William Liang;Guanzhi Wang;De-An Huang;Osbert Bastani;Dinesh Jayaraman;Yuke Zhu;Linxi Fan;Anima Anandkumar</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Reinforcement Learning;Dexterous Manipulation;Reward Learning;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6c6607629a103a06c7c1b52817845f25aa866b8b.pdf" target="_blank">https://openreview.net/pdf/6c6607629a103a06c7c1b52817845f25aa866b8b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.</div>
</div>
<div class='paper-counter'>167/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Qianxu Wang;Haotong Zhang;Congyue Deng;Yang You;Hao Dong;Yixin Zhu;Leonidas Guibas</div>
<div class="field-name">keywords:</div>
<div class="field-value">dexterous manipulaiton;one-shot manipulation transfer;distilled feature field;implicit field</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/29f3e3a8184f5b0229fd686e04fc8b7cddc0cd5a.pdf" target="_blank">https://openreview.net/pdf/29f3e3a8184f5b0229fd686e04fc8b7cddc0cd5a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2ca43d96c9b6b70dfa12130b3083fffc63fa7e25.zip" target="_blank">https://openreview.net/attachment/2ca43d96c9b6b70dfa12130b3083fffc63fa7e25.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans demonstrate remarkable skill in transferring manipulation abilities across objects of varying shapes, poses, and appearances, a capability rooted in their understanding of semantic correspondences between different instances. To equip robots with a similar high-level comprehension, we present SparseDFF, a novel DFF for 3D scenes utilizing large 2D vision models to extract semantic features from sparse RGBD images, a domain where research is limited despite its relevance to many tasks with fixed-camera setups. SparseDFF generates view-consistent 3D DFFs, enabling efficient one-shot learning of dexterous manipulations by mapping image features to a 3D point cloud. Central to SparseDFF is a feature refinement network, optimized with a contrastive loss between views and a point-pruning mechanism for feature continuity. This facilitates the minimization of feature discrepancies w.r.t. end-effector parameters, bridging demonstrations and target manipulations. Validated in real-world scenarios with a dexterous hand, SparseDFF proves effective in manipulating both rigid and deformable objects, demonstrating significant generalization capabilities across object and scene variations.</div>
</div>
<div class='paper-counter'>168/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Tree-Planner: Efficient Close-loop Task Planning with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Mengkang Hu;Yao Mu;Xinmiao Chelsey Yu;Mingyu Ding;Shiguang Wu;Wenqi Shao;Qiguang Chen;Bin Wang;Yu Qiao;Ping Luo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Agents;Large Language Models;Task Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9dc006199def05d273b6f27cca8e7d2dff14b68d.pdf" target="_blank">https://openreview.net/pdf/9dc006199def05d273b6f27cca8e7d2dff14b68d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/fccc835b414762bea35ce317949151d5d4e8b760.pdf" target="_blank">https://openreview.net/attachment/fccc835b414762bea35ce317949151d5d4e8b760.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations.Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness.However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling,  action tree construction, and grounded deciding.Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree.Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information.Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency.By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls,a considerable partof the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2\% compared to the previously best-performing model.Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\% decrease in error corrections.</div>
</div>
<div class='paper-counter'>169/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Domain Randomization via Entropy Maximization</div>
<div class="field-name">authors:</div>
<div class="field-value">Gabriele Tiboni;Pascal Klink;Jan Peters;Tatiana Tommasi;Carlo D'Eramo;Georgia Chalvatzaki</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Sim-to-Real Transfer;Domain Randomization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/82e4dc0af4ef7b9cb3c8a2b5ebb82ca0b5e5e229.pdf" target="_blank">https://openreview.net/pdf/82e4dc0af4ef7b9cb3c8a2b5ebb82ca0b5e5e229.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/abef0c510604558cca92321bac5aeb636783a430.zip" target="_blank">https://openreview.net/attachment/abef0c510604558cca92321bac5aeb636783a430.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel approach for sim-to-real transfer with domain randomization that directly maximizes the entropy of the dynamics distribution during training, while retaining convergence and generalization capabilities.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empirically validate the consistent benefits of DORAEMON in obtaining highly adaptive and generalizable policies, i.e. solving the task at hand across the widest range of dynamics parameters, as opposed to representative baselines from the DR literature. Notably, we also demonstrate the Sim2Real applicability of DORAEMON through its successful zero-shot transfer in a robotic manipulation setup under unknown real-world parameters.</div>
</div>
<div class='paper-counter'>170/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Neural-Symbolic Recursive Machine for Systematic Generalization</div>
<div class="field-name">authors:</div>
<div class="field-value">Qing Li;Yixin Zhu;Yitao Liang;Ying Nian Wu;Song-Chun Zhu;Siyuan Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neuro-symbolic AI;Systematic Generalization;Compositional Generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/440caa44d54237dd2cb02bb7f536e9fdfceadfba.pdf" target="_blank">https://openreview.net/pdf/440caa44d54237dd2cb02bb7f536e9fdfceadfba.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/53b47e622163d9c5441cd1a32768c5f57e6f67e8.zip" target="_blank">https://openreview.net/attachment/53b47e622163d9c5441cd1a32768c5f57e6f67e8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Neural-Symbolic Recursive Machine for systematic generalization, which achieves state-of-the-art performance on SCAN, PCFG, and HINT.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current learning models often struggle with human-like systematic generalization, particularly in learning compositional rules from limited data and extrapolating them to novel combinations. We introduce the Neural-Symbolic Recursive Ma- chine ( NSR), whose core is a Grounded Symbol System ( GSS), allowing for the emergence of combinatorial syntax and semantics directly from training data. The NSR employs a modular design that integrates neural perception, syntactic parsing, and semantic reasoning. These components are synergistically trained through a novel deduction-abduction algorithm. Our findings demonstrate that NSR’s design, imbued with the inductive biases of equivariance and compositionality, grants it the expressiveness to adeptly handle diverse sequence-to-sequence tasks and achieve unparalleled systematic generalization. We evaluate NSR’s efficacy across four challenging benchmarks designed to probe systematic generalization capabilities: SCAN for semantic parsing, PCFG for string manipulation, HINT for arithmetic reasoning, and a compositional machine translation task. The results affirm NSR ’s superiority over contemporary neural and hybrid models in terms of generalization and transferability.</div>
</div>
<div class='paper-counter'>171/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ZeroFlow: Scalable Scene Flow via Distillation</div>
<div class="field-name">authors:</div>
<div class="field-value">Kyle Vedder;Neehar Peri;Nathaniel Eliot Chodosh;Ishan Khatri;ERIC EATON;Dinesh Jayaraman;Yang Liu;Deva Ramanan;James Hays</div>
<div class="field-name">keywords:</div>
<div class="field-value">Scene Flow;Distillation;Scaling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fcb34199f731db7fefa1f87e8c73d5b131c23f9d.pdf" target="_blank">https://openreview.net/pdf/fcb34199f731db7fefa1f87e8c73d5b131c23f9d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3959386584dd451079ad7fa7b74a5f99a381d810.pdf" target="_blank">https://openreview.net/attachment/3959386584dd451079ad7fa7b74a5f99a381d810.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a scalable, human annotation-free distillation pipeline that captures state-of-the-art by leveraging raw data.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds to process full-size point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feedforward methods are considerably faster, running on the order of tens to hundreds of milliseconds for full-size point clouds, but require expensive human supervision. To address both limitations, we propose _Scene Flow via Distillation_, a simple, scalable distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feedforward model. Our instantiation of this framework, _ZeroFlow_, achieves **state-of-the-art** performance on the _Argoverse 2 Self-Supervised Scene Flow Challenge_ while using zero human labels by simply training on large-scale, diverse unlabeled data. At test-time, ZeroFlow is over 1000$\times$ faster than label-free state-of-the-art optimization-based methods on full-size point clouds (34 FPS vs 0.028 FPS) and over 1000$\times$ cheaper to train on unlabeled data compared to the cost of human annotation (\\$394 vs ~\\$750,000). To facilitate further research, we will release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets.</div>
</div>
<div class='paper-counter'>172/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiayuan Gu;Sean Kirmani;Paul Wohlhart;Yao Lu;Montserrat Gonzalez Arenas;Kanishka Rao;Wenhao Yu;Chuyuan Fu;Keerthana Gopalakrishnan;Zhuo Xu;Priya Sundaresan;Peng Xu;Hao Su;Karol Hausman;Chelsea Finn;Quan Vuong;Ted Xiao</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;robot learning;robot manipulation;task representation;behavior cloning;multitask imitation learning;goal conditioning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/99c49fe414f0c5349b9a1f94d32198a847626df5.pdf" target="_blank">https://openreview.net/pdf/99c49fe414f0c5349b9a1f94d32198a847626df5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7b13ad7cc32db0ddb255c91a260ba8c4536f510a.zip" target="_blank">https://openreview.net/attachment/7b13ad7cc32db0ddb255c91a260ba8c4536f510a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies -- they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.</div>
</div>
<div class='paper-counter'>173/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Privileged Sensing Scaffolds Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Edward S. Hu;James Springer;Oleh Rybkin;Dinesh Jayaraman</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;model-based reinforcement learning;world models;robotics;privileged information;asymmetric learning;multimodality;perception;sensing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf" target="_blank">https://openreview.net/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study how privileged, training-time only observation streams can aid skill learning, and instantiate a MBRL algorithm that incorporates privileged sensing into all auxiliary, training-time components of RL to better train the policy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/</div>
</div>
<div class='paper-counter'>174/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Building Cooperative Embodied Agents Modularly with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongxin Zhang;Weihua Du;Jiaming Shan;Qinhong Zhou;Yilun Du;Joshua B. Tenenbaum;Tianmin Shu;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Embodied Intelligence;Multi-Agent Cooperation;Human-AI Interaction;Communication</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf" target="_blank">https://openreview.net/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1731274f5896d9b18709ef40c49c8d05cd933277.zip" target="_blank">https://openreview.net/attachment/1731274f5896d9b18709ef40c49c8d05cd933277.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present CoELA, a modular framework integrating LLMs to address the challenging multi-agent embodied cooperation problem with decentralized control, costly communication, and long-horizon multi-objective tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.</div>
</div>
<div class='paper-counter'>175/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching</div>
<div class="field-name">authors:</div>
<div class="field-value">Aleksandar Makelov;Georg Lange;Atticus Geiger;Neel Nanda</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mechanistic Interpretability;Natural Language Processing;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">visualization or interpretation of learned representations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/41767a08ea94dbfa362b4807a25ed21035e4dc0e.pdf" target="_blank">https://openreview.net/pdf/41767a08ea94dbfa362b4807a25ed21035e4dc0e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e86712c084ae1d55827c06b9609e246f76c653d5.zip" target="_blank">https://openreview.net/attachment/e86712c084ae1d55827c06b9609e246f76c653d5.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show how activation patching can hallucinate meaningful subspaces in a language model by activating dormant pathways.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and manipulation of such subspaces to reverse-engineer computations, employing methods such as activation patching. In this work, we demonstrate that naïve approaches to subspace interventions can give rise to interpretability illusions.Specifically, even if patching along a subspace has the intended end-to-end causal effect on model behavior, this effect may be achieved by activating \emph{a dormant parallel pathway} using a component that is \textit{causally disconnected} from the model output.We demonstrate this in a mathematical example, realize the example empirically in two different settings (the Indirect Object Identification (IOI) task and factual recall), and argue that activating dormant pathways ought to be prevalent in practice.In the context of factual recall, we further show that the illusion is related to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localisation.However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability.To contextualize our findings, we also show what a success case looks like in a task (IOI) where prior manual circuit analysis allows an understanding of the location of the ground truth feature. We explore the additional evidence needed to argue that a patched subspace is faithful.</div>
</div>
<div class='paper-counter'>176/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">What Matters to You? Towards Visual Representation Alignment for Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Thomas Tian;Chenfeng Xu;Masayoshi Tomizuka;Jitendra Malik;Andrea Bajcsy</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot learning;Preference learning;Visual reward learning;Representation alignment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b089a2d0ee33f551f8ee252854a9a7630830fe59.pdf" target="_blank">https://openreview.net/pdf/b089a2d0ee33f551f8ee252854a9a7630830fe59.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/77faf5f630a4bdb20c92744b30a196eb0f268ffd.zip" target="_blank">https://openreview.net/attachment/77faf5f630a4bdb20c92744b30a196eb0f268ffd.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human’s input on what matters to them, exacerbating spurious correlations and ultimately leading to behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X MAGICAL and in robotic manipulation, we find that RAPL’s reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot’s.</div>
</div>
<div class='paper-counter'>177/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images</div>
<div class="field-name">authors:</div>
<div class="field-value">Kuofeng Gao;Yang Bai;Jindong Gu;Shu-Tao Xia;Philip Torr;Zhifeng Li;Wei Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">energy-latency manipulation; large vision-language model; verbose images</div>
<div class="field-name">primary_area:</div>
<div class="field-value">societal considerations including fairness, safety, privacy</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/803c585a3244829aa3083b6aa8aef4fb0f49d3e7.pdf" target="_blank">https://openreview.net/pdf/803c585a3244829aa3083b6aa8aef4fb0f49d3e7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d29d870ff35ed06735c91038fbbe061eac3094f0.zip" target="_blank">https://openreview.net/attachment/d29d870ff35ed06735c91038fbbe061eac3094f0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87× and 8.56× compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications.</div>
</div>
<div class='paper-counter'>178/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Jae-Woo Choi;Youngwoo Yoon;Hyobin Ong;Jaehong Kim;Minsu Jang</div>
<div class="field-name">keywords:</div>
<div class="field-value">task planning;language models;benchmarking;embodied agents;home robots</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf" target="_blank">https://openreview.net/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.</div>
</div>
<div class='paper-counter'>179/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Course Correcting Koopman Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Mahan Fathi;Clement Gehring;Jonathan Pilault;David Kanaa;Pierre-Luc Bacon;Ross Goroshin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Koopman;Autoencoders;Dynamical Systems;Sequence Modeling;Inference-time Methods;Planning;Unsupervised Learning;Representation Learning;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/05d5db50fb619d1e0cd888acf43943df4f6fee0d.pdf" target="_blank">https://openreview.net/pdf/05d5db50fb619d1e0cd888acf43943df4f6fee0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8bf353eea7668c39a57b740083ffac9e690d322e.pdf" target="_blank">https://openreview.net/attachment/8bf353eea7668c39a57b740083ffac9e690d322e.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use Periodic Reencoding as means for correcting the drift of Koopman representations at inference time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.</div>
</div>
<div class='paper-counter'>180/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Video Language Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yilun Du;Sherry Yang;Pete Florence;Fei Xia;Ayzaan Wahid;brian ichter;Pierre Sermanet;Tianhe Yu;Pieter Abbeel;Joshua B. Tenenbaum;Leslie Pack Kaelbling;Andy Zeng;Jonathan Tompson</div>
<div class="field-name">keywords:</div>
<div class="field-value">Planning;Hierarchical Planning;Language Models;Video Models;Long-Horizon Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8f50ade9fed740956ff458c9aaea5d5130948eb7.pdf" target="_blank">https://openreview.net/pdf/8f50ade9fed740956ff458c9aaea5d5130948eb7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a way to solve long horizon decision making tasks by planning through the space of video and language.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data.  To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains -- from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).</div>
</div>
<div class='paper-counter'>181/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuanwen Yue;Sabarinath Mahadevan;Jonas Schult;Francis Engelmann;Bastian Leibe;Konrad Schindler;Theodora Kontogianni</div>
<div class="field-name">keywords:</div>
<div class="field-value">interactive segmentation;3D instance segmentation;point cloud;attention</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/99ef6b1d04aa3c1bd588be1974d4beabdb5d9d1c.pdf" target="_blank">https://openreview.net/pdf/99ef6b1d04aa3c1bd588be1974d4beabdb5d9d1c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7f29a60c66bdc5881f680f613bc3c921576b6f6c.zip" target="_blank">https://openreview.net/attachment/7f29a60c66bdc5881f680f613bc3c921576b6f6c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">During interactive segmentation, a model and a user work together to delineate objects of interest in a 3D point cloud. In an iterative process, the model assigns each data point to an object (or the background), while the user corrects errors in the resulting segmentation and feeds them back into the model. The current best practice formulates the problem as binary classification and segments objects one at a time. The model expects the user to provide positive clicks to indicate regions wrongly assigned to the background and negative clicks on regions wrongly assigned to the object. Sequentially visiting objects is wasteful since it disregards synergies between objects: a positive click for a given object can, by definition, serve as a negative click for nearby objects. Moreover, a direct competition between adjacent objects can speed up the identification of their common boundary. We introduce AGILE3D, an efficient, attention-based model that (1) supports simultaneous segmentation of multiple 3D objects, (2) yields more accurate segmentation masks with fewer user clicks, and (3) offers faster inference. Our core idea is to encode user clicks as spatial-temporal queries and enable explicit interactions between click queries as well as between them and the 3D scene through a click attention module. Every time new clicks are added, we only need to run a lightweight decoder that produces updated segmentation masks. In experiments with four different 3D point cloud datasets, AGILE3D sets a new state-of-the-art. Moreover, we also verify its practicality in real-world setups with real user studies. Project page: https://ywyue.github.io/AGILE3D.</div>
</div>
<div class='paper-counter'>182/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</div>
<div class="field-name">authors:</div>
<div class="field-value">Izzeddin Gur;Hiroki Furuta;Austin V Huang;Mustafa Safdari;Yutaka Matsuo;Douglas Eck;Aleksandra Faust</div>
<div class="field-name">keywords:</div>
<div class="field-value">Web Navigation;Web Automation;Large Language Models;Language Model Agents;Tool Use;Program Synthesis</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf" target="_blank">https://openreview.net/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a modular language model agents for real-world web automation by leveraging the capability of multi-step planning, long context understanding, and program synthesis in LLMs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.</div>
</div>
<div class='paper-counter'>183/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF</div>
<div class="field-name">authors:</div>
<div class="field-value">JangHo Park;Gihyun Kwon;Jong Chul Ye</div>
<div class="field-name">keywords:</div>
<div class="field-value">NeRF;Diffusion model;3D scene editing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/205bd118d65762aad19ebc54c6a767c9ea6e0a95.pdf" target="_blank">https://openreview.net/pdf/205bd118d65762aad19ebc54c6a767c9ea6e0a95.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2c94c50c0d663aed124b968c3da8d7c433667995.zip" target="_blank">https://openreview.net/attachment/2c94c50c0d663aed124b968c3da8d7c433667995.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into  NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this,  here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models.</div>
</div>
<div class='paper-counter'>184/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior</div>
<div class="field-name">authors:</div>
<div class="field-value">Kai Cui;Sascha H. Hauck;Christian Fabian;Heinz Koeppl</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mean Field Control;Multi-Agent Reinforcement Learning;Partial Observability;Collective Behavior</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c1987683d13350c251b90658f2a255e3225d0716.pdf" target="_blank">https://openreview.net/pdf/c1987683d13350c251b90658f2a255e3225d0716.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a MARL algorithm via MFC that scales cooperative problems to many agents and partial information by reducing to a single-agent MDP, with many applications in designing collective behavior.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical results, including a dynamic programming principle, together with optimality guarantees for Dec-POMFC solutions applied to finite swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient methods for MARL via centralized training and decentralized execution, together with policy gradient approximation guarantees. In addition, we improve upon state-of-the-art histogram-based MFC by kernel methods, which is of separate interest also for fully observable MFC. We evaluate numerically on representative collective behavior tasks such as adapted Kuramoto and Vicsek swarming models, being on par with state-of-the-art MARL. Overall, our framework takes a step towards RL-based engineering of artificial collective behavior via MFC.</div>
</div>
<div class='paper-counter'>185/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response</div>
<div class="field-name">authors:</div>
<div class="field-value">Junfeng Long;ZiRui Wang;Quanyi Li;Liu Cao;Jiawei Gao;Jiangmiao Pang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Quadrupedal Locomotion;Internal Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7ebfa7daae934eacc0cd05b7ee6107d2ac5b30dd.pdf" target="_blank">https://openreview.net/pdf/7ebfa7daae934eacc0cd05b7ee6107d2ac5b30dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the classical Internal Model Control principle, we consider these external states as disturbances and introduce Hybrid Internal Model (HIM) to estimate them according to the response of the robot. The response, which we refer to as the hybrid internal embedding, contains the robot’s explicit velocity and implicit stability representation, corresponding to two primary goals for locomotion tasks: explicitly tracking velocity and implicitly maintaining stability. We use contrastive learning to optimize the embedding to be close to the robot’s successor state, in which the response is naturally embedded. HIM has several appealing benefits: It only needs the robot’s proprioceptions, i.e., those from joint encoders and IMU as observations. It innovatively maintains consistent observations between simulation reference and reality that avoids information loss in mimicking learning. It exploits batch-level information that is more robust to noises and keeps better sample efficiency. It only requires 1 hour of training on an RTX 4090 to enable a quadruped robot to traverse any terrain under any disturbances. A wealth of real-world experiments demonstrates its agility, even in high-difficulty tasks and cases never occurred during the training process, revealing remarkable open-world generalizability.</div>
</div>
<div class='paper-counter'>186/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction</div>
<div class="field-name">authors:</div>
<div class="field-value">Guillaume Bono;Leonid Antsfeld;Assem Sadek;Gianluca Monaci;Christian Wolf</div>
<div class="field-name">keywords:</div>
<div class="field-value">Navigation;Embodied AI;Perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf" target="_blank">https://openreview.net/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f84a134b21d0b098ddf46d3a14fdee05504d4afc.zip" target="_blank">https://openreview.net/attachment/f84a134b21d0b098ddf46d3a14fdee05504d4afc.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Instead of learning to reconstruct, we cast the robotic perception task as a navigation task by a blind auxiliary agent generating a learning signal for the main agent</div>
<div class="field-name">abstract:</div>
<div class="field-value">Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.</div>
</div>
<div class='paper-counter'>187/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhaoyi Zhou;Chuning Zhu;Runlong Zhou;Qiwen Cui;Abhishek Gupta;Simon Shaolei Du</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Reinforcement Learning;Return-Conditioned Supervised Learning;Bellman Completeness;Trajectory Stitching</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/92f088b50f95913a055282fd19d3ad41911fe50f.pdf" target="_blank">https://openreview.net/pdf/92f088b50f95913a055282fd19d3ad41911fe50f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We analyze advantage of return-conditioned supervised learning in near-deterministic environments and improves return-conditioned supervised learning to enable trajectory stitching.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be important in sequential decision-making problems. In the presence of function approximation, however, these techniques often diverge due to the absence of Bellman completeness in the function classes considered, a crucial condition for the success of DP-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning (RCSL) are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow *linearly* with the state space size to satisfy Bellman completeness while a constant layer width is enough for RCSL. These findings take a step towards explaining the superior empirical performance of RCSL methods compared to DP-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories. MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness that plagues all dynamic programming algorithms. We propose both theoretical analysis and experimental evaluation to back these claims, outperforming state-of-the-art model-free and model-based offline RL algorithms across several simulated robotics problems.</div>
</div>
<div class='paper-counter'>188/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Online Continual Learning for Interactive Instruction Following Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Byeonghwi Kim;Minhyuk Seo;Jonghyun Choi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Continual Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf" target="_blank">https://openreview.net/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.</div>
</div>
<div class='paper-counter'>189/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaogang Jia;Denis Blessing;Xinkai Jiang;Moritz Reuss;Atalay Donat;Rudolf Lioutikov;Gerhard Neumann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Benchmark;Datasets;Diverse Behaviors</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2f3fdf0514ecf5071b833dfeb74b36176a0de0db.pdf" target="_blank">https://openreview.net/pdf/2f3fdf0514ecf5071b833dfeb74b36176a0de0db.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/cd40700eabe3ab1ed74ee745d330703687a0bd4f.zip" target="_blank">https://openreview.net/attachment/cd40700eabe3ab1ed74ee745d330703687a0bd4f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A simulation benchmark with diverse human demonstrations, and tractable metrics to evaluate imitation learning's ability to capture multi-modal behavior.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills. However, the inherent diversity in human behavior leads to the emergence of multi-modal data distributions, thereby presenting a formidable challenge for existing imitation learning algorithms. Quantifying a model's capacity to capture and replicate this diversity effectively is still an open problem. In this work, we introduce simulation benchmark environments and the corresponding *Datasets with Diverse human Demonstrations for Imitation Learning (D3IL)*, designed explicitly to evaluate a model's ability to learn multi-modal behavior. Our environments are designed to involve multiple sub-tasks that need to be solved, consider manipulation of multiple objects which increases the diversity of the behavior and can only be solved by policies that rely on closed loop sensory feedback. Other available datasets are missing at least one of these challenging properties.To address the challenge of diversity quantification, we introduce tractable metrics that provide valuable insights into a model's ability to acquire and reproduce diverse behaviors. These metrics offer a practical means to assess the robustness and versatility of imitation learning algorithms. Furthermore, we conduct a thorough evaluation of state-of-the-art methods on the proposed task suite. This evaluation serves as a benchmark for assessing their capability to learn diverse behaviors. Our findings shed light on the effectiveness of these methods in tackling the intricate problem of capturing and generalizing multi-modal human behaviors, offering a valuable reference for the design of future imitation learning algorithms.</div>
</div>
<div class='paper-counter'>190/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Tongzhou Mu;Minghua Liu;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reward Learning;Multi-stage Task</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d2aaee0a983fa2894c846b4399e82728a156822c.pdf" target="_blank">https://openreview.net/pdf/d2aaee0a983fa2894c846b4399e82728a156822c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/74e755c3aef53c3a0d8fbe89b8266993518bc85e.pdf" target="_blank">https://openreview.net/attachment/74e755c3aef53c3a0d8fbe89b8266993518bc85e.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose DrS, a novel reward learning approach that learns reusable dense rewards for multi-stage tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demands substantial domain expertise and extensive trial and error. In our work, we propose **DrS** (**D**ense **r**eward learning from **S**tages), a novel approach for learning *reusable* dense rewards for multi-stage tasks in a data-driven manner. By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given. The learned rewards can be *reused* in unseen tasks, thus reducing the human effort for reward engineering. Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms. The learned rewards even achieve comparable performance to human-engineered rewards on some tasks. See our [project page](https://sites.google.com/view/iclr24drs) for more details.</div>
</div>
<div class='paper-counter'>191/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongpeng Cao;Yanbing Mao;Lui Sha;Marco Caccamo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Physics-informed deep reinforcement learning;Safety-critical autonomous systems</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/da97cca0d38172e4f655bd23891b3c1509513545.pdf" target="_blank">https://openreview.net/pdf/da97cca0d38172e4f655bd23891b3c1509513545.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/289020ab57bf742ddf172c907d97beacd14306b7.zip" target="_blank">https://openreview.net/attachment/289020ab57bf742ddf172c907d97beacd14306b7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Physics-regulated DRL</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper proposes the Phy-DRL: a physics-regulated deep reinforcement learning (DRL) framework for safety-critical autonomous systems. The Phy-DRL has three distinguished invariant-embedding designs: i) residual action policy (i.e., integrating data-driven-DRL action policy and physics-model-based action policy), ii) automatically constructed safety-embedded reward, and iii) physics-model-guided neural network (NN) editing, including link editing and activation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically provable safety guarantee and 2) strict compliance of critic and actor networks with physics knowledge about the action-value function and action policy. Finally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot. The experiments validate our theoretical results and demonstrate that Phy-DRL features guaranteed safety compared to purely data-driven DRL and solely model-based design while offering remarkably fewer learning parameters and fast training towards safety guarantee.</div>
</div>
<div class='paper-counter'>192/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">Xavier Puig;Eric Undersander;Andrew Szot;Mikael Dallaire Cote;Tsung-Yen Yang;Ruslan Partsey;Ruta Desai;Alexander Clegg;Michal Hlavac;So Yeon Min;Vladimír Vondruš;Theophile Gervet;Vincent-Pierre Berges;John M Turner;Oleksandr Maksymets;Zsolt Kira;Mrinal Kalakrishnan;Jitendra Malik;Devendra Singh Chaplot;Unnat Jain;Dhruv Batra;Akshara Rai;Roozbeh Mottaghi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Simulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f39a778d333fd408d9ee2e84ba1aa9a9fad05470.pdf" target="_blank">https://openreview.net/pdf/f39a778d333fd408d9ee2e84ba1aa9a9fad05470.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ac36aa06501b2d84b16e4f776320cf4bb0a6b707.zip" target="_blank">https://openreview.net/attachment/ac36aa06501b2d84b16e4f776320cf4bb0a6b707.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities. For more details and visualizations, visit: https://aihabitat.org/habitat3.</div>
</div>
<div class='paper-counter'>193/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning 3D Particle-based Simulators from RGB-D Videos</div>
<div class="field-name">authors:</div>
<div class="field-value">William F Whitney;Tatiana Lopez-Guevara;Tobias Pfaff;Yulia Rubanova;Thomas Kipf;Kim Stachenfeld;Kelsey R Allen</div>
<div class="field-name">keywords:</div>
<div class="field-value">simulation;dynamics;nerf;particle dynamics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c084f0fff026d69efbf43d593934b7e30a668247.pdf" target="_blank">https://openreview.net/pdf/c084f0fff026d69efbf43d593934b7e30a668247.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Learned dynamics models that combine 3D representations and ray-based rendering.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Realistic simulation is critical for applications ranging from robotics to animation. Traditional analytic simulators sometimes struggle to capture sufficiently realistic simulation which can lead to problems including the well known "sim-to-real" gap in robotics. Learned simulators have emerged as an alternative for better capturing real-world physical dynamics, but require access to privileged ground truth physics information such as precise object geometry or particle tracks. Here we propose a method for learning simulators directly from observations. Visual Particle Dynamics (VPD) jointly learns a latent particle-based representation of 3D scenes, a neural simulator of the latent particle dynamics, and a renderer that can produce images of the scene from arbitrary views. VPD learns end to end from posed RGB-D videos and does not require access to privileged information. Unlike existing 2D video prediction models, we show that VPD's 3D structure enables scene editing and long-term predictions. These results pave the way for downstream applications ranging from video editing to robotic planning.</div>
</div>
<div class='paper-counter'>194/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianlang Chen;Xuhong Ren;Qing Guo;Felix Juefei-Xu;Di Lin;Wei Feng;Lei Ma;Jianjun Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Tracking defence;spatial-temporal implicit representation;languange-image model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3ff2f51f19bf7a7c07263268574d69ebfc610fdc.pdf" target="_blank">https://openreview.net/pdf/3ff2f51f19bf7a7c07263268574d69ebfc610fdc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/68d998940505f042f10064d97ab0ab3404c58553.pdf" target="_blank">https://openreview.net/attachment/68d998940505f042f10064d97ab0ab3404c58553.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose to use the language to guide the reconstruction of the adversarial frames through resamplable spatial-temporal implicit representations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.</div>
</div>
<div class='paper-counter'>195/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Planning Abstractions from Language</div>
<div class="field-name">authors:</div>
<div class="field-value">Weiyu Liu;Geng Chen;Joy Hsu;Jiayuan Mao;Jiajun Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Planning and Learning;Learning Abstractions;Compositional Generalization;Robotic Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9d32040b698a4951726efd59207a7258ec83a57c.pdf" target="_blank">https://openreview.net/pdf/9d32040b698a4951726efd59207a7258ec83a57c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A framework that utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction for planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents a framework for learning state and action abstractions in sequential decision-making domains. Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction based on it. PARL consists of three stages: 1) recovering object-level and action concepts, 2) learning state abstractions, abstract action feasibility, and transition models, and 3) applying low-level policies for abstract actions. During inference, given the task description, PARL first makes abstract action plans using the latent transition and feasibility functions, then refines the high-level plan using low-level policies. PARL generalizes across scenarios involving novel object instances and environments, unseen concept compositions, and tasks that require longer planning horizons than settings it is trained on.</div>
</div>
<div class='paper-counter'>196/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Ben Eisner;Yi Yang;Todor Davchev;Mel Vecerik;Jonathan Scholz;David Held</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning from Demonstration;Manipulation;3D Learning;SE(3) Equivariance</div>
<div class="field-name">primary_area:</div>
<div class="field-value">representation learning for computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dd9df4ab36e1ad9afd17e997264a7c5f12256975.pdf" target="_blank">https://openreview.net/pdf/dd9df4ab36e1ad9afd17e997264a7c5f12256975.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack.  In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations. Supplementary information and videos can be found at https://sites.google.com/view/reldist-iclr-2023.</div>
</div>
<div class='paper-counter'>197/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Neural Neighborhood Search for Multi-agent Path Finding</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhongxia Yan;Cathy Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Combinatorial Optimization;Neural Architecture;Multi-agent;Path Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7a97c5a1036b3c34955a1359353fbf512fff31d1.pdf" target="_blank">https://openreview.net/pdf/7a97c5a1036b3c34955a1359353fbf512fff31d1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/316fe71d286baf490ed3dadb3478af1d1fc51bd1.pdf" target="_blank">https://openreview.net/attachment/316fe71d286baf490ed3dadb3478af1d1fc51bd1.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-agent path finding (MAPF) is the combinatorial problem of planning optimal collision-avoiding paths for multiple agents, with application to robotics, logistics, and transportation. Though many recent learning-based works have focused on large-scale combinatorial problems by guiding their decomposition into sequences of smaller subproblems, the combined spatiotemporal and time-restricted nature of MAPF poses a particular challenge for learning-based guidance of iterative approaches like large neighborhood search (LNS), which is already a state-of-the-art approach for MAPF even without learning. We address this challenge of neural-guided LNS for MAPF by designing an architecture which interleaves convolution and attention to efficiently represent MAPF subproblems, enabling practical guidance of LNS in benchmark settings. We demonstrate the speedup of our method over existing state-of-the-art LNS-based methods for MAPF as well as the robustness of our method to unseen settings. Our proposed method expands the horizon of effective deep learning-guided LNS methods into multi-path planning problems, and our proposed representation may be more broadly applicable for representing path-wise interactions.</div>
</div>
<div class='paper-counter'>198/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Unified Human-Scene Interaction via Prompted Chain-of-Contacts</div>
<div class="field-name">authors:</div>
<div class="field-value">Zeqi Xiao;Tai Wang;Jingbo Wang;Jinkun Cao;Wenwei Zhang;Bo Dai;Dahua Lin;Jiangmiao Pang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human-Scene Interaction;Chain-of-Contacts;Unified;LLM</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ac7284562ef813e689f348d02334547591546d9b.pdf" target="_blank">https://openreview.net/pdf/ac7284562ef813e689f348d02334547591546d9b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/108d0d7943fd3252782328d667d2c70d8078786b.pdf" target="_blank">https://openreview.net/attachment/108d0d7943fd3252782328d667d2c70d8078786b.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A unified Human-Scene Interaction framework that supports versatile interactions through language commands</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. The framework defines interaction as ``Chain of Contacts (CoC)", representing steps involving human joint-object part pairs. This concept is inspired by the strong correlation between interaction types and corresponding contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes.</div>
</div>
<div class='paper-counter'>199/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Bo Zhang;Xinyu Cai;Jiakang Yuan;Donglin Yang;Jianfei Guo;Xiangchao Yan;Renqiu Xia;Botian Shi;Min Dou;Tao Chen;Si Liu;Junchi Yan;Yu Qiao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;3D Domain Transfer;Zero-shot 3D Detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/254e0f6f9ac41817ec1e4f4ec69c767f6b847994.pdf" target="_blank">https://openreview.net/pdf/254e0f6f9ac41817ec1e4f4ec69c767f6b847994.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/84f21850a271ea5b6dcfea958210801e97bfa51a.zip" target="_blank">https://openreview.net/attachment/84f21850a271ea5b6dcfea958210801e97bfa51a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, e.g., 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, etc, to verify the zero-shot target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training. Code and simulated points are available at: https://github.com/PJLab-ADG/3DTrans</div>
</div>
<div class='paper-counter'>200/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">I-PHYRE: Interactive Physical Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">Shiqian Li;Kewen Wu;Chi Zhang;Yixin Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Intuitive physics;physical reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fad4695ed5caf629961f820cfffbd439e4662aa5.pdf" target="_blank">https://openreview.net/pdf/fad4695ed5caf629961f820cfffbd439e4662aa5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bf4202ce3dfc8cb3a1fdecda0c659cf93a36497d.zip" target="_blank">https://openreview.net/attachment/bf4202ce3dfc8cb3a1fdecda0c659cf93a36497d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Build human-level interactive agents that can control the physical dynamics by reasoning, planning, and intervention.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the capability to interact with events in real time. To address this, we introduce I-PHYRE, a framework that challenges agents to simultaneously exhibit intuitive physical reasoning, multi-step planning, and in-situ intervention. Here, intuitive physical reasoning refers to a quick, approximate understanding of physics to address complex problems; multi-step denotes the need for extensive sequence planning in I-PHYRE, considering each intervention can significantly alter subsequent choices; and in-situ implies the necessity for timely object manipulation within a scene, where minor timing deviations can result in task failure. We formulate four game splits to scrutinize agents' learning and generalization of essential principles of interactive physical reasoning, fostering learning through interaction with representative scenarios. Our exploration involves three planning strategies and examines several supervised and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The outcomes highlight a notable gap between existing learning algorithms and human performance, emphasizing the imperative for more research in enhancing agents with interactive physical reasoning capabilities. The environment and baselines will be made publicly available.</div>
</div>
<div class='paper-counter'>201/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">Sifan Zhou;Liang Li;Xinyu Zhang;Bo Zhang;Shipeng Bai;Miao Sun;Ziyu Zhao;Xiaobo Lu;Xiangxiang Chu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Quantization;3D Object Detection;Autonomous Driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/842a4e6c9415b683b8eaab8e682c2f2c9616b8e6.pdf" target="_blank">https://openreview.net/pdf/842a4e6c9415b683b8eaab8e682c2f2c9616b8e6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Due to highly constrained computing power and memory, deploying 3D lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization (PTQ) has been widely adopted in 2D vision tasks. However, applying it directly to 3D lidar-based tasks inevitably leads to performance degradation. As a remedy, we propose an effective PTQ method called LiDAR-PTQ, which is particularly curated for 3D lidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ features three main components, (1) a sparsity-based calibration method to determine the initialization of quantization parameters, (2) an adaptive rounding-to-nearest operation to minimize the layerwise reconstruction error, (3) a Task-guided Global Positive Loss (TGPL) to reduce the disparity between the final predictions before and after quantization. Extensive experiments demonstrate that our LiDAR-PTQ can achieve state-of-the-art quantization performance when applied to CenterPoint (both Pillar-based and Voxel-based). To our knowledge, for the very first time in lidar-based 3D detection tasks, the PTQ INT8 model's accuracy is almost the same as the FP32 model while enjoying 3X inference speedup. Moreover, our LiDAR-PTQ is cost-effective being 6X faster than the quantization-aware training method. The code will be released.</div>
</div>
<div class='paper-counter'>202/828</div>
<div class="conference">NeurIPS_cc_2024_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Fei Ni;Jianye HAO;Shiguang Wu;Longxin Kou;Yifu Yuan;Zibin Dong;Jinyi Liu;MingZhi Li;Yuzheng Zhuang;YAN ZHENG</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics manipulation;multi-modal LLM;image generation;instruction following</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2a39fcbdd8617cd0a7fbe9312a20b9b51ea8ab74.pdf" target="_blank">https://openreview.net/pdf/2a39fcbdd8617cd0a7fbe9312a20b9b51ea8ab74.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose PERIA, a novel framework that leverages a MLLM to enable effective task decomposition via holistic language planning and vision planning for robotic manipulation in embodied scenarios.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose **PERIA**(**PE**rceive, **R**eason, **I**magine, **A**ct), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks.</div>
</div>
<div class='paper-counter'>203/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GenRL: Multimodal-foundation world models for generalization in embodied agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Pietro Mazzaglia;Tim Verbelen;Bart Dhoedt;Aaron Courville;Sai Rajeswar</div>
<div class="field-name">keywords:</div>
<div class="field-value">world models;foundations models;reinforcement learning;multitask generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/70954c88d0935070ce03152cf1eb67a2f2ac5a2e.pdf" target="_blank">https://openreview.net/pdf/70954c88d0935070ce03152cf1eb67a2f2ac5a2e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Connecting multimodal foundation models' representations with world models' representations for RL enables specifying task by vision or language prompts and learning the corresponding embodied behaviors in imagination.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination.As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/</div>
</div>
<div class='paper-counter'>204/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rethinking No-reference Image Exposure Assessment from Holism to Pixel:  Models, Datasets and Benchmarks</div>
<div class="field-name">authors:</div>
<div class="field-value">Shuai He;Shuntian Zheng;Anlong Ming;Banyu Wu;Huadong Ma</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Exposure Assessment;Image Quality Assessment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9d159e1b2a972b461ac4b69cc1ad301734642641.pdf" target="_blank">https://openreview.net/pdf/9d159e1b2a972b461ac4b69cc1ad301734642641.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4c033c87c975961e7194fda4f3ecc8ff4de81590.zip" target="_blank">https://openreview.net/attachment/4c033c87c975961e7194fda4f3ecc8ff4de81590.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Pixel-level image exposure assessment from three perspectives: model, dataset, and benchmark.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The past decade has witnessed an increasing demand for enhancing image quality through exposure, and as a crucial prerequisite in this endeavor, Image Exposure Assessment (IEA) is now being accorded serious attention. However, IEA encounters two persistent challenges that remain unresolved over the long term: the accuracy and generalizability of No-reference IEA are inadequate for practical applications; the scope of IEA is confined to qualitative and quantitative analysis of the entire image or subimage, such as providing only a score to evaluate the exposure level, thereby lacking intuitive and precise fine-grained evaluation for complex exposure conditions.  The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark.  1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios. 2) Dataset-level: we elaborately build an exposure-oriented dataset, IEA40K, containing 40K  images, covering 17 typical lighting scenarios, 27 devices, and 50+ scenes, with each image densely annotated by more than 10 experts with pixel-level labels.  3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K. Our P-IEANet not only achieves state-of-the-art (SOTA) performance on all metrics but also seamlessly integrates with existing exposure correction and lighting enhancement methods. To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community. The code and dataset are available in \href{https://github.com/mRobotit/Pixel-level-No-reference-Image-Exposure-Assessment}{\textcolor{red} {here}}.</div>
</div>
<div class='paper-counter'>205/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunnan Wang;Ziqiang Li;Wenyao Zhang;Zequn Zhang;Baao Xie;Xihui Liu;Wenjun Zeng;Xin Jin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Scene Graph; Disentanglement; Diffusion Model; Compositional Image Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/66bc4339c157f7e9cfc224307ac92ad79e98a4b8.pdf" target="_blank">https://openreview.net/pdf/66bc4339c157f7e9cfc224307ac92ad79e98a4b8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4b7d2dd027fefb29263efc4e8b2ca9cc76f24486.zip" target="_blank">https://openreview.net/attachment/4b7d2dd027fefb29263efc4e8b2ca9cc76f24486.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this paper, we leverage the scene graph, a powerful structured representation, for complex image generation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">There has been exciting progress in generating images from natural language or layout conditions. However, these methods struggle to faithfully reproduce complex scenes due to the insufficient modeling of multiple objects and their relationships. To address this issue, we leverage the scene graph, a powerful structured representation, for complex image generation. Different from the previous works that directly use scene graphs for generation, we employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. Specifically, we first propose a Semantics-Layout Variational AutoEncoder (SL-VAE) to jointly derive (layouts, semantics) from the input scene graph, which allows a more diverse and reasonable generation in a one-to-many mapping. We then develop a Compositional Masked Attention (CMA) integrated with a diffusion model, incorporating (layouts, semantics) with fine-grained attributes as generation guidance. To further achieve graph manipulation while keeping the visual content consistent, we introduce a Multi-Layered Sampler (MLS) for an "isolated" image editing effect. Extensive experiments demonstrate that our method outperforms recent competitors based on text, layout, or scene graph, in terms of generation rationality and controllability.</div>
</div>
<div class='paper-counter'>206/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Federated Ensemble-Directed Offline Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Desik Rengarajan;Nitin Ragothaman;Dileep Kalathil;Srinivas Shakkottai</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforcement Learning;Offline Reinforcement Learning;Federated Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ebb3c4a49c535b4cabc2bd5d7686f30b108d2a55.pdf" target="_blank">https://openreview.net/pdf/ebb3c4a49c535b4cabc2bd5d7686f30b108d2a55.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b658741d56ebceb05dfff999f7938a326e2c17d8.zip" target="_blank">https://openreview.net/attachment/b658741d56ebceb05dfff999f7938a326e2c17d8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel federated offline reinforcement learning algorithm</div>
<div class="field-name">abstract:</div>
<div class="field-value">We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Na\"{i}vely combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies.  In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning  Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach.  We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real-world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on a mobile robot. We provide our code and a video of our experiments at \url{https://github.com/DesikRengarajan/FEDORA}.</div>
</div>
<div class='paper-counter'>207/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Large Spatial Model: End-to-end Unposed Images to Semantic 3D</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhiwen Fan;Jian Zhang;Wenyan Cong;Peihao Wang;Renjie Li;Kairun Wen;Shijie Zhou;Achuta Kadambi;Zhangyang Wang;Danfei Xu;Boris Ivanovic;Marco Pavone;Yue Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Reconstruction;3D Scene Understanding;Gaussian Splatting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ee1ddc8c08fa974be5694c1cffee72f12da261ad.pdf" target="_blank">https://openreview.net/pdf/ee1ddc8c08fa974be5694c1cffee72f12da261ad.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a56101d73213494352b81a74c2e7ef24fe402a86.zip" target="_blank">https://openreview.net/attachment/a56101d73213494352b81a74c2e7ef24fe402a86.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a method that utilizes two unposed and uncalibrated images as input, and reconstructs the explicit radiance field, encompassing geometry, appearance, and semantics in real-time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reconstructing and understanding 3D structures from a limited number of images is a classical problem in computer vision. Traditional approaches typically decompose this task into multiple subtasks, involving several stages of complex mappings between different data representations. For example, dense reconstruction using Structure-from-Motion (SfM) requires transforming images into key points, optimizing camera parameters, and estimating structures. Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks. This multi-stage paradigm leads to significant processing times and engineering complexity.In this work, we introduce the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views. Built on a general Transformer-based framework, LSM predicts global geometry via pixel-aligned point maps. To improve spatial attribute regression, we adopt local context aggregation with multi-scale fusion, enhancing the accuracy of fine local details. To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning. Comprehensive experiments on various tasks demonstrate that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.</div>
</div>
<div class='paper-counter'>208/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</div>
<div class="field-name">authors:</div>
<div class="field-value">Yang Dai;Oubo Ma;Longfei Zhang;Xingxing Liang;Shengchao Hu;Mengzhu Wang;Shouling Ji;Jincai Huang;Li Shen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline RL; Trajectory Optimization; Mamba</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e8f05bc8b78365623dc8f45e047f65b46390a923.pdf" target="_blank">https://openreview.net/pdf/e8f05bc8b78365623dc8f45e047f65b46390a923.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An investigation of Mamba in offline RL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL). Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba (dubbed DeMa) in offline RL from the aspect of data structures and essential components with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements since DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify the hidden attention mechanism as a critical factor in its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous methods, outperforming Decision Transformer (DT) with higher performance while using 30\% fewer parameters in Atari, and exceeding DT with only a quarter of the parameters in MuJoCo.</div>
</div>
<div class='paper-counter'>209/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking</div>
<div class="field-name">authors:</div>
<div class="field-value">Roland Stolz;Hanna Krasowski;Jakob Thumm;Michael Eichelbeck;Philipp Gassert;Matthias Althoff</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Policy Gradient;Action Masking;Robotics;Continuous Actions</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8a1eecafd6f9a1b8919878b8b034860552119122.pdf" target="_blank">https://openreview.net/pdf/8a1eecafd6f9a1b8919878b8b034860552119122.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f1d4803a68bdb2523684be6a035fa100b8f5e177.zip" target="_blank">https://openreview.net/attachment/f1d4803a68bdb2523684be6a035fa100b8f5e177.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work introduces three action masking methods for continuous action spaces to focus the exploration of reinforcement learning on state-specific relevant actions, which enhances learning efficiency and effectiveness.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization ( PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.</div>
</div>
<div class='paper-counter'>210/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">$SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yinshuang Xu;Dian Chen;Katherine Liu;Sergey Zakharov;Rares Andrei Ambrus;Kostas Daniilidis;Vitor Campagnolo Guizilini</div>
<div class="field-name">keywords:</div>
<div class="field-value">$SE(3)$ Equivariance;Stereo Depth Estimation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e8039c44b88c7c3803572ada21d4746ef0778d7d.pdf" target="_blank">https://openreview.net/pdf/e8039c44b88c7c3803572ada21d4746ef0778d7d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an $SE(3)$ equivariant model with spherical harmonics ray embeddings and demonstrate its effectiveness in the task of generalized stereo depth estimation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.</div>
</div>
<div class='paper-counter'>211/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion-based Curriculum Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Erdi Sayar;Giovanni Iacca;Ozgur S. Oguz;Alois Knoll</div>
<div class="field-name">keywords:</div>
<div class="field-value">curriculum reinforcement learning;reinforcement learning;diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c6d5c73ad71d17c7a0d816c227738b96c959bc7e.pdf" target="_blank">https://openreview.net/pdf/c6d5c73ad71d17c7a0d816c227738b96c959bc7e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel diffusion based curriculum reinforcement learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Curriculum Reinforcement Learning (CRL) is an approach to facilitate the learning process of agents by structuring tasks in a sequence of increasing complexity. Despite its potential, many existing CRL methods struggle to efficiently guide agents toward desired outcomes, particularly in the absence of domain knowledge. This paper introduces DiCuRL (Diffusion Curriculum Reinforcement Learning), a novel method that leverages conditional diffusion models to generate curriculum goals. To estimate how close an agent is to achieving its goal, our method uniquely incorporates a $Q$-function and a trainable reward function based on Adversarial Intrinsic Motivation within the diffusion model. Furthermore, it promotes exploration through the inherent noising and denoising mechanism present in the diffusion models and is environment-agnostic. This combination allows for the generation of challenging yet achievable goals, enabling agents to learn effectively without relying on domain knowledge. We demonstrate the effectiveness of DiCuRL in three different maze environments and two robotic manipulation tasks simulated in MuJoCo, where it outperforms or matches nine state-of-the-art CRL algorithms from the literature.</div>
</div>
<div class='paper-counter'>212/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel</div>
<div class="field-name">authors:</div>
<div class="field-value">Jialin Li;Marta Zagorowska;Giulia De Pasquale;Alisa Rupenyan;John Lygeros</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safe learning;Bayesian optimization;Time-varying optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">active_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8ccc16f75fc2b8ad413d7dc3ee80d673efeeab6c.pdf" target="_blank">https://openreview.net/pdf/8ccc16f75fc2b8ad413d7dc3ee80d673efeeab6c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Ensuring safety is a key aspect in sequential decision making problems, such as robotics or process control. The complexity of the underlying systems often makes finding the optimal decision challenging, especially when the safety-critical system is time-varying. Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-temporal kernel. The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection. Optimality guarantees are also provided for the algorithm when the optimization problem becomes stationary. We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic data, both regarding safety and optimality. Evaluation on a realistic case study with gas compressors confirms that TVSAFEOPT ensures safety when solving time-varying optimization problems with unknown reward and safety functions.</div>
</div>
<div class='paper-counter'>213/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianbiao Mei;Yukai Ma;Xuemeng Yang;Licheng Wen;Xinyu Cai;Xin Li;Daocheng Fu;Bo Zhang;Pinlong Cai;Min Dou;Botian Shi;Liang He;Yong Liu;Yu Qiao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;Dual-process System;Knowledge-Driven;Vision Language Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b1babf61241a3da282fb13f4bf5bd64b4d8f7e45.pdf" target="_blank">https://openreview.net/pdf/b1babf61241a3da282fb13f4bf5bd64b4d8f7e45.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">LeapAD, a new autonomous driving paradigm inspired by human cognition, improves adaptability and interpretability in complex scenarios through dual-process decision-making and continuous learning from past experiences.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Project page: https://pjlab-adg.github.io/LeapAD</div>
</div>
<div class='paper-counter'>214/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhixiong Nan;Yilong Chen;Tianfei Zhou;Tao Xiang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Object importance estimation;Autonomous vehicles</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/87bd5670b38ec3e2f101018aef40eb48c6c26a89.pdf" target="_blank">https://openreview.net/pdf/87bd5670b38ec3e2f101018aef40eb48c6c26a89.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver's perspective as the input. Although this problem is significant for safer and smarter driving systems, the exploration of this problem remains limited. On one hand, publicly-available large-scale datasets are scarce in the community. To address this dilemma, this paper contributes a new large-scale dataset named Traffic Object Importance (TOI). On the other hand, existing methods often only consider either bottom-up feature or single-fold guidance, leading to limitations in handling highly dynamic and diverse traffic scenarios. Different from existing methods, this paper proposes a model that integrates multi-fold top-down guidance with the bottom-up feature. Specifically, three kinds of top-down guidance factors (i.e., driver intention, semantic context, and traffic rule) are integrated into our model. These factors are important for object importance estimation, but none of the existing methods simultaneously consider them. To our knowledge, this paper proposes the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with bottom-up feature. Extensive experiments demonstrate that our model outperforms state-of-the-art methods by large margins, achieving 23.1% Average Precision (AP) improvement compared with the recently proposed model (i.e., Goal).</div>
</div>
<div class='paper-counter'>215/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Focus On What Matters: Separated Models For Visual-Based RL Generalization</div>
<div class="field-name">authors:</div>
<div class="field-value">Di Zhang;Bowen Lv;Hai Zhang;Feifan Yang;Junqiao Zhao;Hang Yu;Chang Huang;Hongtu Zhou;Chen Ye;changjun jiang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Visual-based RL;Generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4038405ec2477f5290f6738f4c80053e969f1bfe.pdf" target="_blank">https://openreview.net/pdf/4038405ec2477f5290f6738f4c80053e969f1bfe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose SMG, which utilizes a reconstruction-based auxiliary task to extract task-relevant representations from visual observations and further strengths the generalization ability of RL agents with the help of two consistency losses.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A primary challenge for visual-based Reinforcement Learning (RL) is to generalize effectively across unseen environments. Although previous studies have explored different auxiliary tasks to enhance generalization, few adopt image reconstruction due to concerns about exacerbating overfitting to task-irrelevant features during training. Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (\blue{S}eparated \blue{M}odels for \blue{G}eneralization), a novel approach that exploits image reconstruction for generalization. SMG introduces two model branches to extract task-relevant and task-irrelevant representations separately from visual observations via cooperatively reconstruction. Built upon this architecture, we further emphasize the importance of task-relevant features for generalization. Specifically, SMG incorporates two additional consistency losses to guide the agent's focus toward task-relevant areas across different scenarios, thereby achieving free from overfitting. Extensive experiments in DMC demonstrate the SOTA performance of SMG in generalization, particularly excelling in video-background settings. Evaluations on robotic manipulation tasks further confirm the robustness of SMG in real-world applications. Source code is available at \url{https://anonymous.4open.science/r/SMG/}.</div>
</div>
<div class='paper-counter'>216/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FactorSim: Generative Simulation via Factorized Representation</div>
<div class="field-name">authors:</div>
<div class="field-value">Fan-Yun Sun;Harini S I;Angela Yi;Yihan Zhou;Alex Zook;Jonathan Tremblay;Logan Cross;Jiajun Wu;Nick Haber</div>
<div class="field-name">keywords:</div>
<div class="field-value">generative simulation;POMDP;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c3c4eed43ecec8fe574f69437c9137f8c41b7797.pdf" target="_blank">https://openreview.net/pdf/c3c4eed43ecec8fe574f69437c9137f8c41b7797.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework for generating simulations in code to train RL agents and introduce a new benchmark to showcase its efficacy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generating simulations to train intelligent agents in game-playing and robotics from natural language input, user input, or task documentation remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code’s accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.</div>
</div>
<div class='paper-counter'>217/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Zichen Jeff Cui;Hengkai Pan;Aadhithya Iyer;Siddhant Haldar;Lerrel Pinto</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot learning;representation learning;self-supervised learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a80285940d66984b6d99e1990c79614edb3af61b.pdf" target="_blank">https://openreview.net/pdf/a80285940d66984b6d99e1990c79614edb3af61b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">DynaMo, a new self-supervised method for pretraining visual encoders for downstream visuomotor control by explicitly modeling dynamics in the demonstration observations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning has proven to be a powerful tool for training complex visuo-motor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io.</div>
</div>
<div class='paper-counter'>218/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Going Beyond Heuristics by Imposing Policy Improvement as a Constraint</div>
<div class="field-name">authors:</div>
<div class="field-value">Chi-Chang Lee;Zhang-Wei Hong;Pulkit Agrawal</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1983230de4f35516ae3297ba7a6fb88fab7fad02.pdf" target="_blank">https://openreview.net/pdf/1983230de4f35516ae3297ba7a6fb88fab7fad02.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/55b8dcc3d7045b9166063c32638f97a974040774.zip" target="_blank">https://openreview.net/attachment/55b8dcc3d7045b9166063c32638f97a974040774.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a modification to existing RL algorithms to improve the performance when trained with heuristic rewards.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance. Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms. However, such hints may not be optimal, limiting the performance of learned policies. The currently established way of using heuristics is to modify the heuristic reward in a manner that ensures that the optimal policy learned with it remains the same as the optimal policy for the task reward (i.e., optimal policy invariance). However, these methods often fail in practical scenarios with limited training data. We found that while optimal policy invariance ensures convergence to the best policy based on task rewards, it doesn't guarantee better performance than policies trained with biased heuristics under a finite data regime, which is impractical. In this paper, we introduce a new principle tailored for finite data settings. Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy. As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward. Our experiments on robotic locomotion, helicopter control, and manipulation tasks demonstrate that our method consistently outperforms the heuristic policy, regardless of the heuristic rewards' quality.Code is available at https://github.com/Improbable-AI/hepo.</div>
</div>
<div class='paper-counter'>219/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Vision-Language Navigation with Energy-Based Policy</div>
<div class="field-name">authors:</div>
<div class="field-value">Rui Liu;Wenguan Wang;Yi Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Vision-Language Navigation; Vision-Language; Embodied vision</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3086459a83bf86ddd7c3c2a039c2f2248ea6d01f.pdf" target="_blank">https://openreview.net/pdf/3086459a83bf86ddd7c3c2a039c2f2248ea6d01f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (ENP) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, ENP achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.</div>
</div>
<div class='paper-counter'>220/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Kun Zhou;Beichen Zhang;jiapeng wang;Zhipeng Chen;Xin Zhao;Jing Sha;Zhichao Sheng;Shijin Wang;Ji-Rong Wen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Mathematical Reasoning;Data Synthesis</div>
<div class="field-name">primary_area:</div>
<div class="field-value">natural_language_processing</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b61328368f8231b0bef39a7ed803d8db25907822.pdf" target="_blank">https://openreview.net/pdf/b61328368f8231b0bef39a7ed803d8db25907822.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5a90b475df805b2586cd49013f9c59152b311009.zip" target="_blank">https://openreview.net/attachment/5a90b475df805b2586cd49013f9c59152b311009.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.Our code and data will be publicly released in \url{https://github.com/RUCAIBox/JiuZhang3.0}.</div>
</div>
<div class='paper-counter'>221/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BAKU: An Efficient Transformer for Multi-Task Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Siddhant Haldar;Zhuoran Peng;Lerrel Pinto</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot learning;Imitation Learning;Multitask Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/71b48662ea04f445f5d15dd51227f43a7ca9b49c.pdf" target="_blank">https://openreview.net/pdf/71b48662ea04f445f5d15dd51227f43a7ca9b49c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/462f081e6994a2869793fb010c78c528e425f229.zip" target="_blank">https://openreview.net/attachment/462f081e6994a2869793fb010c78c528e425f229.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present BAKU, a simple architecture for multi-task policy learning that provides highly efficient training, particularly in data-scarce problems such as robotics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations. This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world. Thus, there is a pressing need for architectures that can effectively leverage the available training data. In this work, we present BAKU, a simple transformer architecture that enables efficient learning of multi-task robot policies. BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark. On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate. Videos of the robot are best viewed at baku-robot.github.io.</div>
</div>
<div class='paper-counter'>222/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Huy Hoang;Tien Anh Mai;Pradeep Varakantham</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;offline imitation learning;reference reward;supplementary data;ranked dataset</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/21f890aa8acefa4c5640a534a16533bb251a5681.pdf" target="_blank">https://openreview.net/pdf/21f890aa8acefa4c5640a534a16533bb251a5681.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a novel inverse soft-Q learning for offline imitation learning with expert and non-expert demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We focus on offline imitation learning (IL), which aims to mimic an expert's behavior using demonstrations without any interaction with the environment. One of the main challenges in offline IL is the limited support of expert demonstrations, which typically cover only a small fraction of the state-action space. While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations. For example, in treatment optimization problems, there are varying levels of doctor treatments available for different chronic conditions. These range from treatment specialists and experienced general practitioners to less experienced general practitioners. Similarly, when robots are trained to imitate humans in routine tasks, they might learn from individuals with different levels of expertise and efficiency. In this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories. Existing offline IL methods based on behavior cloning or distribution matching often face issues such as overfitting to the limited set of expert demonstrations or inadvertently imitating sub-optimal trajectories from the larger dataset. Our approach, which is based on inverse soft-Q learning, learns from both expert and sub-optimal demonstrations. It assigns higher importance (through learned weights) to aligning with expert demonstrations and lower importance to aligning with sub-optimal ones. A key contribution of our approach, called SPRINQL, is transforming the offline IL problem into a convex optimization over the space of Q functions. Through comprehensive experimental evaluations, we demonstrate that the SPRINQL algorithm achieves state-of-the-art (SOTA) performance on offline IL benchmarks. Code is available at https://github.com/hmhuy0/SPRINQL .</div>
</div>
<div class='paper-counter'>223/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Prediction with Action: Visual Policy Learning via Joint Denoising Process</div>
<div class="field-name">authors:</div>
<div class="field-value">Yanjiang Guo;Yucheng Hu;Jianke Zhang;Yen-Jen Wang;Xiaoyu Chen;Chaochao Lu;Jianyu Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual policy learning;diffusion;image generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/27b4f1716e2587fbc410f94a23ff1e25367e737e.pdf" target="_blank">https://openreview.net/pdf/27b4f1716e2587fbc410f94a23ff1e25367e737e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b56068659dea16ffee93984832caa308416da444.zip" target="_blank">https://openreview.net/attachment/b56068659dea16ffee93984832caa308416da444.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel framework to predict images and robot actions through joint dinoising process.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilities—image prediction and robotic action, respectively—they technically follow similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce \textbf{PAD}, a novel visual policy learning framework that unifies image \textbf{P}rediction and robot \textbf{A}ction within a joint \textbf{D}enoising process.  Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. PAD outperforms previous methods, achieving a significant 38.9\% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0\% success rate increase compared to the strongest baseline. Videos of PAD can be found at https://sites.google.com/view/pad-paper</div>
</div>
<div class='paper-counter'>224/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Reason via Program Generation, Emulation, and Search</div>
<div class="field-name">authors:</div>
<div class="field-value">Nathaniel Weir;Muhammad Khalifa;Linlu Qiu;Orion Weller;Peter Clark</div>
<div class="field-name">keywords:</div>
<div class="field-value">language models;instruction tuning;code generation;reasoning;program search;program emulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">natural_language_processing</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3a26df6098d24990525e293f7f00b96ed7ee7344.pdf" target="_blank">https://openreview.net/pdf/3a26df6098d24990525e293f7f00b96ed7ee7344.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that fine-tuning LMs to generate and then emulate the execution of programs creates models can learn new tasks via program search.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation). However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding. Our goal is to extend a LM’s program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined. To that end, we propose, Code Generation and Emulated EXecution (COGEX). COGEX works by (1) training LMs to generate pseudo-programs and (2) teaching them to emulate their generated program’s execution, including those leaf functions, allowing the LM’s knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one. To adapt the COGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset. We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning. This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.</div>
</div>
<div class='paper-counter'>225/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts</div>
<div class="field-name">authors:</div>
<div class="field-value">Jing-Cheng Pang;Si-Hang Yang;Kaiyuan Li;Jiaji Zhang;Xiong-Hui Chen;Nan Tang;Yang Yu</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;large language models;knowledgeable agents</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/82a85efda24bbab8b3b9acb9f056b2105ab5be11.pdf" target="_blank">https://openreview.net/pdf/82a85efda24bbab8b3b9acb9f056b2105ab5be11.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This study investigates developing knowledgeable agents with RL and LLMs, which achieve low-level control and adapt to novel situations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) traditionally trains agents using interaction data, which limits their capabilities to the scope of the training data. To create more knowledgeable agents, leveraging knowledge from large language models (LLMs) has shown a promising way. Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration. This paper introduces a novel approach, KALM (Knowledgeable Agents from Language Model Rollouts), to learn knowledgeable agents by bridging this gap. KALM extracts knowledge from LLMs in the form of imaginary rollouts, which agents can learn through offline RL. To overcome the limitation that LLMs are inherently text-based and may be incompatible with numerical environmental data, KALM fine-tunes the LLM to perform bidirectional translation between textual goals and rollouts. This process enables the LLM to understand the environment better, facilitating the generation of meaningful rollouts. Experiments on robotic manipulation tasks demonstrate that KALM allows agents to rephrase complex goals and tackle novel tasks requiring new optimal behaviors. KALM achieves a 46% success rate in completing 1400 various novel goals, significantly outperforming the 26% success rate of baseline methods. Project homepage: https://kalmneurips2024.github.io.</div>
</div>
<div class='paper-counter'>226/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Yueming Xu;Haochen Jiang;Zhongyang Xiao;Jianfeng Feng;Li Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">SLAM;Deep learning method;Gaussian splatting;Hybrid pose estimation;Dynamic environment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/003a4a26d3ccd1f63f63eb5a3f55b8f83ce8f36b.pdf" target="_blank">https://openreview.net/pdf/003a4a26d3ccd1f63f63eb5a3f55b8f83ce8f36b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.</div>
</div>
<div class='paper-counter'>227/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis</div>
<div class="field-name">authors:</div>
<div class="field-value">Deepak Sridhar;Abhishek Peri;Rohith Reddy Rachala;Nuno Vasconcelos</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Synthesis; Controllable 2D/3D Synthesis; Diffusion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8f21ac42e948eed5568e8c93b72ba0d79d8038dd.pdf" target="_blank">https://openreview.net/pdf/8f21ac42e948eed5568e8c93b72ba0d79d8038dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a new framework for modeling the joint distribution of images and conditioning variables by adapting Stable Diffusion to enhance prompt compliance, controllability and editing of images.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.   To overcome these limitations, this work introduces a new family of  $\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS).</div>
</div>
<div class='paper-counter'>228/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs</div>
<div class="field-name">authors:</div>
<div class="field-value">Quentin Leboutet;Nina Wiedemann;zhipeng cai;Michael Paulitsch;Kai Yuan</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D articulated objects;diffusion models;generative models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a8c6a5c1a1b447094fcb67f755d8cbf845059d02.pdf" target="_blank">https://openreview.net/pdf/a8c6a5c1a1b447094fcb67f755d8cbf845059d02.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a0e6fcbf36a8a2576351826c2cc667744af2430c.zip" target="_blank">https://openreview.net/attachment/a0e6fcbf36a8a2576351826c2cc667744af2430c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present MIDGArD, a new generative framework for creating 3D articulated objects, separating structure and shape generation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.</div>
</div>
<div class='paper-counter'>229/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DeMo: Decoupling Motion Forecasting into  Directional Intentions and Dynamic States</div>
<div class="field-name">authors:</div>
<div class="field-value">Bozhou Zhang;Nan Song;Li Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Motion Forecasting;Autonomous Driving;Mamba;Attention</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3d4994978d6ca281a3b560f62d73cecc4d2310f7.pdf" target="_blank">https://openreview.net/pdf/3d4994978d6ca281a3b560f62d73cecc4d2310f7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A framework that decouples multi-modal trajectory queries into mode queries for directional intentions and state queries for dynamic states, utilizing Attention and Mamba.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting. In addition, we will make our code and models publicly available.</div>
</div>
<div class='paper-counter'>230/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">Ching-An Cheng;Allen Nie;Adith Swaminathan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimization;Back-Propagation;Automatic Differentiation;LLM;Language Feedback;Execution Trace</div>
<div class="field-name">primary_area:</div>
<div class="field-value">optimization_for_deep_networks</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d03790ef61dce1580c1d113ebc679a0ade344987.pdf" target="_blank">https://openreview.net/pdf/d03790ef61dce1580c1d113ebc679a0ade344987.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2ff35870bd3cac150509fc276e2f13e6e6256f60.zip" target="_blank">https://openreview.net/attachment/2ff35870bd3cac150509fc276e2f13e6e6256f60.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Framework for efficient optimization of heterogenous parameters in general computational workflows</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user’s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization – using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.</div>
</div>
<div class='paper-counter'>231/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruiqi Li;Yiu-ming Cheung</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neural Rendering;Uncertainty Quantification</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8f70d2aaf3fde28609d247eee1e0166da808d459.pdf" target="_blank">https://openreview.net/pdf/8f70d2aaf3fde28609d247eee1e0166da808d459.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0510d95a78e5b2f2187c06e66cb604c5fe9c7164.zip" target="_blank">https://openreview.net/attachment/0510d95a78e5b2f2187c06e66cb604c5fe9c7164.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We quantify the uncertainty in 3D Gaussian Splatting by deviating Gaussians to construction model space samples and learn with variational inference. .</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recently, 3D Gaussian Splatting (3DGS) has become popular in reconstructing dense 3D representations of appearance and geometry. However, the learning pipeline in 3DGS inherently lacks the ability to quantify uncertainty, which is an important factor in applications like robotics mapping and navigation. In this paper, we propose an uncertainty estimation method built upon the Bayesian inference framework. Specifically, we propose a method to build variational multi-scale 3D Gaussians, where we leverage explicit scale information in 3DGS parameters to construct diversified parameter space samples. We develop an offset table technique to draw local multi-scale samples efficiently by offsetting selected attributes and sharing other base attributes. Then, the offset table is learned by variational inference with multi-scale prior. The learned offset posterior can quantify the uncertainty of each individual Gaussian component, and be used in the forward pass to infer the predictive uncertainty. Extensive experimental results on various benchmark datasets show that the proposed method provides well-aligned calibration performance on estimated uncertainty and better rendering quality compared with the previous methods that enable uncertainty quantification with view synthesis. Besides, by leveraging the model parameter uncertainty estimated by our method, we can remove noisy Gaussians automatically, thereby obtaining a high-fidelity part of the reconstructed scene, which is of great help in improving the visual quality.</div>
</div>
<div class='paper-counter'>232/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">E-Motion: Future Motion Simulation via Event Sequence Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Song Wu;Zhiyu Zhu;Junhui Hou;Guangming Shi;Jinjian Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Event-based vision;video diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4468e8e5ba691cc274470c891f728ce7aa135bb1.pdf" target="_blank">https://openreview.net/pdf/4468e8e5ba691cc274470c891f728ce7aa135bb1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b7cd23bba2a4e949afb6a1b5dde875bca9559798.zip" target="_blank">https://openreview.net/attachment/b7cd23bba2a4e949afb6a1b5dde875bca9559798.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion prediction framework.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems. The source code ispublicly available at https://github.com/p4r4mount/E-Motion.</div>
</div>
<div class='paper-counter'>233/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Yongyuan Liang;Tingqiang Xu;Kaizhe Hu;Guangqi Jiang;Furong Huang;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion Model;Policy Learning;Parameter Generation;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/30950d8eec8a2456a781dff694642e9b4c2d048c.pdf" target="_blank">https://openreview.net/pdf/30950d8eec8a2456a781dff694642e9b4c2d048c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a policy network generator that utilizes conditional diffusion models for behavior-to-policy generation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?In this paper, we present **Make-An-Agent**, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by **Make-An-Agent** onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/.</div>
</div>
<div class='paper-counter'>234/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Walsh Hadamard Derived Linear Vector Symbolic Architecture</div>
<div class="field-name">authors:</div>
<div class="field-value">Mohammad Mahmudul Alam;Alexander Oberle;Edward Raff;Stella Biderman;Tim Oates;James Holt</div>
<div class="field-name">keywords:</div>
<div class="field-value">Vector Symbolic Architectures;Holographic Reduced Representations;Hadamard Transformation;HRR;VTB;MAP;HLB</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning_architectures</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/87b4b1c14628316f12b4dc3487e0802deba8e72b.pdf" target="_blank">https://openreview.net/pdf/87b4b1c14628316f12b4dc3487e0802deba8e72b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Starting from the Hadamard transform we develop a simple method for neuro-symbolic manipulation of vectors that has desirable properties for deep learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\mathbb{R}^d$ are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems.</div>
</div>
<div class='paper-counter'>235/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploration by Learning Diverse Skills through Successor State Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Paul-Antoine LE TOLGUENEC;Yann BESSE;Florent Teichteil-Königsbuch;Dennis George Wilson;Emmanuel Rachelson</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Exploration;Deep Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dd7daaa95d0d7d28d2b3debd6bc2adb0031ae0f9.pdf" target="_blank">https://openreview.net/pdf/dd7daaa95d0d7d28d2b3debd6bc2adb0031ae0f9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to perform different skills can encourage agents to explore. In this work, we aim to construct a set of diverse skills that uniformly cover the state space. We propose a formalization of this search for diverse skills, building on a previous definition based on the mutual information between states and skills. We consider the distribution of states reached by a policy conditioned on each skill and leverage the successor state representation to maximize the difference between these skill distributions. We call this approach LEADS: Learning Diverse Skills through Successor State Representations. We demonstrate our approach on a set of maze navigation and robotic control tasks which show that our method is capable of constructing a diverse set of skills which exhaustively cover the state space without relying on reward or exploration bonuses. Our findings demonstrate that this new formalization promotes more robust and efficient exploration by combining mutual information maximization and exploration bonuses.</div>
</div>
<div class='paper-counter'>236/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos</div>
<div class="field-name">authors:</div>
<div class="field-value">Matthew Wallingford;Anand Bhattad;Aditya Kusupati;Vivek Ramanujan;Matt Deitke;Aniruddha Kembhavi;Roozbeh Mottaghi;Wei-Chiu Ma;Ali Farhadi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Novel View Synthesis;3D;Video;360 Video;Large-Scale;Data;Scene Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e92b581e47ffbaa019c305f5181802de017fd66e.pdf" target="_blank">https://openreview.net/pdf/e92b581e47ffbaa019c305f5181802de017fd66e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bd85b6068d813279b5a145c8c87a9a185320139f.zip" target="_blank">https://openreview.net/attachment/bd85b6068d813279b5a145c8c87a9a185320139f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Empowered by the largest real-world, multi-view dataset to date, we present the first model to reasonably synthesize real-world 3D scenes and reconstruct their geometry conditioned on a single image.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.  In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.</div>
</div>
<div class='paper-counter'>237/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Compositional 3D-aware Video Generation with LLM Director</div>
<div class="field-name">authors:</div>
<div class="field-value">Hanxin Zhu;Tianyu He;Anni Tang;Junliang Guo;Zhibo Chen;Jiang Bian</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D-aware Video Generation; LLM; Compositional</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/208a96eeea481035e495ffccce283af8e1625a86.pdf" target="_blank">https://openreview.net/pdf/208a96eeea481035e495ffccce283af8e1625a86.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f503358b9a24890ebeeef71682c24641703b4cbc.zip" target="_blank">https://openreview.net/attachment/f503358b9a24890ebeeef71682c24641703b4cbc.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual elements within the generated video, such as the movement and appearance of specific characters and the manipulation of viewpoints. In this work, we propose a novel paradigm that generates each element in 3D representation separately and then composites them with priors from Large Language Models (LLMs) and 2D diffusion models. Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with flexible control over each element.</div>
</div>
<div class='paper-counter'>238/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Shangding Gu;Laixi Shi;Yuhao Ding;Alois Knoll;Costas Spanos;Adam Wierman;Ming Jin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safe Reinforcement Learning;Sample Manipulation;Efficient Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ba79b8360e1e32df1bc174e2a4c138266533424a.pdf" target="_blank">https://openreview.net/pdf/ba79b8360e1e32df1bc174e2a4c138266533424a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29\% fewer samples than baselines, and reduces training time by 21--38\%.</div>
</div>
<div class='paper-counter'>239/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Discovering plasticity rules that organize and maintain neural circuits</div>
<div class="field-name">authors:</div>
<div class="field-value">David G Bell;Alison Duffy;Adrienne Fairhall</div>
<div class="field-name">keywords:</div>
<div class="field-value">biologically plausible learning rules plasticity self-organization RNNs homeostasis meta-learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neuroscience_and_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2c8b4c60fb82f62769e10bc0419f404b8627168d.pdf" target="_blank">https://openreview.net/pdf/2c8b4c60fb82f62769e10bc0419f404b8627168d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/875501345ad2812b7c03756b233dad7aad0535d1.zip" target="_blank">https://openreview.net/attachment/875501345ad2812b7c03756b233dad7aad0535d1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A supervised approach yields biologically plausible learning rules that self-organize and maintain robust representations of time within RNNs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Intrinsic dynamics within the brain can accelerate learning by providing a prior scaffolding for dynamics aligned with task objectives. Such intrinsic dynamics would ideally self-organize and self-sustain in the face of biological noise including synaptic turnover and cell death. An example of such dynamics is the formation of sequences, a ubiquitous motif in neural activity. The sequence-generating circuit in zebra finch HVC provides a reliable timing scaffold for motor output in song and demonstrates a remarkable capacity for unsupervised recovery following perturbation. Inspired by HVC, we seek a local plasticity rule capable of organizing and maintaining sequence-generating dynamics despite continual network perturbations. We adopt a meta-learning approach introduced by Confavreux et al, which parameterizes a learning rule using basis functions constructed from pre- and postsynaptic activity and synapse size, with tunable time constants. Candidate rules are simulated within initially random networks, and their fitness is evaluated according to a loss function that measures the fidelity with which the resulting dynamics encode time. We use this approach to introduce biological noise, forcing meta-learning to find robust solutions. We first show that, in the absence of perturbations, meta-learning identifies a temporally asymmetric generalization of Oja's rule that reliably organizes sparse sequential activity. When synaptic turnover is introduced, the learned rule incorporates a form of homeostasis, better maintaining robust sequential dynamics relative to other previously proposed rules. Additionally, inspired by recent findings demonstrating that the strength of projections from inhibitory interneurons in HVC also dynamically responds to perturbations, we explore the role of inhibitory plasticity in sequence-generating circuits. We find that learned plasticity adjusts both excitation and inhibition in response to manipulations, outperforming rules applied only to excitatory connections. We demonstrate how plasticity acting on both excitatory and inhibitory synapses can better shape excitatory cell dynamics to scaffold timing representations.</div>
</div>
<div class='paper-counter'>240/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongchao Zhang;Zhizhen Qin;Sicun Gao;Andrew Clark</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safe Control;Barrier Functions;Control Barrier Functions;Neural Networks</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8c8be656daa65c9db0d7eaaf0f5e2cbcf3137202.pdf" target="_blank">https://openreview.net/pdf/8c8be656daa65c9db0d7eaaf0f5e2cbcf3137202.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/30b91db354495f76f642059c3cd273ea85e52ea7.zip" target="_blank">https://openreview.net/attachment/30b91db354495f76f642059c3cd273ea85e52ea7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a ReLU NCBF synthesis framework with efficient exact verification for robotic safety. Key insights: ReLU NCBF can be verified in linear pieces, boundary pieces are safety-critical, and limiting them improves efficiency.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.</div>
</div>
<div class='paper-counter'>241/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Text-Aware Diffusion for Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Calvin Luo;Mandy He;Zilai Zeng;Chen Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion models;reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7969af175f3dac2cdf426c6629415a5f6951c777.pdf" target="_blank">https://openreview.net/pdf/7969af175f3dac2cdf426c6629415a5f6951c777.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2716387f28f6530aaef3405e7cf114b39fc09cd6.zip" target="_blank">https://openreview.net/attachment/2716387f28f6530aaef3405e7cf114b39fc09cd6.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Pre-trained, frozen diffusion models generate dense zero-shot reward signals for text-conditioned policy learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Training an agent to achieve particular goals or perform desired behaviors is often accomplished through reinforcement learning, especially in the absence of expert demonstrations.  However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable. To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning.  We hypothesize that large-scale pretrained generative models encode rich priors that can supervise a policy to behave not only in a text-aligned manner, but also in alignment with a notion of naturalness summarized from internet-scale training data.  In our experiments, we demonstrate that TADPoLe is able to learn policies for novel goal-achievement and continuous locomotion behaviors specified by natural language, in both Humanoid and Dog environments. The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation. We further show that TADPoLe performs competitively when applied to robotic manipulation tasks in the Meta-World environment, without having access to any in-domain demonstrations.</div>
</div>
<div class='paper-counter'>242/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffSF: Diffusion Models for Scene Flow Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yushan Zhang;Bastian Wandt;Maria Magnusson;Michael Felsberg</div>
<div class="field-name">keywords:</div>
<div class="field-value">Scene Flow Estimation;Denoising Diffusion Models;Uncertainty</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/469a83f616f15f8f0d5c0e23e222481187df6199.pdf" target="_blank">https://openreview.net/pdf/469a83f616f15f8f0d5c0e23e222481187df6199.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models. In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks. Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions. The code is available at https://github.com/ZhangYushan3/DiffSF.</div>
</div>
<div class='paper-counter'>243/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees</div>
<div class="field-name">authors:</div>
<div class="field-value">Negin Musavi;Ziyao Guo;Geir Dullerud;Yingying Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">set-membership;least-squares;nonlinear systems;non-asymptotic guarantees</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/93c6f92221521c38dae4a17d2c9fb3a257c92ed1.pdf" target="_blank">https://openreview.net/pdf/93c6f92221521c38dae4a17d2c9fb3a257c92ed1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ae7d9dab406eeaf52c397476e92c7c51911b0bc1.zip" target="_blank">https://openreview.net/attachment/ae7d9dab406eeaf52c397476e92c7c51911b0bc1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper generalizes the system estimation conditions for nonlinear control systems under i.i.d. inputs and provides non-asymptotic analysis.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper focuses on the system identification of an important class of nonlinear systems: nonlinear systems that are linearly parameterized, which enjoy wide applications in robotics and other mechanical systems. We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters. We provide non-asymptotic convergence rates for LSE and SME under  i.i.d. control inputs and control policies with i.i.d. random perturbations, both of which are considered as non-active-exploration inputs. Compared with the counter-example based on piecewise-affine systems in the literature, the success of non-active exploration in our setting relies on a key assumption about the system dynamics: we require the system functions to be real-analytic. Our results, together with the piecewise-affine counter-example, reveal the importance of differentiability in nonlinear system identification through non-active exploration. Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example.</div>
</div>
<div class='paper-counter'>244/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Cooperative Trajectory Representations for Motion Forecasting</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongzhi Ruan;Haibao Yu;Wenxian Yang;Siqi Fan;Zaiqing Nie</div>
<div class="field-name">keywords:</div>
<div class="field-value">Cooperative Autonomous Driving;Motion Forecasting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/262f0f4a1a5579270c8d2e1921cd2c8b943c1a59.pdf" target="_blank">https://openreview.net/pdf/262f0f4a1a5579270c8d2e1921cd2c8b943c1a59.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/36df1818fba82926f5a9bc27ec8ca3641e9a8de3.zip" target="_blank">https://openreview.net/attachment/36df1818fba82926f5a9bc27ec8ca3641e9a8de3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. In this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. Specifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. V2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.Experimental results on both V2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.Find the project at https://github.com/AIR-THU/V2X-Graph.</div>
</div>
<div class='paper-counter'>245/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hanzhang Zhou;Zijian Feng;Zixiao Zhu;Junlang Qian;Kezhi Mao</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM Bias;In-Context Learning;Attention and FFN Manipulation;Prompt Brittleness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">natural_language_processing</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/358d90ff7d8d269005fc19a00baabfed20752d30.pdf" target="_blank">https://openreview.net/pdf/358d90ff7d8d269005fc19a00baabfed20752d30.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Unveiling internal mechanisms that lead to LLM bias and mitigate such bias by manipulation of LLM internal structures.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness—sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.</div>
</div>
<div class='paper-counter'>246/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow</div>
<div class="field-name">authors:</div>
<div class="field-value">Chen-Hao Chao;Chien Feng;Wei-Fang Sun;Cheng-Kuang Lee;Simon See;Chun-Yi Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Maximum Entropy Reinforcement Learning;Normalizing Flows;Energy-Based Models;Energy-Based Normalizing Flow</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec522f54f9fae11a7e5da3c9da4418013805b5c3.pdf" target="_blank">https://openreview.net/pdf/ec522f54f9fae11a7e5da3c9da4418013805b5c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Existing Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) methods for continuous action spaces are typically formulated based on actor-critic frameworks and optimized through alternating steps of policy evaluation and policy improvement. In the policy evaluation steps, the critic is updated to capture the soft Q-function. In the policy improvement steps, the actor is adjusted in accordance with the updated soft Q-function. In this paper, we introduce a new MaxEnt RL framework modeled using Energy-Based Normalizing Flows (EBFlow). This framework integrates the policy evaluation steps and the policy improvement steps, resulting in a single objective training process. Our method enables the calculation of the soft value function used in the policy evaluation target without Monte Carlo approximation. Moreover, this design supports the modeling of multi-modal action distributions while facilitating efficient action sampling. To evaluate the performance of our method, we conducted experiments on the MuJoCo benchmark suite and a number of high-dimensional robotic tasks simulated by Omniverse Isaac Gym. The evaluation results demonstrate that our method achieves superior performance compared to widely-adopted representative baselines.</div>
</div>
<div class='paper-counter'>247/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space</div>
<div class="field-name">authors:</div>
<div class="field-value">Maximilian Stölzle;Cosimo Della Santina</div>
<div class="field-name">keywords:</div>
<div class="field-value">Dynamical Systems;Control Theory;Robotics;Decision and Control;Deep Autoencoders</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fac38d41d80be55ab5f6463f5ba220894dd5d996.pdf" target="_blank">https://openreview.net/pdf/fac38d41d80be55ab5f6463f5ba220894dd5d996.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We leverage input-to-state stable coupled oscillator networks for conducting model-based control in latent space.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Even though a variety of methods have been proposed in the literature, efficient and effective latent-space control (i.e., control in a learned low-dimensional space) of physical systems remains an open challenge.We argue that a promising avenue is to leverage powerful and well-understood closed-form strategies from control theory literature in combination with learned dynamics, such as potential-energy shaping.We identify three fundamental shortcomings in existing latent-space models that have so far prevented this powerful combination: (i) they lack the mathematical structure of a physical system, (ii) they do not inherently conserve the stability properties of the real systems, (iii) these methods do not have an invertible mapping between input and latent-space forcing.This work proposes a novel Coupled Oscillator Network (CON) model that simultaneously tackles all these issues. More specifically, (i) we show analytically that CON is a Lagrangian system - i.e., it possesses well-defined potential and kinetic energy terms. Then, (ii) we provide formal proof of global Input-to-State stability using Lyapunov arguments.Moving to the experimental side, we demonstrate that CON reaches SoA performance when learning complex nonlinear dynamics of mechanical systems directly from images.An additional methodological innovation contributing to achieving this third goal is an approximated closed-form solution for efficient integration of network dynamics, which eases efficient training.We tackle (iii) by approximating the forcing-to-input mapping with a decoder that is trained to reconstruct the input based on the encoded latent space force.Finally, we leverage these three properties and show that they enable latent-space control. We use an integral-saturated PID with potential force compensation and demonstrate high-quality performance on a soft robot using raw pixels as the only feedback information.</div>
</div>
<div class='paper-counter'>248/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion-Reward Adversarial Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Chun-Mao Lai;Hsiang-Chun Wang;Ping-Chun Hsieh;Yu-Chiang Frank Wang;Min-Hung Chen;Shao-Hua Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Adversarial Imitation Learning;Diffusion Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/066237eeddc2c1e8b6468563b612bea809593220.pdf" target="_blank">https://openreview.net/pdf/066237eeddc2c1e8b6468563b612bea809593220.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work proposes a novel adversarial imitation learning framework that integrates a diffusion model into generative adversarial imitation learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, we propose Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more robust and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator, and design diffusion rewards based on the classifier’s output for policy learning. Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL’s effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more robust and smoother rewards. Project page: https://nturobotlearninglab.github.io/DRAIL/</div>
</div>
<div class='paper-counter'>249/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EGODE: An Event-attended Graph ODE Framework for Modeling Rigid Dynamics</div>
<div class="field-name">authors:</div>
<div class="field-value">Jingyang Yuan;Gongbo Sun;Zhiping Xiao;Hang Zhou;Xiao Luo;Junyu Luo;Yusheng Zhao;Wei Ju;Ming Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Keywords: Interacting Dynamical System;Rigid Simulation;Neural ODE;Graph Neural Network</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_learning_for_physical_sciences</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c4c3ac7cd0379357d24027e69a2515268b592a50.pdf" target="_blank">https://openreview.net/pdf/c4c3ac7cd0379357d24027e69a2515268b592a50.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper studies the problem of rigid dynamics modeling, which has a wide range of applications in robotics, graphics, and mechanical design. The problem is partly solved by graph neural network (GNN) simulators. However, these approaches cannot effectively handle the relationship between intrinsic continuity and instantaneous changes in rigid dynamics. Moreover, they usually neglect hierarchical structures across mesh nodes and objects in systems. In this paper, we propose a novel approach named Event-attend Graph ODE (EGODE) for effective rigid dynamics modeling. In particular, we describe the rigid system using both mesh node representations and object representations. To model continuous dynamics across hierarchical structures, we use a coupled graph ODE framework for the evolution of both types of representations over a long period. In addition, to capture instantaneous changes during the collision, we introduce an event module, which can effectively estimate the occurrence of the collision and update the states of both mesh node and object representations during evolution. Extensive experiments on a range of benchmark datasets validate the superiority of the proposed EGODE compared to various state-of-the-art baselines. The source code can be found at https://github.com/yuanjypku/EGODE.</div>
</div>
<div class='paper-counter'>250/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention</div>
<div class="field-name">authors:</div>
<div class="field-value">Haomeng Zhang;Chiao An Yang;Raymond A. Yeh</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D grounding;vision and language</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a00e4f772e2813276a2eaadaf70c65a0f5e563ee.pdf" target="_blank">https://openreview.net/pdf/a00e4f772e2813276a2eaadaf70c65a0f5e563ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ca568c48e4ed9a856f096acff29913e6e90c1dc7.zip" target="_blank">https://openreview.net/attachment/ca568c48e4ed9a856f096acff29913e6e90c1dc7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-object 3D Grounding involves locating 3D boxes based on a given query phrase from a point cloud. It is a challenging and significant task that has numerous applications in visual understanding, human-computer interaction, and robotics. To tackle this challenge, we introduce D-LISA, a two-stage approach that incorporates three innovations. First, a dynamic vision module that enables a variable and learnable number of box proposals. Second, a dynamic camera positioning that extracts features for each proposal. Third, a language-informed spatial attention module that better reasons over the proposals to output the final prediction. Empirically, experiments show that our method outperforms the state-of-the-art methods on multi-object 3D grounding by 12.8% (absolute) and is competitive in single-object 3D grounding.</div>
</div>
<div class='paper-counter'>251/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Variational Distillation of Diffusion Policies into Mixture of Experts</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongyi Zhou;Denis Blessing;Ge Li;Onur Celik;Xiaogang Jia;Gerhard Neumann;Rudolf Lioutikov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diverse Behavior Learning;Model Distillation;Diffusion Models;Variational Inference</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/198eed271f6d6fdbd5ab59ac4223f2e082726e32.pdf" target="_blank">https://openreview.net/pdf/198eed271f6d6fdbd5ab59ac4223f2e082726e32.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work introduces Variational Diffusion Distillation (VDD), a novel method for distilling denoising diffusion policies into a Mixture of Experts (MoE).</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work introduces Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. Diffusion Models are the current state-of-the-art in generative modeling due to their exceptional ability to accurately learn and represent complex, multi-modal distributions. This ability allows Diffusion Models to replicate the inherent diversity in human behavior, making them the preferred models in behavior learning such as Learning from Human Demonstrations (LfD).However, diffusion models come with some drawbacks, including the intractability of likelihoods and long inference times due to their iterative sampling process. The inference times, in particular, pose a significant challenge to real-time applications such as robot control.In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train.VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the benefits of Mixture Models.Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.VDD demonstrates across nine complex behavior learning tasks, that it is able to: i) accurately distill complex distributions learned by the diffusion model, ii) outperform existing state-of-the-art distillation methods, and iii) surpass conventional methods for training MoE. The code and videos are available at https://intuitive-robots.github.io/vdd-website.</div>
</div>
<div class='paper-counter'>252/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiawei Gao;Ziqin Wang;Zeqi Xiao;Jingbo Wang;Tai Wang;Jinkun Cao;Xiaolin Hu;Si Liu;Jifeng Dai;Jiangmiao Pang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Physics-based Character Animation;Human-Object Interactions;Multi-Agent Cooperation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dd44c497d9be71bd929e7c858aa8fbc75b131dac.pdf" target="_blank">https://openreview.net/pdf/dd44c497d9be71bd929e7c858aa8fbc75b131dac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/75ad5950dd8b5f0d43ac2b8aaf07a53453b3efd3.zip" target="_blank">https://openreview.net/attachment/75ad5950dd8b5f0d43ac2b8aaf07a53453b3efd3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel framework for physics-based animation of multi-person carrying tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce **Coo**perative **H**uman-**O**bject **I**nteraction (**CooHOI**), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.</div>
</div>
<div class='paper-counter'>253/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Your contrastive learning problem is secretly a distribution alignment problem</div>
<div class="field-name">authors:</div>
<div class="field-value">Zihao Chen;Chi-Heng Lin;Ran Liu;Jingyun Xiao;Eva L Dyer</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimal transport;Distribution alignment;Noise contrastive estimation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c7c6656445f089a46f221ffe1c65abf802905470.pdf" target="_blank">https://openreview.net/pdf/c7c6656445f089a46f221ffe1c65abf802905470.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ac0f8972f8ccc0fd106026e5f4cfb924252c73bc.zip" target="_blank">https://openreview.net/attachment/ac0f8972f8ccc0fd106026e5f4cfb924252c73bc.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this work, we introduce a novel framework for representation learning that recasts contrastive estimation as a distribution alignment problem.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the success of contrastive learning (CL) in vision and language, its theoretical foundations and mechanisms for building representations remain poorly understood. In this work, we build connections between noise contrastive estimation losses widely used in CL and distribution alignment with entropic optimal transport (OT). This connection allows us to develop a family of different losses and multistep iterative variants for existing CL methods. Intuitively, by using more information from the distribution of latents, our approach allows a more distribution-aware  manipulation of the relationships within augmented sample sets.We provide theoretical insights and experimental evidence demonstrating the benefits of our approach for generalized contrastive alignment. Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment.By reframing contrastive learning as an alignment problem and leveraging existing optimization tools for OT, our work provides new insights and connections between different self-supervised learning models in addition to new tools that can be more easily adapted to incorporate domain knowledge into learning.</div>
</div>
<div class='paper-counter'>254/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery</div>
<div class="field-name">authors:</div>
<div class="field-value">Alexander Rutherford;Michael Beukman;Timon Willi;Bruno Lacerda;Nick Hawes;Jakob Nicolaus Foerster</div>
<div class="field-name">keywords:</div>
<div class="field-value">MARL;UED;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/450f8065819bc0d3298b71c62f255c51668b3b69.pdf" target="_blank">https://openreview.net/pdf/450f8065819bc0d3298b71c62f255c51668b3b69.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An improved score function for unsupervised environment design in binary outcome settings, which we use to train agents for real-world tasks, and an improved adversarial evaluation protocol that assesses policy robustness.</div>
<div class="field-name">abstract:</div>
<div class="field-value">What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks.This work investigates how existing UED methods select training environments, focusing on task prioritisation metrics.Surprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate.As a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities. Put differently, current methods fail to predict intuitive measures of *learnability*. Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always.Based on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR).We open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability.</div>
</div>
<div class='paper-counter'>255/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions</div>
<div class="field-name">authors:</div>
<div class="field-value">Zizhao Wang;Jiaheng Hu;Caleb Chuck;Stephen Chen;Roberto Martín-Martín;Amy Zhang;Scott Niekum;Peter Stone</div>
<div class="field-name">keywords:</div>
<div class="field-value">unsupervised skill discovery</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/02c32e154257dc57f63f03ad314a8242d5f3dbdb.pdf" target="_blank">https://openreview.net/pdf/02c32e154257dc57f63f03ad314a8242d5f3dbdb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free interactions with environments. Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states. However, in complex environments with many state factors (e.g., household environments with many objects), learning skills that cover all possible states is impossible, and naively encouraging state diversity often leads to simple skills that are not ideal for solving downstream tasks. This work introduces Skill Discovery from Local Dependencies (SkiLD), which leverages state factorization as a natural inductive bias to guide the skill learning process. The key intuition guiding SkiLD is that skills that induce \textbf{diverse interactions} between state factors are often more valuable for solving downstream tasks. To this end, SkiLD develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment. We evaluate SkiLD in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where SkiLD successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage.</div>
</div>
<div class='paper-counter'>256/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals</div>
<div class="field-name">authors:</div>
<div class="field-value">Abhiram Iyer;Sarthak Chandra;Sugandha Sharma;Ila R Fiete</div>
<div class="field-name">keywords:</div>
<div class="field-value">grid cells;cognitive mapping;cognitive maps;self-supervised learning;entorhinal cortex;path integration;neuroscience;dimensionality reduction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neuroscience_and_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/daa997d06d5eaa24f486f64213887a18931dacce.pdf" target="_blank">https://openreview.net/pdf/daa997d06d5eaa24f486f64213887a18931dacce.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Grid cells in the medial entorhinal cortex create remarkable periodic maps of explored space during navigation. Recent studies show that they form similar maps of abstract cognitive spaces. Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink). Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces. To sidestep the computational cost of learning representations for each high-dimensional sensory input, the brain extracts self-consistent, low-dimensional descriptions of displacements across abstract spaces, leveraging the spatial velocity integration of grid cells to efficiently build maps of different domains.Our neural network model for abstract velocity extraction factorizes the content of these abstract domains from displacements within the domains to generate content-independent and self-consistent, low-dimensional velocity estimates. Crucially, it uses a self-supervised geometric consistency constraint that requires displacements along closed loop trajectories to sum to zero, an integration that is itself performed by the downstream grid cell circuit over learning. This process results in high fidelity estimates of velocities and allowed transitions in abstract domains, a crucial prerequisite for efficient map generation in these high-dimensional environments. We also show how our method outperforms traditional dimensionality reduction and deep-learning based motion extraction networks on the same set of tasks.This is the first neural network model to explain how grid cells can flexibly represent different abstract spaces and makes the novel prediction that they should do so while maintaining their population correlation and manifold structure across domains. Fundamentally, our model sheds light on the mechanistic origins of cognitive flexibility and transfer of representations across vastly different domains in brains, providing a potential self-supervised learning (SSL) framework for leveraging similar ideas in transfer learning and data-efficient generalization in machine learning and robotics.</div>
</div>
<div class='paper-counter'>257/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Optimizing Automatic Differentiation with Deep Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jamie Lohoff;Emre Neftci</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Automatic Differentiation;AlphaZero;Deep Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5e8452cabdcbe57ca595b5b7a5bc46375ef01533.pdf" target="_blank">https://openreview.net/pdf/5e8452cabdcbe57ca595b5b7a5bc46375ef01533.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/11b75e5e32d5316bf9bd3888a9fb1792442d40c2.zip" target="_blank">https://openreview.net/attachment/11b75e5e32d5316bf9bd3888a9fb1792442d40c2.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We trained a neural network to produce new automatic differentiation algorithms using Deep RL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.Finding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can execute the obtained elimination orders.</div>
</div>
<div class='paper-counter'>258/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaidong Zhang;Pengzhen Ren;Bingqian Lin;Junfan Lin;Shikui Ma;Hang Xu;Xiaodan Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot manipulation; World model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/070d36407fda81f907f8e4ea5020c264b9e28d52.pdf" target="_blank">https://openreview.net/pdf/070d36407fda81f907f8e4ea5020c264b9e28d52.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d05e98e7d4eb4305c7ec7a7591cce04fb8327f25.zip" target="_blank">https://openreview.net/attachment/d05e98e7d4eb4305c7ec7a7591cce04fb8327f25.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Language-guided robotic manipulation is a challenging task that requires an embodied agent to follow abstract user instructions to accomplish various complex manipulation tasks. Previous work generally maps instructions and visual perceptions directly to low-level executable actions, neglecting the modeling of critical waypoints (e.g., key states of “close to/grab/move up” in action trajectories) in manipulation tasks.To address this issue, we propose a PImitive-driVen waypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses solely on the prediction of task-relevant waypoints. Specifically, PIVOT-R consists of a Waypoint-aware World Model (WAWM) and a lightweight action prediction module. The former performs primitive action parsing and primitive-driven waypoint prediction, while the latter focuses on decoding low-level actions. Additionally, we also design an asynchronous hierarchical executor (AHE) for PIVOT-R, which can use different execution frequencies for different modules of the model, thereby helping the model reduce computational redundancy and improve model execution efficiency. Our PIVOT-R outperforms state-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving an average relative improvement of 19.45% across four levels of instruction tasks. Moreover, compared to the synchronously executed PIVOT-R, the execution efficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop in performance. These results provide compelling evidence that our PIVOT-R can significantly improve both the performance and efficiency of robotic manipulation.</div>
</div>
<div class='paper-counter'>259/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Sriyash Poddar;Yanming Wan;Hamish Ivison;Abhishek Gupta;Natasha Jaques</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning from human feedback;preference based reward learning;pluralistic alignment;multimodal reward modeling;variational inference</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/100f9da6a2c6f9d3d08c26432688a8a57f9cbd85.pdf" target="_blank">https://openreview.net/pdf/100f9da6a2c6f9d3d08c26432688a8a57f9cbd85.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups. To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods. Our proposed techniques are based on a latent variable formulation - inferring a novel user-specific latent and learning reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment.</div>
</div>
<div class='paper-counter'>260/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Referring Human Pose and Mask Estimation In the Wild</div>
<div class="field-name">authors:</div>
<div class="field-value">Bo Miao;Mingtao Feng;Zijie Wu;Mohammed Bennamoun;Yongsheng Gao;Ajmal Saeed Mian</div>
<div class="field-name">keywords:</div>
<div class="field-value">Referring Human Pose and Mask Estimation;Object Segmentation;Human-AI Interaction;Vision-and-Language;Multimodal Reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3a47c08a3759b5f79ef8599d6d6317ead8042340.pdf" target="_blank">https://openreview.net/pdf/3a47c08a3759b5f79ef8599d6d6317ead8042340.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A new prompt reasoning task that predicts poses and masks of specified individuals using text or positional prompts to advance human-AI interaction, supported by a new large-scale dataset and a unified promptable approach.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017.</div>
</div>
<div class='paper-counter'>261/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Pretrained Optimization Model for Zero-Shot Black Box Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaobin Li;Kai Wu;Yujian Betterest Li;Xiaoyu Zhang;Handing Wang;Jing Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Pretrained Optimization Models;Black-box Optimization;Zero-shot Optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">optimization</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/24b6392a5d62ba6e70b5ee17d81ef7efa1ed0e88.pdf" target="_blank">https://openreview.net/pdf/24b6392a5d62ba6e70b5ee17d81ef7efa1ed0e88.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work considers zero-shot black box optimization with a population-based pretrained optimization model (POM).</div>
<div class="field-name">abstract:</div>
<div class="field-value">Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/.</div>
</div>
<div class='paper-counter'>262/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LaKD: Length-agnostic Knowledge Distillation for Trajectory Prediction with Any Length Observations</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuhang Li;Changsheng Li;Ruilin Lv;Rongqing Li;Ye Yuan;Guoren Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Trajectory Prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7cf0348cc3747c46278bb98d27d152a16c5722d3.pdf" target="_blank">https://openreview.net/pdf/7cf0348cc3747c46278bb98d27d152a16c5722d3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Trajectory prediction is a crucial technology to help systems avoid traffic accidents, ensuring safe autonomous driving. Previous methods typically use a fixed-length and sufficiently long trajectory of an agent as observations to predict its future trajectory. However, in real-world scenarios, we often lack the time to gather enough trajectory points before making predictions, e.g., when a car suddenly appears due to an obstruction, the system must make immediate predictions to prevent a collision. This poses a new challenge for trajectory prediction systems, requiring them to be capable of making accurate predictions based on observed trajectories of arbitrary lengths, leading to the failure of existing methods. In this paper, we propose a Length-agnostic Knowledge Distillation framework, named LaKD,  which can make accurate trajectory predictions, regardless of the length of observed data. Specifically, considering the fact that long trajectories, containing richer temporal information but potentially additional interference, may perform better or worse than short trajectories, we devise a dynamic length-agnostic knowledge distillation mechanism for exchanging information among trajectories of arbitrary lengths, dynamically determining the transfer direction based on prediction performance. In contrast to traditional knowledge distillation, LaKD employs a unique model that simultaneously serves as both the teacher and the student, potentially causing knowledge collision during the distillation process. Therefore, we design a dynamic soft-masking mechanism, where we first calculate the importance of neuron units and then apply soft-masking to them, so as to safeguard critical units from disruption during the knowledge distillation process. In essence, LaKD is a general and principled framework that can be naturally compatible with existing trajectory prediction models of different architectures. Extensive experiments on three benchmark datasets, Argoverse 1, nuScenes and Argoverse 2, demonstrate the effectiveness of our approach.</div>
</div>
<div class='paper-counter'>263/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CountGD: Multi-Modal Open-World Counting</div>
<div class="field-name">authors:</div>
<div class="field-value">Niki Amini-Naieni;Tengda Han;Andrew Zisserman</div>
<div class="field-name">keywords:</div>
<div class="field-value">multi-modal open-world counting;vision-language foundation model;open-world object counting;class-agnostic counting;text-specified counting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/673724e078a1d785fdd87105db2bf8e2fe667b77.pdf" target="_blank">https://openreview.net/pdf/673724e078a1d785fdd87105db2bf8e2fe667b77.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose CountGD, a state-of-the-art multi-modal open-world object counting model that can count arbitrary objects given visual exemplars, text, or both together, fusing the modalities to accurately estimate the object count.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The goal of this paper is to improve the generality and accuracy of open-vocabulary object counting in images. To improve the generality, we repurpose an open-vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars. In turn, these new capabilities -- being able to specify the target object by multi-modalites (text and exemplars) -- lead to an improvement in counting accuracy. We make three contributions: First, we introduce the first open-world counting model, CountGD,  where the prompt can be specified by a text description or visual exemplars or both; Second, we show that the performance of the model significantly improves the state of the art on multiple counting benchmarks -- when using text only, CountGD outperforms all previous text-only works, and when using both text and visual exemplars, we outperform all previous models; Third, we carry out a preliminary study into different interactions between the text and visual exemplar prompts, including the cases where they reinforce each other and where one restricts the other. The code and an app to test the model are available at https://www.robots.ox.ac.uk/vgg/research/countgd/.</div>
</div>
<div class='paper-counter'>264/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Unlock the Intermittent Control Ability of Model Free Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiashun Liu;Jianye HAO;Xiaotian Hao;Yi Ma;YAN ZHENG;Yujing Hu;Tangjie Lv</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforcement Learning ; Representation Learning; Intermittent Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/866a97e9a7f9d894309e4373c61c5ed45679250b.pdf" target="_blank">https://openreview.net/pdf/866a97e9a7f9d894309e4373c61c5ed45679250b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/68ecee19febaab7ae8d7954a0d43b80ffbce3ad3.zip" target="_blank">https://openreview.net/attachment/68ecee19febaab7ae8d7954a0d43b80ffbce3ad3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We observe that previous DRL methods fail to learn effective policies in intermittent control scenarios because of the discontinue interaction and propose a plugin method for DRL to address  such problems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Intermittent control problems are common in real world. The interactions between the decision maker and the executor can be discontinuous (intermittent) due to various types of interruptions, e.g. unstable communication channel. Due to intermittent interaction, agents are unable to acquire the state sent by the executor and cannot transmit actions to the executor within a period of time step, i.e. bidirectional blockage, which may lead to inefficiencies of reinforcement learning policies and prevent the executors from completing the task. Such problem is not well studied in the RL community. In this paper, we model Intermittent control problem as an Intermittent Control Markov Decision Process, i.e agents are expected to generate action sequences corresponding to the unavailable states and transmit them before disabling interactions to ensure the smooth and effective motion of executors. However, directly generating multiple future actions in the original action space has unnatural motion issue and exploration difficulty. We propose **M**ulti-step **A**ction **R**epre**S**entation (**MARS**), which encodes a sequence of actions from the original action space to a compact and decodable latent space. Then based on the latent action sequence representation, the mainstream RL methods can be easily optimized to learn a smooth and efficient motion policy. Extensive experiments on simulation tasks and real-world robotic grasping tasks show that MARS significantly improves the learning efficiency and final performances compared with existing baselines.</div>
</div>
<div class='paper-counter'>265/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robust Reinforcement Learning from Corrupted Human Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Alexander Bukharin;Ilgee Hong;Haoming Jiang;Zichong Li;Qingru Zhang;Zixuan Zhang;Tuo Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning from Human Feedback;Robust Reward Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ed58795eedc15829f681216397668d1bc4d6e894.pdf" target="_blank">https://openreview.net/pdf/ed58795eedc15829f681216397668d1bc4d6e894.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new robust reward learning method for RLHF.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning from human feedback (RLHF) provides a principled framework for aligning AI systems with human preference data. For various reasons, e.g., personal bias, context ambiguity, lack of training, etc, human annotators may give incorrect or inconsistent preference labels.  To tackle this challenge, we propose a robust RLHF approach -- $R^3M$, which models the potentially corrupted preference label as sparse outliers. Accordingly, we formulate the robust reward learning as an $\ell_1$-regularized maximum likelihood estimation problem. Computationally, we develop an efficient alternating optimization algorithm, which only incurs negligible computational overhead compared with the standard RLHF approach. Theoretically, we prove that under proper regularity conditions, $R^3M$ can consistently learn the underlying reward and identify outliers, provided that the number of outlier labels scales sublinearly with the preference sample size. Furthermore, we remark that $R^3M$ is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO). Our experiments on robotic control and natural language generation with large language models (LLMs) show that $R^3M$ improves robustness of the reward  against several types of perturbations to the preference data.</div>
</div>
<div class='paper-counter'>266/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Instruction-Guided Visual Masking</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinliang Zheng;Jianxiong Li;Sijie Cheng;Yinan Zheng;Jiaming Li;Jihao Liu;Yu Liu;Jingjing Liu;Xianyuan Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value">MultiModal Instruction Following;Visual Grounding;Large MultiModal Models;Embodied AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8b3b0bd660a93a59b896bef55f7699a1728e3544.pdf" target="_blank">https://openreview.net/pdf/8b3b0bd660a93a59b896bef55f7699a1728e3544.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/931ef549c761e32f7c550b8d5dc0d6bc3d30f629.zip" target="_blank">https://openreview.net/attachment/931ef549c761e32f7c550b8d5dc0d6bc3d30f629.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A generic and powerful visual grounding model that enhances broad multimodal instruction following tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model. By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions. Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs. We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples. Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks. Code, model and data are available at https://github.com/2toinf/IVM.</div>
</div>
<div class='paper-counter'>267/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Splatter a Video: Video Gaussian Representation  for Versatile Processing</div>
<div class="field-name">authors:</div>
<div class="field-value">Yang-Tian Sun;Yi-Hua Huang;Lin Ma;Xiaoyang Lyu;Yan-Pei Cao;XIAOJUAN QI</div>
<div class="field-name">keywords:</div>
<div class="field-value">Video Representation; Video Processing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d8203dcb42e1329589c7539b6ee7b267032da700.pdf" target="_blank">https://openreview.net/pdf/d8203dcb42e1329589c7539b6ee7b267032da700.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6678bef74e5a63997b4ddcdfc3911762cf3d7353.zip" target="_blank">https://openreview.net/attachment/6678bef74e5a63997b4ddcdfc3911762cf3d7353.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A method to represent casual videos using 3D Gaussians without estimating camera pose</div>
<div class="field-name">abstract:</div>
<div class="field-value">Video representation is a long-standing problem that is crucial for various downstream tasks, such as tracking, depth prediction, segmentation, view synthesis, and editing. However, current methods either struggle to model complex motions due to the absence of 3D structure or rely on implicit 3D representations that are ill-suited for manipulation tasks. To address these challenges, we introduce a novel explicit 3D representation—video Gaussian representation—that embeds a video into 3D Gaussians. Our proposed representation models video appearance in a 3D canonical space using explicit Gaussians as proxies and associates each Gaussian with 3D motions for video motion. This approach offers a more intrinsic and explicit representation than layered atlas or volumetric pixel matrices. To obtain such a representation, we distill 2D priors, such as optical flow and depth, from foundation models to regularize learning in this ill-posed setting.Extensive applications demonstrate the versatility of our new video representation. It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation.</div>
</div>
<div class='paper-counter'>268/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration</div>
<div class="field-name">authors:</div>
<div class="field-value">Renlang Huang;Yufan Tang;Jiming Chen;Liang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Point cloud registration;Rigid transformation estimation;Feature matching;Correspondence;Deep learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0ebedb57564b241db75971993a3c60eeffde03dd.pdf" target="_blank">https://openreview.net/pdf/0ebedb57564b241db75971993a3c60eeffde03dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/15f826882be95a94275b8fd0c9b5709896f4cfec.zip" target="_blank">https://openreview.net/attachment/15f826882be95a94275b8fd0c9b5709896f4cfec.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel consistency-aware spot-guided Transformer for versatile and hierarchical point cloud registration, achieving state-of-the-art accuracy, efficiency, and robustness on both outdoor and indoor benchmarks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Deep learning-based feature matching has shown great superiority for point cloud registration in the absence of pose priors. Although coarse-to-fine matching approaches are prevalent, the coarse matching of existing methods is typically sparse and loose without consideration of geometric consistency, which makes the subsequent fine matching rely on ineffective optimal transport and hypothesis-and-selection methods for consistency. Therefore, these methods are neither efficient nor scalable for real-time applications such as odometry in robotics. To address these issues, we design a consistency-aware spot-guided Transformer (CAST), which incorporates a spot-guided cross-attention module to avoid interfering with irrelevant areas, and a consistency-aware self-attention module to enhance matching capabilities with geometrically consistent correspondences. Furthermore, a lightweight fine matching module for both sparse keypoints and dense features can estimate the transformation accurately. Extensive experiments on both outdoor LiDAR point cloud datasets and indoor RGBD point cloud datasets demonstrate that our method achieves state-of-the-art accuracy, efficiency, and robustness.</div>
</div>
<div class='paper-counter'>269/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</div>
<div class="field-name">authors:</div>
<div class="field-value">Feng Lu;Xinyao Zhang;Canming Ye;Shuting Dong;Lijun Zhang;Xiangyuan Lan;Chun Yuan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Place Recognition;Image Descriptors;VLAD;SuperVLAD;1-Cluster VLAD</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/14fe7ad61d0cefc622031da6004d154b817b1346.pdf" target="_blank">https://openreview.net/pdf/14fe7ad61d0cefc622031da6004d154b817b1346.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization. Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD. These features are suitable for large-scale VPR and robust against viewpoint changes. However, the VLAD-based aggregation methods usually learn a large number of (e.g., 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features. More importantly, when there is a domain gap between the data in training and inference, the cluster centers determined on the training set are usually improper for inference, resulting in a performance drop. To this end, we first attempt to improve NetVLAD by removing the cluster center and setting only a small number of (e.g., only 4) clusters. The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains. We name this method SuperVLAD. In addition, by introducing ghost clusters that will not be retained in the final output, we further propose a very low-dimensional 1-Cluster VLAD descriptor, which has the same dimension as the output of GeM pooling but performs notably better. Experimental results suggest that, when paired with a transformer-based backbone, our SuperVLAD shows better domain generalization performance than NetVLAD with significantly fewer parameters. The proposed method also surpasses state-of-the-art methods with lower feature dimensions on several benchmark datasets. The code is available at https://github.com/lu-feng/SuperVLAD.</div>
</div>
<div class='paper-counter'>270/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoran Lu;Ruihai Wu;Yitong Li;Sijie Li;Ziyu Zhu;Chuanruo Ning;Yan Shen;Longzan Luo;Yuanpei Chen;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Garment Manipulation;Simulation;Benchmark</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/54c17c13caa1af39b2d17587bf979aa4e4816bb2.pdf" target="_blank">https://openreview.net/pdf/54c17c13caa1af39b2d17587bf979aa4e4816bb2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b01ca617521b357eb4f35adab70ffa12b3f05357.zip" target="_blank">https://openreview.net/attachment/b01ca617521b357eb4f35adab70ffa12b3f05357.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose GarmentLab for garment manipulation with realistic and rich simulations and diverse benchmark tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which exhibit offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work.</div>
</div>
<div class='paper-counter'>271/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration</div>
<div class="field-name">authors:</div>
<div class="field-value">Borja G. León;Francesco Riccio;Kaushik Subramanian;Peter R. Wurman;Peter Stone</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Policy Diversity;Generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/797c73757517c0a099940cf56bbd136b9ef349b8.pdf" target="_blank">https://openreview.net/pdf/797c73757517c0a099940cf56bbd136b9ef349b8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Novel algorithm for learning diverse near-optimal policies capable of generalizing within and out-of distribution</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to approach the same problem from different angles is a cornerstone of human intelligence that leads to robust solutions and effective adaptation to problem variations. In contrast, current RL methodologies tend to lead to policies that settle on a single solution to a given problem, making them brittle to problem variations. Replicating human flexibility in reinforcement learning agents is the challenge that we explore in this work. We tackle this challenge by extending state-of-the-art approaches to introduce DUPLEX, a method that explicitly defines a diversity objective with constraints and makes robust estimates of policies’ expected behavior through successor features. The trained agents can (i) learn a diverse set of near-optimal policies in complex highly-dynamic environments and (ii) exhibit competitive and diverse skills in out-of-distribution (OOD) contexts. Empirical results indicate that DUPLEX improves over previous methods and successfully learns competitive driving styles in a hyper-realistic simulator (i.e., GranTurismo ™ 7) as well as diverse and effective policies in several multi-context robotics MuJoCo simulations with OOD gravity forces and height limits. To the best of our knowledge, our method is the first to achieve diverse solutions in complex driving simulators and OOD robotic contexts. DUPLEX agents demonstrating diverse behaviors can be found at https://ai.sony/publications/Discovering-Creative-Behaviors-through-DUPLEX-Diverse-Universal-Features-for-Policy-Exploration/.</div>
</div>
<div class='paper-counter'>272/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MatrixNet: Learning over symmetry groups using learned group representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Lucas Laird;Circe Hsu;Asilata Bapat;Robin Walters</div>
<div class="field-name">keywords:</div>
<div class="field-value">Group Theory;Representation Theory;Feature learning;geometric deep learning;homomorphic</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning_architectures</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ecb23533f22a338db6c7d0e723faedd242783ac0.pdf" target="_blank">https://openreview.net/pdf/ecb23533f22a338db6c7d0e723faedd242783ac0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">When learning over symmetry groups, a learned group representation of group elements works well as a feature  representation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use knownsymmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set. Our code is available at https://github.com/lucas-laird/MatrixNet.</div>
</div>
<div class='paper-counter'>273/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps</div>
<div class="field-name">authors:</div>
<div class="field-value">Nikita Starodubcev;Mikhail Khoroshikh;Artem Babenko;Dmitry Baranchuk</div>
<div class="field-name">keywords:</div>
<div class="field-value">text-guided image editing;consistency distillation;diffusion models;image inversion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dbb548a54050f82ad788c1ff54b1ab069059edbd.pdf" target="_blank">https://openreview.net/pdf/dbb548a54050f82ad788c1ff54b1ab069059edbd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5f600231049b6920f2163393f45592f0d8e186b8.zip" target="_blank">https://openreview.net/attachment/5f600231049b6920f2163393f45592f0d8e186b8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">zero shot image editing with consistency distillation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion distillation represents a highly promising direction for achieving faithful text-to-image generation in a few sampling steps. However, despite recent successes, existing distilled models still do not provide the full spectrum of diffusion abilities, such as real image inversion, which enables many precise image manipulation methods. This work aims to enrich distilled text-to-image diffusion models with the ability to effectively encode real images into their latent space. To this end, we introduce invertible Consistency Distillation (iCD), a generalized consistency distillation framework that facilitates both high-quality image synthesis and accurate image encoding in only 3-4 inference steps. Though the inversion problem for text-to-image diffusion models gets exacerbated by high classifier-free guidance scales, we notice that dynamic guidance significantly reduces reconstruction errors without noticeable degradation in generation performance. As a result, we demonstrate that iCD equipped with dynamic guidance may serve as a highly effective tool for zero-shot text-guided image editing, competing with more expensive state-of-the-art alternatives.</div>
</div>
<div class='paper-counter'>274/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Few-Shot Task Learning through Inverse Generative Modeling</div>
<div class="field-name">authors:</div>
<div class="field-value">Aviv Netanyahu;Yilun Du;Antonia Bronars;Jyothish Pari;Joshua B. Tenenbaum;Tianmin Shu;Pulkit Agrawal</div>
<div class="field-name">keywords:</div>
<div class="field-value">few shot learning;inverse generative modeling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/053a4767d0b787d3a3a3e198cb6ee55524fe4c95.pdf" target="_blank">https://openreview.net/pdf/053a4767d0b787d3a3a3e198cb6ee55524fe4c95.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains -- object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts.</div>
</div>
<div class='paper-counter'>275/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Enhancing Preference-based Linear Bandits via Human Response Time</div>
<div class="field-name">authors:</div>
<div class="field-value">Shen Li;Yuyang Zhang;Zhaolin Ren;Claire Liang;Na Li;Julie Shah</div>
<div class="field-name">keywords:</div>
<div class="field-value">human response time;preference learning;linear bandits;dueling bandits;psychology;economics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">bandits</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b32d10afd0c5117bb0b9ac42cf07b7786e40cbd9.pdf" target="_blank">https://openreview.net/pdf/b32d10afd0c5117bb0b9ac42cf07b7786e40cbd9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Leveraging human response times to accelerate preference learning from binary choices</div>
<div class="field-name">abstract:</div>
<div class="field-value">Interactive preference learning systems infer human preferences by presenting queries as pairs of options and collecting binary choices. Although binary choices are simple and widely used, they provide limited information about preference strength. To address this, we leverage human response times, which are inversely related to preference strength, as an additional signal. We propose a computationally efficient method that combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology. Theoretical and empirical analyses show that for queries with strong preferences, response times complement choices by providing extra information about preference strength, leading to significantly improved utility estimation. We incorporate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that using response times significantly accelerates preference learning compared to choice-only approaches. Additional materials, such as code, slides, and talk video, are available at https://shenlirobot.github.io/pages/NeurIPS24.html.</div>
</div>
<div class='paper-counter'>276/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Navigating Chemical Space with Latent Flows</div>
<div class="field-name">authors:</div>
<div class="field-value">Guanghao Wei;Yining Huang;Chenru Duan;Yue Song;Yuanqi Du</div>
<div class="field-name">keywords:</div>
<div class="field-value">Dynamical System;Optimal Transport;Molecular Discovery;Deep Generative Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_learning_for_physical_sciences</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/61e516bc27676bd44f2fe394a5a9589f144a5b39.pdf" target="_blank">https://openreview.net/pdf/61e516bc27676bd44f2fe394a5a9589f144a5b39.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3a20f1bd8ba0c6b1820554a7b4957ad5a8754fc8.zip" target="_blank">https://openreview.net/attachment/3a20f1bd8ba0c6b1820554a7b4957ad5a8754fc8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery.In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings.Codes and demos are publicly available on GitHub at [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow).</div>
</div>
<div class='paper-counter'>277/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Reconstruction of Manipulated Garment with Guided Deformation Prior</div>
<div class="field-name">authors:</div>
<div class="field-value">Ren Li;Corentin Dumery;Zhantao Deng;Pascal Fua</div>
<div class="field-name">keywords:</div>
<div class="field-value">Garment reconstruction;Deformation priors;Geometric deformations;Garment manipulation;3D to UV mapping;Non-rigid reconstruction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/465505c3b984943682d03d8d99c6f111462fb4db.pdf" target="_blank">https://openreview.net/pdf/465505c3b984943682d03d8d99c6f111462fb4db.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bc0e1cde9870198eaf0aac1ec98b32608ebbb1c7.zip" target="_blank">https://openreview.net/attachment/bc0e1cde9870198eaf0aac1ec98b32608ebbb1c7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Modeling the shape of garments has received much attention, but most existing approaches assume the garments to be worn by someone, which constrains the range of shapes they can assume. In this work, we address shape recovery when garments are being manipulated instead of worn, which gives rise to an even larger range of possible shapes. To this end, we leverage the implicit sewing patterns (ISP) model for garment modeling and extend it by adding a diffusion-based deformation prior to represent these shapes. To recover 3D garment shapes from incomplete 3D point clouds acquired when the garment is folded, we map the points to UV space, in which our priors are learned, to produce partial UV maps, and then fit the priors to recover complete UV maps and 2D to 3D mappings. Experimental results demonstrate the superior reconstruction accuracy of our method compared to previous ones, especially when dealing with large non-rigid deformations arising from the manipulations.</div>
</div>
<div class='paper-counter'>278/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Youpeng Wen;Junfan Lin;Yi Zhu;Jianhua Han;Hang Xu;Shen Zhao;Xiaodan Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation learning;Video prediction;Robot Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e13dd37a46369b87a3c1fbc7cd460d8ed7a7039e.pdf" target="_blank">https://openreview.net/pdf/e13dd37a46369b87a3c1fbc7cd460d8ed7a7039e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ef685f0a923478afc37e1d7dc44ef9f00ffe4a58.zip" target="_blank">https://openreview.net/attachment/ef685f0a923478afc37e1d7dc44ef9f00ffe4a58.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose \textbf{VidMan} (\textbf{Vid}eo Diffusion for Robot \textbf{Man}ipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7\% relative improvement, and demonstrates over 9\% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.</div>
</div>
<div class='paper-counter'>279/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors</div>
<div class="field-name">authors:</div>
<div class="field-value">Chenyang Ma;Kai Lu;Ta-Ying Cheng;Niki Trigoni;Andrew Markham</div>
<div class="field-name">keywords:</div>
<div class="field-value">VLM Spatial Reasoning;Zero-Shot</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/867437d5e4ce3abc8790c6ec15f3bd74162253dc.pdf" target="_blank">https://openreview.net/pdf/867437d5e4ce3abc8790c6ec15f3bd74162253dc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.</div>
</div>
<div class='paper-counter'>280/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Siddharth Nayak;Adelmo Morrison Orozco;Marina Ten Have;Jackson Zhang;Vittal Thirumalai;Darren Chen;Aditya Kapoor;Eric Robinson;Karthik Gopalakrishnan;James Harrison;Anuj Mahajan;brian ichter;Hamsa Balakrishnan</div>
<div class="field-name">keywords:</div>
<div class="field-value">multi-agent robotics;large language models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/30511ccbe77a9b3fbd9e34c29efcb47db4044e7c.pdf" target="_blank">https://openreview.net/pdf/30511ccbe77a9b3fbd9e34c29efcb47db4044e7c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/47f9a482f268b2e6fe171e39f70dc5f020978129.zip" target="_blank">https://openreview.net/attachment/47f9a482f268b2e6fe171e39f70dc5f020978129.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">a VLM-based planner for long-horizon multi-agent robotics in partially observable setting that does not rely on privileged information from the simulator/oracle.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability of Language Models (LMs) to understand natural language makes them a powerful tool for parsing human instructions into task plans for autonomous robots. Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base. However, LMs in their standard form face challenges with long-horizon tasks, particularly in partially observable multi-agent settings. We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments. LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators. Additionally, we present MAP-THOR, a comprehensive test suite encompassing household tasks of varying complexity within the AI2-THOR environment. Experiments show that LLaMAR achieves a 30\% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search \& Rescue tasks. Code can be found at [https://github.com/nsidn98/LLaMAR](https://github.com/nsidn98/LLaMAR)</div>
</div>
<div class='paper-counter'>281/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Huzi Cheng;Joshua W Brown</div>
<div class="field-name">keywords:</div>
<div class="field-value">goal-conditioned RL;planning;multi-task RL;vmPFC;goal-directed behavior;cognitive control;spatial navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neuroscience_and_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a9641cd247e6e7a43fd893f0d42fd17484c244b6.pdf" target="_blank">https://openreview.net/pdf/a9641cd247e6e7a43fd893f0d42fd17484c244b6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduced a new goal reduction mechanism that outperforms RL algorithms in multi-goal tasks and models brain activity.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks.Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge.Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models.Moreover, the goal-reducer, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task.This goal reduction mechanism also models human problem-solving.Comparing the model's performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between an goal-reducer agent's components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors.</div>
</div>
<div class='paper-counter'>282/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Rohan R Paleja;Michael Joseph Munje;Kimberlee Chestnut Chang;Reed Jensen;Matthew Gombolay</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human-Machine Teaming;Adaptive AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value">human-AI_interaction</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc456c7cc9a61128a809d224d0a8b36455e1efcc.pdf" target="_blank">https://openreview.net/pdf/dc456c7cc9a61128a809d224d0a8b36455e1efcc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop approaches that enable iterative, mixed-initiative team development allowing end- users to interactively reprogram interpretable AI teammates and summarize our user study findings into guidelines for future research.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human’s actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and that black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.</div>
</div>
<div class='paper-counter'>283/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Zaijing Li;Yuquan Xie;Rui Shao;Gongwei Chen;Dongmei Jiang;Liqiang Nie</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multimodal Agent;Multimodal Large Language Models;Multimodal In-context Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d57bc4c4a18c0395e77ebc2cfa868685e62bbb3c.pdf" target="_blank">https://openreview.net/pdf/d57bc4c4a18c0395e77ebc2cfa868685e62bbb3c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2da2ea8f7e2e0c249cb277f09a4d63b4a5e01f19.zip" target="_blank">https://openreview.net/attachment/2da2ea8f7e2e0c249cb277f09a4d63b4a5e01f19.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a powerful agent with hybrid multimodal memory architecture, Optimus-1, in Minecraft.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.</div>
</div>
<div class='paper-counter'>284/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TinyTTA: Efficient Test-time Adaptation via Early-exit Ensembles on Edge Devices</div>
<div class="field-name">authors:</div>
<div class="field-value">Hong Jia;Young D. Kwon;Alessio Orsino;Ting Dang;Domenico Talia;Cecilia Mascolo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Test-time adaptation;efficiency;edge device;microcontroller</div>
<div class="field-name">primary_area:</div>
<div class="field-value">infrastructure</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fd2667bca18208a1af603508605fa4b6b2717abe.pdf" target="_blank">https://openreview.net/pdf/fd2667bca18208a1af603508605fa4b6b2717abe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Efficient Test-time Adaptation Framework for Microcontrollers along with an MCU TTA library.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The increased adoption of Internet of Things (IoT) devices has led to the generation of large data streams with applications in healthcare, sustainability, and robotics. In some cases, deep neural networks have been deployed directly on these resource-constrained units to limit communication overhead, increase efficiency and privacy, and enable real-time applications. However, a common challenge in this setting is the continuous adaptation of models necessary to accommodate changing environments, i.e., data distribution shifts. Test-time adaptation (TTA) has emerged as one potential solution, but its validity has yet to be explored in resource-constrained hardware settings, such as those involving microcontroller units (MCUs). TTA on constrained devices generally suffers from i) memory overhead due to the full backpropagation of a large pre-trained network, ii) lack of support for normalization layers on MCUs, and iii) either memory exhaustion with large batch sizes required for updating or poor performance with small batch sizes. In this paper, we propose TinyTTA, to enable, for the first time, efficient TTA on constrained devices with limited memory. To address the limited memory constraints, we introduce a novel self-ensemble and batch-agnostic early-exit strategy for TTA, which enables continuous adaptation with small batch sizes for reduced memory usage, handles distribution shifts, and improves latency efficiency. Moreover, we develop the TinyTTA Engine, a first-of-its-kind MCU library that enables on-device TTA. We validate TinyTTA on a Raspberry Pi Zero 2W and an STM32H747 MCU. Experimental results demonstrate that TinyTTA improves TTA accuracy by up to 57.6\%, reduces memory usage by up to six times, and achieves faster and more energy-efficient TTA. Notably, TinyTTA is the only framework able to run TTA on MCU STM32H747 with a 512 KB memory constraint while maintaining high performance.</div>
</div>
<div class='paper-counter'>285/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to be Smooth: An End-to-End Differentiable Particle Smoother</div>
<div class="field-name">authors:</div>
<div class="field-value">Ali Younis;Erik B. Sudderth</div>
<div class="field-name">keywords:</div>
<div class="field-value">particle;filter;smoother;non-parametric;generative;discriminative;multi-modal;belief propagation;monte carlo;sequential;kernel density estimation;mixture;importance sampling;localization;autonomous driving;smoothing;filtering</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/436d21646700dafd6966a41a37cb937e415cb353.pdf" target="_blank">https://openreview.net/pdf/436d21646700dafd6966a41a37cb937e415cb353.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An end-to-end differentiable framework for training discriminative particle smoothing algorithms, with state-of-the-art performance at city-scale localization from imagery.</div>
<div class="field-name">abstract:</div>
<div class="field-value">For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes.  Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models.  Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers.  Our "two-filter" smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models.  The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps.</div>
</div>
<div class='paper-counter'>286/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Huayu Chen;Kaiwen Zheng;Hang Su;Jun Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion;offline;RL;alignment;continuous control;DPO</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/36b802a6ae74a169344f720fea7757c283bcdf0d.pdf" target="_blank">https://openreview.net/pdf/36b802a6ae74a169344f720fea7757c283bcdf0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/fa87c729eb34bf611056732a4277e0245b8dca0a.zip" target="_blank">https://openreview.net/attachment/fa87c729eb34bf611056732a4277e0245b8dca0a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an alignment strategy for diffusion models in continuous control</div>
<div class="field-name">abstract:</div>
<div class="field-value">Drawing upon recent advances in language model alignment, we formulate offline Reinforcement Learning as a two-stage optimization problem: First pretraining expressive generative policies on reward-free behavior datasets, then finetuning these policies to align with task-specific annotations like Q-values. This strategy allows us to leverage abundant and diverse behavior data to enhance generalization and enable rapid adaptation to downstream tasks using minimal annotations. In particular, we introduce Efficient Diffusion Alignment (EDA) for solving continuous control problems. EDA utilizes diffusion models for behavior modeling. However, unlike previous approaches, we represent diffusion policies as the derivative of a scalar neural network with respect to action inputs. This representation is critical because it enables direct density calculation for diffusion models, making them compatible with existing LLM alignment theories. During policy fine-tuning, we extend preference-based alignment methods like Direct Preference Optimization (DPO) to align diffusion behaviors with continuous Q-functions. Our evaluation on the D4RL benchmark shows that EDA exceeds all baseline methods in overall performance. Notably, EDA maintains about 95\% of performance and still outperforms several baselines given only 1\% of Q-labelled data during fine-tuning.</div>
</div>
<div class='paper-counter'>287/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhou FANG;Yong-Lu Li;Lixin Yang;Cewu Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Articulated Objects;Arbitrary Manipulation;Real Images</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0c8b1bca39b4c4112369176bd348e00d66ce91d7.pdf" target="_blank">https://openreview.net/pdf/0c8b1bca39b4c4112369176bd348e00d66ce91d7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks. Recently, several image editing methods based on diffusion models have been proposed to manipulate articulated objects according to text prompts. However, these methods often generate weird artifacts or even fail in real images. To this end, we introduce the Part-Aware Diffusion Model to approach the manipulation of articulated objects in real images. First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently. Then we propose dynamic feature maps to transfer the appearance of objects from input images to edited ones, meanwhile generating the novel-appearing parts reasonably. Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works. Additionally, we verify our method on 3D articulated object understanding forembodied robot scenarios and the promising results prove that our method supports this task strongly. The project page is https://mvig-rhos.com/pa_diffusion.</div>
</div>
<div class='paper-counter'>288/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Assist Humans without Inferring Rewards</div>
<div class="field-name">authors:</div>
<div class="field-value">Vivek Myers;Evan Ellis;Sergey Levine;Benjamin Eysenbach;Anca Dragan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human-AI Collaboration;Unsupervised Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">human-AI_interaction</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8dcb15a417bba528b0aecb9d1bdb8958d2a06698.pdf" target="_blank">https://openreview.net/pdf/8dcb15a417bba528b0aecb9d1bdb8958d2a06698.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a scalable algorithm for assisting humans without inferring their objectives.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area—scalability to high-dimensional settings—with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems. Our code is available at https://github.com/vivekmyers/empowerment_successor_representations.</div>
</div>
<div class='paper-counter'>289/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing</div>
<div class="field-name">authors:</div>
<div class="field-value">Qibo Qiu;Shun Zhang;Haiming Gao;Honghui Yang;Haochao Ying;Wenxiao Wang;Xiaofei He</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Foundation Model;Visual Place Recognition;Parameter Efficiency Fine-Tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ba161d12803881f620c602fb54d33c2a0e5a14a8.pdf" target="_blank">https://openreview.net/pdf/ba161d12803881f620c602fb54d33c2a0e5a14a8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a novel and effective Parameter Efficiency Fine-Tuning (PEFT) pipeline of adapting a visual foundation model in the visual place recognition task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual Place Recognition (VPR) is essential for mobile robots as it enables them to retrieve images from a database closest to their current location. The progress of Visual Foundation Models (VFMs) has significantly advanced VPR by capturing representative descriptors in images. However, existing fine-tuning efforts for VFMs often overlook the crucial role of probing in effectively adapting these descriptors for improved image representation. In this paper, we propose the Centroid-Free Probing (CFP) stage, making novel use of second-order features for more effective use of descriptors from VFMs. Moreover, to control the preservation of task-specific information adaptively based on the context of the VPR, we introduce the Dynamic Power Normalization (DPN) module in both the recalibration and CFP stages, forming a novel Parameter Efficiency Fine-Tuning (PEFT) pipeline (EMVP) tailored for the VPR task. Extensive experiments demonstrate the superiority of the proposed CFP over existing probing methods. Moreover, the EMVP pipeline can further enhance fine-tuning performance in terms of accuracy and efficiency. Specifically, it achieves 93.9\%, 96.5\%, and 94.6\% Recall@1 on the MSLS Validation, Pitts250k-test, and SPED datasets, respectively, while saving 64.3\% of trainable parameters compared with the existing SOTA PEFT method.</div>
</div>
<div class='paper-counter'>290/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</div>
<div class="field-name">authors:</div>
<div class="field-value">Nikhil Behari;Edwin Zhang;YUNFAN ZHAO;Aparna Taneja;Dheeraj Mysore Nagaraj;Milind Tambe</div>
<div class="field-name">keywords:</div>
<div class="field-value">public health;multi-agent systems;multi-armed bandits;LLMs</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_learning_for_healthcare</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/022671b21acf1eef6ce2d8aef3660c7c6285d9a4.pdf" target="_blank">https://openreview.net/pdf/022671b21acf1eef6ce2d8aef3660c7c6285d9a4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f7b4f28c6d2d23d64fd2f671ff80e3786a66e485.zip" target="_blank">https://openreview.net/attachment/f7b4f28c6d2d23d64fd2f671ff80e3786a66e485.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations.  We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input.</div>
</div>
<div class='paper-counter'>291/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LLM-based Skill Diffusion for Zero-shot Policy Adaptation</div>
<div class="field-name">authors:</div>
<div class="field-value">Woo Kyung Kim;Youngseok Lee;Jooyoung Kim;Honguk Woo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Planning;Diffusion Model;Large Language Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6f0b49a3aa38dc5aaa6b697129957b4d3fabfcb.pdf" target="_blank">https://openreview.net/pdf/b6f0b49a3aa38dc5aaa6b697129957b4d3fabfcb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1576c08c38a326aac821d743260db177ec3a5a60.zip" target="_blank">https://openreview.net/attachment/1576c08c38a326aac821d743260db177ec3a5a60.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">LLM-based Skill Diffusion for Zero-shot Policy Adaptation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.</div>
</div>
<div class='paper-counter'>292/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploring the trade-off between deep-learning and explainable models for brain-machine interfaces</div>
<div class="field-name">authors:</div>
<div class="field-value">Luis Hernan Cubillos;Guy Revach;Matthew Mender;Joseph T Costello;Hisham Temmar;Aren Hite;Diksha Anoop Kumar Zutshi;Dylan Michael Wallace;Xiaoyong Ni;Madison M. Kelberman;Matt Willsey;Ruud Van Sloun;Nir Shlezinger;Parag Ganapati Patil;Anne Draelos;Cynthia Chestek</div>
<div class="field-name">keywords:</div>
<div class="field-value">brain-machine interfaces;neural decoders;safety;kalman filter;real-time processing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neuroscience_and_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e668d56c878fa03d1e227267a3d0e5ccff829595.pdf" target="_blank">https://openreview.net/pdf/e668d56c878fa03d1e227267a3d0e5ccff829595.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/203c25a384ec61159d6ad44dc3d76593f520f005.zip" target="_blank">https://openreview.net/attachment/203c25a384ec61159d6ad44dc3d76593f520f005.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show an explainable brain decoder that combines the Kalman filter and RNNs to predict finger movements with high accuracy</div>
<div class="field-name">abstract:</div>
<div class="field-value">People with brain or spinal cord-related paralysis often need to rely on others for basic tasks, limiting their independence. A potential solution is brain-machine interfaces (BMIs), which could allow them to voluntarily control external devices (e.g., robotic arm) by decoding brain activity to movement commands. In the past decade, deep-learning decoders have achieved state-of-the-art results in most BMI applications, ranging from speech production to finger control. However, the 'black-box' nature of deep-learning decoders could lead to unexpected behaviors, resulting in major safety concerns in real-world physical control scenarios. In these applications, explainable but lower-performing decoders, such as the Kalman filter (KF), remain the norm. In this study, we designed a BMI decoder based on KalmanNet, an extension of the KF that augments its operation with recurrent neural networks to compute the Kalman gain. This results in a varying “trust” that shifts between inputs and dynamics. We used this algorithm to predict finger movements from the brain activity of two monkeys. We compared KalmanNet results offline (pre-recorded data, $n=13$ days) and online (real-time predictions, $n=5$ days) with a simple KF and two recent deep-learning algorithms: tcFNN (non-ReFIT version) and LSTM. KalmanNet achieved comparable or better results than other deep learning models in offline and online modes, relying on the dynamical model for stopping while depending more on neural inputs for initiating movements. We further validated this mechanism by implementing a heteroscedastic KF that used the same strategy, and it also approached state-of-the-art performance while remaining in the explainable domain of standard KFs. However, we also see two downsides to KalmanNet. KalmanNet shares the limited generalization ability of existing deep-learning decoders, and its usage of the KF as an inductive bias limits its performance in the presence of unseen noise distributions. Despite this trade-off, our analysis successfully integrates traditional controls and modern deep-learning approaches to motivate high-performing yet still explainable BMI designs.</div>
</div>
<div class='paper-counter'>293/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators</div>
<div class="field-name">authors:</div>
<div class="field-value">Allen Nie;Yash Chandak;Christina J. Yuan;Anirudhan Badrinath;Yannis Flet-Berliac;Emma Brunskill</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline reinforcement learning;off policy evaluation;statistics;ensemble method</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0a3ac0bcceb34aa4f7547d107636259e738fca17.pdf" target="_blank">https://openreview.net/pdf/0a3ac0bcceb34aa4f7547d107636259e738fca17.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/77dfeec8fd2d974bd2d89a41e9e2c9d7be8ae5d4.zip" target="_blank">https://openreview.net/attachment/77dfeec8fd2d974bd2d89a41e9e2c9d7be8ae5d4.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an estimator-agnostic meta-algorithm to combine multiple OPE estimates that achieves lower MSE in Offline RL tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL.</div>
</div>
<div class='paper-counter'>294/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Weichao Zeng;Yan Shu;Zhenhang Li;Dongbao Yang;Yu Zhou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Synthesis;Image Editing;Scene Text Editing.</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/54defa44a9993f1a3bcc94db4b764e2c4153b3d9.pdf" target="_blank">https://openreview.net/pdf/54defa44a9993f1a3bcc94db4b764e2c4153b3d9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">TextCtrl is a novel diffusion-based approach that performs high-fidelity scene text editing through explicit text style disentanglement and glyph structure representation as well as adaptive inference control.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose TextCtrl, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation,  TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which deconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy. Project page: https://github.com/weichaozeng/TextCtrl.</div>
</div>
<div class='paper-counter'>295/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SCaR: Refining Skill Chaining for Long-Horizon Robotic Manipulation via Dual Regularization</div>
<div class="field-name">authors:</div>
<div class="field-value">Zixuan Chen;Ze Ji;Jing Huo;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">long-horizon manipulation;sub-task;skill chaining;robot learning;reinforcement learning;imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/76bc76a0939cc1aacfacd42b34dfefe5dd606c46.pdf" target="_blank">https://openreview.net/pdf/76bc76a0939cc1aacfacd42b34dfefe5dd606c46.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Long-horizon robotic manipulation tasks typically involve a series of interrelated sub-tasks spanning multiple execution stages. Skill chaining offers a feasible solution for these tasks by pre-training the skills for each sub-task and linking them sequentially. However, imperfections in skill learning or disturbances during execution can lead to the accumulation of errors in skill chaining process, resulting in execution failures. In this paper, we investigate how to achieve stable and smooth skill chaining for long-horizon robotic manipulation tasks. Specifically, we propose a novel skill chaining framework called Skill Chaining via Dual Regularization (SCaR). This framework applies dual regularization to sub-task skill pre-training and fine-tuning, which not only enhances the intra-skill dependencies within each sub-task skill but also reinforces the inter-skill dependencies between sequential sub-task skills, thus ensuring smooth skill chaining and stable long-horizon execution. We evaluate the SCaR framework on two representative long-horizon robotic manipulation simulation benchmarks: IKEA furniture assembly and kitchen organization. Additionally, we conduct a simple real-world validation in tabletop robot pick-and-place tasks. The experimental results show that, with the support of SCaR, the robot achieves a higher success rate in long-horizon tasks compared to relevant baselines and demonstrates greater robustness to perturbations.</div>
</div>
<div class='paper-counter'>296/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Grasp as You Say: Language-guided Dexterous Grasp Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yi-Lin Wei;Jian-Jian Jiang;Chengyi Xing;Xiantuo Tan;Xiao-Ming Wu;Hao Li;Mark Cutkosky;Wei-Shi Zheng</div>
<div class="field-name">keywords:</div>
<div class="field-value">Dexterous Grasp Generation;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9af5f19707a436dc0f91369baa5913a2abb8ac3f.pdf" target="_blank">https://openreview.net/pdf/9af5f19707a436dc0f91369baa5913a2abb8ac3f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/21fc9d7449e3ee01624d27ca7a45285d5c527908.zip" target="_blank">https://openreview.net/attachment/21fc9d7449e3ee01624d27ca7a45285d5c527908.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper explores a novel task "Dexterous Grasp as You Say'' (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language. However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named DexGYSNet, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system. Equipped with this dataset, we introduce the DexGYSGrasp framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity. To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. Extensive experiments are conducted on DexGYSNet and real world environments for validation.</div>
</div>
<div class='paper-counter'>297/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</div>
<div class="field-name">authors:</div>
<div class="field-value">Yang Yue;Yulin Wang;Bingyi Kang;Yizeng Han;Shenzhi Wang;Shiji Song;Jiashi Feng;Gao Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">embodied AI;dynamic network;CALVIN benchmark;multimodal large language model;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning_architectures</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b35ead1ac4b6efee2d45d2cb2fcd36c575bded67.pdf" target="_blank">https://openreview.net/pdf/b35ead1ac4b6efee2d45d2cb2fcd36c575bded67.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/61aaeb342d459077610f18a2e41ef2a8a5d567a1.zip" target="_blank">https://openreview.net/attachment/61aaeb342d459077610f18a2e41ef2a8a5d567a1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Propose a dynamic multimodal large language model framework for saving computational and GPU memory costs in robotic execution</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data.These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks, whose feasibility has been recently verified~\cite{rt-2,rt-x}.However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs usually incorporates storing billions of parameters and performing tremendous computation, imposing significant hardware demands.In our paper, we seek to address this challenge by leveraging an intriguing observation: relatively easier situations make up the bulk of the procedure of controlling robots to fulfill diverse tasks, and they generally require far smaller models to obtain the correct robotic actions.Motivated by this observation, we propose a \emph{DynamicEarly-Exit for Robotic MLLM} (DeeR) framework that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to cease processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (\emph{i.e.}, power consumption), as well as peak computational consumption (\emph{i.e.}, latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance.Moreover, we design a tailored training method for integrating temporal information on top of such multi-exit architectures to predict actions reasonably. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs by 5.2-6.5x and GPU memory by 2x without compromising performance.Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.</div>
</div>
<div class='paper-counter'>298/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning rigid-body simulators over implicit shapes for large-scale scenes and vision</div>
<div class="field-name">authors:</div>
<div class="field-value">Yulia Rubanova;Tatiana Lopez-Guevara;Kelsey R Allen;William F Whitney;Kim Stachenfeld;Tobias Pfaff</div>
<div class="field-name">keywords:</div>
<div class="field-value">graph networks;learned simulation;physics;rigid body simulation;scaling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">graph_neural_networks</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a025a4908402e558708ed28771812dd10af193dd.pdf" target="_blank">https://openreview.net/pdf/a025a4908402e558708ed28771812dd10af193dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Simulating large scenes with many rigid objects is crucial for a variety of applications, such as robotics, engineering, film and video games. Rigid interactions are notoriously hard to model: small changes to the initial state or the simulation parameters can lead to large changes in the final state. Recently, learned simulators based on graph networks (GNNs) were developed as an alternative to hand-designed simulators like MuJoCo and Bullet. They are able to accurately capture dynamics of real objects directly from real-world observations. However, current state-of-the-art learned simulators operate on meshes and scale poorly to scenes with many objects or detailed shapes. Here we present SDF-Sim, the first learned rigid-body simulator designed for scale. We use learned signed-distance functions (SDFs) to represent the object shapes and to speed up distance computation. We design the simulator to leverage SDFs and avoid the fundamental bottleneck of the previous simulators associated with collision detection.For the first time in literature, we demonstrate that we can scale the GNN-based simulators to scenes with hundreds of objects and up to 1.1 million nodes, where mesh-based approaches run out of memory. Finally, we show that SDF-Sim can be applied to real world scenes by extracting SDFs from multi-view images.</div>
</div>
<div class='paper-counter'>299/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoran He;Chenjia Bai;Ling Pan;Weinan Zhang;Bin Zhao;Xuelong Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Actionless Video Pre-Training;Video-Based Policy Learning;Discrete Diffusion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/27d1f274745ac6699344ffdee0a03b2011997c97.pdf" target="_blank">https://openreview.net/pdf/27d1f274745ac6699344ffdee0a03b2011997c97.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0678f4ccc9a5af22676a5b2d7c92f1ed7aecc9c0.zip" target="_blank">https://openreview.net/attachment/0678f4ccc9a5af22676a5b2d7c92f1ed7aecc9c0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a video-based multi-task policy using discrete diffusion, facilitating efficient policy learning by incorporating foresight from predicted videos.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. However, it remains a challenge due to the domain gap between humans and robots. Moreover, it is difficult to extract useful information representing the dynamic world from human videos, because of its noisy and multimodal data structure. In this paper, we introduce a novel framework to tackle these challenges, which leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning with a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior performance.</div>
</div>
<div class='paper-counter'>300/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Minghao Chen;Yihang Li;Yanting Yang;Shiyu Yu;Binbin Lin;Xiaofei He</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;AI Agents;planning;decision making;programming</div>
<div class="field-name">primary_area:</div>
<div class="field-value">other</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fa7bc0e7a61d8579a98e5eadb278e05289e2611c.pdf" target="_blank">https://openreview.net/pdf/fa7bc0e7a61d8579a98e5eadb278e05289e2611c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/28e6868a100f6b4746f5bffeaaa012ebba16c879.zip" target="_blank">https://openreview.net/attachment/28e6868a100f6b4746f5bffeaaa012ebba16c879.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper introduces AutoManual, a framework that enables LLM agents to autonomously adapt to new environments by interacting and generating comprehensive instruction manuals through online rule optimization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\% with GPT-4-turbo and 86.2\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.</div>
</div>
<div class='paper-counter'>301/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</div>
<div class="field-name">authors:</div>
<div class="field-value">Lirui Wang;Xinlei Chen;Jialiang Zhao;Kaiming He</div>
<div class="field-name">keywords:</div>
<div class="field-value">heterogeneous robot learning;heterogeneous pre-trained transformer;scaling law for robotics;robotic foundation model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c13c57a0e7c8c79cd64396bfd32a10ba0b9c7238.pdf" target="_blank">https://openreview.net/pdf/c13c57a0e7c8c79cd64396bfd32a10ba0b9c7238.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Heterogeneous Pre-trained Transformers (HPT) that pre-train policy representation across different robot embodiments and tasks, scale it to 1B parameters and 50 datasets, and demonstrate transfer in simulation and real world evaluation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (liruiw.github.io/hpt) for code and videos.</div>
</div>
<div class='paper-counter'>302/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">QueST: Self-Supervised Skill Abstractions for Learning Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Atharva Mete;Haotian Xue;Albert Wilcox;Yongxin Chen;Animesh Garg</div>
<div class="field-name">keywords:</div>
<div class="field-value">Behavior Clonning;Action Tokenization;Self Supervised Skill Abstraction;Few-shot Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6e4a1a752de62e19eb4b95bd7f3502742333da3d.pdf" target="_blank">https://openreview.net/pdf/6e4a1a752de62e19eb4b95bd7f3502742333da3d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">QueST is a multitask latent-variable behavior model that learns sharable low-level skills by representing temporal action abstractions (1-2 secs motion) with a sequence of discrete codebook entries (skill-tokens).</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is apromising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture sharable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST’s architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io.</div>
</div>
<div class='paper-counter'>303/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Leveraging Separated World Model for Exploration in Visually Distracted Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaichen Huang;Shenghua Wan;Minghao Shao;Hai-Hang Sun;Le Gan;Shuai Feng;De-Chuan Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value">unsupervised RL;separate world model;visual inputs with distractors;minimax optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/da1cee185df1821d9b61f94c8cb37febcf9131a3.pdf" target="_blank">https://openreview.net/pdf/da1cee185df1821d9b61f94c8cb37febcf9131a3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9edbac60d0b27068eb1216bb54fdec95be91c117.zip" target="_blank">https://openreview.net/attachment/9edbac60d0b27068eb1216bb54fdec95be91c117.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Model-based unsupervised reinforcement learning (URL) has gained prominence for reducing environment interactions and learning general skills using intrinsic rewards. However, distractors in observations can severely affect intrinsic reward estimation, leading to a biased exploration process, especially in environments with visual inputs like images or videos. To address this challenge, we propose a bi-level optimization framework named Separation-assisted eXplorer (SeeX). In the inner optimization, SeeX trains a separated world model to extract exogenous and endogenous information, minimizing uncertainty to ensure task relevance. In the outer optimization, it learns a policy on imaginary trajectories generated within the endogenous state space to maximize task-relevant uncertainty. Evaluations on multiple locomotion and manipulation tasks demonstrate SeeX's effectiveness.</div>
</div>
<div class='paper-counter'>304/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Zero-Shot Transfer of Neural ODEs</div>
<div class="field-name">authors:</div>
<div class="field-value">Tyler Ingebrand;Adam Thorpe;ufuk topcu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Zero-shot Transfer;Neural ODE;Model-based control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6a650a5c71241b227459d9edda79c36d9a8fac28.pdf" target="_blank">https://openreview.net/pdf/6a650a5c71241b227459d9edda79c36d9a8fac28.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a method for zero-shot transfer and dynamics prediction via function encoders using neural ODE basis functions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.</div>
</div>
<div class='paper-counter'>305/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Anthony Liang;Guy Tennenholtz;ChihWei Hsu;Yinlam Chow;Erdem Biyik;Craig Boutilier</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Bayesian Reinforcement Learning;Meta-Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/816ee5b0296158f0938d78ab4b84abe989d5fbfc.pdf" target="_blank">https://openreview.net/pdf/816ee5b0296158f0938d78ab4b84abe989d5fbfc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate task inference in environments where the latent state evolves at varying rates.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions---parts of the episode where the latent state is fixed---and propose three key modifications to existing meta-RL methods: (i) consistency of latent information within sessions, (ii) session masking, and (iii) prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, illustrating the efficacy of DynaMITE-RL over state-of-the-art baselines in both online and offline RL settings.</div>
</div>
<div class='paper-counter'>306/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetics</div>
<div class="field-name">authors:</div>
<div class="field-value">Lennert De Smet;Pedro Zuidberg Dos Martires</div>
<div class="field-name">keywords:</div>
<div class="field-value">Probability theory;neurosymbolic AI;neuro-symbolic AI;neural-symbolic AI;integer arithmetic;linear integer arithmetic;integer programming;discrete random variables</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/db84620c031d715ef1d6a5c2cf2e96187e730928.pdf" target="_blank">https://openreview.net/pdf/db84620c031d715ef1d6a5c2cf2e96187e730928.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/92be14081cccd83de41c7e58077c077ad82a624f.zip" target="_blank">https://openreview.net/attachment/92be14081cccd83de41c7e58077c077ad82a624f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a differentiable tensorisation of linear arithmetics over integer-valued random variables that extends the horizon of exact probabilistic inference by exploiting the fast Fourier transform.</div>
<div class="field-name">abstract:</div>
<div class="field-value">As illustrated by the success of integer linear programming, linear integer arithmetics is a powerful tool for modelling combinatorial problems. Furthermore, the probabilistic extension of linear programming has been used to formulate problems in neurosymbolic AI. However, two key problems persist that prevent the adoption of neurosymbolic techniques beyond toy problems. First, probabilistic inference is inherently hard, #P-hard to be precise. Second, the discrete nature of integers renders the construction of meaningful gradients challenging, which is problematic for learning. In order to mitigate these issues, we formulate linear arithmetics over integer-valued random variables as tensor manipulations that can be implemented in a straightforward fashion using modern deep learning libraries. At the core of our formulation lies the observation that the addition of two integer-valued random variables can be performed by adapting the fast Fourier transform to probabilities in the log-domain. By relying on tensor operations we obtain a differentiable data structure, which unlocks, virtually for free, gradient-based learning. In our experimental validation we show that tensorising probabilistic integer linear arithmetics and leveraging the fast Fourier transform allows us to push the state of the art by several orders of magnitude in terms of inference and learning times.</div>
</div>
<div class='paper-counter'>307/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scalable Bayesian Optimization via Focalized Sparse Gaussian Processes</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunyue Wei;Vincent Zhuang;Saraswati Soedarmadji;Yanan Sui</div>
<div class="field-name">keywords:</div>
<div class="field-value">Scalable Bayesian optimization;Sparse Gaussian process</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3b2fc5597a3c16b9823e4dcaa2fc4a20f006d647.pdf" target="_blank">https://openreview.net/pdf/3b2fc5597a3c16b9823e4dcaa2fc4a20f006d647.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4b477203e0e6585d567febc7a54380d36afb262b.zip" target="_blank">https://openreview.net/attachment/4b477203e0e6585d567febc7a54380d36afb262b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Bayesian optimization is an effective technique for black-box optimization, but its applicability is typically limited to low-dimensional and small-budget problems due to the cubic complexity of computing the Gaussian process (GP) surrogate. While various approximate GP models have been employed to scale Bayesian optimization to larger sample sizes, most suffer from overly-smooth estimation and focus primarily on problems that allow for large online samples.  In this work, we argue that Bayesian optimization algorithms with sparse GPs can more efficiently allocate their representational power to relevant regions of the search space. To achieve this, we propose focalized GP, which leverages a novel variational loss function to achieve stronger local prediction, as well as FocalBO, which hierarchically optimizes the focalized GP acquisition function over progressively smaller search spaces. Experimental results demonstrate that FocalBO can efficiently leverage large amounts of offline and online data to achieve state-of-the-art performance on robot morphology design and to control a 585-dimensional musculoskeletal system.</div>
</div>
<div class='paper-counter'>308/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongcheng Wang;Peiqi Liu;Wenzhe Cai;Mingdong Wu;Zhengyu Qian;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mudolar Object Navigation;Demand-driven Navigation;Attribute Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/93905ccec59fb2772c8a3940329393bf93a88986.pdf" target="_blank">https://openreview.net/pdf/93905ccec59fb2772c8a3940329393bf93a88986.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d6c640cadeb1f93b7e357a68cefe5117b65fb4f6.zip" target="_blank">https://openreview.net/attachment/d6c640cadeb1f93b7e357a68cefe5117b65fb4f6.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a multi-object demand-driven navigation benchmark  and train an coarse-to-fine attribute-based exploration agent to solve this task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as "I am thirsty." The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.</div>
</div>
<div class='paper-counter'>309/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Weikang Wan;Ziyu Wang;Yufei Wang;Zackory Erickson;David Held</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;model-based reinforcement learning;differentiable trajectory optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning_architectures</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/797ea8f1f520e879eef0d48f51e53fa63d677421.pdf" target="_blank">https://openreview.net/pdf/797ea8f1f520e879eef0d48f51e53fa63d677421.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces DiffTORI, which uses Differentiable Trajectory Optimization as the policy representation to generate actions for deep reinforcement and imitation learning, and outperforms prior state-of-the-art methods in both domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper introduces DiffTORI, which utilizes $\textbf{Diff}$erentiable $\textbf{T}$rajectory $\textbf{O}$ptimization as the policy representation to generate actions for deep $\textbf{R}$einforcement and $\textbf{I}$mitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization.  As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTORI addresses the “objective mismatch” issue of prior model-based RL algorithms, as the dynamics model in DiffTORI is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTORI  for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feedforward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods in both domains.</div>
</div>
<div class='paper-counter'>310/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generating compositional scenes via Text-to-image RGBA Instance Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Alessandro Fontanella;Petru-Daniel Tudosiu;Yongxin Yang;Shifeng Zhang;Sarah Parisot</div>
<div class="field-name">keywords:</div>
<div class="field-value">RGBA generation;scene composition;diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cc8fd4de4e2d11fe0c63beefe323fbdecde1aabb.pdf" target="_blank">https://openreview.net/pdf/cc8fd4de4e2d11fe0c63beefe323fbdecde1aabb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a multi-layer approach for text-to-image diffusion models that improves fine-grained control over object attributes and layout by generating isolated RGBA images and blending them into detailed composite scenes.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-image diffusion generative models can generate high quality images at the cost of tedious prompt engineering. Controllability can be improved by introducing layout conditioning, however existing methods lack layout editing ability and fine-grained control over object attributes. The concept of multi-layer generation holds great potential to address these limitations, however generating image instances concurrently to scene composition limits control over fine-grained object attributes, relative positioning in 3D space and scene manipulation abilities. In this work, we propose a novel multi-stage generation paradigm that is designed for fine-grained control, flexibility and interactivity. To ensure control over instance attributes, we devise a novel training paradigm to adapt a diffusion model to generate isolated scene components as RGBA images with transparency information. To build complex images, we employ these pre-generated instances and introduce a multi-layer composite generation process that smoothly assembles components in realistic scenes. Our experiments show that our RGBA diffusion model is capable of generating diverse and high quality instances with precise control over object attributes. Through multi-layer composition, we demonstrate that our approach allows to build and manipulate images from highly complex prompts with fine-grained control over object appearance and location, granting a higher degree of control than competing methods.</div>
</div>
<div class='paper-counter'>311/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Tang;Haifeng Xu;Ruimin Zhang;Derek Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">prophet inequality;optimal stopping;robustness;strategic information revealing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">algorithmic_game_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5dda2558f65809779f2ce50bbc3fac437cf946ba.pdf" target="_blank">https://openreview.net/pdf/5dda2558f65809779f2ce50bbc3fac437cf946ba.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Prophet inequality concerns a basic optimal stopping problem and states that simple threshold stopping policies --- i.e., accepting the first reward larger than a certain threshold --- can achieve tight $\frac{1}{2}$-approximation to the optimal prophet value. Motivated by its economic applications, this paper studies the robustness of this approximation to natural strategic manipulations in which each random reward is associated with a self-interested player who may selectively reveal his realized reward to the searcher in order to maximize his probability of being selected. We say a threshold policy is $\alpha$(-strategically)-robust if it (a) achieves the  $\alpha$-approximation to the prophet value for strategic players; and (b) meanwhile remains a  $\frac{1}{2}$-approximation in the standard non-strategic setting.Starting with a characterization of each player's optimal information revealing strategy, we demonstrate the intrinsic robustness of prophet inequalities to strategic reward signaling through the following results:(1) for arbitrary reward distributions, there is a threshold policy that is $\frac{1-\frac{1}{e}}{2}$-robust, and this ratio is tight;(2) for i.i.d. reward distributions, there is a threshold policy that is $\frac{1}{2}$-robust, which is tight for the setting; and (3) for log-concave (but non-identical) reward distributions, the $\frac{1}{2}$-robustness can also be achieved under certain regularity assumptions.</div>
</div>
<div class='paper-counter'>312/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Seongwoong Cho;Donggyun Kim;Jinwoo Lee;Seunghoon Hong</div>
<div class="field-name">keywords:</div>
<div class="field-value">few-shot learning;imitation learning;behavior cloning;transformers;meta-learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a2f6c54273665fef8c3639341d09cdc7e5faee0c.pdf" target="_blank">https://openreview.net/pdf/a2f6c54273665fef8c3639341d09cdc7e5faee0c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalizing across robot embodiments and tasks is crucial for adaptive robotic systems. Modular policy learning approaches adapt to new embodiments but are limited to specific tasks, while few-shot imitation learning (IL) approaches often focus on a single embodiment.In this paper, we introduce a few-shot behavior cloning framework to simultaneously generalize to unseen embodiments and tasks using a few (e.g., five) reward-free demonstrations. Our framework leverages a joint-level input-output representation to unify the state and action spaces of heterogeneous embodiments and employs a novel structure-motion state encoder that is parameterized to capture both shared knowledge across all embodiments and embodiment-specific knowledge. A matching-based policy network then predicts actions from a few demonstrations, producing an adaptive policy that is robust to over-fitting. Evaluated in the DeepMind Control suite, our framework termed Meta-Controller demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches.</div>
</div>
<div class='paper-counter'>313/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yu Zheng;Guangming Wang;Jiuming Liu;Marc Pollefeys;Hesheng Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">LiDAR Point Cloud Semantic Segmentation;2D Projection;Quantized Information Loss</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5a871230d47f5a2d7722af373313a33ddcdb2d8e.pdf" target="_blank">https://openreview.net/pdf/5a871230d47f5a2d7722af373313a33ddcdb2d8e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/adc4ed77808cf07c9f07636f3c2ae24f75251167.zip" target="_blank">https://openreview.net/attachment/adc4ed77808cf07c9f07636f3c2ae24f75251167.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose spherical frustum structure to avoid quantized information loss in conventional 2D spherical projection for LiDAR point cloud semantic segmentation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">LiDAR point cloud semantic segmentation enables the robots to obtain fine-grained semantic information of the surrounding environment. Recently, many works project the point cloud onto the 2D image and adopt the 2D Convolutional Neural Networks (CNNs) or vision transformer for LiDAR point cloud semantic segmentation. However, since more than one point can be projected onto the same 2D position but only one point can be preserved, the previous 2D projection-based segmentation methods suffer from inevitable quantized information loss, which results in incomplete geometric structure, especially for small objects. To avoid quantized information loss, in this paper, we propose a novel spherical frustum structure, which preserves all points projected onto the same 2D position. Additionally, a hash-based representation is proposed for memory-efficient spherical frustum storage. Based on the spherical frustum structure, the Spherical Frustum sparse Convolution (SFC) and Frustum Farthest Point Sampling (F2PS) are proposed to convolve and sample the points stored in spherical frustums respectively. Finally, we present the Spherical Frustum sparse Convolution Network (SFCNet) to adopt 2D CNNs for LiDAR point cloud semantic segmentation without quantized information loss. Extensive experiments on the SemanticKITTI and nuScenes datasets demonstrate that our SFCNet outperforms previous 2D projection-based semantic segmentation methods based on conventional spherical projection and shows better performance on small object segmentation by preserving complete geometric structure. Codes will be available at https://github.com/IRMVLab/SFCNet.</div>
</div>
<div class='paper-counter'>314/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STL: Still Tricky Logic (for System Validation, Even When Showing Your Work)</div>
<div class="field-name">authors:</div>
<div class="field-value">Isabelle Hurley;Rohan R Paleja;Ashley Suh;Jaime Daniel Pena;Ho Chit Siu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Explainability;Formal Methods;Human Experiments;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">interpretability_and_explainability</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e89140e0edb441e10662ae4657a9bfe74f9e2b15.pdf" target="_blank">https://openreview.net/pdf/e89140e0edb441e10662ae4657a9bfe74f9e2b15.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/31faef665a2c0967511c84b7dbcd0c45d31a50bc.zip" target="_blank">https://openreview.net/attachment/31faef665a2c0967511c84b7dbcd0c45d31a50bc.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Formal methods are not particularly "explainable" for system validation even when applying best practices from education research</div>
<div class="field-name">abstract:</div>
<div class="field-value">As learned control policies become increasingly common in autonomous systems, there is increasing need to ensure that they are interpretable and can be checked by human stakeholders. Formal specifications have been proposed as ways to produce human-interpretable policies for autonomous systems that can still be learned from examples. Previous work showed that despite claims of interpretability, humans are unable to use formal specifications presented in a variety of ways to validate even simple robot behaviors. This work uses active learning, a standard pedagogical method, to attempt to improve humans' ability to validate policies in signal temporal logic (STL). Results show that overall validation accuracy is not high, at 65\% $\pm$ 15% (mean $\pm$ standard deviation), and that the three conditions of no active learning, active learning, and active learning with feedback do not significantly differ from each other. Our results suggest that the utility of formal specifications for human interpretability is still unsupported but point to other avenues of development which may enable improvements in system validation.</div>
</div>
<div class='paper-counter'>315/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">NeuralFluid: Nueral Fluidic System Design and Control with Differentiable Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yifei Li;Yuchen Sun;Pingchuan Ma;Eftychios Sifakis;Tao Du;Bo Zhu;Wojciech Matusik</div>
<div class="field-name">keywords:</div>
<div class="field-value">differentiable simulation;fluid learning;fluid simulation;fluid control;fluid optimization;fluid design;inverse problems;navier-stokes;adjoint method;fluid dynamics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_learning_for_physical_sciences</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6be38b86468a5d1764ac6fa70b4d9ff250e0f78e.pdf" target="_blank">https://openreview.net/pdf/6be38b86468a5d1764ac6fa70b4d9ff250e0f78e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/641e706a52b3f33590dd9c4aedbb04ed552981e8.zip" target="_blank">https://openreview.net/attachment/641e706a52b3f33590dd9c4aedbb04ed552981e8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present NeuralFluid, a novel framework to explore neural control and design of complex fluidic systems with dynamic solid boundaries. Our system features a fast differentiable Navier-Stokes solver with solid-fluid interface handling, a low-dimensional differentiable parametric geometry representation, a control-shape co-design algorithm, and gym-like simulation environments to facilitate various fluidic control design applications. Additionally, we present a benchmark of design, control, and learning tasks on high-fidelity, high-resolution dynamic fluid environments that pose challenges for existing differentiable fluid simulators. These tasks include designing the control of artificial hearts, identifying robotic end-effector shapes, and controlling a fluid gate. By seamlessly incorporating our differentiable fluid simulator into a learning framework, we demonstrate successful design, control, and learning results that surpass gradient-free solutions in these benchmark tasks.</div>
</div>
<div class='paper-counter'>316/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robot Policy Learning with Temporal Optimal Transport Reward</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuwei Fu;Haichao Zhang;Di Wu;Wei Xu;Benoit Boulet</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Imitation Learning;Optimal Transport</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/546d5a3bfcb9e2fdc8b68c1bf6c486d493da366e.pdf" target="_blank">https://openreview.net/pdf/546d5a3bfcb9e2fdc8b68c1bf6c486d493da366e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Temporal optimal transport reward for policy learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reward specification is one of the most tricky problems in Reinforcement Learning, which usually requires tedious hand engineering in practice. One promising approach to tackle this challenge is to adopt existing expert video demonstrations for policy learning. Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations. For example, reward labeling via Optimal Transport (OT) has been shown to be an effective strategy to generate a proxy reward by measuring the alignment between the robot trajectory and the expert demonstrations. However, previous work mostly overlooks that the OT reward is invariant to temporal order information, which could bring extra noise to the reward signal. To address this issue, in this paper, we introduce the Temporal Optimal Transport (TemporalOT) reward to incorporate temporal order information for learning a more accurate OT-based proxy reward. Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method. Our code is available at: https://github.com/fuyw/TemporalOT.</div>
</div>
<div class='paper-counter'>317/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EfficientCAPER: An End-to-End Framework for Fast and Robust Category-Level Articulated Object Pose Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinyi Yu;Haonan Jiang;Li Zhang;Lin Yuanbo Wu;Linlin Ou;Liu Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Articulated Object;Pose Estimation;Joint-centric Pose Modeling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/96b175aca73fb75e7bfc3e332487048318ed6d6a.pdf" target="_blank">https://openreview.net/pdf/96b175aca73fb75e7bfc3e332487048318ed6d6a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Human life is populated with articulated objects. Pose estimation for category-level articulated objects is a significant challenge due to their inherent complexity and diverse kinematic structures. Current methods for this task usually meet the problems of insufficient consideration of kinematic constraints, self-occlusion, and optimization requirements. In this paper, we propose EfficientCAPER, an end-to-end Category-level Articulated object Pose EstimatoR, eliminating the need for optimization functions as post-processing and utilizing the kinematic structure for joint-centric pose modeling, thus enhancing the efficiency and applicability. Given a partial point cloud as input, the EfficientCAPER firstly estimates the pose for the free part of an articulated object using decoupled rotation representation. Next, we canonicalize the input point cloud to estimate constrained parts' poses by predicting the joint parameters and states as replacements. Evaluations on three diverse datasets, ArtImage, ReArtMix, and RobotArm, show EfficientCAPER's effectiveness and generalization ability to real-world scenarios. The framework exhibits excellent static pose estimation performance for articulated objects, contributing to the advancement of category-level pose estimation. Codes will be made publicly available.</div>
</div>
<div class='paper-counter'>318/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ACES: Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Julien Pourcel;Cédric Colas;Gaia Molinaro;Pierre-Yves Oudeyer;Laetitia Teodorescu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diversity search;code generation;quality-diversity;open-endedness;generative models;evolutionary algorithms;code models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ebfcd198369769c06e4e8bd93508befd24cd2a69.pdf" target="_blank">https://openreview.net/pdf/ebfcd198369769c06e4e8bd93508befd24cd2a69.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a new open-ended algorithm to automate the generation of diverse and challenging programming puzzles to evaluate LLM-based problem solvers.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science. We propose a method that aims to automate this process by harnessing the power of state-of-the-art generative models to produce a diversity of challenging yet solvable problems, here in the context of Python programming puzzles. Inspired by the intrinsically motivated literature, Autotelic CodE Search (ACES) jointly optimizes for the diversity and difficulty of generated problems. We represent problems in a space of LLM-generated semantic descriptors describing the programming skills required to solve them (e.g. string manipulation, dynamic programming, etc.) and measure their difficulty empirically as a linearly decreasing function of the success rate of \textit{Llama-3-70B}, a state-of-the-art LLM problem solver. ACES iteratively prompts a large language model to generate difficult problems achieving a diversity of target semantic descriptors (goal-directed exploration) using previously generated problems as in-context examples. ACES generates problems that are more diverse and more challenging than problems produced by baseline methods and three times more challenging than problems found in existing Python programming benchmarks on average across 11 state-of-the-art code LLMs.</div>
</div>
<div class='paper-counter'>319/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Gunshi Gupta;Karmesh Yadav;Yarin Gal;Dhruv Batra;Zsolt Kira;Cong Lu;Tim G. J. Rudner</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Representation Learning for Control;Diffusion Models;Foundation Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e9965fa3272e80dd82a71cd2d4d9f81875b6d8a2.pdf" target="_blank">https://openreview.net/pdf/e9965fa3272e80dd82a71cd2d4d9f81875b6d8a2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We investigate representations from pre-trained text-to-image diffusion models for control tasks and showcase competitive performance across a wide range of tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding—a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.</div>
</div>
<div class='paper-counter'>320/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Oracle-Efficient Reinforcement Learning for Max Value Ensembles</div>
<div class="field-name">authors:</div>
<div class="field-value">Marcel Hussing;Michael Kearns;Aaron Roth;Sikata Bela Sengupta;Jessica Sorrell</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning Theory;Ensembling;Max-Following;Learning Theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c449006bee001a0eb99144ee2b3b61cb1f6c07c4.pdf" target="_blank">https://openreview.net/pdf/c449006bee001a0eb99144ee2b3b61cb1f6c07c4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bb469519a489aabca86558d48f18b35b9afd94e1.zip" target="_blank">https://openreview.net/attachment/bb469519a489aabca86558d48f18b35b9afd94e1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We provide an efficient algorithm to learn an approximate max-following policy using K constituent policies in large state spaces.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance). One line of research attempting to address these difficultiesmakes the natural assumption that we are given a collection of base or *constituent* policies (possibly heuristic) upon which we would like to improve in a scalable manner. In this work we aim to compete with the *max-following policy*, which at each state follows the action of whichever constituent policy has the highest value. The max-following policy is always at least as good as the best constituent policy, and may be considerably better. Our main result is an efficient algorithm that learns to compete with the max-following policy, given only access to the constituent policies (but not their value functions). In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions. We illustrate our algorithm's experimental effectiveness and behavior on several robotic simulation testbeds.</div>
</div>
<div class='paper-counter'>321/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiaming Liu;Mengzhen Liu;Zhenyu Wang;Pengju An;Xiaoqi Li;Kaichen Zhou;Senqiao Yang;Renrui Zhang;Yandong Guo;Shanghang Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Manipulation;Multimodal Large Language Model;State Space Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d121b003663e64b010981e0a4b9e271adebd7c2d.pdf" target="_blank">https://openreview.net/pdf/d121b003663e64b010981e0a4b9e271adebd7c2d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a8ec83e43dde9f6210ec7009438418ab8d320803.zip" target="_blank">https://openreview.net/attachment/a8ec83e43dde9f6210ec7009438418ab8d320803.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models.</div>
</div>
<div class='paper-counter'>322/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Andrew Wagenmaker;Kevin Huang;Liyiming Ke;Kevin Jamieson;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;sim2real transfer;exploration;sample complexity;naive exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e749c63c44906a2b940f81830331ae3d5f02f741.pdf" target="_blank">https://openreview.net/pdf/e749c63c44906a2b940f81830331ae3d5f02f741.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d0608a0b1ebe1e8c439d47b9d48cc38186ea11da.zip" target="_blank">https://openreview.net/attachment/d0608a0b1ebe1e8c439d47b9d48cc38186ea11da.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate that, even when direct sim-to-real transfer fails, transferring exploratory policies from a simulator can enable efficient RL in the real-world with only a least-squares regression oracle.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In order to mitigate the sample complexity of real-world reinforcement learning, common practice is to first train a policy in a simulator where samples are cheap, and then deploy this policy in the real world, with the hope that it generalizes effectively. Such \emph{direct sim2real} transfer is not guaranteed to succeed, however, and in cases where it fails, it is unclear how to best utilize the simulator. In this work, we show that in many regimes, while direct sim2real transfer may fail, we can utilize the simulator to learn a set of \emph{exploratory} policies which enable efficient exploration in the real world. In particular, in the setting of low-rank MDPs, we show that coupling these exploratory policies with simple, practical approaches---least-squares regression oracles and naive randomized exploration---yields a polynomial sample complexity in the real world, an exponential improvement over direct sim2real transfer, or learning without access to a simulator. To the best of our knowledge, this is the first evidence that simulation transfer yields a provable gain in reinforcement learning in settings where direct sim2real transfer fails. We validate our theoretical results on several realistic robotic simulators and a real-world robotic sim2real task, demonstrating that transferring exploratory policies can yield substantial gains in practice as well.</div>
</div>
<div class='paper-counter'>323/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">An-Chieh Cheng;Hongxu Yin;Yang Fu;Qiushan Guo;Ruihan Yang;Jan Kautz;Xiaolong Wang;Sifei Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Vision-Language Models;Spatial Reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/72621ba2893cdc746a92fa241286edca2ca9aab0.pdf" target="_blank">https://openreview.net/pdf/72621ba2893cdc746a92fa241286edca2ca9aab0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A powerful region-level VLM adept at 3D spatial reasoning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs’ spatial perception and reasoning capabilities. SpatialRGPT advances VLMs’ spatial understanding through two key innovations: (i) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (ii) a flexible ``plugin'' module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in Vision-Language Models (VLMs). Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT.</div>
</div>
<div class='paper-counter'>324/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting</div>
<div class="field-name">authors:</div>
<div class="field-value">Yian Wang;Xiaowen Qiu;Jiageng Liu;Zhehuan Chen;Jiting Cai;Yufei Wang;Tsun-Hsuan Wang;Zhou Xian;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Scene Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8273d02953e792d948c4d80d19f084e9d21a2ac4.pdf" target="_blank">https://openreview.net/pdf/8273d02953e792d948c4d80d19f084e9d21a2ac4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Creating large-scale interactive 3D environments is essential for the development of Robotics and Embodied AI research. However, generating diverse embodied environments with realistic detail and considerable complexity remains a significant challenge. Current methods, including manual design, procedural generation, diffusion-based scene generation, and large language model (LLM) guided scene design, are hindered by limitations such as excessive human effort, reliance on predefined rules or training datasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image generative models better capture scene and object configuration than LLMs, we address these challenges by introducing $\textit{Architect}$, a generative framework that creates complex and realistic 3D embodied environments leveraging diffusion-based 2D image inpainting. In detail, we utilize foundation visual perception models to obtain each generated object from the image and leverage pre-trained depth estimation models to lift the generated 2D image to 3D space. While there are still challenges that the camera parameters and scale of depth are still absent in the generated image, we address those problems by ''controlling'' the diffusion model by $\textit{hierarchical inpainting}$. Specifically, having access to ground-truth depth and camera parameters in simulation, we first render a photo-realistic image of only the background. Then, we inpaint the foreground in this image, passing the geometric cues to the inpainting model in the background, which informs the camera parameters.This process effectively controls the camera parameters and depth scale for the generated image, facilitating the back-projection from 2D image to 3D point clouds. Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate the placement of large furniture and small objects to enrich the scene. This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments. Experimental results demonstrate that $\textit{Architect}$ outperforms existing methods in producing realistic and complex environments, making it highly suitable for Embodied AI and robotics applications.</div>
</div>
<div class='paper-counter'>325/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffuBox: Refining 3D Object Detection with Point Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiangyu Chen;Zhenzhen Liu;Katie Z Luo;Siddhartha Datta;Adhitya Polavaram;Yan Wang;Yurong You;Boyi Li;Marco Pavone;Wei-Lun Chao;Mark Campbell;Bharath Hariharan;Kilian Q Weinberger</div>
<div class="field-name">keywords:</div>
<div class="field-value">Domain Adaptation;Denoising Diffusion Models;3D Object Detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2f2a7ab4b745917ee48546f9f819fd5a70aeabd2.pdf" target="_blank">https://openreview.net/pdf/2f2a7ab4b745917ee48546f9f819fd5a70aeabd2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b11599306a1ae877e0ec65ecbf79d3788f529e89.zip" target="_blank">https://openreview.net/attachment/b11599306a1ae877e0ec65ecbf79d3788f529e89.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Ensuring robust 3D object detection and localization is crucial for many applications in robotics and autonomous driving. Recent models, however, face difficulties in maintaining high performance when applied to domains with differing sensor setups or geographic locations, often resulting in poor localization accuracy due to domain shift. To overcome this challenge, we introduce a novel diffusion-based box refinement approach. This method employs a domain-agnostic diffusion model, conditioned on the LiDAR points surrounding a coarse bounding box, to simultaneously refine the box's location, size, and orientation. We evaluate this approach under various domain adaptation settings, and our results reveal significant improvements across different datasets, object classes and detectors. Our PyTorch implementation is available at https://github.com/cxy1997/DiffuBox.</div>
</div>
<div class='paper-counter'>326/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">Shuaihang Yuan;Hao Huang;Yu Hao;Congcong Wen;Anthony Tzes;Yi Fang</div>
<div class="field-name">keywords:</div>
<div class="field-value">zero-shot object goal navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0c845936c3d701205d5f448d0cb8a11a947c234a.pdf" target="_blank">https://openreview.net/pdf/0c845936c3d701205d5f448d0cb8a11a947c234a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Zero-Shot Object Goal Navigation (ZS-OGN) enables robots to navigate toward objects of unseen categories without prior training. Traditional approaches often leverage categorical semantic information for navigation guidance, which struggles when only partial objects are observed or detailed and functional representations of the environment are lacking. To resolve the above two issues, we propose \textit{Geometric-part and Affordance Maps} (GAMap), a novel method that integrates object parts and affordance attributes for navigation guidance. Our method includes a multi-scale scoring approach to capture geometric-part and affordance attributes of objects at different scales. Comprehensive experiments conducted on the HM3D and Gibson benchmark datasets demonstrate improvements in Success Rates and Success weighted by Path Length, underscoring the efficacy of our geometric-part and affordance-guided navigation approach in enhancing robot autonomy and versatility, without any additional task-specific training or fine-tuning with the semantics of unseen objects and/or the locomotions of the robot.</div>
</div>
<div class='paper-counter'>327/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hang Yin;Xiuwei Xu;Zhenyu Wu;Jie Zhou;Jiwen Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">navigation;scene graph;large language model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c46925571b42316c576d17cd679b1c2ae959ca3a.pdf" target="_blank">https://openreview.net/pdf/c46925571b42316c576d17cd679b1c2ae959ca3a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ec56eb74a135d801244bc005369dbdfdced12d20.zip" target="_blank">https://openreview.net/attachment/ec56eb74a135d801244bc005369dbdfdced12d20.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a scene graph representation to prompt LLM for zero-shot object navigation, which achieves state-of-the-art performance while being explainable.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we propose a new framework for zero-shot object navigation.Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges.Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error.We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than \textbf{10\%} SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.Code of this project will be released in the final version.</div>
</div>
<div class='paper-counter'>328/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Humanoid Locomotion as Next Token Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Ilija Radosavovic;Bike Zhang;Baifeng Shi;Jathushan Rajasegaran;Sarthak Kamat;Trevor Darrell;Koushil Sreenath;Jitendra Malik</div>
<div class="field-name">keywords:</div>
<div class="field-value">Real-World Humanoid Control;Next Token Prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2f48ec003c19b29d4023a570d0f23d939a099cb2.pdf" target="_blank">https://openreview.net/pdf/2f48ec003c19b29d4023a570d0f23d939a099cb2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We cast real-world humanoid control as a next token prediction problem.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor sequences. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, such as videos without actions. We train our model on a dataset of sequences from a prior neural network policy, a model-based controller, motion capture, and YouTube videos of humans. We show that our model enables a real humanoid robot to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor sequences.</div>
</div>
<div class='paper-counter'>329/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Adaptive Preference Scaling for Reinforcement Learning with Human Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Ilgee Hong;Zichong Li;Alexander Bukharin;Yixiao Li;Haoming Jiang;Tianbao Yang;Tuo Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning from Human Feedback;Large Language Models;Alignment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eaa267a47dd2c488be1aad5e49a25710d060880a.pdf" target="_blank">https://openreview.net/pdf/eaa267a47dd2c488be1aad5e49a25710d060880a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new adaptive preference loss for RLHF.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.</div>
</div>
<div class='paper-counter'>330/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Omnigrasp: Grasping Diverse Objects with Simulated Humanoids</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhengyi Luo;Jinkun Cao;Sammy Christen;Alexander Winkler;Kris M. Kitani;Weipeng Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Physics Simulation;Humanoid Control;Dexterous Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/08974bf790f4c1e69d0de5e867da7b9a5b9e0e44.pdf" target="_blank">https://openreview.net/pdf/08974bf790f4c1e69d0de5e867da7b9a5b9e0e44.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/80c7141d2f1de281a87376244b60956f4cf9a531.zip" target="_blank">https://openreview.net/attachment/80c7141d2f1de281a87376244b60956f4cf9a531.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Full body and dexterous humanoid grasping and object manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.</div>
</div>
<div class='paper-counter'>331/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Zikang Zhou;Haibo HU;Xinhong Chen;Jianping Wang;Nan Guan;Kui Wu;Yung-Hui Li;Yu-Kai Huang;Chun Jason Xue</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-Agent Systems;Transformers;Generative Models;Autonomous Driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8a93c63c8997805fca102b403710312eae5c929f.pdf" target="_blank">https://openreview.net/pdf/8a93c63c8997805fca102b403710312eae5c929f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A fully autoregressive Transformer with next-patch prediction mechanism for multi-agent behavior simulation in autonomous driving</div>
<div class="field-name">abstract:</div>
<div class="field-value">Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems. Existing data-driven simulators primarily use an encoder-decoder architecture to encode the historical trajectories before decoding the future. However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization. Given these limitations, we propose BehaviorGPT, a homogeneous and fully autoregressive Transformer designed to simulate the sequential behavior of multiple agents. Crucially, our approach discards the traditional separation between "history" and "future" by modeling each time step as the "current" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3) to mitigate the negative effects of autoregressive modeling, in which models are trained to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. Despite having merely 3M model parameters, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a realism score of 0.7473 and a minADE score of 1.4147, demonstrating its exceptional performance in traffic agent simulation.</div>
</div>
<div class='paper-counter'>332/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Vision Foundation Model Enables Generalizable Object Pose Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">Kai Chen;Yiyao Ma;Xingyu Lin;Stephen James;Jianshu Zhou;Yun-Hui Liu;Pieter Abbeel;Qi Dou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Object Pose Estimation;Vision Foundation Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/361ff94a1b419acf5dbfca68f2a1f1362ffc8472.pdf" target="_blank">https://openreview.net/pdf/361ff94a1b419acf5dbfca68f2a1f1362ffc8472.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper presents a new framework for generalizable object pose estimation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.</div>
</div>
<div class='paper-counter'>333/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Ke Sun;Shen Chen;Taiping Yao;Hong Liu;Xiaoshuai Sun;Shouhong Ding;Rongrong Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deepfake Detection;Face forgery Detection;Stable Diffusion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/de3d7bfc4ad38cae0ae8bc83a64c953f5ed81fdc.pdf" target="_blank">https://openreview.net/pdf/de3d7bfc4ad38cae0ae8bc83a64c953f5ed81fdc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/df156f8197d7e2973e0afc803a3c4997f0a18f38.zip" target="_blank">https://openreview.net/attachment/df156f8197d7e2973e0afc803a3c4997f0a18f38.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. The code are available in https://github.com/skJack/DiffusionFake.git.</div>
</div>
<div class='paper-counter'>334/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CRT-Fusion: Camera, Radar, Temporal Fusion Using  Motion Information  for 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">Jisong Kim;Minjae Seong;Jun Won Choi</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Object Detection;Sensor Fusion;Temporal Fusion;Radar;Camera</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning_architectures</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4324eb5080025d064864306ccd9a5422f55d18ca.pdf" target="_blank">https://openreview.net/pdf/4324eb5080025d064864306ccd9a5422f55d18ca.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">CRT-Fusion is a novel framework that significantly improves the accuracy and robustness of 3D object detection by effectively integrating radar-camera information and temporal cues, explicitly considering the motion of dynamic objects.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird's-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection.</div>
</div>
<div class='paper-counter'>335/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STONE: A Submodular Optimization Framework for Active 3D Object Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">RUIYU MAO;Sarthak Kumar Maharana;Rishabh K Iyer;Yunhui Guo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Active learning;3D object detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">active_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/766f68b508af225d0f0c51faa951bacc71e1769d.pdf" target="_blank">https://openreview.net/pdf/766f68b508af225d0f0c51faa951bacc71e1769d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6c7e03ea70527e82031e3c6cb702d2e4f8ead05e.zip" target="_blank">https://openreview.net/attachment/6c7e03ea70527e82031e3c6cb702d2e4f8ead05e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Unified active 3D object detection framework based on submodular optimization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">3D object detection is fundamentally important for various emerging applications, including autonomous driving and robotics. A key requirement for training an accurate 3D object detector is the availability of a large amount of LiDAR-based point cloud data. Unfortunately, labeling point cloud data is extremely challenging, as accurate 3D bounding boxes and semantic labels are required for each potential object. This paper proposes a unified active 3D object detection framework, for greatly reducing the labeling cost of training 3D object detectors. Our framework is based on a novel formulation of submodular optimization, specifically tailored to the problem of active 3D object detection. In particular, we address two fundamental challenges associated with active 3D object detection: data imbalance and the need to cover the distribution of the data, including LiDAR-based point cloud data of varying difficulty levels. Extensive experiments demonstrate that our method achieves state-of-the-art performance with high computational efficiency compared to existing active learning methods. The code is available at [https://github.com/RuiyuM/STONE](https://github.com/RuiyuM/STONE)</div>
</div>
<div class='paper-counter'>336/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Training an Open-Vocabulary Monocular 3D Detection Model without 3D Data</div>
<div class="field-name">authors:</div>
<div class="field-value">Rui Huang;Henry Zheng;Yan Wang;Zhuofan Xia;Marco Pavone;Gao Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Monocular 3D Detection;Open-Vocabulary 3D Detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1b5443632dd3e212aedfac2690b7cf97f64fd4c8.pdf" target="_blank">https://openreview.net/pdf/1b5443632dd3e212aedfac2690b7cf97f64fd4c8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Open-vocabulary 3D object detection has recently attracted considerable attention due to its broad applications in autonomous driving and robotics, which aims to effectively recognize novel classes in previously unseen domains. However, existing point cloud-based open-vocabulary 3D detection models are limited by their high deployment costs. In this work, we propose a novel open-vocabulary monocular 3D object detection framework, dubbed OVM3D-Det, which trains detectors using only RGB images, making it both cost-effective and scalable to publicly available data. Unlike traditional methods, OVM3D-Det does not require high-precision LiDAR or 3D sensor data for either input or generating 3D bounding boxes. Instead, it employs open-vocabulary 2D models and pseudo-LiDAR to automatically label 3D objects in RGB images, fostering the learning of open-vocabulary monocular 3D detectors. However, training 3D models with labels directly derived from pseudo-LiDAR is inadequate due to imprecise boxes estimated from noisy point clouds and severely occluded objects. To address these issues, we introduce two innovative designs: adaptive pseudo-LiDAR erosion and bounding box refinement with prior knowledge from large language models. These techniques effectively calibrate the 3D labels and enable RGB-only training for 3D detectors. Extensive experiments demonstrate the superiority of OVM3D-Det over baselines in both indoor and outdoor scenarios. The code will be released.</div>
</div>
<div class='paper-counter'>337/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoyu Dong;Huiqiao Fu;Wentao Xu;Zhehao Zhou;Chunlin Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Evolutionary adversarial simulator identification;reinforcement learning;sim-to-real transfer</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9f84677e14cfc0a9b91d7a8b6a7d3e6c11c2356a.pdf" target="_blank">https://openreview.net/pdf/9f84677e14cfc0a9b91d7a8b6a7d3e6c11c2356a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel approach of Evolutionary Adversarial Simulator Identification (EASI) by combining Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning (RL) controllers have demonstrated remarkable performance in complex robot control tasks. However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots. Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs. In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges. Specifically, we consider the problem of sim-to-real as a search problem, where ES acts as a generator in adversarial competition with a neural network discriminator, aiming to find physical parameter distributions that make the state transitions between simulation and reality as similar as possible. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions. EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world. We demonstrate the performance of EASI in both sim-to-sim and sim-to-real tasks, showing superior performance compared to existing sim-to-real algorithms.</div>
</div>
<div class='paper-counter'>338/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers</div>
<div class="field-name">authors:</div>
<div class="field-value">Gautham Vasan;Mohamed Elsayed;Seyed Alireza Azimi;Jiamin He;Fahim Shahriar;Colin Bellinger;Martha White;A. Rupam Mahmood</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Robotics;Deep Learning;Incremental Learning;Real-time Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/75f25cb2e84509442ee1a9e9d3072b54da61c396.pdf" target="_blank">https://openreview.net/pdf/75f25cb2e84509442ee1a9e9d3072b54da61c396.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/00cd51923ef47d96808a3eba1877db0edcb96685.zip" target="_blank">https://openreview.net/attachment/00cd51923ef47d96808a3eba1877db0edcb96685.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Action Value Gradient (AVG), a novel incremental policy gradient method for real-time learning on robots with limited onboard computation, eliminating the need for large replay buffers, target networks or batch updates.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during *incremental learning*, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method --- *Action Value Gradient (AVG)* and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.</div>
</div>
<div class='paper-counter'>339/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can Large Language Model Agents Simulate Human Trust Behavior?</div>
<div class="field-name">authors:</div>
<div class="field-value">Chengxing Xie;Canyu Chen;Feiran Jia;Ziyu Ye;Shiyang Lai;Kai Shu;Jindong Gu;Adel Bibi;Ziniu Hu;David Jurgens;James Evans;Philip Torr;Bernard Ghanem;Guohao Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM Agent;Human Simulation;Behavioral Alignment;Trust Games</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_learning_for_social_sciences</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/83af3ec1753188a2e392e049fc2d839ffd53ab72.pdf" target="_blank">https://openreview.net/pdf/83af3ec1753188a2e392e049fc2d839ffd53ab72.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We discover that LLM agents generally exhibit trust behavior in Trust Games and GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the potential to simulate human trust behavior with LLM agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.</div>
</div>
<div class='paper-counter'>340/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space</div>
<div class="field-name">authors:</div>
<div class="field-value">Leo Schwinn;David Dobre;Sophie Xhonneux;Gauthier Gidel;Stephan Günnemann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Adversarial Attacks;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cb40ba22037c6e16bd208e3c462a084bedb51d23.pdf" target="_blank">https://openreview.net/pdf/cb40ba22037c6e16bd208e3c462a084bedb51d23.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1484b1907cddbb05d4fd926413e65b448c3ef02b.zip" target="_blank">https://openreview.net/attachment/1484b1907cddbb05d4fd926413e65b448c3ef02b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose continuous embedding attacks as a threat model in large language models (LLMs), demonstrating their effectiveness in removing safety alignment and extracting unlearned information.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current research in adversarial robustness of LLMs focuses on \textit{discrete} input manipulations in the natural language space, which can be directly transferred to \textit{closed-source} models. However, this approach neglects the steady progression of \textit{open-source} models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the \textit{embedding space attack}, which directly attacks the \textit{continuous} embedding representation of input tokens.We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.</div>
</div>
<div class='paper-counter'>341/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions</div>
<div class="field-name">authors:</div>
<div class="field-value">Guangyan Chen;Meiling Wang;Te Cui;Yao Mu;Haoyang Lu;Tianxing Zhou;Zicai Peng;Mengxiao Hu;Haizhou Li;Li Yuan;Yi Yang;Yufeng Yue</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multimodal language models;Vision language models;Robotic manipulation;Code generation;Visual imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e9b1a837e503d1861ece741d0a2b937f77eea435.pdf" target="_blank">https://openreview.net/pdf/e9b1a837e503d1861ece741d0a2b937f77eea435.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e9f7a3d906392c24f5b22eebdcb688cca4fc31fd.zip" target="_blank">https://openreview.net/attachment/e9f7a3d906392c24f5b22eebdcb688cca4fc31fd.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">VLMimic is a novel visual imitation learning paradigm that leverages VLMs to directly learn skills with fine-grained action levels, from a limited number of human videos, outperforming baselines in both simulated and real-world experiments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual imitation learning (VIL) provides an efficient and intuitive strategy for robotic systems to acquire novel skills. Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable performance in vision and language reasoning capabilities for VIL tasks. Despite the progress, current VIL methods naively employ VLMs to learn high-level plans from human videos, relying on pre-defined motion primitives for executing physical interactions, which remains a major bottleneck. In this work, we present VLMimic, a novel paradigm that harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos. Specifically, VLMimic first grounds object-centric movements from human videos, and learns skills using hierarchical constraint representations, facilitating the derivation of skills with fine-grained action levels from limited human videos. These skills are refined and updated through an iterative comparison strategy, enabling efficient adaptation to unseen environments. Our extensive experiments exhibit that our VLMimic, using only 5 human videos, yields significant improvements of over 27% and 21% in RLBench and real-world manipulation tasks, and surpasses baselines by more than 37% in long-horizon tasks. Code and videos are available on our anonymous homepage.</div>
</div>
<div class='paper-counter'>342/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Causal Inference in the Closed-Loop: Marginal Structural Models for Sequential Excursion Effects</div>
<div class="field-name">authors:</div>
<div class="field-value">Alexander W. Levis;Gabriel Loewinger;Francisco Pereira</div>
<div class="field-name">keywords:</div>
<div class="field-value">marginal structural models;optogenetics;excursion effects;neuroscience;dynamic treatment regimes;micro-randomized trials;sequentially randomized experiments</div>
<div class="field-name">primary_area:</div>
<div class="field-value">causal_inference</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d29c280f804bad36b2451f2e49f236e6099ba176.pdf" target="_blank">https://openreview.net/pdf/d29c280f804bad36b2451f2e49f236e6099ba176.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a non-parametric causal inference framework for closed-loop optogenetics behavioral experiments to enable excursion effect estimation for treatment sequences greater than length one in the presence of positivity violations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing "closed-loop" designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends "excursion effect" methods---popular in the mobile health literature---to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.</div>
</div>
<div class='paper-counter'>343/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense</div>
<div class="field-name">authors:</div>
<div class="field-value">Mingkun Zhang;Keping Bi;Wei Chen;Quanrun Chen;Jiafeng Guo;Xueqi Cheng</div>
<div class="field-name">keywords:</div>
<div class="field-value">Adversarial Defense;Diffusion Model;Causal</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec959caf9431c61f33c1cab95d86c6bb04acc70e.pdf" target="_blank">https://openreview.net/pdf/ec959caf9431c61f33c1cab95d86c6bb04acc70e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8b473d3f2b729dbeea660d8bbc8776cf1cb40cf7.zip" target="_blank">https://openreview.net/attachment/8b473d3f2b729dbeea660d8bbc8776cf1cb40cf7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors for adversarial defense on image classification task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39\% (+4.01\%) on CIFAR-10, 56.25\% (+3.13\%) on CIFAR-100, and 82.62\% (+4.93\%) on GTSRB (German Traffic Sign Recognition Benchmark). The code is available athttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff.</div>
</div>
<div class='paper-counter'>344/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Model-based Diffusion for Trajectory Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Chaoyi Pan;Zeji Yi;Guanya Shi;Guannan Qu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion;Trajectory Optimization;Motion Planning;Robotics;Sampling-based Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/de9314b0cb13ef6263570ff2f4dcc2e0c40b3c12.pdf" target="_blank">https://openreview.net/pdf/de9314b0cb13ef6263570ff2f4dcc2e0c40b3c12.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ebff0ade19155b743239159f21802220d1199fd2.zip" target="_blank">https://openreview.net/attachment/ebff0ade19155b743239159f21802220d1199fd2.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Model-Based Diffusion (MBD) solves trajectory optimization by using model information for score computation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as model-based diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD’s ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes are available in the supplementary materials.</div>
</div>
<div class='paper-counter'>345/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities</div>
<div class="field-name">authors:</div>
<div class="field-value">Hao Dong;Yue Zhao;Eleni Chatzi;Olga Fink</div>
<div class="field-name">keywords:</div>
<div class="field-value">Out-of-Distribution Detection;Multimodal Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">safety_in_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9d70465c6462352f6d0ba5d124979dd81b7f1cd0.pdf" target="_blank">https://openreview.net/pdf/9d70465c6462352f6d0ba5d124979dd81b7f1cd0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b73cc5a2ca7401a9eb504022832ed44962c21438.zip" target="_blank">https://openreview.net/attachment/b73cc5a2ca7401a9eb504022832ed44962c21438.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel benchmark MultiOOD for more realistic Multimodal OOD Detection, as well as an Agree-and-Disagree (A2D) algorithm and an NP-Mix outlier synthesis method to improve Multimodal OOD Detection performances.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. To support accessibility and reproducibility, our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.</div>
</div>
<div class='paper-counter'>346/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuanlin Duan;Guofeng Cui;He Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Goal-Conditioned Reinforcement Learning;Model-Based Reinforcement Learning;Exploration Strategies;World Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0296492067a445a2ab25d467127d7927c3912518.pdf" target="_blank">https://openreview.net/pdf/0296492067a445a2ab25d467127d7927c3912518.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose "Cluster Edge Exploration" (CE$^2$), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space, and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE$^2$ demonstrates superior efficiency in exploration compared to baseline methods and ablations.</div>
</div>
<div class='paper-counter'>347/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">IPM-LSTM: A Learning-Based Interior Point Method for Solving Nonlinear Programs</div>
<div class="field-name">authors:</div>
<div class="field-value">Xi Gao;Jinxin Xiong;Akang Wang;Qihong Duan;Jiang Xue;Qingjiang Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">learning to optimize;interior point method;nonlinear constrained programs</div>
<div class="field-name">primary_area:</div>
<div class="field-value">optimization</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6542683c66b26377894da282285f49029b749bbf.pdf" target="_blank">https://openreview.net/pdf/6542683c66b26377894da282285f49029b749bbf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Solving constrained nonlinear programs (NLPs) is of great importance in various domains such as power systems, robotics, and wireless communication networks. One widely used approach for addressing NLPs is the interior point method (IPM). The most computationally expensive procedure in IPMs is to solve systems of linear equations via matrix factorization. Recently, machine learning techniques have been adopted to expedite classic optimization algorithms. In this work, we propose using Long Short-Term Memory (LSTM) neural networks to approximate the solution of linear systems and integrate this approximating step into an IPM. The resulting approximate NLP solution is then utilized to warm-start an interior point solver. Experiments on various types of NLPs, including Quadratic Programs and Quadratically Constrained Quadratic Programs, show that our approach can significantly accelerate NLP solving, reducing iterations by up to 60% and solution time by up to 70% compared to the default solver.</div>
</div>
<div class='paper-counter'>348/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Any2Policy: Learning Visuomotor Policy with Any-Modality</div>
<div class="field-name">authors:</div>
<div class="field-value">Yichen Zhu;Zhicai Ou;Feifei Feng;Jian Tang</div>
<div class="field-name">keywords:</div>
<div class="field-value">multi-modal;robot learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5d47166afae1292bf33c3eca4946a29d4f83a7ae.pdf" target="_blank">https://openreview.net/pdf/5d47166afae1292bf33c3eca4946a29d4f83a7ae.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/42734737f3aa1930054544a5200f4920030e202b.zip" target="_blank">https://openreview.net/attachment/42734737f3aa1930054544a5200f4920030e202b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans can communicate and observe media with different modalities, such as texts, sounds, and images. For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities. Current robotic learning methodologies often focus on single-modal task specification and observation, thereby limiting their ability to process rich multi-modal information. Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents. This system empowers robots to handle tasks using various modalities, whether in combinations like text-image, audio-image, text-point cloud, or in isolation. Our innovative approach involves training a versatile modality network that adapts to various inputs and connects with policy networks for effective control. Because of the lack of existing multi-modal robotics datasets for evaluation, we assembled a comprehensive real-world dataset encompassing 30 robotic tasks. Each task in this dataset is richly annotated across multiple modalities, providing a robust foundation for assessment. We conducted extensive validation of our proposed unified modality embodied agent using several simulation benchmarks, including Franka Kitchen, Meta-World, and Maniskill2, as well as in our real-world settings. Our experiments showcase the promising capability of building embodied agents that can adapt to diverse multi-modal in a unified framework.</div>
</div>
<div class='paper-counter'>349/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BricksRL: A Platform for Democratizing Robotics and  Reinforcement Learning Research and Education with LEGO</div>
<div class="field-name">authors:</div>
<div class="field-value">Sebastian Dittert;Vincent Moens;Gianni De Fabritiis</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics;Reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2da707af995bbe5a2fd1bf93469260c40d2c5dd5.pdf" target="_blank">https://openreview.net/pdf/2da707af995bbe5a2fd1bf93469260c40d2c5dd5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present BricksRL, a platform designed to democratize access to robotics for reinforcement learning research and education. BricksRL facilitates the creation, design, and training of custom LEGO robots in the real world by interfacing them with the TorchRL library for reinforcement learning agents. The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables state-of-the-art reinforcement learning training on GPUs for a wide variety of LEGO builds. This offers a flexible and cost-efficient approach for scaling and also provides a robust infrastructure for robot-environment-algorithm communication. We present various experiments across tasks and robot configurations, providing built plans and training results. Furthermore, we demonstrate that inexpensive LEGO robots can be trained end-to-end in the real world to achieve simple tasks, with training times typically under 120 minutes on a normal laptop. Moreover, we show how users can extend the capabilities, exemplified by the successful integration of non-LEGO sensors. By enhancing accessibility to both robotics and reinforcement learning, BricksRL establishes a strong foundation for democratized robotic learning in research and educational settings.</div>
</div>
<div class='paper-counter'>350/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities</div>
<div class="field-name">authors:</div>
<div class="field-value">Adriel Saporta;Aahlad Manas Puli;Mark Goldstein;Rajesh Ranganath</div>
<div class="field-name">keywords:</div>
<div class="field-value">multimodal;contrastive learning;representation learning;total correlation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d74d24cb967cf692598fb4675185995408e9941e.pdf" target="_blank">https://openreview.net/pdf/d74d24cb967cf692598fb4675185995408e9941e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Symile is a simple contrastive learning approach that captures higher-order information between any number of modalities and provides a flexible, architecture-agnostic objective for learning modality-specific representations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Contrastive learning methods, such as CLIP, leverage naturally paired data—for example, images and their corresponding text captions—to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.</div>
</div>
<div class='paper-counter'>351/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Dylan J Foster;Adam Block;Dipendra Misra</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation learning;statistical learning theory;reinforcement learning theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/35b6f0ccafd8894a3060f640ea7a24f2f4a405ba.pdf" target="_blank">https://openreview.net/pdf/35b6f0ccafd8894a3060f640ea7a24f2f4a405ba.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Through a new analysis of behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline imitation learning, closing an apparent gap between online and offline IL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, behavior cloning (BC) is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner’s access to the expert. We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks. Through a new analysis of BC with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.</div>
</div>
<div class='paper-counter'>352/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Distributional Successor Features Enable Zero-Shot Policy Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Chuning Zhu;Xinqi Wang;Tyler Han;Simon Shaolei Du;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforcement Learning;Multitask Transfer;Diffusion Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5af1d63f8fee561edc23591be026790e53a2d23f.pdf" target="_blank">https://openreview.net/pdf/5af1d63f8fee561edc23591be026790e53a2d23f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Intelligent agents must be generalists, capable of quickly adapting to various tasks. In reinforcement learning (RL), model-based RL learns a dynamics model of the world, in principle enabling transfer to arbitrary reward functions through planning. However, autoregressive model rollouts suffer from compounding error, making model-based RL ineffective for long-horizon problems. Successor features offer an alternative by modeling a policy's long-term state occupancy, reducing policy evaluation under new rewards to linear regression. Yet, policy optimization with successor features can be challenging. This work proposes a novel class of models, i.e., Distributional Successor Features for Zero-Shot Policy Optimization (DiSPOs), that learn a distribution of successor features of a stationary dataset's behavior policy, along with a policy that acts to realize different successor features within the dataset. By directly modeling long-term outcomes in the dataset, DiSPOs avoid compounding error while enabling a simple scheme for zero-shot policy optimization across reward functions. We present a practical instantiation of DiSPOs using diffusion models and show their efficacy as a new class of transferable models, both theoretically and empirically across various simulated robotics problems. Videos and code are available at https://weirdlabuw.github.io/dispo/.</div>
</div>
<div class='paper-counter'>353/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Memorize What Matters: Emergent Scene Decomposition from Multitraverse</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiming Li;Zehong Wang;Yue Wang;Zhiding Yu;Zan Gojcic;Marco Pavone;Chen Feng;Jose M. Alvarez</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;Self-Supervised Learning;3D Gaussian Splatting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/87c09e2b7e2f39416a87c855d382e4d74db73b8a.pdf" target="_blank">https://openreview.net/pdf/87c09e2b7e2f39416a87c855d382e4d74db73b8a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Self-supervised 2D ephemerality segmentation and 3D environmental mapping in urban scenes</div>
<div class="field-name">abstract:</div>
<div class="field-value">Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust 3D representation learning problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residual mining, and robust optimization, 3DGM simultaneously performs 2D segmentation and 3D mapping without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.</div>
</div>
<div class='paper-counter'>354/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion Imitation from Observation</div>
<div class="field-name">authors:</div>
<div class="field-value">Bo-Ruei Huang;Chun-Kai Yang;Chun-Mao Lai;Dai-Jie Wu;Shao-Hua Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation from Observation;Learning from Observation;Imitation Learning;Diffusion Model;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/47a8ac81d50bd6fbb33165af250615515ee96f6a.pdf" target="_blank">https://openreview.net/pdf/47a8ac81d50bd6fbb33165af250615515ee96f6a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning from Observation (LfO) aims to imitate experts by learning from state-only demonstrations without requiring action labels. Existing adversarial imitation learning approaches learn a generator agent policy to produce state transitions that are indistinguishable to a discriminator that learns to classify agent and expert state transitions. Despite its simplicity in formulation, these methods are often sensitive to hyperparameters and brittle to train. Motivated by the recent success of diffusion models in generative modeling, we propose to integrate a diffusion model into the adversarial imitation learning from observation framework. Specifically, we employ a diffusion model to capture expert and agent transitions by generating the next state, given the current state. Then, we reformulate the learning objective to train the diffusion model as a binary classifier and use it to provide ``realness'' rewards for policy learning. Our proposed framework, Diffusion Imitation from Observation (DIFO), demonstrates superior performance in various continuous control domains, including navigation, locomotion, manipulation, and games.</div>
</div>
<div class='paper-counter'>355/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Team-Fictitious Play for Reaching Team-Nash Equilibrium in Multi-team Games</div>
<div class="field-name">authors:</div>
<div class="field-value">Ahmed Said Dönmez;Yüksel Arslantaş;Muhammed O. Sayin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-agent reinforcement learning;fictitious play;multi-team games</div>
<div class="field-name">primary_area:</div>
<div class="field-value">algorithmic_game_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d6fc0a044595d08ac10b7e6e074c1db9cf5a7b27.pdf" target="_blank">https://openreview.net/pdf/d6fc0a044595d08ac10b7e6e074c1db9cf5a7b27.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/95d98f8438270780be11563fb8a0d932cb24cb75.zip" target="_blank">https://openreview.net/attachment/95d98f8438270780be11563fb8a0d932cb24cb75.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A new variant of fictitious play provably converging to Team-Nash equilibrium in multi-team zero-sum games.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-team games, prevalent in robotics and resource management, involve team members striving for a joint best response against other teams. Team-Nash equilibrium (TNE) predicts the outcomes of such coordinated interactions. However, can teams of self-interested agents reach TNE? We introduce Team-Fictitious Play (Team-FP), a new variant of fictitious play where agents respond to the last actions of team members and the beliefs formed about other teams with some inertia in action updates. This design is essential in team coordination beyond the classical fictitious play dynamics. We focus on zero-sum potential team games (ZSPTGs) where teams can interact pairwise while the team members do not necessarily have identical payoffs. We show that Team-FP reaches near TNE in ZSPTGs with a quantifiable error bound. We extend Team-FP dynamics to multi-team Markov games for model-based and model-free cases. The convergence analysis tackles the challenge of non-stationarity induced by evolving opponent strategies based on the optimal coupling lemma and stochastic differential inclusion approximation methods. Our work strengthens the foundation for using TNE to predict the behavior of decentralized teams and offers a practical rule for team learning in multi-team environments. We provide extensive simulations of Team-FP dynamics and compare its performance with other widely studied dynamics such as smooth fictitious play and multiplicative weights update. We further explore how different parameters impact the speed of convergence.</div>
</div>
<div class='paper-counter'>356/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Active Perception for Grasp Detection via Neural Graspness Field</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoxiang Ma;Modi Shi;Boyang Gao;Di Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Grasping;Active Perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eba3836261b94868013e1ac7d9ec431d41bfc9c5.pdf" target="_blank">https://openreview.net/pdf/eba3836261b94868013e1ac7d9ec431d41bfc9c5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper tackles the challenge of active perception for robotic grasp detection in cluttered environments. Incomplete 3D geometry information can negatively affect the performance of learning-based grasp detection methods, and scanning the scene from multiple views introduces significant time costs. To achieve reliable grasping performance with efficient camera movement, we propose an active grasp detection framework based on the Neural Graspness Field (NGF), which models the scene incrementally and facilitates next-best-view planning. Constructed in real-time as the camera moves, the NGF effectively models the grasp distribution in 3D space by rendering graspness predictions from each view. For next-best-view planning, we aim to reduce the uncertainty of the NGF through a graspness inconsistency-guided policy, selecting views based on discrepancies between NGF outputs and a pre-trained graspness network. Additionally, we present a neural graspness sampling method that decodes graspness values from the NGF to improve grasp pose detection results. Extensive experiments on the GraspNet-1Billion benchmark demonstrate significant performance improvements compared to previous works. Real-world experiments show that our method achieves a superior trade-off between grasping performance and time costs.</div>
</div>
<div class='paper-counter'>357/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunuo Chen;Tianyi Xie;Zeshun Zong;Xuan Li;Feng Gao;Yin Yang;Ying Nian Wu;Chenfanfu Jiang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Text-to-3D Generation;Score Distillation Sampling;Physics-based Simulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/73f39a34aec6d1a695fa2762599bb14ea814a8e3.pdf" target="_blank">https://openreview.net/pdf/73f39a34aec6d1a695fa2762599bb14ea814a8e3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/29d2f8587e635a0781143d348bf074b60f991602.zip" target="_blank">https://openreview.net/attachment/29d2f8587e635a0781143d348bf074b60f991602.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks. Generated models frequently fail to maintain balance when placed in physics-based simulations or 3D printed. This balance is crucial for satisfying user design intentions in interactive gaming, embodied AI, and robotics, where stable models are needed for reliable interaction. Additionally, stable models ensure that 3D-printed objects, such as figurines for home decoration, can stand on their own without requiring additional supports. To fill this gap, we introduce Atlas3D, an automatic and easy-to-implement method that enhances existing Score Distillation Sampling (SDS)-based text-to-3D tools. Atlas3D ensures the generation of self-supporting 3D models that adhere to physical laws of stability under gravity, contact, and friction. Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks. We verify Atlas3D's efficacy through extensive generation tasks and validate the resulting 3D models in both simulated and real-world environments.</div>
</div>
<div class='paper-counter'>358/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zunnan Xu;Yukang Lin;Haonan Han;Sicheng Yang;Ronghui Li;Yachao Zhang;Xiu Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gesture Synthesis;Selective State Space Models;Human Computer Interaction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8bcc8170ea92265935bb03237ad7c86156d39602.pdf" target="_blank">https://openreview.net/pdf/8bcc8170ea92265935bb03237ad7c86156d39602.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/490dcfbf7b65a4c8748fe26722e4020c36ebede7.zip" target="_blank">https://openreview.net/attachment/490dcfbf7b65a4c8748fe26722e4020c36ebede7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model to improve gesture synthesis. However, the high computational complexity of these techniques limits the application in reality. In this study, we explore the potential of state space models (SSMs).Direct application of SSMs in gesture synthesis encounters difficulties, which stem primarily from the diverse movement dynamics of various body parts. The generated gestures may also exhibit unnatural jittering issues.To address these, we implement a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures.Built upon the selective scan mechanism, we introduce MambaTalk, which integrates hybrid fusion modules, local and global scans to refine latent space representations.Subjective and objective experiments demonstrate that our method surpasses the performance of state-of-the-art models. Our project is publicly available at~\url{https://kkakkkka.github.io/MambaTalk/}.</div>
</div>
<div class='paper-counter'>359/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hypothesis Testing the Circuit Hypothesis in LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">Claudia Shi;Nicolas Beltran-Velez;Achille Nazaret;Carolina Zheng;Adrià Garriga-Alonso;Andrew Jesson;Maggie Makar;David Blei</div>
<div class="field-name">keywords:</div>
<div class="field-value">Hypothesis testings;mechanistic interpretability;circuit</div>
<div class="field-name">primary_area:</div>
<div class="field-value">interpretability_and_explainability</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d42b43708ca0c06c98f6b5d7a422bd9082f54bdf.pdf" target="_blank">https://openreview.net/pdf/d42b43708ca0c06c98f6b5d7a422bd9082f54bdf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/041cbfc03bb2389ffd6e1c8f782b0a33d9ae8e5f.zip" target="_blank">https://openreview.net/attachment/041cbfc03bb2389ffd6e1c8f782b0a33d9ae8e5f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Hypothesis tests and empirical studies on how well existing circuits align with idealized properties of circuits</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis?In this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. The criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal.We apply these tests to six circuits described in the research literature. We find that synthetic circuits -- circuits that are hard-coded in the model -- align with the idealized properties. Circuits discovered in Transformer models satisfy the criteria to varying degrees.To facilitate future empirical studies of circuits, we created the \textit{circuitry} package, a wrapper around the \textit{TransformerLens} library, which abstracts away lower-level manipulations of hooks and activations. The software is available at \url{https://github.com/blei-lab/circuitry}.</div>
</div>
<div class='paper-counter'>360/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MeMo: Meaningful, Modular Controllers via Noise Injection</div>
<div class="field-name">authors:</div>
<div class="field-value">Megan Tjandrasuwita;Jie Xu;Armando Solar-Lezama;Wojciech Matusik</div>
<div class="field-name">keywords:</div>
<div class="field-value">modular neural network policy;policy transfer;imitation learning;reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/619a53e8af5c6f75d27c1f217291ef9088bf8a5c.pdf" target="_blank">https://openreview.net/pdf/619a53e8af5c6f75d27c1f217291ef9088bf8a5c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/eac1e028508a2ecc66aaf7330c76e9aaa5371953.zip" target="_blank">https://openreview.net/attachment/eac1e028508a2ecc66aaf7330c76e9aaa5371953.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a method for pretraining modular controllers from a single robot and environment that significantly speeds up RL training for locomotion and grasping when reused on more complex morphologies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robots are often built from standardized assemblies, (e.g. arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together. In this paper we demonstrate a new approach that takes a single robot and its controller as input and produces a set of modular controllers for each of these assemblies such that when a new robot is built from the same parts, its control can be quickly learned by reusing the modular controllers. We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a novel modularity objective to learn an appropriate division of labor among the modules. We demonstrate that this objective can be optimized simultaneously with standard behavior cloning loss via noise injection. We benchmark our framework in locomotion and grasping environments on simple to complex robot morphology transfer. We also show that the modules help in task transfer. On both structure and task transfer, MeMo achieves improved training efficiency to graph neural network and Transformer baselines.</div>
</div>
<div class='paper-counter'>361/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploring Low-Dimensional Subspace in Diffusion Models for Controllable Image Editing</div>
<div class="field-name">authors:</div>
<div class="field-value">Siyi Chen;Huijie Zhang;Minzhe Guo;Yifu Lu;Peng Wang;Qing Qu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion model;precise image manipulation;low-rank</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2ecc789f39a91123bffb6022a9d0889986ab90f2.pdf" target="_blank">https://openreview.net/pdf/2ecc789f39a91123bffb6022a9d0889986ab90f2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free **LO**w-rank **CO**ntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace.Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the [project website](https://chicychen.github.io/LOCO).</div>
</div>
<div class='paper-counter'>362/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap Features</div>
<div class="field-name">authors:</div>
<div class="field-value">Chengkai Hou;Zhengrong Xue;Bingyang Zhou;Jinghan Ke;Lin Shao;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Keypoint Detection  Deformable Object  3D Point Cloud</div>
<div class="field-name">primary_area:</div>
<div class="field-value">machine_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1cac2d0a643843b33dd76da7279a9dbbedad0142.pdf" target="_blank">https://openreview.net/pdf/1cac2d0a643843b33dd76da7279a9dbbedad0142.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9141bb880197347fac23c9422106c6aff96aa264.zip" target="_blank">https://openreview.net/attachment/9141bb880197347fac23c9422106c6aff96aa264.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Detecting 3D keypoints with semantic consistency is widely used in many scenarios such as pose estimation, shape registration and robotics. Currently, most unsupervised 3D keypoint detection methods focus on the rigid-body objects. However, when faced with deformable objects, the keypoints they identify do not preserve semantic consistency well. In this paper, we introduce an innovative unsupervised keypoint detector Key-Grid for both the rigid-body and deformable objects, which is an autoencoder framework. The encoder predicts keypoints and the decoder utilizes the generated keypoints to reconstruct the objects. Unlike previous work, we leverage the identified keypoint in formation to form a 3D grid feature heatmap called grid heatmap, which is used in the decoder section. Grid heatmap is a novel concept that represents the latent variables for grid points sampled uniformly in the 3D cubic space, where these variables are the shortest distance between the grid points and the “skeleton” connected by keypoint pairs. Meanwhile, we incorporate the information from each layer of the encoder into the decoder section. We conduct an extensive evaluation of Key-Grid on a list of benchmark datasets. Key-Grid achieves the state-of-the-art performance on the semantic consistency and position accuracy of keypoints. Moreover, we demonstrate the robustness of Key-Grid to noise and downsampling. In addition, we achieve SE-(3) invariance of keypoints though generalizing Key-Grid to a SE(3)-invariant backbone.</div>
</div>
<div class='paper-counter'>363/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Motion Forecasting in Continuous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Nan Song;Bozhou Zhang;Xiatian Zhu;Li Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Motion forecasting; Autonomous Driving;</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2a125e7a1eda481ef4cf87ee407026f462a9ee79.pdf" target="_blank">https://openreview.net/pdf/2a125e7a1eda481ef4cf87ee407026f462a9ee79.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Motion forecasting for agents in autonomous driving is highly challenging due to the numerous possibilities for each agent's next action and their complex interactions in space and time. In real applications, motion forecasting takes place repeatedly and continuously as the self-driving car moves. However, existing forecasting methods typically process each driving scene within a certain range independently, totally ignoring the situational and contextual relationships between successive driving scenes. This significantly simplifies the forecasting task, making the solutions suboptimal and inefficient to use in practice. To address this fundamental limitation, we propose a novel motion forecasting framework for continuous driving, named RealMotion.It comprises two integral streams both at the scene level:(1) The scene context stream progressively accumulates historical scene information until the present moment, capturing temporal interactive relationships among scene elements.(2) The agent trajectory stream optimizes current forecasting by sequentially relaying past predictions.Besides, a data reorganization strategy is introduced to narrow the gap between existing benchmarks and real-world applications, consistent with our network. These approaches enable exploiting more broadly the situational and progressive insights of dynamic motion across space and time. Extensive experiments on Argoverse series with different settings demonstrate that our RealMotion achieves state-of-the-art performance, along with the advantage of efficient real-world inference.</div>
</div>
<div class='paper-counter'>364/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaiqu Liang;Zixu Zhang;Jaime Fernández Fisac</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Conformal Prediction;Uncertainty Quantification;Foundation Models for Decision Making</div>
<div class="field-name">primary_area:</div>
<div class="field-value">natural_language_processing</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/34d4b3c9d68e98e8ce07734791b988c7fb03a20a.pdf" target="_blank">https://openreview.net/pdf/34d4b3c9d68e98e8ce07734791b988c7fb03a20a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a7a718feecbbf45b8b7f156a7b9c10493025770c.zip" target="_blank">https://openreview.net/attachment/a7a718feecbbf45b8b7f156a7b9c10493025770c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper propose introspective planning to guide Large Language Models (LLMs) planning with uncertainty awareness, and achieve a tighter confidence bound with conformal prediction.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning. We propose introspective planning, a systematic approach that guides LLMs to refine their own uncertainty in alignment with inherent task ambiguity. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a new safe mobile manipulation benchmark, indicate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Additionally, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.</div>
</div>
<div class='paper-counter'>365/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">iVideoGPT: Interactive VideoGPTs are Scalable World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Jialong Wu;Shaofeng Yin;Ningya Feng;Xu He;Dong Li;Jianye HAO;Mingsheng Long</div>
<div class="field-name">keywords:</div>
<div class="field-value">world model;model-based reinforcement learning;video prediction;visual planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a848046c24eaa7568067026c8fceb7851d74ba96.pdf" target="_blank">https://openreview.net/pdf/a848046c24eaa7568067026c8fceb7851d74ba96.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose iVideoGPT, an autoregressive transformer architecture for scalable world models, pre-train it on millions of trajectories and adapt it to a wide range of tasks, including video prediction, visual planning, and model-based RL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals—visual observations, actions, and rewards—into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications. Code and pre-trained models are available at https://thuml.github.io/iVideoGPT.</div>
</div>
<div class='paper-counter'>366/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Strategic Littlestone Dimension: Improved Bounds on Online Strategic Classification</div>
<div class="field-name">authors:</div>
<div class="field-value">Saba Ahmadi;Kunhe Yang;Hanrui Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">online strategic classification;Littlestone dimension;manipulation graph</div>
<div class="field-name">primary_area:</div>
<div class="field-value">online_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/51a421f67e2a16b771ccba68a6143fb519646f92.pdf" target="_blank">https://openreview.net/pdf/51a421f67e2a16b771ccba68a6143fb519646f92.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We study the problem of online binary classification in settings where strategic agents can modify their observable features to receive a positive classification. We model the set of feasible manipulations by a directed graph over the feature space, and assume the learner only observes the manipulated features instead of the original ones. We introduce the Strategic Littlestone Dimension, a new combinatorial measure that captures the joint complexity of the hypothesis class and the manipulation graph. We demonstrate that it characterizes the instance-optimal mistake bounds for deterministic learning algorithms in the realizable setting. We also achieve improved regret in the agnostic setting by a refined agnostic-to-realizable reduction that accounts for the additional challenge of not observing agents' original features. Finally, we relax the assumption that the learner knows the manipulation graph, instead assuming their knowledge is captured by a family of graphs. We derive regret bounds in both the realizable setting where all agents manipulate according to the same graph within the graph family, and the agnostic setting where the manipulation graphs are chosen adversarially and not consistently modeled by a single graph in the family.</div>
</div>
<div class='paper-counter'>367/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering</div>
<div class="field-name">authors:</div>
<div class="field-value">Ido Sobol;Chenfeng Xu;Or Litany</div>
<div class="field-name">keywords:</div>
<div class="field-value">Novel View Synthesis;Image Generative Models;Diffusion Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2b060ee47100b88216ae60b75e7ca3abafc26b1f.pdf" target="_blank">https://openreview.net/pdf/2b060ee47100b88216ae60b75e7ca3abafc26b1f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Training-free boosting of diffusion based novel view generation through attention map manipulations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences. Recent advancements in diffusion models, particularly the Zero-1-to-3 model, have been widely adopted for generating plausible views, videos, and 3D models. However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint. In this work, we propose Zero-to-Hero, a novel test-time approach that enhances view synthesis by manipulating attention maps during the denoising process of Zero-1-to-3. By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability and authenticity. This process improves geometric consistency without requiring retraining or significant computational resources. Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions. These processes are further supported by a specialized sampling schedule. Experimental results demonstrate substantial improvements in fidelity and consistency, validated on a diverse set of out-of-distribution objects. Additionally, we demonstrate the general applicability and effectiveness of Zero-to-Hero in multi-view, and image generation conditioned on semantic maps and pose.</div>
</div>
<div class='paper-counter'>368/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinmin He;Kai Li;Yifan Zang;Haobo Fu;QIANG FU;Junliang Xing;Jian Cheng</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-task Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/718f8a0162937d8a72dd87918f6855d8654402fe.pdf" target="_blank">https://openreview.net/pdf/718f8a0162937d8a72dd87918f6855d8654402fe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/66317c521caab84531b2f5397b8f61dce0fb24c7.zip" target="_blank">https://openreview.net/attachment/66317c521caab84531b2f5397b8f61dce0fb24c7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks. Existing approaches primarily focus on parameter sharing with carefully designed network structures or tailored optimization procedures. However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition. To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies, generating better training trajectories. In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance. CTPG is a general framework adaptable to existing parameter sharing approaches. Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks.</div>
</div>
<div class='paper-counter'>369/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generative Adversarial Model-Based Optimization via Source Critic Regularization</div>
<div class="field-name">authors:</div>
<div class="field-value">Michael S Yao;Yimeng Zeng;Hamsa Bastani;Jacob R. Gardner;James Gee;Osbert Bastani</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Optimization;Bayesian Optimization;Surrogate Objectives</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5e9c7395873a86fe0b8430642a4679960f1a27ef.pdf" target="_blank">https://openreview.net/pdf/5e9c7395873a86fe0b8430642a4679960f1a27ef.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that source critic adversarial networks can effectively regularize offline optimizers for generative tasks in medicine and the sciences.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline model-based optimization seeks to optimize against a learned surrogate model without querying the true oracle objective function during optimization. Such tasks are commonly encountered in protein design, robotics, and clinical medicine where evaluating the oracle function is prohibitively expensive. However, inaccurate surrogate model predictions are frequently encountered along offline optimization trajectories. To address this limitation, we propose *generative adversarial model-based optimization* using **adaptive source critic regularization (aSCR)**—a task- and optimizer- agnostic framework for constraining the optimization trajectory to regions of the design space where the surrogate function is reliable. We propose a computationally tractable algorithm to dynamically adjust the strength of this constraint, and show how leveraging aSCR with standard Bayesian optimization outperforms existing methods on a suite of offline generative design tasks. Our code is available at https://github.com/michael-s-yao/gabo.</div>
</div>
<div class='paper-counter'>370/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding</div>
<div class="field-name">authors:</div>
<div class="field-value">Yanmin Wu;Jiarui Meng;Haijie LI;Chenming Wu;Yahao Shi;Xinhua Cheng;Chen Zhao;Haocheng Feng;Errui Ding;Jingdong Wang;Jian Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Understanding;3D Gaussian Splatting;open-vocabulary</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fd2da0776f6305648e3c07de2e0a2c9fc6d08e5d.pdf" target="_blank">https://openreview.net/pdf/fd2da0776f6305648e3c07de2e0a2c9fc6d08e5d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) that possesses the capability for 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level.Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. The source code is available at our project page https://3d-aigc.github.io/OpenGaussian.</div>
</div>
<div class='paper-counter'>371/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar</div>
<div class="field-name">authors:</div>
<div class="field-value">Fangqiang Ding;Xiangyu Wen;Yunzhou Zhu;Yiming Li;Chris Xiaoxuan Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">4D Imaging Radar;3D Occupancy Prediction;Scene Understanding;Autonomous Driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a7c6663d15027d21973269a119bc1ad67116b73f.pdf" target="_blank">https://openreview.net/pdf/a7c6663d15027d21973269a119bc1ad67116b73f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5eeba321f9601e9642eff87213fceba2b23aed9a.zip" target="_blank">https://openreview.net/attachment/5eeba321f9601e9642eff87213fceba2b23aed9a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a pioneering method that utilizes 4D imaging radar sensors for robust 3D occupancy prediction even against adverse weathers.</div>
<div class="field-name">abstract:</div>
<div class="field-value">3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction. Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details. RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms. To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation. We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset. The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods. Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies.</div>
</div>
<div class='paper-counter'>372/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MotionBooth: Motion-Aware Customized Text-to-Video Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianzong Wu;Xiangtai Li;Yanhong Zeng;Jiangning Zhang;Qianyu Zhou;Yining Li;Yunhai Tong;Kai Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">custimized generation;video motion generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">diffusion_based_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a3f3aa90fe0891b06247e492408e30d8fb44fe1b.pdf" target="_blank">https://openreview.net/pdf/a3f3aa90fe0891b06247e492408e30d8fb44fe1b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/fe4cf92efbfeec443e01ff5234bd49647d7d68da.zip" target="_blank">https://openreview.net/attachment/fe4cf92efbfeec443e01ff5234bd49647d7d68da.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently fine-tune a text-to-video model to capture the object's shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject's learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Models and codes will be made publicly available.</div>
</div>
<div class='paper-counter'>373/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Qingwen Bu;Jia Zeng;Li Chen;Yanchao Yang;Guyue Zhou;Junchi Yan;Ping Luo;Heming Cui;Yi Ma;Hongyang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Manipulation;Visuomotor Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6d661ae5f68b379172437a497e57e1f2a5dd4f7e.pdf" target="_blank">https://openreview.net/pdf/6d661ae5f68b379172437a497e57e1f2a5dd4f7e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.</div>
</div>
<div class='paper-counter'>374/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Yipu Chen;Haotian Xue;Yongxin Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion Model;Adversarial Attacks;Robot Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative_models</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec80acb56b94cb34d7b6f867a138de80110d613e.pdf" target="_blank">https://openreview.net/pdf/ec80acb56b94cb34d7b6f867a138de80110d613e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/463b68d0420ac8a8a150970a1d9c895b3863ed95.zip" target="_blank">https://openreview.net/attachment/463b68d0420ac8a8a150970a1d9c895b3863ed95.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models have emerged as a promising approach for behavior cloning (BC), leveraging their exceptional ability to model multi-modal distributions. Diffusion policies (DP) have elevated BC performance to new heights, demonstrating robust efficacy across diverse tasks, coupled with their inherent flexibility and ease of implementation. Despite the increasing adoption of Diffusion Policies (DP) as a foundation for policy generation, the critical issue of safety remains largely unexplored. While previous attempts have targeted deep policy networks, DP used diffusion models as the policy network, making it ineffective to be attacked using previous methods because of its chained structure and randomness injected. In this paper, we undertake a comprehensive examination of DP safety concerns by introducing adversarial scenarios, encompassing offline and online attacks, global and patch-based attacks. We propose DP-Attacker, a suite of algorithms that can craft effective adversarial attacks across all aforementioned scenarios. We conduct attacks on pre-trained diffusion policies across various manipulation tasks. Through extensive experiments, we demonstrate that DP-Attacker has the capability to significantly decrease the success rate of DP for all scenarios. Particularly in offline scenarios, we exhibit the generation of highly transferable perturbations applicable to all frames. Furthermore, we illustrate the creation of adversarial physical patches that, when applied to the environment, effectively deceive the model. Video results areput in: https://sites.google.com/view/dp-attacker-videos/.</div>
</div>
<div class='paper-counter'>375/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Trajectory Diffusion for ObjectGoal Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinyao Yu;Sixian Zhang;Xinhang Song;Xiaorong Qin;Shuqiang Jiang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;visual navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2024 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4a98ca3c5b5cc11841f2d3f230131fedb22a7c9a.pdf" target="_blank">https://openreview.net/pdf/4a98ca3c5b5cc11841f2d3f230131fedb22a7c9a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4da1b9e141eee82a9ed0cb11f842bd009bf89857.zip" target="_blank">https://openreview.net/attachment/4da1b9e141eee82a9ed0cb11f842bd009bf89857.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Object goal navigation requires an agent to navigate to a specified object in an unseen environment based on visual observations and user-specified goals. Human decision-making in navigation is sequential, planning a most likely sequence of actions toward the goal. However, existing ObjectNav methods, both end-to-end learning methods and modular methods, rely on single-step planning. They output the next action based on the current model input, which easily overlooks temporal consistency and leads to myopic planning.To this end, we aim to learn sequence planning for ObjectNav. Specifically, we propose trajectory diffusion to learn the distribution of trajectory sequences conditioned on the current observation and the goal. We utilize DDPM and automatically collected optimal trajectory segments to train the trajectory diffusion.Once the trajectory diffusion model is trained, it can generate a temporally coherent sequence of future trajectory for agent based on its current observations.Experimental results on the Gibson and MP3D datasets demonstrate that the generated trajectories effectively guide the agent, resulting in more accurate and efficient navigation.</div>
</div>
<div class='paper-counter'>376/828</div>
<div class="conference">NeurIPS_cc_2023_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FAMO: Fast Adaptive Multitask Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Bo Liu;Yihao Feng;Peter Stone;qiang liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">multitask learning;multitask optimization;conflicting gradients;knowledge transfer</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7b86009f1e0203f3e6e2ef8927789fb197f564de.pdf" target="_blank">https://openreview.net/pdf/7b86009f1e0203f3e6e2ef8927789fb197f564de.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/20956f7a2cad082fcdb5409f26a4d2b539f2ad80.zip" target="_blank">https://openreview.net/attachment/20956f7a2cad082fcdb5409f26a4d2b539f2ad80.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a fast adaptive task weighting method for multitask optimization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using $\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at \url{https://github.com/Cranial-XIX/FAMO}.</div>
</div>
<div class='paper-counter'>377/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Sihan Xu;Ziqiao Ma;Yidong Huang;Honglak Lee;Joyce Chai</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image to image translation;latent diffusion models;conditional diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a793a0f07dbdfca6a7ec129b5d1bccc9bdb80114.pdf" target="_blank">https://openreview.net/pdf/a793a0f07dbdfca6a7ec129b5d1bccc9bdb80114.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c06a4628bbb3916b7eb5fa1f6d77d03c2f348e1b.zip" target="_blank">https://openreview.net/attachment/c06a4628bbb3916b7eb5fa1f6d77d03c2f348e1b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a simple neural network called CycleNet that incorporates cycle consistency into conditional diffusion models for text-guided image manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Project homepage: https://cyclenetweb.github.io/</div>
</div>
<div class='paper-counter'>378/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Model-Based Control with Sparse Neural Dynamics</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziang Liu;Genggeng Zhou;Jeff He;Tobia Marcucci;Li Fei-Fei;Jiajun Wu;Yunzhu Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">model learning;model-based control;neural network sparsification;mixed-integer programming;trajectory optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5ee9114f8ac04a00605e7f9ee11adfbf5d23fd68.pdf" target="_blank">https://openreview.net/pdf/5ee9114f8ac04a00605e7f9ee11adfbf5d23fd68.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7b23fded0927bccf8d265e5d25fa1da8fd942ee2.zip" target="_blank">https://openreview.net/attachment/7b23fded0927bccf8d265e5d25fa1da8fd942ee2.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose to sparsify neural dynamics models for more effective closed-loop, model-based planning using the mixed-integer formulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning predictive models from observations using deep neural networks (DNNs) is a promising new approach to many real-world planning and control problems. However, common DNNs are too unstructured for effective planning, and current control methods typically rely on extensive sampling or local gradient descent. In this paper, we propose a new framework for integrated model learning and predictive control that is amenable to efficient optimization algorithms. Specifically, we start with a ReLU neural model of the system dynamics and, with minimal losses in prediction accuracy, we gradually sparsify it by removing redundant neurons. This discrete sparsification process is approximated as a continuous problem, enabling an end-to-end optimization of both the model architecture and the weight parameters. The sparsified model is subsequently used by a mixed-integer predictive controller, which represents the neuron activations as binary variables and employs efficient branch-and-bound algorithms. Our framework is applicable to a wide variety of DNNs, from simple multilayer perceptrons to complex graph neural dynamics. It can efficiently handle tasks involving complicated contact dynamics, such as object pushing, compositional object sorting, and manipulation of deformable objects. Numerical and hardware experiments show that, despite the aggressive sparsification, our framework can deliver better closed-loop performance than existing state-of-the-art methods.</div>
</div>
<div class='paper-counter'>379/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Sheng-Yen Chou;Pin-Yu Chen;Tsung-Yi Ho</div>
<div class="field-name">keywords:</div>
<div class="field-value">backdoor;diffusion model;trustworthy</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1714ec5af2bb79e4c1a3364bb68cee070e5fe4e5.pdf" target="_blank">https://openreview.net/pdf/1714ec5af2bb79e4c1a3364bb68cee070e5fe4e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8d44cea9fcc5f1d9e2a493638dc0fab9fa4a5d83.pdf" target="_blank">https://openreview.net/attachment/8d44cea9fcc5f1d9e2a493638dc0fab9fa4a5d83.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.</div>
</div>
<div class='paper-counter'>380/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Imitation Learning from Imperfection: Theoretical Justifications and Algorithms</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziniu Li;Tian Xu;Zeyu Qin;Yang Yu;Zhi-Quan Luo</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;distribution shift;policy optimization;data selection</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/22d0b59e25f27418f040c4b051afff0d0d1693ca.pdf" target="_blank">https://openreview.net/pdf/22d0b59e25f27418f040c4b051afff0d0d1693ca.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e21515ff4231e431ef5098ddf7d8299ea75515f8.pdf" target="_blank">https://openreview.net/attachment/e21515ff4231e431ef5098ddf7d8299ea75515f8.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show the potential of improving imitation learning by leveraging diverse data sources through effective data selection.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for sequential decision-making tasks. But, their effectiveness is hampered when faced with limited expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data has been proposed, which enhances learning by incorporating an additional yet imperfect dataset obtained inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the potential inclusion of out-of-expert-distribution samples. In this work, we propose a mathematical formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a naive approach—applying the behavioral cloning (BC) algorithm concept to the combined set of expert and supplementary data—may fall short of vanilla BC, which solely relies on expert data. This deficiency arises due to the distribution shift between the two data sources. To address this issue, we propose a new importance-sampling-based technique for selecting data within the expert distribution. We prove that the proposed method eliminates the gap of the naive approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that our method outperforms previous state-of-the-art methods in tasks including robotic locomotion control, Atari video games, and image classification. Overall, our work underscores the potential of improving IL by leveraging diverse data sources through effective data selection.</div>
</div>
<div class='paper-counter'>381/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Self-Supervised Reinforcement Learning that Transfers using Random Features</div>
<div class="field-name">authors:</div>
<div class="field-value">Boyuan Chen;Chuning Zhu;Pulkit Agrawal;Kaiqing Zhang;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">deep reinforcement learning;self-supervised learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3ebd4b1cebdcdab7974434f9468facf8430c57cb.pdf" target="_blank">https://openreview.net/pdf/3ebd4b1cebdcdab7974434f9468facf8430c57cb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5ff5406f3501b71797ac9ae0173ec21a4de4ee06.zip" target="_blank">https://openreview.net/attachment/5ff5406f3501b71797ac9ae0173ec21a4de4ee06.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems with new reward functions. Our method is self-supervised in that it can be trained on offline datasets without reward labels, but can then be quickly deployed on new tasks. We validate that our proposed method enables transfer across tasks on a variety of manipulation and locomotion domains in simulation, opening the door to generalist decision-making agents.</div>
</div>
<div class='paper-counter'>382/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Marcel Torne Villasevil;Max Balsells I Pamies;Zihan Wang;Samedh Desai;Tao Chen;Pulkit Agrawal;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning from human preferences;self-supervised learning;exploration in reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a40335f3a755b1158686f87fd0bc8a70a931d939.pdf" target="_blank">https://openreview.net/pdf/a40335f3a755b1158686f87fd0bc8a70a931d939.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/db8e1d223e573fc1f49485a430a2cab9605f4ac3.zip" target="_blank">https://openreview.net/attachment/db8e1d223e573fc1f49485a430a2cab9605f4ac3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Solving long-horizon robotic tasks with crowdsourced human feedback</div>
<div class="field-name">abstract:</div>
<div class="field-value">Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision making tasks with a non-trivial element of exploration requires either specifying carefully designed reward functions or relying on indiscriminate, novelty seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we propose a technique - Human Guided Exploration (HUGE), that is able to leverage low-quality feedback from non-expert users, which is infrequent, asynchronous and noisy, to guide exploration for reinforcement learning, without requiring careful reward specification. The key idea is to separate the challenges of directed exploration and policy learning - human feedback is used to direct exploration, while self-supervised policy learning is used to independently learn unbiased behaviors from the collected data. We show that this procedure can leverage noisy, asynchronous human feedback to learn tasks with no hand-crafted reward design or exploration bonuses. We show that HUGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots.</div>
</div>
<div class='paper-counter'>383/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ReDS: Offline RL With Heteroskedastic Datasets via Support Constraints</div>
<div class="field-name">authors:</div>
<div class="field-value">Anikait Singh;Aviral Kumar;Quan Vuong;Yevgen Chebotar;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline RL;support constraints;heteroskedastic data</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/af33007ef0d73079ce68fac45fe86afa1887aab7.pdf" target="_blank">https://openreview.net/pdf/af33007ef0d73079ce68fac45fe86afa1887aab7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/903f9c8be6018d03569bef6a1ffb906daa4fb169.zip" target="_blank">https://openreview.net/attachment/903f9c8be6018d03569bef6a1ffb906daa4fb169.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that conventional distributional constraint RL algorithms are need with heteroskedatic datasets. We propose an offline RL method to handle such settings.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline reinforcement learning (RL) learns policies entirely from static datasets. Practical applications of offline RL will inevitably require learning from datasets where the variability of demonstrated behaviors changes non-uniformly across the state space. For example, at a red light, nearly all human drivers behave similarly by stopping, but when merging onto a highway, some drivers merge quickly, efficiently, and safely, while many hesitate or merge dangerously. Both theoretically and empirically, we show that typical offline RL methods, which are based on distribution constraints fail to learn from data with such non-uniform variability, due to the requirement to stay close to the behavior policy **to the same extent** across the state space. Ideally, the learned policy should be free to choose **per state** how closely to follow the behavior policy to maximize long-term return, as long as the learned policy stays within the support of the behavior policy. To instantiate this principle, we reweight the data distribution in conservative Q-learning (CQL) to obtain an approximate support constraint formulation. The reweighted distribution is a mixture of the current policy and an additional policy trained to mine poor actions that are likely under the behavior policy. Our method, CQL (ReDS), is theoretically motivated, and improves performance across a wide range of offline RL problems in games, navigation, and pixel-based manipulation.</div>
</div>
<div class='paper-counter'>384/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongxin Li;Jingran Su;Yuntao Chen;Qing Li;Zhaoxiang Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Model; Task Planning; Embodied AI; Robotics; Software Automation; Decision making</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e39e19f886957d5341513a6082960eccfe1ec02b.pdf" target="_blank">https://openreview.net/pdf/e39e19f886957d5341513a6082960eccfe1ec02b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/dba4c5e83fb1ee3bd3642b7cc62db2ac43acc14d.zip" target="_blank">https://openreview.net/attachment/dba4c5e83fb1ee3bd3642b7cc62db2ac43acc14d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an LLM-based autonomous agent that manipulates complex software by following natural language instructions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/.</div>
</div>
<div class='paper-counter'>385/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Parallel-mentoring for Offline Model-based Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Can Chen;Christopher Beckham;Zixuan Liu;Xue Liu;Christopher Pal</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline model-based optimization;bi-level optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/447e9dd103eda449af07c8e75b96480c57e33ec3.pdf" target="_blank">https://openreview.net/pdf/447e9dd103eda449af07c8e75b96480c57e33ec3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0adeb2f51d36d4754ce48e0218d8c3439de1b380.zip" target="_blank">https://openreview.net/attachment/0adeb2f51d36d4754ce48e0218d8c3439de1b380.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a simple and novel method called parallel-mentoring to improve the ensemble robustness by facilitating the exchange of weak ranking supervision signals among proxies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study offline model-based optimization to maximize a black-box objective function with a static dataset of designs and scores. These designs encompass a variety of domains, including materials, robots, DNA sequences, and proteins. A common approach trains a proxy on the static dataset and performs gradient ascent to obtain new designs. However, this often results in poor designs due to the proxy inaccuracies for out-of-distribution designs. Recent studies indicate that (a) gradient ascent with a mean ensemble of proxies generally outperforms simple gradient ascent, and (b) a trained proxy provides weak ranking supervision signals for design selection. Motivated by (a) and (b), we propose $\textit{parallel-mentoring}$ as an effective and novel method that facilitates mentoring among proxies, creating a more robust ensemble to mitigate the out-of-distribution issue. We focus on the three-proxy case in the main paper and our method consists of two modules. The first module, $\textit{voting-based pairwise supervision}$, operates on three parallel proxies and captures their ranking supervision signals as pairwise comparison labels. These labels are combined through majority voting to generate consensus labels, which incorporates ranking supervision signals from all proxies and enables mutual mentoring. Yet, label noise arises due to possible incorrect consensus. To alleviate this, we introduce an $\textit{adaptive soft-labeling}$ module with soft-labels initialized as consensus labels. Based on bi-level optimization, this module fine-tunes proxies in the inner level and learns more accurate labels in the outer level to adaptively mentor proxies, resulting in a more robust ensemble. Experiments validate the effectiveness of our method. Our code is available here.</div>
</div>
<div class='paper-counter'>386/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rewiring Neurons in Non-Stationary Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhicheng Sun;Yadong MU</div>
<div class="field-name">keywords:</div>
<div class="field-value">continual learning;reinforcement learning;brain-inspired learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/85f193c9b9af583af8f8dfbbef12c329e8884955.pdf" target="_blank">https://openreview.net/pdf/85f193c9b9af583af8f8dfbbef12c329e8884955.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f46287ba07e648c1a3d91ec1b2c393adc5bcadd0.pdf" target="_blank">https://openreview.net/attachment/f46287ba07e648c1a3d91ec1b2c393adc5bcadd0.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new rewiring approach for continual reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The human brain rewires itself for neuroplasticity in the presence of new tasks. We are inspired to harness this key process in continual reinforcement learning, prioritizing adaptation to non-stationary environments. In distinction to existing rewiring approaches that rely on pruning or dynamic routing, which may limit network capacity and plasticity, this work presents a novel rewiring scheme by permuting hidden neurons. Specifically, the neuron permutation is parameterized to be end-to-end learnable and can rearrange all available synapses to explore a large span of weight space, thereby promoting adaptivity. In addition, we introduce two main designs to steer the rewiring process in continual reinforcement learning: first, a multi-mode rewiring strategy is proposed which diversifies the policy and encourages exploration when encountering new environments. Secondly, to ensure stability on history tasks, the network is devised to cache each learned wiring while subtly updating its weights, allowing for retrospective recovery of any previous state appropriate for the task. Meanwhile, an alignment mechanism is curated to achieve better plasticity-stability tradeoff by jointly optimizing cached wirings and weights. Our proposed method is comprehensively evaluated on 18 continual reinforcement learning scenarios ranging from locomotion to manipulation, demonstrating its advantages over state-of-the-art competitors in performance-efficiency tradeoffs. Code is available at https://github.com/feifeiobama/RewireNeuron.</div>
</div>
<div class='paper-counter'>387/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">What’s Left? Concept Grounding with Logic-Enhanced Foundation Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Joy Hsu;Jiayuan Mao;Joshua B. Tenenbaum;Jiajun Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">concept learning;visual reasoning;large language models;neuro-symbolic learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4d7303a06a683331c65270470df54eaab0bf5a04.pdf" target="_blank">https://openreview.net/pdf/4d7303a06a683331c65270470df54eaab0bf5a04.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A unified concept learning and reasoning framework that integrates LLMs with learnable grounding modules across domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning—using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like “*left*” can also be grounded in 3D, temporal, and action data, as in moving to your *left*. This limited generalization stems from these inference-only methods’ inability to learn or adapt pre-trained models to a new domain. We propose the **L**ogic-**E**nhanced **F**ounda**T**ion Model (**LEFT**), a unified framework that *learns* to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT’s executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.</div>
</div>
<div class='paper-counter'>388/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ELDEN: Exploration via Local Dependencies</div>
<div class="field-name">authors:</div>
<div class="field-value">Zizhao Wang;Jiaheng Hu;Peter Stone;Roberto Martín-Martín</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning; intrinsic motivation; exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ad00f738ab67bbba4f0ec31b2d628904a1d30bf5.pdf" target="_blank">https://openreview.net/pdf/ad00f738ab67bbba4f0ec31b2d628904a1d30bf5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/21ec7994023e4947b3fe978d65ee5d8402cdd4cb.pdf" target="_blank">https://openreview.net/attachment/21ec7994023e4947b3fe978d65ee5d8402cdd4cb.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An exploration method for reinforcement learning based on novel local dependencies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme --- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.</div>
</div>
<div class='paper-counter'>389/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VOCE: Variational Optimization with Conservative Estimation for Offline Safe Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiayi Guan;Guang Chen;Jiaming Ji;Long Yang;Ao Zhou;Zhijun Li;changjun jiang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline safe reinforcement learning;Pessimistic conservative estimation;Variational optimization;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/62663449eadaaffe849b2df8061910045d9ff6c7.pdf" target="_blank">https://openreview.net/pdf/62663449eadaaffe849b2df8061910045d9ff6c7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3d9f717ab9d376873b6f246cb24212f3c4855373.zip" target="_blank">https://openreview.net/attachment/3d9f717ab9d376873b6f246cb24212f3c4855373.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline safe reinforcement learning (RL) algorithms promise to learn policies that satisfy safety constraints directly in offline datasets without interacting with the environment. This arrangement is particularly important in scenarios with high sampling costs and potential dangers, such as autonomous driving and robotics. However, the influence of safety constraints and out-of-distribution (OOD) actions have made it challenging for previous methods to achieve high reward returns while ensuring safety. In this work, we propose a Variational Optimization with Conservative Eestimation algorithm (VOCE) to solve the problem of optimizing safety policies in the offline dataset. Concretely, we reframe the problem of offline safe RL using probabilistic inference, which introduces variational distributions to make the optimization of policies more flexible. Subsequently, we utilize pessimistic estimation methods to estimate the Q-value of cost and reward, which mitigates the extrapolation errors induced by OOD actions. Finally, extensive experiments demonstrate that the VOCE algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety.</div>
</div>
<div class='paper-counter'>390/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Hanna Ziesche;Leonel Rozo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Policy optimization;robot learning;reinforcement learning;Gaussian mixture models;optimal transport;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1e1dd7dd62f17b38cfaba8e36a359dda14a4565b.pdf" target="_blank">https://openreview.net/pdf/1e1dd7dd62f17b38cfaba8e36a359dda14a4565b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b758d8ef9eae92e2f5b0e345d7ad0c25d6c3e855.zip" target="_blank">https://openreview.net/attachment/b758d8ef9eae92e2f5b0e345d7ad0c25d6c3e855.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We leverage the structure of Gaussian mixture policies to cast the policy optimization as a Wasserstein gradient flow over the GMMs space.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities.  When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives.  Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms.  We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermore, we leverage the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of the GMM policy via Riemannian optimization. We evaluate our approach on common robotic settings: Reaching motions, collision-avoidance behaviors, and multi-goal tasks. Our results show that our method outperforms common policy optimization baselines in terms of task success rate and low-variance solutions.</div>
</div>
<div class='paper-counter'>391/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PHOTOSWAP: Personalized Subject Swapping in Images</div>
<div class="field-name">authors:</div>
<div class="field-value">Jing Gu;Yilin Wang;Nanxuan Zhao;Tsu-Jui Fu;Wei Xiong;Qing Liu;Zhifei Zhang;HE Zhang;Jianming Zhang;HyunJoon Jung;Xin Eric Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">image editing;diffusion model;text to image generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9ee3408b1e355f82e723648dd35688547a25f129.pdf" target="_blank">https://openreview.net/pdf/9ee3408b1e355f82e723648dd35688547a25f129.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/176fbd6a4d2685594804aaea17c52bef7175f60c.pdf" target="_blank">https://openreview.net/attachment/176fbd6a4d2685594804aaea17c52bef7175f60c.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Subject swapping with a personal item</div>
<div class="field-name">abstract:</div>
<div class="field-value">In an era where images and visual content dominate our digital landscape, the ability to manipulate and personalize these images has become a necessity.Envision seamlessly substituting a tabby cat lounging on a sunlit window sill in a photograph with your own playful puppy, all while preserving the original charm and composition of the image. We present \emph{Photoswap}, a novel approach that enables this immersive image editing experience through personalized subject swapping in existing images.\emph{Photoswap} first learns the visual concept of the subject from reference images and then swaps it into the target image using pre-trained diffusion models in a training-free manner. We establish that a well-conceptualized visual subject can be seamlessly transferred to any image with appropriate self-attention and cross-attention manipulation, maintaining the pose of the swapped subject and the overall coherence of the image. Comprehensive experiments underscore the efficacy and controllability of \emph{Photoswap} in personalized subject swapping. Furthermore, \emph{Photoswap} significantly outperforms baseline methods in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing.</div>
</div>
<div class='paper-counter'>392/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion Self-Guidance for Controllable Image Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Dave Epstein;Allan Jabri;Ben Poole;Alexei A Efros;Aleksander Holynski</div>
<div class="field-name">keywords:</div>
<div class="field-value">generative models;image editing;diffusion;guidance</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ce0509649acd6033731bb6b76e3ff6a86aea5b90.pdf" target="_blank">https://openreview.net/pdf/ce0509649acd6033731bb6b76e3ff6a86aea5b90.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Use attention and features from the denoising forward pass to guide diffusion sampling - change object size, position, and appearance without affecting the rest of the scene, and much more!</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large-scale generative models are capable of producing high-quality images from detailed prompts. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides precise control over properties of the generated image by guiding the internal representations of diffusion models. We demonstrate that the size, location, and appearance of objects can be extracted from these representations, and show how to use them to steer the sampling process. Self-guidance operates similarly to standard classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We demonstrate the flexibility and effectiveness of self-guided generation through a wide range of challenging image manipulations, such as modifying the position or size of a single object (keeping the rest of the image unchanged), merging the appearance of objects in one image with the layout of another, composing objects from multiple images into one, and more. We also propose a new method for reconstruction using self-guidance, which allows extending our approach to editing real images.</div>
</div>
<div class='paper-counter'>393/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Cross-Domain Policy Adaptation via Value-Guided Data Filtering</div>
<div class="field-name">authors:</div>
<div class="field-value">Kang Xu;Chenjia Bai;Xiaoteng Ma;Dong Wang;Bin Zhao;Zhen Wang;Xuelong Li;Wei Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning; Domain Adaptation; Online Dynamics Adaptation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/303c20708abc9f07c5e15e47319db117dc92f242.pdf" target="_blank">https://openreview.net/pdf/303c20708abc9f07c5e15e47319db117dc92f242.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/25a478efb95f4bfea6610841d40fd6b7cfd670af.zip" target="_blank">https://openreview.net/attachment/25a478efb95f4bfea6610841d40fd6b7cfd670af.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalizing policies across different domains with dynamics mismatch poses a significant challenge in reinforcement learning. For example, a robot learns the policy in a simulator, but when it is deployed in the real world, the dynamics of the environment may be different. Given the source and target domain with dynamics mismatch, we consider the online dynamics adaptation problem, in which case the agent can access sufficient source domain data while online interactions with the target domain are limited. Existing research has attempted to solve the problem from the dynamics discrepancy perspective. In this work, we reveal the limitations of these methods and explore the problem from the value difference perspective via a novel insight on the value consistency across domains. Specifically, we present the Value-Guided Data Filtering (VGDF) algorithm, which selectively shares transitions from the source domain based on the proximity of paired value targets across the two domains. Empirical results on various environments with kinematic and morphology shifts demonstrate that our method achieves superior performance compared to prior approaches.</div>
</div>
<div class='paper-counter'>394/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Parsel🐍: Algorithmic Reasoning with Language Models by Composing Decompositions</div>
<div class="field-name">authors:</div>
<div class="field-value">Eric Zelikman;Qian Huang;Gabriel Poesia;Noah Goodman;Nick Haber</div>
<div class="field-name">keywords:</div>
<div class="field-value">reasoning;language models;code synthesis;decomposition</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/71fdc840d8db15ba5f93d82efbe8c173c6bfefac.pdf" target="_blank">https://openreview.net/pdf/71fdc840d8db15ba5f93d82efbe8c173c6bfefac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/64363b03300c24ccfa1bc3607fd44766b3cbba71.pdf" target="_blank">https://openreview.net/attachment/64363b03300c24ccfa1bc3607fd44766b3cbba71.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Language models can solve algorithmic reasoning tasks by decomposing them, solving subparts, and composing them.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel.</div>
</div>
<div class='paper-counter'>395/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas</div>
<div class="field-name">authors:</div>
<div class="field-value">Kensen Shi;Hanjun Dai;Wen-Ding Li;Kevin Ellis;Charles Sutton</div>
<div class="field-name">keywords:</div>
<div class="field-value">Program Synthesis;Programming By Example;Lambdas;Functional Programming</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a159a127f75c894a613753f4a4ffd7606a85a126.pdf" target="_blank">https://openreview.net/pdf/a159a127f75c894a613753f4a4ffd7606a85a126.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We design, train, and evaluate a neural program synthesizer that can construct arbitrary lambda functions for use within higher-order functions to perform looping computations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-based techniques in an integer list manipulation domain.</div>
</div>
<div class='paper-counter'>396/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints</div>
<div class="field-name">authors:</div>
<div class="field-value">Dohyeong Kim;Kyungjae Lee;Songhwai Oh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;Safety;Multiple Constraints;Distributional Critic</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b139d81382775ba6142cf8d616aea7535590a28c.pdf" target="_blank">https://openreview.net/pdf/b139d81382775ba6142cf8d616aea7535590a28c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c29d388294366221d21f58152127e70d0d2f5249.zip" target="_blank">https://openreview.net/attachment/c29d388294366221d21f58152127e70d0d2f5249.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a safe reinforcement learning method based on the trust region method and distributional critics for multiple constraints.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance.Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints.To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a safe distributional actor-critic (SDAC).Our main contributions are as follows: 1) introducing a gradient integration method to manage infeasibility issues in multi-constrained problems, ensuring theoretical convergence, and 2) developing a TD($\lambda$) target distribution to estimate risk-averse constraints with low biases. We evaluate SDAC through extensive experiments involving multi- and single-constrained robotic tasks.While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in multi-constrained tasks and 1.78 times fewer constraint violations in single-constrained tasks compared to safe RL baselines.Code is available at: https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.</div>
</div>
<div class='paper-counter'>397/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Generalizable Agents via Saliency-guided Features Decorrelation</div>
<div class="field-name">authors:</div>
<div class="field-value">Sili Huang;Yanchao Sun;Jifeng Hu;Siyuan Guo;Hechang Chen;Yi Chang;Lichao Sun;Bo Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/59d46abd0dcdde36b337831f9b2eae764f27406b.pdf" target="_blank">https://openreview.net/pdf/59d46abd0dcdde36b337831f9b2eae764f27406b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1cacb6a30beede257612bf61c67cadd9b97dfa03.pdf" target="_blank">https://openreview.net/attachment/1cacb6a30beede257612bf61c67cadd9b97dfa03.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. The variations can arise in both task-irrelevant features, such as background noise, and task-relevant features, such as robot configurations, that are related to the optimal decisions. To achieve generalization in both situations, agents are required to accurately understand the impact of changed features on the decisions, i.e., establishing the true associations between changed features and decisions in the policy model. However, due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations through sample reweighting. Concretely, SGFD consists of two core techniques: Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex non-linear correlations in high-dimensional images, while the saliency map is designed to identify the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to minimize the estimated correlations related to changed features, thereby achieving decorrelation in visual RL tasks. Our experimental results demonstrate that SGFD can generalize well on a wide range of test environments and significantly outperforms state-of-the-art methods in handling both task-irrelevant variations and task-relevant variations.</div>
</div>
<div class='paper-counter'>398/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Enhancing Robot Program Synthesis Through Environmental Context</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianyi Chen;Qidi Wang;Zhen Dong;Liwei Shen;Xin Peng</div>
<div class="field-name">keywords:</div>
<div class="field-value">program synthesis;partial envrionment;robotic programming;domain-specific language</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eb9f9758fbf8e512eb46a939bd6dee1dd8c008d3.pdf" target="_blank">https://openreview.net/pdf/eb9f9758fbf8e512eb46a939bd6dee1dd8c008d3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8cfd1bcffd8a7ea0e4f59488e5a9370768b3227c.pdf" target="_blank">https://openreview.net/attachment/8cfd1bcffd8a7ea0e4f59488e5a9370768b3227c.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a framework that learns to synthesize a robot program by rectifying potentially erroneous code segments, with the aid of partially observed environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Program synthesis aims to automatically generate an executable program that conforms to the given specification. Recent advancements have demonstrated that deep neural methodologies and large-scale pretrained language models are highly proficient in capturing program semantics.For robot programming, prior works have facilitated program synthesis by incorporating global environments. However, the assumption of acquiring a comprehensive understanding of the entire environment is often excessively challenging to achieve.In this work, we present a framework that learns to synthesize a program by rectifying potentially erroneous code segments, with the aid of partially observed environments. To tackle the issue of inadequate attention to partial observations, we propose to first learn an environment embedding space that can implicitly evaluate the impacts of each program token based on the precondition. Furthermore, by employing a graph structure, the model can aggregate both environmental and syntactic information flow and furnish smooth program rectification guidance.Extensive experimental evaluations and ablation studies on the partially observed VizDoom domain authenticate that our method offers superior generalization capability across various tasks and greater robustness when encountering noises.</div>
</div>
<div class='paper-counter'>399/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Optimal Exploration for Model-Based RL in Nonlinear Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Andrew Wagenmaker;Guanya Shi;Kevin Jamieson</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;control theory;system identification;experiment design;active learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a450180468afbf4ec1f7a3b952e175adc1d41a8e.pdf" target="_blank">https://openreview.net/pdf/a450180468afbf4ec1f7a3b952e175adc1d41a8e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop an approach which optimally explores unknown nonlinear dynamical systems with the goal of learning a good controller.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system (Mania et al., 2020), in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters.In this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems (Wagenmaker et al., 2021), we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems.</div>
</div>
<div class='paper-counter'>400/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinyu Sun;Peihao Chen;Jugang Fan;Jian Chen;Thomas H. Li;Mingkui Tan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Navigation;Image-Goal Navigation;Embodied AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/591a8caa8ca2d94aaa792d688d62c03e9b7da397.pdf" target="_blank">https://openreview.net/pdf/591a8caa8ca2d94aaa792d688d62c03e9b7da397.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bcd7e29891682dc70e985e8c28853e7632133ca2.pdf" target="_blank">https://openreview.net/attachment/bcd7e29891682dc70e985e8c28853e7632133ca2.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We proposed a fine-grained goal prompting method for image-goal navigation. It significantly outperforms baselines, surpassing SOTA by 8% in success rate with 1/50 model size.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems like household robots. The agent is required to well understand and reason the location of the navigation goal from a picture shot in the goal position. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (\sexyname) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (\textit{i.e.,} Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8\% with only 1/50 model size.</div>
</div>
<div class='paper-counter'>401/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Arbitrarily Scalable Environment Generators via Neural Cellular Automata</div>
<div class="field-name">authors:</div>
<div class="field-value">Yulun Zhang;Matthew Christopher Fontaine;Varun Bhatt;Stefanos Nikolaidis;Jiaoyang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-robot systems;quality diversity;automatic environment generation;neural cellular automata</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/16ea6add522a9c2f52a5a785d7995d5f312bd506.pdf" target="_blank">https://openreview.net/pdf/16ea6add522a9c2f52a5a785d7995d5f312bd506.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/268353c448088f825fb91cbb077070d9965f2567.zip" target="_blank">https://openreview.net/attachment/268353c448088f825fb91cbb077070d9965f2567.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We study the problem of generating arbitrarily large environments to improve the throughput of multi-robot systems. Prior work proposes Quality Diversity (QD) algorithms as an effective method for optimizing the environments of automated warehouses. However, these approaches optimize only relatively small environments, falling short when it comes to replicating real-world warehouse sizes. The challenge arises from the exponential increase in the search space as the environment size increases. Additionally, the previous methods have only been tested with up to 350 robots in simulations, while practical warehouses could host thousands of robots. In this paper, instead of optimizing environments, we propose to optimize Neural Cellular Automata (NCA) environment generators via QD algorithms. We train a collection of NCA generators with QD algorithms in small environments and then generate arbitrarily large environments from the generators at test time. We show that NCA environment generators maintain consistent, regularized patterns regardless of environment size, significantly enhancing the scalability of multi-robot systems in two different domains with up to 2,350 robots. Additionally, we demonstrate that our method scales a single-agent reinforcement learning policy to arbitrarily large environments with similar patterns. We include the source code at https://github.com/lunjohnzhang/warehouse_env_gen_nca_public.</div>
</div>
<div class='paper-counter'>402/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generating Behaviorally Diverse Policies with Latent Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Shashank Hegde;Sumeet Batra;K.R. Zentner;Gaurav S. Sukhatme</div>
<div class="field-name">keywords:</div>
<div class="field-value">Latent Diffusion;Quality Diversity;Reinforcement Learning;Graph Neural Networks</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/08a198942d41d981c92302387aca88bf6945dbe9.pdf" target="_blank">https://openreview.net/pdf/08a198942d41d981c92302387aca88bf6945dbe9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A latent diffusion model that generates a behaviorally diverse set of neural network policies for robotic control</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of theoriginal collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original humanoid archive coverage. Further, the conditioning mechanism of diffusion models allowsfor flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home.</div>
</div>
<div class='paper-counter'>403/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</div>
<div class="field-name">authors:</div>
<div class="field-value">Anagh Malik;Parsa Mirdehghan;Sotiris Nousias;Kyros Kutulakos;David B. Lindell</div>
<div class="field-name">keywords:</div>
<div class="field-value">neural radiance fields;3D reconstruction;single-photon lidar;computational imaging</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a52815f4d6b3d74bba72be4a5454c4ecaa767cff.pdf" target="_blank">https://openreview.net/pdf/a52815f4d6b3d74bba72be4a5454c4ecaa767cff.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5ef701db8fa940b8c206172914a0f3cbfe723e97.pdf" target="_blank">https://openreview.net/attachment/5ef701db8fa940b8c206172914a0f3cbfe723e97.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a NeRF-based approach for view synthesis of transient, picosecond-scale measurements captured by a lidar.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and geometry from multiview imagery. Recent work has also begun to explore how to use additional supervision from lidar or depth sensor measurements in the NeRF framework. However, previous lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek to render such histograms from novel views. Different from conventional NeRFs, the approach relies on a time-resolved version of the volume rendering equation to render the lidar measurements and capture transient light transport phenomena at picosecond timescales. We evaluate our method on a first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show that our approach recovers improved geometry and conventional appearance compared to point cloud-based supervision when training on few input viewpoints. Transient NeRFs may be especially useful for applications which seek to simulate raw lidar measurements for downstream tasks in autonomous driving, robotics, and remote sensing.</div>
</div>
<div class='paper-counter'>404/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment</div>
<div class="field-name">authors:</div>
<div class="field-value">Hao Liu;Wilson Yan;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Model;VQVAE;Vector Quantization;Multimodal</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/40e87f105717c5bb581dfdc6af44627a7ec5da31.pdf" target="_blank">https://openreview.net/pdf/40e87f105717c5bb581dfdc6af44627a7ec5da31.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8cf6c82a0c1e1f2713f8449b71ff74e986bbe1d5.pdf" target="_blank">https://openreview.net/attachment/8cf6c82a0c1e1f2713f8449b71ff74e986bbe1d5.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">LQAE aligns text and images without paired data, achieving superior performance in few-shot learning tasks such as image classification and VQA.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of natural language tasks. However, a key limitation is that these language models fundamentally lack grounding to visual perception - a crucial attribute needed to extend to real world tasks such as in visual-question answering and robotics. While prior works have largely connected image to text through pretraining or fine-tuning, learning such alignments are generally costly due to a combination of curating massive datasets and large computational burdens. In order to resolve these limitations, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language model denoisers (e.g., BERT). Our main idea is to encode images as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then feed a masked version of the quantized embeddings into a BERT to reconstruct the original input. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. We show LQAE learns text-aligned image tokens that enable few-shot multi-modal learning with large language models, outperforming baseline methods in tasks such as image classification and VQA while requiring as few as 1-10 image-text pairs.</div>
</div>
<div class='paper-counter'>405/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Neural Lighting Simulation for Urban Scenes</div>
<div class="field-name">authors:</div>
<div class="field-value">Ava Pun;Gary Sun;Jingkang Wang;Yun Chen;Ze Yang;Sivabalan Manivasagam;Wei-Chiu Ma;Raquel Urtasun</div>
<div class="field-name">keywords:</div>
<div class="field-value">Scene Relighting;Lighting Estimation;Camera Simulation;Self-Driving;Lighting Simulation;Scene Editing</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3827ed968be833ad549d8670b8c3901f2e972433.pdf" target="_blank">https://openreview.net/pdf/3827ed968be833ad549d8670b8c3901f2e972433.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/cdc1bf42af62266ab947bf363959ca9a00546730.zip" target="_blank">https://openreview.net/attachment/cdc1bf42af62266ab947bf363959ca9a00546730.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a neural lighting simulation approach to simulate realistic videos under different lighting conditions</div>
<div class="field-name">abstract:</div>
<div class="field-value">Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results  than prior work. Importantly,  training perception models on data generated by LightSim can significantly improve their performance. Our project page is available at https://waabi.ai/lightsim/.</div>
</div>
<div class='paper-counter'>406/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yanjie Ze;Yuyao Liu;Ruizhe Shi;Jiaxin Qin;Zhecheng Yuan;Jiashun Wang;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Reinforcement Learning;Representation Learning;Dexterous Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f85de9f722c2be5f4dbe8ddbaa94d04d47716d14.pdf" target="_blank">https://openreview.net/pdf/f85de9f722c2be5f4dbe8ddbaa94d04d47716d14.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f65cd06609629b8e30695dd3aad5a9bf92ec6405.pdf" target="_blank">https://openreview.net/attachment/f65cd06609629b8e30695dd3aad5a9bf92ec6405.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose H-InDex, a hand-informed visual representation for dexterous manipulation with reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human $\textbf{H}$and-$\textbf{In}$formed visual representation learning framework to solve difficult $\textbf{Dex}$terous manipulation tasks ($\textbf{H-InDex}$) with reinforcement learning. Our framework consists of three stages: $\textit{(i)}$ pre-training representations with 3D human hand pose estimation, $\textit{(ii)}$ offline adapting representations with self-supervised keypoint detection, and $\textit{(iii)}$ reinforcement learning with exponential moving average BatchNorm. The last two stages only modify $0.36$% parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study $\textbf{12}$ challenging dexterous manipulation tasks and find that $\textbf{H-InDex}$ largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code and videos are available at https://yanjieze.com/H-InDex .</div>
</div>
<div class='paper-counter'>407/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation</div>
<div class="field-name">authors:</div>
<div class="field-value">David Brandfonbrener;Ofir Nachum;Joan Bruna</div>
<div class="field-name">keywords:</div>
<div class="field-value">representation learning;imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d0347c6116fe48f2348808f72b369da76357ede8.pdf" target="_blank">https://openreview.net/pdf/d0347c6116fe48f2348808f72b369da76357ede8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ad18fb879bf02b129b5453faadf9e4beccdd5eb7.zip" target="_blank">https://openreview.net/attachment/ad18fb879bf02b129b5453faadf9e4beccdd5eb7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling -- i.e.,  predicting an action given the observations appearing before and after it in the demonstration -- is well-suited to this setting. We provide empirical evidence of this claim through evaluations on a variety of simulated visuomotor manipulation problems. While previous work has attempted various theoretical explanations regarding the benefit of inverse dynamics modeling, we find that these arguments are insufficient to explain the empirical advantages often observed in our settings, and so we derive a novel analysis using a simple but general environment model.</div>
</div>
<div class='paper-counter'>408/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Abide by the law and follow the flow: conservation laws for gradient flows</div>
<div class="field-name">authors:</div>
<div class="field-value">Sibylle Marcotte;Rémi Gribonval;Gabriel Peyré</div>
<div class="field-name">keywords:</div>
<div class="field-value">Implicit bias;conservation laws;gradient flow;linear neural network;matrix factorization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7b766b2698788a69d39692e5b346421ae2b4c83e.pdf" target="_blank">https://openreview.net/pdf/7b766b2698788a69d39692e5b346421ae2b4c83e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bce115bec8e52f5168e05eb8ae1397c1d50bbcb9.pdf" target="_blank">https://openreview.net/attachment/bce115bec8e52f5168e05eb8ae1397c1d50bbcb9.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", that define quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the maximal number of independent conservation lawsby performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms to: a) compute a family of polynomial laws; b) compute the maximal number of (not necessarily polynomial) independent conservation laws. We provide showcase examples that we fully work out theoretically. Besides, applying the two algorithms confirms for a number of ReLU network architectures that all known laws are recovered by the algorithm, and that there are no other independent laws. Such computational tools pave the way to understanding desirable properties of optimization initialization in large machine learning models.</div>
</div>
<div class='paper-counter'>409/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?</div>
<div class="field-name">authors:</div>
<div class="field-value">Jialu Gao;Kaizhe Hu;Guowei Xu;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Reinforcement Learning;Large Generative Models;Image Editing;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d9fc561288af1fedc900eac51112c3bd68ae6a6c.pdf" target="_blank">https://openreview.net/pdf/d9fc561288af1fedc900eac51112c3bd68ae6a6c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/39b3ddc8b00e238754edff0009bc8b3a44269652.pdf" target="_blank">https://openreview.net/attachment/39b3ddc8b00e238754edff0009bc8b3a44269652.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as "wiping" a stain off a table. Subsequently, LfVoid trains an ensembled goal discriminator on the generated image to provide reward signals for a reinforcement learning agent, guiding it to achieve the goal. The ability of LfVoid to learn with zero in-domain training on expert demonstrations or true goal observations (the void) is attributed to the utilization of knowledge from web-scale generative models. We evaluate LfVoid across three simulated tasks and validate its feasibility in the corresponding real-world scenarios. In addition, we offer insights into the key considerations for the effective integration of visual generative models into robot learning workflows. We posit that our work represents an initial step towards the broader application of pre-trained visual generative models in the robotics field. Our project page: https://lfvoid-rl.github.io/.</div>
</div>
<div class='paper-counter'>410/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Metropolis Sampling for Constrained Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Nic Fishman;Leo Klarner;Emile Mathieu;Michael John Hutchinson;Valentin De Bortoli</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion model;generative modelling;manifold;constraints;proteins;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/53ec438372798bf2a7dfb94164dca01f7ee9fa9c.pdf" target="_blank">https://openreview.net/pdf/53ec438372798bf2a7dfb94164dca01f7ee9fa9c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a tractable extension of Riemannian diffusion models to arbitrarily constrained manifolds.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling on image domains. In addition, their extension to Riemannian manifolds has facilitated a range of applications across the natural sciences. While many of these problems stand to benefit from the ability to specify arbitrary, domain-informed constraints, this setting is not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by constructing novel noising processes based on the reflected Brownian motion and logarithmic barrier methods. However, the associated samplers are either computationally burdensome or only apply to convex subsets of Euclidean space. In this paper, we introduce an alternative, simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We demonstrate the scalability and flexibility of our approach on a range of problem settings with convex and non-convex constraints, including applications from geospatial modelling, robotics and protein design.</div>
</div>
<div class='paper-counter'>411/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Adaptive Online Replanning with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Siyuan Zhou;Yilun Du;Shun Zhang;Mengdi Xu;Yikang Shen;Wei Xiao;Dit-Yan Yeung;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Decision making;Robotics;Planning-based</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d071ca735af8d27368640c939173aab5aa7c5448.pdf" target="_blank">https://openreview.net/pdf/d071ca735af8d27368640c939173aab5aa7c5448.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/64d4092143e24a116339eb9095b15fc8b2df2a73.pdf" target="_blank">https://openreview.net/attachment/64d4092143e24a116339eb9095b15fc8b2df2a73.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a new data-driven replanning approach based on diffusion models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models have risen a promising approach to data-driven planning, and have demonstrated impressive robotic control, reinforcement learning, and video planning performance. Given an effective planner, an important question to consider is replanning -- when given plans should be regenerated due to both action execution error and external environment changes.  Direct plan execution, without replanning, is problematic as errors from individual actions rapidly accumulate and environments are partially observable and stochastic. Simultaneously, replanning at each timestep incurs a substantial computational cost, and may prevent successful task execution, as different generated plans prevent consistent progress to any particular goal. In this paper, we explore how we may effectively replan with diffusion models. We propose a principled approach to determine when to replan, based on the diffusion model's estimated likelihood of existing generated plans. We further present an approach to replan existing trajectories to ensure that new plans follow the same goal state as the original trajectory, which may efficiently bootstrap off previously generated plans.  We illustrate how a combination of our proposed additions significantly improves the performance of diffusion planners leading to 38\% gains over past diffusion planning approaches on Maze2D and further enables handling of stochastic and long-horizon robotic control tasks.</div>
</div>
<div class='paper-counter'>412/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Active Vision Reinforcement Learning under Limited Visual Observability</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinghuan Shang;Michael S Ryoo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Active Reinforcement Learning;Visual Reinforcement Learning;Active Vision;Active Perception;Partial Observability;Sensorimotor</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/53573f728362a797bfc5cd2e7015edb927da3575.pdf" target="_blank">https://openreview.net/pdf/53573f728362a797bfc5cd2e7015edb927da3575.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/51cc3f7add8ecb543174e09cb9f1634e6403ac0f.pdf" target="_blank">https://openreview.net/attachment/51cc3f7add8ecb543174e09cb9f1634e6403ac0f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Learning how to choose useful visual observations in Active-RL task by sensorimotor reward.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we investigate Active Vision Reinforcement Learning (ActiveVision-RL), where an embodied agent simultaneously learns action policy for the task while also controlling its visual observations in partially observable environments. We denote the former as motor policy and the latter as sensory policy. For example, humans solve real world tasks by hand manipulation (motor policy) together with eye movements (sensory policy). ActiveVision-RL poses challenges on coordinating two policies given their mutual influence. We propose SUGARL, Sensorimotor Understanding Guided Active Reinforcement Learning, a framework that models motor and sensory policies separately, but jointly learns them using with an intrinsic sensorimotor reward. This learnable reward is assigned by sensorimotor reward module, incentivizes the sensory policy to select observations that are optimal to infer its own motor action, inspired by the sensorimotor stage of humans. Through a series of experiments, we show the effectiveness of our method across a range of observability conditions and its adaptability to existed RL algorithms. The sensory policies learned through our method are observed to exhibit effective active vision strategies.</div>
</div>
<div class='paper-counter'>413/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Iteratively Learn Diverse Strategies with State Distance Information</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Fu;Weihua Du;Jingwei Li;Sunli Chen;Jingzhao Zhang;Yi Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diverse behavior;multi-agent reinforcement learning;deep reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/49c7a5ec8776c177fb9e1d08cee27df6ecdeb9d9.pdf" target="_blank">https://openreview.net/pdf/49c7a5ec8776c177fb9e1d08cee27df6ecdeb9d9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/36d79ab2a5dc3b845c405e021b10174d329693cb.zip" target="_blank">https://openreview.net/attachment/36d79ab2a5dc3b845c405e021b10174d329693cb.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Based on the analysis of existing approaches, we develop an iterative RL algorithm for discovering diverse high-reward strategies with provable convergence properties.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In complex reinforcement learning (RL) problems, policies with similar rewards may have substantially different behaviors. It remains a fundamental challenge to optimize rewards while also discovering as many *diverse* strategies as possible, which can be crucial in many practical applications. Our study examines two design choices for tackling this challenge, i.e., *diversity measure* and *computation framework*. First, we find that with existing diversity measures, visually indistinguishable policies can still yield high diversity scores. To accurately capture the behavioral difference, we propose to incorporate the state-space distance information into the diversity measure. In addition, we examine two common computation frameworks for this problem, i.e., population-based training (PBT) and iterative learning (ITR). We show that although PBT is the precise problem formulation, ITR can achieve comparable diversity scores with higher computation efficiency, leading to improved solution quality in practice. Based on our analysis, we further combine ITR with two tractable realizations of the state-distance-based diversity measures and develop a novel diversity-driven RL algorithm, *State-based Intrinsic-reward Policy Optimization* (SIPO), with provable convergence properties. We empirically examine SIPO across three domains from robot locomotion to multi-agent games. In all of our testing environments, SIPO consistently produces strategically diverse and human-interpretable policies that cannot be discovered by existing baselines.</div>
</div>
<div class='paper-counter'>414/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Inverse Preference Learning: Preference-based RL without a Reward Function</div>
<div class="field-name">authors:</div>
<div class="field-value">Joey Hejna;Dorsa Sadigh</div>
<div class="field-name">keywords:</div>
<div class="field-value">preference learning;preference-based reinforcement learning;human-in-the-loop reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/44ce5e80b1ce6b3e7e9153d4c55d3869ca704fed.pdf" target="_blank">https://openreview.net/pdf/44ce5e80b1ce6b3e7e9153d4c55d3869ca704fed.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9c22ecdf439692006c31ae5497ebfd93c2db468e.zip" target="_blank">https://openreview.net/attachment/9c22ecdf439692006c31ae5497ebfd93c2db468e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We design an offline preference-based RL algorithm that does not require learning a reward function.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na\"ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.</div>
</div>
<div class='paper-counter'>415/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping</div>
<div class="field-name">authors:</div>
<div class="field-value">Tianhao Wu;Mingdong Wu;Jiyao Zhang;Yunchong Gan;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human-asissting Dexterous Grasping;Score-matching;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ed024a4118d603505c1830547a06339e4473091f.pdf" target="_blank">https://openreview.net/pdf/ed024a4118d603505c1830547a06339e4473091f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/fa9fc84e8eb27b83711546be66a67c27b896a4be.zip" target="_blank">https://openreview.net/attachment/fa9fc84e8eb27b83711546be66a67c27b896a4be.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel human-asissting dexterous grasping task and propose a RL framework incorporated with score-matching.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry.  We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field (GraspGF), and a history-conditional residual policy.  GraspGF learns 'how' to grasp by estimating the gradient of a synthesised success grasping example set, while the residual policy determines 'when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demonstrate the superiority of our proposed method compared to baselines, highlighting the user-awareness and practicality in real-world applications. The codes and demonstrations can be viewed at https://sites.google.com/view/graspgf.</div>
</div>
<div class='paper-counter'>416/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought</div>
<div class="field-name">authors:</div>
<div class="field-value">Huaxiaoyue Wang;Gonzalo Gonzalez-Pumariega;Yash Sharma;Sanjiban Choudhury</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Model;Code Generation;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d65bfb8fa76b313f9c5758f2135a74a50f59c18d.pdf" target="_blank">https://openreview.net/pdf/d65bfb8fa76b313f9c5758f2135a74a50f59c18d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment.</div>
</div>
<div class='paper-counter'>417/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Reduced Policy Optimization for Continuous Control with Hard Constraints</div>
<div class="field-name">authors:</div>
<div class="field-value">Shutong Ding;Jingya Wang;Yali Du;Ye Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Hard Constraint;Generalized Reduced Gradient</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/438d66ece286c70eab4457c2ba7cc56e3804504c.pdf" target="_blank">https://openreview.net/pdf/438d66ece286c70eab4457c2ba7cc56e3804504c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/09525341ad9239b4326a81b68cfc3f96ef6e31a8.zip" target="_blank">https://openreview.net/attachment/09525341ad9239b4326a81b68cfc3f96ef6e31a8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a reduced policy optimization algorithm that combines RL with generalized reduced gradient for continuos control tasks with hard constraints, and develop three new benchmarks for RL with hard constraints.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.</div>
</div>
<div class='paper-counter'>418/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Norm-guided latent space exploration for text-to-image generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Dvir Samuel;Rami Ben-Ari;Nir Darshan;Haggai Maron;Gal Chechik</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion models;few-shot learning;long-tail learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/903a44cbe7f7ce473db033995fc35f98ae24cdc3.pdf" target="_blank">https://openreview.net/pdf/903a44cbe7f7ce473db033995fc35f98ae24cdc3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1d5ce6f64506341cf4378550b3a78313980e6ff5.pdf" target="_blank">https://openreview.net/attachment/1d5ce6f64506341cf4378550b3a78313980e6ff5.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-image diffusion models show great potential in synthesizing a large variety of concepts in new compositions and scenarios. However, the latent space of initial seeds is still not well understood and its structure was shown to impact the generation of various concepts. Specifically, simple operations like interpolation and finding the centroid of a set of seeds perform poorly when using standard Euclidean or spherical metrics in the latent space. This paper makes the observation that, in current training procedures, diffusion models observed inputs with a narrow range of norm values. This has strong implications for methods that rely on seed manipulation for image generation, with applications to few-shot and long-tail learning tasks. To address this issue, we propose a novel method for interpolating between two seeds and demonstrate that it defines a new non-Euclidean metric that takes into account a norm-based prior on seeds. We describe a simple yet efficient algorithm for approximating this interpolation procedure and use it to further define centroids in the latent seed space. We show that our new interpolation and centroid techniques significantly enhance the generation of rare concept images. This further leads to state-of-the-art performance on few-shot and long-tail benchmarks, improving prior approaches in terms of generation speed, image quality, and semantic content.</div>
</div>
<div class='paper-counter'>419/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Iterative Reachability Estimation for Safe Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Milan Ganai;Zheng Gong;Chenning Yu;Sylvia Lee Herbert;Sicun Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Constraints;Safety;Hamilton Jacobi Reachability;Deep Reinforcement Learning;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc357aec3d276b912db77bac823899298ab3fd72.pdf" target="_blank">https://openreview.net/pdf/dc357aec3d276b912db77bac823899298ab3fd72.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4aca5113f63ae76f5201818641898e3fc3d03f31.zip" target="_blank">https://openreview.net/attachment/4aca5113f63ae76f5201818641898e3fc3d03f31.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel Reachability Estimation for Safe Policy Optimization algorithm for stochastic safety-constrained reinforcement learning with safety guarantees and optimal convergence analysis.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Ensuring safety is important for the practical deployment of reinforcement learning (RL). Various challenges must be addressed, such as handling stochasticity in the environments, providing rigorous guarantees of persistent state-wise safety satisfaction, and avoiding overly conservative behaviors that sacrifice performance. We propose a new framework, Reachability Estimation for Safe Policy Optimization (RESPO), for safety-constrained RL in general stochastic settings. In the feasible set where there exist violation-free policies, we optimize for rewards while maintaining persistent safety. Outside this feasible set, our optimization produces the safest behavior by guaranteeing entrance into the feasible set whenever possible with the least cumulative discounted violations. We introduce a class of algorithms using our novel reachability estimation function to optimize in our proposed framework and in similar frameworks such as those concurrently handling multiple hard and soft constraints. We theoretically establish that our algorithms almost surely converge to locally optimal policies of our safe optimization framework. We evaluate the proposed methods on a diverse suite of safe RL environments from Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both reward performance and safety compared with state-of-the-art baselines.</div>
</div>
<div class='paper-counter'>420/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhiao Huang;Feng Chen;Yewen Pu;Chunru Lin;Hao Su;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Differentiable physics; Soft body manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8e78ca917c3abb2a676e0846aee3aae00a1f757b.pdf" target="_blank">https://openreview.net/pdf/8e78ca917c3abb2a676e0846aee3aae00a1f757b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b976352ee8d3fc59920e0f9fdfd79d891a655840.pdf" target="_blank">https://openreview.net/attachment/b976352ee8d3fc59920e0f9fdfd79d891a655840.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce DiffVL, a method that enables non-expert users to communicate soft-body manipulation tasks -- a combination of vision and natural language, given in multiple stages -- that can be readily leveraged by a differential physics solver.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Combining gradient-based trajectory optimization with differentiable physics simulation is an efficient technique for solving soft-body manipulation problems.Using a well-crafted optimization objective, the solver can quickly converge onto a valid trajectory.However, writing the appropriate objective functions requires expert knowledge, making it difficult to collect a large set of naturalistic problems from non-expert users.We introduce DiffVL, a method that enables non-expert users to communicate soft-body manipulation tasks -- a combination of vision and natural language, given in multiple stages -- that can be readily leveraged by a differential physics solver. We have developed GUI tools that enable non-expert users to specify 100 tasks inspired by real-life soft-body manipulations from online videos, which we'll make public.We leverage large language models to translate task descriptions into machine-interpretable optimization objectives. The optimization objectives can help differentiable physics solvers to solve these long-horizon multistage tasks that are challenging for previous baselines.</div>
</div>
<div class='paper-counter'>421/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">XAGen: 3D Expressive Human Avatars Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhongcong Xu;Jianfeng Zhang;Jun Hao Liew;Jiashi Feng;Mike Zheng Shou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human Avatar;3D-aware GAN</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0df6a8c42379f1d067dde6028e3392d8608e78cb.pdf" target="_blank">https://openreview.net/pdf/0df6a8c42379f1d067dde6028e3392d8608e78cb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d6f14900489efbc661fa3a614d6bec3b0b98d9a4.pdf" target="_blank">https://openreview.net/attachment/d6f14900489efbc661fa3a614d6bec3b0b98d9a4.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in 3D-aware GAN models have enabled the generation of realistic and controllable human body images. However, existing methods focus on the control of major body joints, neglecting the manipulation of expressive attributes, such as facial expressions, jaw poses, hand poses, and so on. In this work, we present XAGen, the first 3D generative model for human avatars capable of expressive control over body, face, and hands. To enhance the fidelity of small-scale regions like face and hands, we devise a multi-scale and multi-part 3D representation that models fine details. Based on this representation, we propose a multi-part rendering technique that disentangles the synthesis of body, face, and hands to ease model training and enhance geometric quality. Furthermore, we design multi-part discriminators that evaluate the quality of the generated avatars with respect to their appearance and fine-grained control capabilities. Experiments show that XAGen surpasses state-of-the-art methods in terms of realism, diversity, and expressive control abilities. Code and data will be made available at https://showlab.github.io/xagen.</div>
</div>
<div class='paper-counter'>422/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Compositional Foundation Models for Hierarchical Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Anurag Ajay;Seungwook Han;Yilun Du;Shuang Li;Abhi Gupta;Tommi S. Jaakkola;Joshua B. Tenenbaum;Leslie Pack Kaelbling;Akash Srivastava;Pulkit Agrawal</div>
<div class="field-name">keywords:</div>
<div class="field-value">Foundation Models;Composition;Hierarchical Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f2234f57462acdebeab0e9f191b319f27f790cfd.pdf" target="_blank">https://openreview.net/pdf/f2234f57462acdebeab0e9f191b319f27f790cfd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/434d3d852f62b8955e6e807d768955372b0e178b.zip" target="_blank">https://openreview.net/attachment/434d3d852f62b8955e6e807d768955372b0e178b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A foundation model which leverages multiple expert foundation model, trained individually on language, vision and action data, jointly together to solve long-horizon tasks</div>
<div class="field-name">abstract:</div>
<div class="field-value">To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.</div>
</div>
<div class='paper-counter'>423/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiao Zang;Miao Yin;Jinqi Xiao;Saman Zonouz;Bo Yuan</div>
<div class="field-name">keywords:</div>
<div class="field-value">graph neural network;deep learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c1eb8844a9df3f0dbfc3b40b5201068a129251b1.pdf" target="_blank">https://openreview.net/pdf/c1eb8844a9df3f0dbfc3b40b5201068a129251b1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/94d15a1526259515c0cac4c8dd396f193c813e67.pdf" target="_blank">https://openreview.net/attachment/94d15a1526259515c0cac4c8dd396f193c813e67.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Motion planning, which aims to find a high-quality collision-free path in the configuration space, is a fundamental task in robotic systems. Recently, learning-based motion planners, especially the graph neural network-powered, have shown promising planning performance. However, though the state-of-the-art GNN planner can efficiently extract and learn graph information, its inherent mechanism is not well suited for graph search process, hindering its further performance improvement. To address this challenge and fully unleash the potential of GNN in motion planning, this paper proposes GraphMP, a neural motion planner for both low and high-dimensional planning tasks. With the customized model architecture and training mechanism design, GraphMP can simultaneously perform efficient graph pattern extraction and graph search processing, leading to strong planning performance. Experiments on a variety of environments, ranging from 2D Maze to 14D dual KUKA robotic arm, show that our proposed GraphMP achieves significant improvement on path quality and planning speed over the state-of-the-art learning-based and classical planners; while preserving the competitive success rate.</div>
</div>
<div class='paper-counter'>424/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Parallel Sampling of Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Andy Shih;Suneel Belkhale;Stefano Ermon;Dorsa Sadigh;Nima Anari</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion models;parallel sampling</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/41890d156ae31068d246f67f7af49c07fcd75470.pdf" target="_blank">https://openreview.net/pdf/41890d156ae31068d246f67f7af49c07fcd75470.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/83ef4ef7a2a4710daac304ded2c2ab931aa5b325.zip" target="_blank">https://openreview.net/attachment/83ef4ef7a2a4710daac304ded2c2ab931aa5b325.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We improve sampling speed of diffusion models by 2-4x using parallel computation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.</div>
</div>
<div class='paper-counter'>425/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EDGI: Equivariant Diffusion for Planning with Embodied Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Johann Brehmer;Joey Bose;Pim De Haan;Taco Cohen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Planning;Diffusion models;Equivariance;Equivariant generative models</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1fbba77049c62a9573302e3d761a841d38e952ae.pdf" target="_blank">https://openreview.net/pdf/1fbba77049c62a9573302e3d761a841d38e952ae.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/34e1d19ee1991b5ae99e6f916ab0329dff21ff88.pdf" target="_blank">https://openreview.net/attachment/34e1d19ee1991b5ae99e6f916ab0329dff21ff88.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We embed a new SE(3) × ℤ × Sₙ-equivariant architecture in a diffusion model, solving planning problems for embodied agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group SE(3), the discrete-time translation group ℤ, and the object permutation group Sₙ. EDGI follows the Diffuser framework by Janner et al. (2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new SE(3) × ℤ × Sₙ-equivariant diffusion model that supports multiple representations. We integrate this model in a planning loop, where conditioning and classifier guidance let us softly break the symmetry for specific tasks as needed. On object manipulation and navigation tasks, EDGI is substantially more sample efficient and generalizes better across the symmetry group than non-equivariant models.</div>
</div>
<div class='paper-counter'>426/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Universal Policies via Text-Guided Video Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yilun Du;Sherry Yang;Bo Dai;Hanjun Dai;Ofir Nachum;Joshua B. Tenenbaum;Dale Schuurmans;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">sequential decision making;general-purpose agent;video diffusion</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/33b7c3fe90b1829c51b1a54681e02d57004f37c6.pdf" target="_blank">https://openreview.net/pdf/33b7c3fe90b1829c51b1a54681e02d57004f37c6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0461f7825a836efb5d24f1e7e79cdc544673c109.zip" target="_blank">https://openreview.net/attachment/0461f7825a836efb5d24f1e7e79cdc544673c109.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We cast sequential decision making as a text-conditioned video generation problem and propose UniPi which can generalize to combinatorial and multi-task environments and be able to utilize broad internet-scale text-video datasets.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.</div>
</div>
<div class='paper-counter'>427/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Kissing to Find a Match: Efficient Low-Rank Permutation Representation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hannah Dröge;Zorah Lähner;Yuval Bahat;Onofre Martorell Nadal;Felix Heide;Michael Moeller</div>
<div class="field-name">keywords:</div>
<div class="field-value">low rank;permutation;kissing number;matrix factorization;assigment problem</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/596678081f85be3fb04f7fd587fe900024246553.pdf" target="_blank">https://openreview.net/pdf/596678081f85be3fb04f7fd587fe900024246553.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4b3862b25309d8c80b18dba9b76a1ea2741edeb7.pdf" target="_blank">https://openreview.net/attachment/4b3862b25309d8c80b18dba9b76a1ea2741edeb7.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an efficient representation for permutation matrices that is well-suited for optimization in learning frameworks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Permutation matrices play a key role in matching and assignment problems across the fields, especially in computer vision and robotics. However, memory for explicitly representing permutation matrices grows quadratically with the size of the problem, prohibiting large problem instances. In this work, we propose to tackle the curse of dimensionality of large  permutation matrices by approximating them using low-rank matrix factorization, followed by a nonlinearity. To this end, we rely on the Kissing number theory to infer the minimal rank required for representing a permutation matrix of a given size, which is significantly smaller than the problem size. This leads to a drastic reduction in computation and memory costs, e.g., up to $3$ orders of magnitude less memory for a problem of size $n=20000$, represented using $8.4\times10^5$ elements in two small matrices instead of using a single huge matrix with $4\times 10^8$ elements. The proposed representation allows for accurate representations of large permutation matrices, which in turn enables handling large problems that would have been infeasible otherwise. We demonstrate the applicability and merits of the proposed approach through a series of experiments on a range of problems that involve predicting permutation matrices, from linear and quadratic assignment to shape matching problems.</div>
</div>
<div class='paper-counter'>428/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms</div>
<div class="field-name">authors:</div>
<div class="field-value">Shenao Zhang;Boyi Liu;Zhaoran Wang;Tuo Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Model-Based Reinforcement Learning;Policy Gradient</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bc50cf99d86d2e3ceb7bda5550580056d828d321.pdf" target="_blank">https://openreview.net/pdf/bc50cf99d86d2e3ceb7bda5550580056d828d321.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.</div>
</div>
<div class='paper-counter'>429/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation</div>
<div class="field-name">authors:</div>
<div class="field-value">Wenhao Ding;Laixi Shi;Yuejie Chi;Ding Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;robustness;causality;spurious correlation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a9a33a9a5d5a133b319fccec594e47af36a78d98.pdf" target="_blank">https://openreview.net/pdf/a9a33a9a5d5a133b319fccec594e47af36a78d98.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/33b6d70f1bcb43786325d4d51ddde7f494f88712.pdf" target="_blank">https://openreview.net/attachment/33b6d70f1bcb43786325d4d51ddde7f494f88712.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Robust State-Confounded Markov Decision Processes to make RL agent robust against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and causal structure, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in avoiding learning spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks.</div>
</div>
<div class='paper-counter'>430/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions</div>
<div class="field-name">authors:</div>
<div class="field-value">Chengzhi Cao;Chao Yang;Ruimao Zhang;Shuang Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Logic rule;human actions;sports analyze</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/92b647db51aeef5356cc2619b5a6472827d72670.pdf" target="_blank">https://openreview.net/pdf/92b647db51aeef5356cc2619b5a6472827d72670.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ffd4baebff22cb7dd3c7489b24c33d51413db02f.pdf" target="_blank">https://openreview.net/attachment/ffd4baebff22cb7dd3c7489b24c33d51413db02f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose an interpretable model to uncover the behavioral patterns of human movements by analyzing their trajectories. Our approach is based on the belief that human actions are driven by intentions and are influenced by environmental factors such as spatial relationships with surrounding objects. To model this, we use a set of spatial-temporal logic rules that include intention variables as principles. These rules are automatically discovered and used to capture the dynamics of human actions. To learn the model parameters and rule content, we design an EM learning algorithm that treats the unknown rule content as a latent variable. In the E-step, we evaluate the posterior over the latent rule content, and in the M-step, we optimize the rule generator and model parameters by maximizing the expected log-likelihood. Our model has wide-ranging applications in areas such as sports analytics, robotics, and autonomous cars. We demonstrate the model's superior interpretability and prediction performance on both pedestrian and NBA basketball player datasets, achieving promising results.</div>
</div>
<div class='paper-counter'>431/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Cross-Episodic Curriculum for Transformer Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Lucy Xiaoyang Shi;Yunfan Jiang;Jake Grigsby;Linxi Fan;Yuke Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Transformers;In-context Learning;Reinforcement Learning;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fce810149a981354afad34f9f90063e7674fb0a6.pdf" target="_blank">https://openreview.net/pdf/fce810149a981354afad34f9f90063e7674fb0a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4f1d3e38a3d5a8d7bfdb25dfa72b1419f5918a3e.pdf" target="_blank">https://openreview.net/attachment/4f1d3e38a3d5a8d7bfdb25dfa72b1419f5918a3e.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Cross-Episodic Curriculum enhances Transformer agents’ learning efficiency and generalization by leveraging cross-episodic experiences, proving effective across RL and IL settings.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer’s context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced on the project website https://cec-agent.github.io/ to facilitate research on Transformer agent learning.</div>
</div>
<div class='paper-counter'>432/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Modulate pre-trained Models in RL</div>
<div class="field-name">authors:</div>
<div class="field-value">Thomas Schmied;Markus Hofmarcher;Fabian Paischer;Razvan Pascanu;Sepp Hochreiter</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Transformer;Decision Transformer;Multi-task learning;Continual learning;NLP;Fine-tuning;Prompt Tuning;Parameter efficient Fine-tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2353475a66038ec7650d3f7473fff63c25317f44.pdf" target="_blank">https://openreview.net/pdf/2353475a66038ec7650d3f7473fff63c25317f44.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study shows that with most fine-tuning approaches, the performance on pre-training tasks deteriorates significantly. Therefore, we propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation of learned skills by modulating the information flow of the frozen pre-trained model via a learnable modulation pool. Our method achieves state-of-the-art performance on the Continual-World benchmark, while retaining performance on the pre-training tasks. Finally, to aid future research in this area, we release a dataset encompassing 50 Meta-World and 16 DMControl tasks.</div>
</div>
<div class='paper-counter'>433/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Text Promptable Surgical Instrument Segmentation with Vision-Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zijian Zhou;Oluwatosin Alabi;Meng Wei;Tom Vercauteren;Miaojing Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Surgical Instrument Segmentation;Vision Language Models;Text Promptable Segmentation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4c5281bbd7ffcd723dbcf773ff1ba822333b6cd0.pdf" target="_blank">https://openreview.net/pdf/4c5281bbd7ffcd723dbcf773ff1ba822333b6cd0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d3a3376f1d88a52f281da9646c70b19b19ba5975.pdf" target="_blank">https://openreview.net/attachment/d3a3376f1d88a52f281da9646c70b19b19ba5975.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we propose a novel text promptable surgical instrument segmentation approach to overcome challenges associated with diversity and differentiation of surgical instruments in minimally invasive surgeries. We redefine the task as text promptable, thereby enabling a more nuanced comprehension of surgical instruments and adaptability to new instrument types. Inspired by recent advancements in vision-language models, we leverage pretrained image and text encoders as our model backbone and design a text promptable mask decoder consisting of attention- and convolution-based prompting schemes for surgical instrument segmentation prediction. Our model leverages multiple text prompts for each surgical instrument through a new mixture of prompts mechanism, resulting in enhanced segmentation performance. Additionally, we introduce a hard instrument area reinforcement module to improve image feature comprehension and segmentation precision. Extensive experiments on several surgical instrument segmentation datasets demonstrate our model's superior performance and promising generalization capability. To our knowledge, this is the first implementation of a promptable approach to surgical instrument segmentation, offering significant potential for practical application in the field of robotic-assisted surgery. Code is available at https://github.com/franciszzj/TP-SIS.</div>
</div>
<div class='paper-counter'>434/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MoVie: Visual Model-Based Policy Adaptation for View Generalization</div>
<div class="field-name">authors:</div>
<div class="field-value">Sizhe Yang;Yanjie Ze;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">visual reinforcement learning;visual generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d5a3b594b87cc102304ba554e4cd051e2d94424a.pdf" target="_blank">https://openreview.net/pdf/d5a3b594b87cc102304ba554e4cd051e2d94424a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/41791080de6bb6163b3017631a90977c0be42c6f.pdf" target="_blank">https://openreview.net/attachment/41791080de6bb6163b3017631a90977c0be42c6f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Code and videos are available at https://yangsizhe.github.io/MoVie/.</div>
</div>
<div class='paper-counter'>435/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos</div>
<div class="field-name">authors:</div>
<div class="field-value">Matthew Chang;Aditya Prakash;Saurabh Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">Inpainting;Diffusion;Robot Learning;Egocentric Vision</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dfbd322d9bc20b8b81201fc51ee34ff3b0a1983d.pdf" target="_blank">https://openreview.net/pdf/dfbd322d9bc20b8b81201fc51ee34ff3b0a1983d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b17795477889f2674fbdafb3d2e67e3163669279.zip" target="_blank">https://openreview.net/attachment/b17795477889f2674fbdafb3d2e67e3163669279.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop an inpainting diffusion model to factorize egocentric videos into agent and environment representations, and show the power of the factored representation for downstream tasks in vision and robot learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The analysis and use of egocentric videos for robotics tasks is made challenging by occlusion and the visual mismatch between the human hand and a robot end-effector. Past work views the human hand as a nuisance and removes it from the scene. However, the hand also provides a valuable signal for learning. In this work, we propose to extract a factored representation of the scene that separates the agent (human hand) and the environment. This alleviates both occlusion and mismatch while preserving the signal, thereby easing the design of models for downstream robotics tasks. At the heart of this factorization is our proposed Video Inpainting via Diffusion Model (VIDM) that leverages both a prior on real-world images (through a large-scale pre-trained diffusion model) and the appearance of the object in earlier frames of the video (through attention). Our experiments demonstrate the effectiveness of VIDM at improving the in-painting quality in egocentric videos and the power of our factored representation for numerous tasks: object detection, 3D reconstruction of manipulated objects, and learning of reward functions, policies, and affordances from videos.</div>
</div>
<div class='paper-counter'>436/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Zirui Zhao;Wee Sun Lee;David Hsu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Task Planning;Large Language Models;Human-Robot Interaction</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c1fbe62958ce601ca40bd33e74eacbb62df288db.pdf" target="_blank">https://openreview.net/pdf/c1fbe62958ce601ca40bd33e74eacbb62df288db.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f78054a18bdeea5bf1e8cf71e138ea2c7ec35ae6.pdf" target="_blank">https://openreview.net/attachment/f78054a18bdeea5bf1e8cf71e138ea2c7ec35ae6.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use Large Language Models as both the commonsense world model and the heuristic policy within the Monte Carlo Tree Search framework, enabling better-reasoned decision-making for daily tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large-scale task planning is a major challenge.  Recent work exploits large  language models (LLMs) directly as a policy and shows surprisingly  interesting results.  This paper shows that LLMs provide a  commonsense model of the world in addition to a policy that acts on it.  The world model and the policy can be combined in a search  algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task  planning.  In our new LLM-MCTS algorithm, the LLM-induced world model  provides a commonsense prior belief for MCTS to achieve effective reasoning;  the LLM-induced policy acts as a heuristic to guide the search, vastly  improving search efficiency. Experiments show that LLM-MCTS outperforms  both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide  margin, for complex, novel tasks.   Further experiments and analyses on multiple tasks --  multiplication, travel planning, object rearrangement --  suggest minimum description length (MDL)  as a general guiding principle: if the  description length of the world model is substantially smaller than that of  the  policy, using LLM as a world model for model-based planning is likely better  than using LLM solely as a policy.</div>
</div>
<div class='paper-counter'>437/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching</div>
<div class="field-name">authors:</div>
<div class="field-value">Junsheng Zhou;Baorui Ma;Wenyuan Zhang;Yi Fang;Yu-Shen Liu;Zhizhong Han</div>
<div class="field-name">keywords:</div>
<div class="field-value">LiDAR Point Clouds;2D images;Cross-modality registration;Matching</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1b0cc4e7c98315e52ed7a5dd8fd0cbaabab0d0ff.pdf" target="_blank">https://openreview.net/pdf/1b0cc4e7c98315e52ed7a5dd8fd0cbaabab0d0ff.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3df1f16f13058ef12f31ccf94a313ed87fc2b0a0.pdf" target="_blank">https://openreview.net/attachment/3df1f16f13058ef12f31ccf94a313ed87fc2b0a0.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Cross-modality registration between 2D images captured by cameras and 3D point clouds from LiDARs is a crucial task in computer vision and robotic. Previous methods estimate 2D-3D correspondences by matching point and pixel patterns learned by neural networks, and use Perspective-n-Points (PnP) to estimate rigid transformation during post-processing. However, these methods struggle to map points and pixels to a shared latent space robustly since points and pixels have very different characteristics with patterns learned in different manners (MLP and CNN), and they also fail to construct supervision directly on the transformation since the PnP is non-differentiable, which leads to unstable registration results. To address these problems, we propose to learn a structured cross-modality latent space to represent pixel features and 3D features via a differentiable probabilistic PnP solver. Specifically, we design a triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D elements using both voxels and points to learn the cross-modality latent space with pixels. We design both the voxel and pixel branch based on CNNs to operate convolutions on voxels/pixels represented in grids, and integrate an additional point branch to regain the information lost during voxelization. We train our framework end-to-end by imposing supervisions directly on the predicted pose distribution with a probabilistic PnP solver. To explore distinctive patterns of cross-modality features, we design a novel loss with adaptive-weighted optimization for cross-modality feature description. The experimental results on KITTI and nuScenes datasets show significant improvements over the state-of-the-art methods.</div>
</div>
<div class='paper-counter'>438/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Siming Lan;Rui Zhang;Qi Yi;Jiaming Guo;Shaohui Peng;Yunkai Gao;Fan Wu;Ruizhi Chen;Zidong Du;Xing Hu;Xishan Zhang;Ling Li;Yunji Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;multi-task learning;contrastive learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4da74979d1bec2038d413c66925c2affcbd91aef.pdf" target="_blank">https://openreview.net/pdf/4da74979d1bec2038d413c66925c2affcbd91aef.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/cc6fd049a2e972d7f21ef902a045aca9058c2004.zip" target="_blank">https://openreview.net/attachment/cc6fd049a2e972d7f21ef902a045aca9058c2004.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods.In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, alleviating the negative transfer within the task and improving the generalization ability and the performance for multi-task RL.We conducted the experiment on Meta-World, a multi-task RL benchmark containing various robotics manipulation tasks. Experimental results show that CMTA outperforms learning each task individually for the first time and achieves substantial performance improvements over the baselines.</div>
</div>
<div class='paper-counter'>439/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiankai Sun;Yiqi Jiang;Jianing Qiu;Parth Talpur Nobel;Mykel Kochenderfer;Mac Schwager</div>
<div class="field-name">keywords:</div>
<div class="field-value">Uncertainty;Conformal Prediction;Dynamics Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8fde541eed2bb30f6071042844bbb417203468c0.pdf" target="_blank">https://openreview.net/pdf/8fde541eed2bb30f6071042844bbb417203468c0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2ecc4ea663e3dc768de4f0a96c64d4c41aea6275.pdf" target="_blank">https://openreview.net/attachment/2ecc4ea663e3dc768de4f0a96c64d4c41aea6275.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic applications often involve working in environments that are uncertain, dynamic, and partially observable. Recently, diffusion models have been proposed for learning trajectory prediction models trained from expert demonstrations, which can be used for planning in robot tasks. Such models have demonstrated a strong ability to overcome challenges such as multi-modal action distributions, high-dimensional output spaces, and training instability. It is crucial to quantify the uncertainty of these dynamics models when using them for planning. In this paper, we quantify the uncertainty of diffusion dynamics models using Conformal Prediction (CP). Given a finite number of exchangeable expert trajectory examples (called the “calibration set”), we use CP to obtain a set in the trajectory space (called the “coverage region”) that is guaranteed to contain the output of the diffusion model with a user-defined probability (called the “coverage level”). In PlanCP, inspired by concepts from conformal prediction, we modify the loss function for training the diffusion model to include a quantile term to encourage more robust performance across the variety of training examples. At test time, we then calibrate PlanCP with a conformal prediction process to obtain coverage sets for the trajectory prediction with guaranteed coverage level. We evaluate our algorithm on various planning tasks and model-based offline reinforcement learning tasks and show that it reduces the uncertainty of the learned trajectory prediction model. As a by-product, our algorithm PlanCP outperforms prior algorithms on existing offline RL benchmarks and challenging continuous planning tasks. Our method can be combined with most model-based planning approaches to produce uncertainty estimates of the closed-loop system.</div>
</div>
<div class='paper-counter'>440/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Mixed-Initiative Multiagent Apprenticeship Learning for Human Training of Robot Teams</div>
<div class="field-name">authors:</div>
<div class="field-value">Esmaeil Seraj;Jerry Yuyang Xiong;Mariah L Schrum;Matthew Gombolay</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning from Demonstration;Multi-Robot Systems;Teaching Robot Teams</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fe81cd3ddf306f231f510fc993d7267ed064c6d6.pdf" target="_blank">https://openreview.net/pdf/fe81cd3ddf306f231f510fc993d7267ed064c6d6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5e6e9f604c7429c9957d9bb8c9f0a95900ee1f3f.pdf" target="_blank">https://openreview.net/attachment/5e6e9f604c7429c9957d9bb8c9f0a95900ee1f3f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this work we present MixTURE, a multi-agent learning from demonstration approach that enables directly teaching teams of heterogeneous robots under a Dec-POMDP paradigm using real human generated data.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Extending recent advances in Learning from Demonstration (LfD) frameworks to multi-robot settings poses critical challenges such as environment non-stationarity due to partial observability which is detrimental to the applicability of existing methods. Although prior work has shown that enabling communication among agents of a robot team can alleviate such issues, creating inter-agent communication under existing Multi-Agent LfD (MA-LfD) frameworks requires the human expert to provide demonstrations for both environment actions and communication actions, which necessitates an efficient communication strategy on a known message spaces. To address this problem, we propose Mixed-Initiative Multi-Agent Apprenticeship Learning (MixTURE). MixTURE enables robot teams to learn from a human expert-generated data a preferred policy to accomplish a collaborative task, while simultaneously learning emergent inter-agent communication to enhance team coordination. The key ingredient to MixTURE's success is automatically learning a communication policy, enhanced by a mutual-information maximizing reverse model that rationalizes the underlying expert demonstrations without the need for human generated data or an auxiliary reward function. MixTURE outperforms a variety of relevant baselines on diverse data generated by human experts in complex heterogeneous domains. MixTURE is the first MA-LfD framework to enable learning multi-robot collaborative policies directly from real human data, resulting in ~44% less human workload, and ~46% higher usability score.</div>
</div>
<div class='paper-counter'>441/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Provably Safe Reinforcement Learning with Step-wise Violation Constraints</div>
<div class="field-name">authors:</div>
<div class="field-value">Nuoya Xiong;Yihan Du;Longbo Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">safe reinforcement learning;step-wise violation;reinforcement learning theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/89325e22e9f902654729410715a100b34489c214.pdf" target="_blank">https://openreview.net/pdf/89325e22e9f902654729410715a100b34489c214.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8a4ba93123920f542a3c40218bfcbd8ccac1da98.pdf" target="_blank">https://openreview.net/attachment/8a4ba93123920f542a3c40218bfcbd8ccac1da98.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper study the safe reinforcement learning and reward-free exploration with step-wise violation constraints</div>
<div class="field-name">abstract:</div>
<div class="field-value">We investigate a novel safe reinforcement learning problem with step-wise violation constraints. Our problem differs from existing works in that we focus on stricter step-wise violation constraints and do not assume the existence of safe actions, making our formulation more suitable for safety-critical applications that need to ensure safety in all decision steps but may not always possess safe actions, e.g., robot control and autonomous driving.We propose an efficient algorithm SUCBVI, which guarantees $\widetilde{\mathcal{O}}(\sqrt{ST})$ or gap-dependent $\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}} + S^2AH^2)$ step-wise violation and $\widetilde{\mathcal{O}}(\sqrt{H^3SAT})$ regret. Lower bounds are provided to validate the optimality in both violation and  regret performance with respect to the number of states $S$ and the total number of steps $T$. Moreover, we further study an innovative safe reward-free exploration problem with step-wise violation constraints. For this problem, we design algorithm SRF-UCRL to find a near-optimal safe policy, which achieves nearly state-of-the-art  sample complexity $\widetilde{\mathcal{O}}((\frac{S^2AH^2}{\varepsilon}+\frac{H^4SA}{\varepsilon^2})(\log(\frac{1}{\delta})+S))$, and guarantees $\widetilde{\mathcal{O}}(\sqrt{ST})$ violation during exploration.  Experimental results demonstrate the  superiority of our algorithms in safety performance and corroborate our theoretical results.</div>
</div>
<div class='paper-counter'>442/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can Language Models Solve Graph Problems in Natural Language?</div>
<div class="field-name">authors:</div>
<div class="field-value">Heng Wang;Shangbin Feng;Tianxing He;Zhaoxuan Tan;Xiaochuang Han;Yulia Tsvetkov</div>
<div class="field-name">keywords:</div>
<div class="field-value">large language models;graph reasoning;structured reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/756b1f7b1f822853cd68c1925e52eb13bdda2e78.pdf" target="_blank">https://openreview.net/pdf/756b1f7b1f822853cd68c1925e52eb13bdda2e78.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6e3b61522a39dda6fc5831a757f63263134d6280.zip" target="_blank">https://openreview.net/attachment/6e3b61522a39dda6fc5831a757f63263134d6280.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question.</div>
</div>
<div class='paper-counter'>443/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">NAP: Neural 3D Articulated Object Prior</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiahui Lei;Congyue Deng;Bokui Shen;Leonidas Guibas;Kostas Daniilidis</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D articulated objects;diffusion models;generative models</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a0e84642feb7a7870f9b1a4dc6dca5dbccb22c02.pdf" target="_blank">https://openreview.net/pdf/a0e84642feb7a7870f9b1a4dc6dca5dbccb22c02.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9c02cf466204a9c7218c49e409192ad1b293bb98.pdf" target="_blank">https://openreview.net/attachment/9c02cf466204a9c7218c49e409192ad1b293bb98.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose Neural 3D Articulated object Prior (NAP), the first 3D deep generative model to synthesize 3D articulated object models. Despite the extensive research on generating 3D static objects, compositions, or scenes, there are hardly any approaches on capturing the distribution of articulated objects, a common object category for human and robot interaction. To generate articulated objects, we first design a novel articulation tree/graph parameterization and then apply a diffusion-denoising probabilistic model over this representation where articulated objects can be generated via denoising from random complete graphs. In order to capture both the geometry and the motion structure whose distribution will affect each other, we design a graph denoising network for learning the reverse diffusion process. We propose a novel distance that adapts widely used 3D generation metrics to our novel task to evaluate generation quality. Experiments demonstrate our high performance in articulated object generation as well as its applications on conditioned generation, including Part2Motion, PartNet-Imagination, Motion2Part, and GAPart2Object.</div>
</div>
<div class='paper-counter'>444/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Concept Algebra for (Score-Based) Text-Controlled Generative Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zihao Wang;Lin Gui;Jeffrey Negrea;Victor Veitch</div>
<div class="field-name">keywords:</div>
<div class="field-value">disentanglement; representation learning; text-controlled generative models; diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5298b51885527822bc4cfb5a894ca0b6ff983875.pdf" target="_blank">https://openreview.net/pdf/5298b51885527822bc4cfb5a894ca0b6ff983875.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7aa56b05e8e715ec766b0214f56c2cedb575ba35.zip" target="_blank">https://openreview.net/attachment/7aa56b05e8e715ec766b0214f56c2cedb575ba35.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. A key property of such models is that they can compose disparate concepts in a 'disentangled' manner.This suggests these models have internal representations that encode concepts in a 'disentangled' manner. Here, we focus on the idea that concepts are encoded as subspaces of some representation space.  We formalize what this means, show there's a natural choice for the representation, and develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples using Stable Diffusion.</div>
</div>
<div class='paper-counter'>445/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Active representation learning for general task space with applications in robotics</div>
<div class="field-name">authors:</div>
<div class="field-value">Yifang Chen;Yingbing Huang;Simon Shaolei Du;Kevin Jamieson;Guanya Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">active learning;representation learning;robotics;theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a44a7ae3473d6e741ee196ed3b8a3c931aafd2b6.pdf" target="_blank">https://openreview.net/pdf/a44a7ae3473d6e741ee196ed3b8a3c931aafd2b6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c2c132c9ad34b53b9decde45f9ce95d4687b6048.pdf" target="_blank">https://openreview.net/attachment/c2c132c9ad34b53b9decde45f9ce95d4687b6048.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Representation learning based on multi-task pretraining has become a powerful approach in many domains. In particular, task-aware representation learning aims to learn an optimal representation for a specific target task by sampling data from a set of source tasks, while task-agnostic representation learning seeks to learn a universal representation for a class of tasks.  In this paper, we propose a general and versatile algorithmic and theoretic framework for \emph{active representation learning}, where the learner optimally chooses which source tasks to sample from. This framework, along with a tractable meta algorithm, allows most arbitrary target and source task spaces (from discrete to continuous), covers both task-aware and task-agnostic settings, and is compatible with deep representation learning practices. We provide several instantiations under this framework, from bilinear and feature-based nonlinear to general nonlinear cases. In the bilinear case, by leveraging the non-uniform spectrum of the task representation and the calibrated source-target relevance, we prove that the sample complexity to achieve $\varepsilon$-excess risk on target scales with $(k^*)^2 ||v^*||_2^2 \varepsilon^{-2}$ where $k^*$ is the effective dimension of the target and $||v^*||_2^2 \in (0,1]$ represents the connection between source and target space. Compared to the passive one, this can save up to $\frac{1}{d_W}$ of sample complexity, where $d_W$ is the task space dimension. Finally, we demonstrate different instantiations of our meta algorithm in synthetic datasets and robotics problems, from pendulum simulations to real-world drone flight datasets. On average, our algorithms outperform baselines by 20%-70%.</div>
</div>
<div class='paper-counter'>446/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruihai Wu;Kai Cheng;Yan Zhao;Chuanruo Ning;Guanqi Zhan;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Affordance for Robotics;Articulated Object Manipulation;Occlusion Handling</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6c88495201cd13a2f9940f9ea958ed2d8ed568f6.pdf" target="_blank">https://openreview.net/pdf/6c88495201cd13a2f9940f9ea958ed2d8ed568f6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f7452b1fe6324d67dc136f175f758c97ed246c74.zip" target="_blank">https://openreview.net/attachment/f7452b1fe6324d67dc136f175f758c97ed246c74.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose the affordance learning task that considers environment constraints, and propose a novel and data-efficient learning framework.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containing a single occluder and generalizing to scenes with complex occluder combinations. Experiments demonstrate the effectiveness of our proposed approach in learning affordance considering environment constraints.</div>
</div>
<div class='paper-counter'>447/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Relax, it doesn’t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis</div>
<div class="field-name">authors:</div>
<div class="field-value">Mehdi Azabou;Michael Jacob Mendelson;Nauman Ahad;Maks Sorokin;Shantanu Thakoor;Carolina Urzay;Eva L Dyer</div>
<div class="field-name">keywords:</div>
<div class="field-value">animal behavior;behavioral neuroscience;self-supervised learning;multi-timescale</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/307c6dca91df2facd1757a88cc7c494fb5f468ac.pdf" target="_blank">https://openreview.net/pdf/307c6dca91df2facd1757a88cc7c494fb5f468ac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a self-supervised learning model to analyze animal tracking data and to reveal the factors that underlie animal behavior across different timescales.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Unconstrained and natural  behavior consists of dynamics that are complex and  unpredictable, especially when trying to predict what will happen  multiple steps into the future. While some success has been found in building representations of animal behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for animal behavior that combines two novel components: (i) an action-prediction objective that aims to predict the  distribution of actions over future timesteps, and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly benchmarks. In all of these cases, we show that our model can build representations that capture the many different factors that drive behavior and solve a wide range of downstream tasks.</div>
</div>
<div class='paper-counter'>448/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">Chuanruo Ning;Ruihai Wu;Haoran Lu;Kaichun Mo;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">articulated object manipulation;few-shot learning;visual affordance for robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7baa38d6db0f2b2e62a1251ba9db108abfec1fc6.pdf" target="_blank">https://openreview.net/pdf/7baa38d6db0f2b2e62a1251ba9db108abfec1fc6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0fb5e86ebb0a3d5850193a3cfe65148451d3aa5b.zip" target="_blank">https://openreview.net/attachment/0fb5e86ebb0a3d5850193a3cfe65148451d3aa5b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Leveraging similarity among local geometries across object categories to perform cross-category few-shot affordance learning for articulated object manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Articulated object manipulation is a fundamental yet challenging task in robotics. Due to significant geometric and semantic variations across object categories, previous manipulation models struggle to generalize to novel categories. Few-shot learning is a promising solution for alleviating this issue by allowing robots to perform a few interactions with unseen objects. However, extant approaches often necessitate costly and inefficient test-time interactions with each unseen instance. Recognizing this limitation, we observe that despite their distinct shapes, different categories often share similar local geometries essential for manipulation, such as pullable handles and graspable edges - a factor typically underutilized in previous few-shot learning works. To harness this commonality, we introduce 'Where2Explore', an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates the geometric similarity across different categories, identifying local areas that differ from shapes in the training categories for efficient exploration while concurrently transferring affordance knowledge to similar parts of the objects. Extensive experiments in simulated and real-world environments demonstrate our framework's capacity for efficient few-shot exploration and generalization.</div>
</div>
<div class='paper-counter'>449/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment</div>
<div class="field-name">authors:</div>
<div class="field-value">Zihui Xue;Kristen Grauman</div>
<div class="field-name">keywords:</div>
<div class="field-value">fine-grained video understanding;egocentric video;self-supervised learning;temporal alignment</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a88e226c00401c562932d4c6e21700aeee39d165.pdf" target="_blank">https://openreview.net/pdf/a88e226c00401c562932d4c6e21700aeee39d165.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9522a2e3745c925297c6377339f5499327e21558.zip" target="_blank">https://openreview.net/attachment/9522a2e3745c925297c6377339f5499327e21558.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">a self-supervised embedding approach to learn fine-grained action features invariant to the egocentric and exocentric viewpoints</div>
<div class="field-name">abstract:</div>
<div class="field-value">The egocentric and exocentric viewpoints of a human activity look dramatically different, yet invariant representations to link them are essential for many potential applications in robotics and augmented reality.  Prior work is limited to learning view-invariant features from paired synchronized viewpoints.  We relax that strong data assumption and propose to learn fine-grained action features that are invariant to the viewpoints by aligning egocentric and exocentric videos in time, even when not captured simultaneously or in the same environment. To this end, we propose AE2, a self-supervised embedding approach with two key designs: (1) an object-centric encoder that explicitly focuses on regions corresponding to hands and active objects; (2) a contrastive-based alignment objective that leverages temporally reversed frames as negative samples. For evaluation, we establish a benchmark for fine-grained video understanding in the ego-exo context, comprising four datasets---including an ego tennis forehand dataset we collected, along with dense per-frame labels we annotated for each dataset. On the four datasets, our AE2 method strongly outperforms prior work in a variety of fine-grained downstream tasks, both in regular and cross-view settings.</div>
</div>
<div class='paper-counter'>450/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Michael Beukman;Devon Jarvis;Richard Klein;Steven James;Benjamin Rosman</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Reinforment Learning;Contextual Markov Decision Process;Neural Network Architecture</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e5ea8a4b045e26fa25d59aea640135db7afdfd1a.pdf" target="_blank">https://openreview.net/pdf/e5ea8a4b045e26fa25d59aea640135db7afdfd1a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce the decision adapter, an improved way to incorporate context into an agent's policy in contextual reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">While reinforcement learning has achieved remarkable successes in several domains, its real-world application is limited due to many methods failing to generalise to unfamiliar conditions. In this work, we consider the problem of generalising to new transition dynamics, corresponding to cases in which the environment's response to the agent's actions differs. For example, the gravitational force exerted on a robot depends on its mass and changes the robot's mobility. Consequently, in such cases, it is necessary to condition an agent's actions on extrinsic state information and pertinent contextual information reflecting how the environment responds. While the need for context-sensitive policies has been established, the manner in which context is incorporated architecturally has received less attention. Thus, in this work, we present an investigation into how context information should be incorporated into behaviour learning to improve generalisation.  To this end, we introduce a neural network architecture, the Decision Adapter, which generates the weights of an adapter module and conditions the behaviour of an agent on the context information. We show that the Decision Adapter is a useful generalisation of a previously proposed architecture and empirically demonstrate that it results in superior generalisation performance compared to previous approaches in several environments. Beyond this, the Decision Adapter is more robust to irrelevant distractor variables than several alternative methods.</div>
</div>
<div class='paper-counter'>451/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ATMAN: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Björn Deiseroth;Mayukh Deb;Samuel Weinbach;Manuel Brack;Patrick Schramowski;Kristian Kersting</div>
<div class="field-name">keywords:</div>
<div class="field-value">explainability;attention manipulation;perturbation;large language model;multi-modality;generative decoder;efficiency;transformer</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/026ce233d91b5bd56ecdac317ddfaaef28077d37.pdf" target="_blank">https://openreview.net/pdf/026ce233d91b5bd56ecdac317ddfaaef28077d37.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/32aa1413f18d3d5a24da31bd0f2c95f8f168df6a.pdf" target="_blank">https://openreview.net/attachment/32aa1413f18d3d5a24da31bd0f2c95f8f168df6a.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show how attention manipulation can be used as a perturbation method; to steer or explain large language models outputs w.r.t. the input relevance; on text and text/visual models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of additional memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use explanations in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method relying on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.</div>
</div>
<div class='paper-counter'>452/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Importance-aware Co-teaching for Offline Model-based Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Ye Yuan;Can Chen;Zixuan Liu;Willie Neiswanger;Xue Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline model-based optimization;co-teaching;meta-learning;sample reweighting</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/60b0b52612345b72636f0ad607e0b7e9b6d4bc91.pdf" target="_blank">https://openreview.net/pdf/60b0b52612345b72636f0ad607e0b7e9b6d4bc91.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9f0ce0b071f376d7e31b978cb8a04d300ec0e6d6.pdf" target="_blank">https://openreview.net/attachment/9f0ce0b071f376d7e31b978cb8a04d300ec0e6d6.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline model-based optimization aims to find a design that maximizes a property of interest using only an offline dataset, with applications in robot, protein, and molecule design, among others. A prevalent approach is gradient ascent, where a proxy model is trained on the offline dataset and then used to optimize the design. This method suffers from an out-of-distribution issue, where the proxy is not accurate for unseen designs. To mitigate this issue, we explore using a pseudo-labeler to generate valuable data for fine-tuning the proxy. Specifically, we propose $\textit{\textbf{I}mportance-aware \textbf{C}o-\textbf{T}eaching for Offline Model-based Optimization}~(\textbf{ICT})$. This method maintains three symmetric proxies with their mean ensemble as the final proxy, and comprises two steps. The first step is $\textit{pseudo-label-driven co-teaching}$. In this step, one proxy is iteratively selected as the pseudo-labeler for designs near the current optimization point, generating pseudo-labeled data.  Subsequently, a co-teaching process identifies small-loss samples as valuable data and exchanges them between the other two proxies for fine-tuning, promoting knowledge transfer.  This procedure is repeated three times, with a different proxy chosen as the pseudo-labeler each time, ultimately enhancing the ensemble performance.To further improve accuracy of pseudo-labels, we perform a secondary step of $\textit{meta-learning-based sample reweighting}$,which assigns importance weights to samples in the pseudo-labeled dataset and updates them via meta-learning. ICT achieves state-of-the-art results across multiple design-bench tasks, achieving the best mean rank $3.1$ and median rank $2$ among $15$ methods.Our source code can be accessed here.</div>
</div>
<div class='paper-counter'>453/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Wonje Choi;Woo Kyung Kim;SeungHyun Kim;Honguk Woo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Prompt Learining;Domain Adaptation;Embodied AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2b304d9f4f08bdb5216c3656ac73dbae1ea1b755.pdf" target="_blank">https://openreview.net/pdf/2b304d9f4f08bdb5216c3656ac73dbae1ea1b755.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5413a8e37735c6cfb57284964cfddf9e449d7dc6.zip" target="_blank">https://openreview.net/attachment/5413a8e37735c6cfb57284964cfddf9e449d7dc6.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</div>
<div class="field-name">abstract:</div>
<div class="field-value">For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enables efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factors that significantly affects the agent's egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that ConPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.</div>
</div>
<div class='paper-counter'>454/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Boundary Guided Learning-Free Semantic Control with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Ye Zhu;Yu Wu;Zhiwei Deng;Olga Russakovsky;Yan Yan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion probabilistic models;learning-free applications;high-dimensional semantic boundary;markov mixing</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/77449eacfd8bc6a501e475e91abb9f7490b99337.pdf" target="_blank">https://openreview.net/pdf/77449eacfd8bc6a501e475e91abb9f7490b99337.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/445570c47b1566e4d8d9cc297d65b4a48ec1b881.zip" target="_blank">https://openreview.net/attachment/445570c47b1566e4d8d9cc297d65b4a48ec1b881.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce one of the first learning-free methods for effective and efficient image semantic control and editing via pre-trained unconditional denoising diffusion base models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Applying pre-trained generative denoising diffusion models (DDMs) for downstream tasks such as image semantic editing usually requires either fine-tuning DDMs or learning auxiliary editing networks in the existing literature. In this work, we present our BoundaryDiffusion method for efficient, effective and light-weight semantic control with frozen pre-trained DDMs, without learning any extra networks. As one of the first learning-free diffusion editing works, we start by seeking a more comprehensive understanding of the intermediate high-dimensional latent spaces by theoretically and empirically analyzing their probabilistic and geometric behaviors in the Markov chain. We then propose to further explore the critical step in the denoising trajectory that characterizes the convergence of a pre-trained DDM and introduce an automatic search method. Last but not least, in contrast to the conventional understanding that DDMs have relatively poor semantic behaviors (in generic latent spaces), we prove that the critical latent space we found already forms semantic subspace boundaries at the generic level in unconditional DDMs, which allows us to do controllable manipulation by guiding the denoising trajectory towards the targeted boundary via a single-step operation. We conduct extensive experiments on multiple DPMs architectures (DDPM, iDDPM) and datasets (CelebA, CelebA-HQ, LSUN-church, LSUN-bedroom, AFHQ-dog) with different resolutions (64, 256), achieving superior or state-of-the-art performance in various task scenarios (image semantic editing, text-based editing, unconditional semantic control) to demonstrate the effectiveness.</div>
</div>
<div class='paper-counter'>455/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Minyoung Hwang;Gunmin Lee;Hogun Kee;Chan Woo Kim;Kyungjae Lee;Songhwai Oh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning; Reinforcement Learning from Human Feedback; Preference-based Reinforcement Learning; Human-Robot Interaction</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/03fd7bb2cab83230de26f0813f2dfa2c166e0863.pdf" target="_blank">https://openreview.net/pdf/03fd7bb2cab83230de26f0813f2dfa2c166e0863.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8bcd064973417dd39fe5638d7734f6ada66922f4.zip" target="_blank">https://openreview.net/attachment/8bcd064973417dd39fe5638d7734f6ada66922f4.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel RLHF framework called SeqRank, which significantly improves the human feedback efficiency and performance of RLHF.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning from human feedback (RLHF) alleviates the problem of designing a task-specific reward function in reinforcement learning by learning it from human preference. However, existing RLHF models are considered inefficient as they produce only a single preference data from each human feedback. To tackle this problem, we propose a novel RLHF framework called SeqRank, that uses sequential preference ranking to enhance the feedback efficiency. Our method samples trajectories in a sequential manner by iteratively selecting a defender from the set of previously chosen trajectories $\mathcal{K}$ and a challenger from the set of unchosen trajectories $\mathcal{U}\setminus\mathcal{K}$, where $\mathcal{U}$ is the replay buffer. We propose two trajectory comparison methods with different defender sampling strategies: (1) sequential pairwise comparison that selects the most recent trajectory and (2) root pairwise comparison that selects the most preferred trajectory from $\mathcal{K}$. We construct a data structure and rank trajectories by preference to augment additional queries. The proposed method results in at least 39.2% higher average feedback efficiency than the baseline and also achieves a balance between feedback efficiency and data dependency. We examine the convergence of the empirical risk and the generalization bound of the reward model with Rademacher complexity. While both trajectory comparison methods outperform conventional pairwise comparison, root pairwise comparison improves the average reward in locomotion tasks and the average success rate in manipulation tasks by 29.0% and 25.0%, respectively. The source code and the videos are provided in the supplementary material.</div>
</div>
<div class='paper-counter'>456/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Geometric Algebra Transformer</div>
<div class="field-name">authors:</div>
<div class="field-value">Johann Brehmer;Pim De Haan;Sönke Behrends;Taco Cohen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Geometry;geometric algebra;equivariance;transformer</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ca06b85b86759bbdc5b5011af7903f18dbb87471.pdf" target="_blank">https://openreview.net/pdf/ca06b85b86759bbdc5b5011af7903f18dbb87471.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">GATr is a new general-purpose network for geometric data, based on geometric algebra representations + E(3) equivariance + a transformer architecture</div>
<div class="field-name">abstract:</div>
<div class="field-value">Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.</div>
</div>
<div class='paper-counter'>457/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization</div>
<div class="field-name">authors:</div>
<div class="field-value">Mahyar Fazlyab;Taha Entesari;Aniket Roy;Rama Chellappa</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Learning;Adversarial Robustness;Certified Radius;Lipschitz Constants</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/17ab79dff97fb924179854bb2ca5bb4533172240.pdf" target="_blank">https://openreview.net/pdf/17ab79dff97fb924179854bb2ca5bb4533172240.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/09f89e73e9b3f4ba68cbebc989733e93a479ee5b.pdf" target="_blank">https://openreview.net/attachment/09f89e73e9b3f4ba68cbebc989733e93a479ee5b.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A new training procedure to improve the robustness of deep classifiers using Lipschitz regularization and margin maximization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. In this paper, we propose a differentiable regularizer that is a lower bound on the distance of the data points to the classification boundary. The proposed regularizer requires knowledge of the model's Lipschitz constant along certain directions. To this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the Lipschitz constant of neural networks accurately and efficiently.  The relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the monotonicity and Lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their Lipschitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art.</div>
</div>
<div class='paper-counter'>458/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Zihao Wang;Shaofei Cai;Guanzhou Chen;Anji Liu;Xiaojian Ma;Yitao Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">open-ended learning;multi task;large language models;zero-shot planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/55b58f4d498cc7605484fc24b7dbee97f5e0a58f.pdf" target="_blank">https://openreview.net/pdf/55b58f4d498cc7605484fc24b7dbee97f5e0a58f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel interactive planning approach using LLMs that brings a planning-based agent to the apex of Minecraft (an open-world environment) for the first time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the achievability of the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose ``$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect'' ($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal $\textbf{Selector}$, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\texttt{ObtainDiamond}$ grand challenge with our approach.</div>
</div>
<div class='paper-counter'>459/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VoxDet: Voxel Learning for Novel Instance Detection</div>
<div class="field-name">authors:</div>
<div class="field-value">Bowen Li;Jiashun Wang;Yaoyu Hu;Chen Wang;Sebastian Scherer</div>
<div class="field-name">keywords:</div>
<div class="field-value">Unseen object detection;instance perception;voxel representation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/99461e15e7a826c499270e277a457ad66964d10e.pdf" target="_blank">https://openreview.net/pdf/99461e15e7a826c499270e277a457ad66964d10e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/75fa65594031b88f23df056580581553ebb72955.zip" target="_blank">https://openreview.net/attachment/75fa65594031b88f23df056580581553ebb72955.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work proposes VoxDet, a novel instance detector based on geometry-invariant voxel learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on $2 \mathrm{D}$ representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a $3 \mathrm{D}$ reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools, where 20 unique instances are video-recorded with camera extrinsic. RoboTools also provides 24 challenging cluttered scenarios with more than $9 \mathrm{k}$ box annotations. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and RoboTools benchmarks, where VoxDet outperforms various $2 \mathrm{D}$ baselines remarkably with faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D novel instance detection tasks.</div>
</div>
<div class='paper-counter'>460/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Latent exploration for Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Alberto Silvio Chiappa;Alessandro Marin Vargas;Ann Huang;Alexander Mathis</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;efficient exploration;curse of dimensionality;motor control;musculoskeletal control</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c5394776a16ed77983cd8e048b45c1af4311b90d.pdf" target="_blank">https://openreview.net/pdf/c5394776a16ed77983cd8e048b45c1af4311b90d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a monolithic framework for implicitly exploring applicable to both off- and on-policy algorithms that learns more energy efficient policies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which can be seamlessly integrated with on- and off-policy algorithms. We demonstrate that the noisy actions generated by perturbing the network's activations can be modeled as a multivariate Gaussian distribution with a full covariance matrix. In the PyBullet locomotion tasks, Lattice-SAC achieves state of the art results, and reaches 18\% higher reward than unstructured exploration in the Humanoid environment. In the musculoskeletal control environments of MyoSuite, Lattice-PPO achieves higher reward in most reaching and object manipulation tasks, while also finding more energy-efficient policies with reductions of 20-60\%. Overall, we demonstrate the effectiveness of structured action noise in time and actuator space for complex motor control tasks. The code is available at: https://github.com/amathislab/lattice.</div>
</div>
<div class='paper-counter'>461/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Wenlong Huang;Fei Xia;Dhruv Shah;Danny Driess;Andy Zeng;Yao Lu;Pete Florence;Igor Mordatch;Sergey Levine;Karol Hausman;brian ichter</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;language models;embodied agents</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7dc83574bcaf3b1245b01026782fe6b45e8c3b23.pdf" target="_blank">https://openreview.net/pdf/7dc83574bcaf3b1245b01026782fe6b45e8c3b23.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We formulate a token decoding procedure applying large language model to robotics settings. Tokens are selected based on likelihood under the language model and a set of grounded models, such as affordance, safety, and preference functions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.</div>
</div>
<div class='paper-counter'>462/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</div>
<div class="field-name">authors:</div>
<div class="field-value">Paul Pu Liang;Yun Cheng;Xiang Fan;Chun Kai Ling;Suzanne Nie;Richard J. Chen;Zihao Deng;Nicholas Allen;Randy Auerbach;Faisal Mahmood;Ruslan Salakhutdinov;Louis-Philippe Morency</div>
<div class="field-name">keywords:</div>
<div class="field-value">multimodal learning;feature interactions;partial information decomposition;information theory;quantification;model selection</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b13b51ca6ddc73b2e8a80fefc4d109e2e25f9933.pdf" target="_blank">https://openreview.net/pdf/b13b51ca6ddc73b2e8a80fefc4d109e2e25f9933.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate the usefulness of information theoretic concepts of redundancy, uniqueness, and synergy in quantifying multimodal datasets, models, and model selection. Our proposed estimators allow us to scale these to large, continuous domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.</div>
</div>
<div class='paper-counter'>463/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Smoothed Online Learning for Prediction in Piecewise Affine Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Adam Block;Max Simchowitz;Russ Tedrake</div>
<div class="field-name">keywords:</div>
<div class="field-value">Smoothed Online Learning;Piecewise Affine Prediction;Learning Dynamics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7221eedc0cae7f32704c3024b2acd1d8c568ef0e.pdf" target="_blank">https://openreview.net/pdf/7221eedc0cae7f32704c3024b2acd1d8c568ef0e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/68d9c9ae14c9310a95a97ff8ad82941210cbd106.pdf" target="_blank">https://openreview.net/attachment/68d9c9ae14c9310a95a97ff8ad82941210cbd106.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this paper, we provide the first provably oracle-efficient, no-regret algorithm for prediction in piecewise affine systems</div>
<div class="field-name">abstract:</div>
<div class="field-value">The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics.  Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches.  This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle.  We further apply our results to the problems of one-step prediction and multi-step simulation regret in piecewise affine dynamical systems, where the learner is tasked with simulating trajectories and regret is measured in terms of the Wasserstein distance between simulated and true data.  Along the way, we develop several technical tools of more general interest.</div>
</div>
<div class='paper-counter'>464/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Accelerating Exploration with Unlabeled Prior Data</div>
<div class="field-name">authors:</div>
<div class="field-value">Qiyang Li;Jason Zhang;Dibya Ghosh;Amy Zhang;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/984c60bb13b1b87e5c84cbf69e1b8726dc8453e9.pdf" target="_blank">https://openreview.net/pdf/984c60bb13b1b87e5c84cbf69e1b8726dc8453e9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.</div>
</div>
<div class='paper-counter'>465/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</div>
<div class="field-name">authors:</div>
<div class="field-value">Yao Mu;Qinglong Zhang;Mengkang Hu;Wenhai Wang;Mingyu Ding;Jun Jin;Bin Wang;Jifeng Dai;Yu Qiao;Ping Luo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Multi-modal Foundation Model;Embodied Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c6fd810b6afce13a0fd3e68501e7443b1cf733c0.pdf" target="_blank">https://openreview.net/pdf/c6fd810b6afce13a0fd3e68501e7443b1cf733c0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f2e0bf80a50e171403948a0ceb7398d88660ed64.pdf" target="_blank">https://openreview.net/attachment/f2e0bf80a50e171403948a0ceb7398d88660ed64.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning.(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.</div>
</div>
<div class='paper-counter'>466/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Video Prediction Models as Rewards for Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Alejandro Escontrela;Ademi Adeniji;Wilson Yan;Ajay Jain;Xue Bin Peng;Ken Goldberg;Youngwoon Lee;Danijar Hafner;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;generative modeling;learning from demonstrations;video prediction;unsupervised reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bfb07c06724b1ed9b855b299cde80eaff58b76b1.pdf" target="_blank">https://openreview.net/pdf/bfb07c06724b1ed9b855b299cde80eaff58b76b1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">VIPER uses the conditional likelihoods from pre-trained video prediction models as rewards to learn policies from action and reward-free video.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com</div>
</div>
<div class='paper-counter'>467/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhenbo Song;Xianghui Ze;Jianfeng Lu;Yujiao Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optical flow;correspondence learning;cross-view;camera localization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e7b1c4e8200174361ccb3f8a50c9a98ddcb18192.pdf" target="_blank">https://openreview.net/pdf/e7b1c4e8200174361ccb3f8a50c9a98ddcb18192.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/cff0df98b344ac3349079735c19e1a27415bc579.zip" target="_blank">https://openreview.net/attachment/cff0df98b344ac3349079735c19e1a27415bc579.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper addresses the problem of estimating the 3-DoF camera pose for a ground-level image with respect to a satellite image that encompasses the local surroundings. We propose a novel end-to-end approach that leverages the learning of dense pixel-wise flow fields in pairs of ground and satellite images to calculate the camera pose. Our approach differs from existing methods by constructing the feature metric at the pixel level, enabling full-image supervision for learning distinctive geometric configurations and visual appearances across views. Specifically, our method employs two distinct convolution networks for ground and satellite feature extraction. Then, we project the ground feature map to the bird's eye view (BEV) using a fixed camera height assumption to achieve preliminary geometric alignment. To further establish the content association between the BEV and satellite features, we introduce a residual convolution block to refine the projected BEV feature. Optical flow estimation is performed on the refined BEV feature map and the satellite feature map using flow decoder networks based on RAFT. After obtaining dense flow correspondences, we apply the least square method to filter matching inliers and regress the ground camera pose. Extensive experiments demonstrate significant improvements compared to state-of-the-art methods. Notably, our approach reduces the median localization error by 89\%, 19\%, 80\%, and 35\% on the KITTI, Ford multi-AV, VIGOR, and Oxford RobotCar datasets, respectively.</div>
</div>
<div class='paper-counter'>468/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Domain Adaptive Imitation Learning with Visual Observation</div>
<div class="field-name">authors:</div>
<div class="field-value">Sungho Choi;Seungyul Han;Woojun Kim;Jongseong Chae;Whiyoung Jung;Youngchul Sung</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Deep Reinforcement Learning;Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e052cebe3716febd624084fee588f0d6f94988a6.pdf" target="_blank">https://openreview.net/pdf/e052cebe3716febd624084fee588f0d6f94988a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b22ed0f05aa1ac784bcae0b8c81cad06cb632a03.zip" target="_blank">https://openreview.net/attachment/b22ed0f05aa1ac784bcae0b8c81cad06cb632a03.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new learning architecture and a reward generation procedure for domain-adaptive IL with visual observation to improve behavior feature extraction and learner policy performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we consider domain-adaptive imitation learning with visual observation, where an agent in a target domain learns to perform a task by observing expert demonstrations in a source domain. Domain adaptive imitation learning arises in practical scenarios where a robot, receiving visual sensory data, needs to mimic movements by visually observing other robots from different angles or observing robots of different shapes. To overcome the domain shift in cross-domain imitation learning with visual observation, we propose a novel framework for extracting domain-independent behavioral features from input observations that can be used to train the learner, based on dual feature extraction and image reconstruction. Empirical results demonstrate that our approach outperforms previous algorithms for imitation learning from visual observation with domain shift.</div>
</div>
<div class='paper-counter'>469/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On the Pareto Front of Multilingual Neural Machine Translation</div>
<div class="field-name">authors:</div>
<div class="field-value">Liang Chen;Shuming Ma;Dongdong Zhang;Furu Wei;Baobao Chang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multilinugal Neural Machine Translation;Multitask Learning;Pareto Optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c40800d5c2f43a8055778719282965a2c873e57a.pdf" target="_blank">https://openreview.net/pdf/c40800d5c2f43a8055778719282965a2c873e57a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose the Double Power Law to predict the unique trade-off behavior in Multilingual NMT.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we study how the performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, data sizes, and language directions, we find it interesting that the performance of certain translation direction does not always improve with the increase of its weight in the multi-task optimization objective. Accordingly, scalarization method leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus, which poses a great challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy, and the number of tasks. Finally, we formulate the sample ratio selection problem in MNMT as an optimization problem based on the Double Power Law. Extensive experiments show that it achieves better performance than temperature searching and gradient manipulation methods with only 1/5 to 1/2 of the total training budget. We release the code at https://github.com/pkunlp-icler/ParetoMNMT for reproduction.</div>
</div>
<div class='paper-counter'>470/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Data Quality in Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Suneel Belkhale;Yuchen Cui;Dorsa Sadigh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Robotics;Data Quality</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/282b1364dc49d7bb2204ef9b71ac2fbf06863355.pdf" target="_blank">https://openreview.net/pdf/282b1364dc49d7bb2204ef9b71ac2fbf06863355.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5f904f4f776844c511e76817d0286f060e96a9e0.zip" target="_blank">https://openreview.net/attachment/5f904f4f776844c511e76817d0286f060e96a9e0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We formalize data quality in imitation learning and analyze performance for models trained on different data sources in several environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In supervised learning, the question of data quality and curation has been sidelined in recent years in favor of increasingly more powerful and expressive models that can ingest internet-scale data. However, in offline learning for robotics, we simply lack internet scale data, and so high quality datasets are a necessity. This is especially true in imitation learning (IL), a sample efficient paradigm for robot learning using expert demonstrations. Policies learned through IL suffer from state distribution shift at test time due to compounding errors in action prediction, which leads to unseen states that the policy cannot recover from.Instead of designing new algorithms to address distribution shift, an alternative perspective is to develop new ways of assessing and curating datasets. There is growing evidence that the same IL algorithms can have substantially different performance across different datasets. This calls for a formalism for defining metrics of "data quality" that can further be leveraged for data curation.In this work, we take the first step toward formalizing data quality for imitation learning through the lens of distribution shift: a high quality dataset encourages the policy to stay in distribution at test time. We propose two fundamental properties that are necessary for a high quality datasets: i) action divergence: the mismatch between the expert and learned policy at certain states; and ii) transition diversity: the noise present in the system for a given state and action. We investigate the combined effect of these two key properties in imitation learning theoretically, and we empirically analyze models trained on a variety of different data sources. We show that state diversity is not always beneficial, and we demonstrate how action divergence and transition diversity interact in practice.</div>
</div>
<div class='paper-counter'>471/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BCDiff: Bidirectional Consistent Diffusion for Instantaneous Trajectory Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Rongqing Li;Changsheng Li;Dongchun Ren;Guangyi Chen;Ye Yuan;Guoren Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Trajectory prediction;instantaneous observation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8557f9cb53bd4a2e8cffc0b9e2752e6b94be8002.pdf" target="_blank">https://openreview.net/pdf/8557f9cb53bd4a2e8cffc0b9e2752e6b94be8002.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3a33ec63c44798101b5d2f36f01d75f68b29cca2.pdf" target="_blank">https://openreview.net/attachment/3a33ec63c44798101b5d2f36f01d75f68b29cca2.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The objective of pedestrian trajectory prediction is to estimate the future paths of pedestrians by leveraging historical observations, which plays a vital role in ensuring the safety of self-driving vehicles and navigation robots. Previous works usually rely on a sufficient amount of observation time to accurately predict future trajectories. However, there are many real-world situations where the model lacks sufficient time to observe, such as when pedestrians abruptly emerge from blind spots, resulting in inaccurate predictions and even safety risks. Therefore, it is necessary to perform trajectory prediction based on instantaneous observations, which has rarely been studied before. In this paper, we propose a Bi-directional Consistent Diffusion framework tailored for instantaneous trajectory prediction, named BCDiff. At its heart, we develop two coupled diffusion models by designing a mutual guidance mechanism which can bidirectionally and consistently generate unobserved historical trajectories and future trajectories step-by-step,  to utilize the complementary information between them. Specifically, at each step, the predicted unobserved historical trajectories and limited observed trajectories guide one diffusion model to generate future trajectories, while the predicted future trajectories and observed trajectories guide the other diffusion model to predict unobserved historical trajectories. Given the presence of relatively high noise in the generated trajectories during the initial steps, we introduce a gating mechanism to learn the weights between the predicted trajectories and the limited observed trajectories for automatically balancing their contributions. By means of this iterative and mutually guided generation process, both the future and unobserved historical trajectories undergo continuous refinement, ultimately leading to accurate predictions. Essentially, BCDiff is an encoder-free framework that can be compatible with existing trajectory prediction models in principle. Experiments show that our proposed BCDiff significantly improves the accuracy of instantaneous trajectory prediction on the ETH/UCY and Stanford Drone datasets, compared to related approaches.</div>
</div>
<div class='paper-counter'>472/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yasheng SUN;Yifan Yang;Houwen Peng;Yifei Shen;Yuqing Yang;Han Hu;Lili Qiu;Hideki Koike</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Manipulation;Visual Instruction</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2a95aec73f18bd298f4cd60155b1492a7ddebb86.pdf" target="_blank">https://openreview.net/pdf/2a95aec73f18bd298f4cd60155b1492a7ddebb86.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/07c9e19a43791d8ceba84cc217917f2b66a403c2.pdf" target="_blank">https://openreview.net/attachment/07c9e19a43791d8ceba84cc217917f2b66a403c2.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">While language-guided image manipulation has made remarkable progress, the challenge of how to instruct the manipulation process faithfully reflecting human intentions persists. An accurate and comprehensive description of a manipulation task using natural language is laborious and sometimes even impossible, primarily due to the inherent uncertainty and ambiguity present in linguistic expressions. Is it feasible to accomplish image manipulation without resorting to external cross-modal language information? If this possibility exists, the inherent modality gap would be effortlessly eliminated. In this paper, we propose a novel  manipulation methodology, dubbed ImageBrush, that learns visual instructions for more accurate image editing.Our key idea is to employ a pair of transformation images as visual instructions, which not only precisely captures human intention but also facilitates accessibility in real-world scenarios. Capturing visual instructions is particularly challenging because it involves extracting the underlying intentions solely from visual demonstrations and then applying this operation to a new image. To address this challenge, we formulate visual instruction learning as a diffusion-based inpainting problem, where the contextual information is fully exploited through an iterative process of generation. A visual prompting encoder is carefully devised to enhance the model's capacity in uncovering human intent behind the visual instructions. Extensive experiments show that our method generates engaging manipulation results conforming to the transformations entailed in demonstrations. Moreover, our model exhibits robust generalization capabilities on various downstream tasks such as pose transfer, image translation and video inpainting.</div>
</div>
<div class='paper-counter'>473/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziyi Wu;Jingyu Hu;Wuyue Lu;Igor Gilitschenski;Animesh Garg</div>
<div class="field-name">keywords:</div>
<div class="field-value">Unsupervised object-centric learning;diffusion model;generative modeling</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f2d3a79a58f247d1daef0de12eeaf801dee8761d.pdf" target="_blank">https://openreview.net/pdf/f2d3a79a58f247d1daef0de12eeaf801dee8761d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We incorporate diffusion models to the Slot Attention framework and achieve better segmentation and generation results</div>
<div class="field-name">abstract:</div>
<div class="field-value">Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization.Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery.In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing.However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities.In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation.We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data.Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation across six datasets.Furthermore, our learned object features can be utilized by existing object-centric dynamics models, improving video prediction quality and downstream temporal reasoning tasks.Finally, we demonstrate the scalability of SlotDiffusion to unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained image encoders.</div>
</div>
<div class='paper-counter'>474/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RoboCLIP: One Demonstration is Enough to Learn Robot Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Sumedh Anand Sontakke;Jesse Zhang;Séb Arnold;Karl Pertsch;Erdem Biyik;Dorsa Sadigh;Chelsea Finn;Laurent Itti</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Vision and Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d4ce41c713f1b7bb1260a9c4ff15d714a4bfab0a.pdf" target="_blank">https://openreview.net/pdf/d4ce41c713f1b7bb1260a9c4ff15d714a4bfab0a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1876245eca7c88a662739f291a384c91a911b390.zip" target="_blank">https://openreview.net/attachment/1876245eca7c88a662739f291a384c91a911b390.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use Video-and-Language Models to generate rewards for reinforcement learning agents by specifying tasks using natural language and out-of-domain demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations instead of using an extrinsic reward function but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration. Visit our website at https://sites.google.com/view/roboclip/home for experiment videos.</div>
</div>
<div class='paper-counter'>475/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Online POMDP Planning with Anytime Deterministic Guarantees</div>
<div class="field-name">authors:</div>
<div class="field-value">Moran Barenboim;Vadim Indelman</div>
<div class="field-name">keywords:</div>
<div class="field-value">POMDPs;Planning under uncertainty;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/31ecd9c310a9a42fbe5ff1ce56868b0a0bc4cc42.pdf" target="_blank">https://openreview.net/pdf/31ecd9c310a9a42fbe5ff1ce56868b0a0bc4cc42.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/402f661f6875f980e7280326546fa98a8d2f29ee.pdf" target="_blank">https://openreview.net/attachment/402f661f6875f980e7280326546fa98a8d2f29ee.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that iseasier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior node. Then, since a complete belief update may be computationally demanding, we extend the bounds to support reduction of both the state and the observation spaces. We demonstrate how our guarantees can be integrated with existing state-of-the-art solvers that sample a subset of states and observations. As a result, the returned solution holds deterministic bounds relative to the optimal policy. Lastly, we substantiate our findings with supporting experimental results.</div>
</div>
<div class='paper-counter'>476/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PAPR: Proximity Attention Point Rendering</div>
<div class="field-name">authors:</div>
<div class="field-value">Yanshu Zhang;Shichong Peng;Seyed Alireza Moazenipourasil;Ke Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">point cloud learning;point cloud rendering</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d9641f8767774569b74f913abd82091b2cbb548c.pdf" target="_blank">https://openreview.net/pdf/d9641f8767774569b74f913abd82091b2cbb548c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A point-based 3D representation and rendering method that can learn scene surface geometry using point cloud from scratch</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning.  Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, influence score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: zero-shot geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website at https://zvict.github.io/papr/.</div>
</div>
<div class='paper-counter'>477/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping</div>
<div class="field-name">authors:</div>
<div class="field-value">Feng Zhang;Ming Tian;Zhiqiang Li;Bin Xu;Qingbo Lu;Changxin Gao;Nong Sang</div>
<div class="field-name">keywords:</div>
<div class="field-value">tone mapping; learnable local laplacian filter; laplacian pyramid; 3D lookup table</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f4bc6298372f773f40d8cc8337a786223a07d54c.pdf" target="_blank">https://openreview.net/pdf/f4bc6298372f773f40d8cc8337a786223a07d54c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2ae5a599cd366ba171e508c514d8180ffbcfde1c.pdf" target="_blank">https://openreview.net/attachment/2ae5a599cd366ba171e508c514d8180ffbcfde1c.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Tone mapping aims to convert high dynamic range (HDR) images to low dynamic range (LDR) representations, a critical task in the camera imaging pipeline. In recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained attention due to their ability to strike a favorable balance between enhancement performance and computational efficiency. However, these methods often fail to deliver satisfactory results in local areas since the look-up table is a global operator for tone mapping, which works based on pixel values and fails to incorporate crucial local information. To this end, this paper aims to address this issue by exploring a novel strategy that integrates global and local operators by utilizing closed-form Laplacian pyramid decomposition and reconstruction. Specifically, we employ image-adaptive 3D LUTs to manipulate the tone in the low-frequency image by leveraging the specific characteristics of the frequency information. Furthermore, we utilize local Laplacian filters to refine the edge details in the high-frequency components in an adaptive manner. Local Laplacian filters are widely used to preserve edge details in photographs, but their conventional usage involves manual tuning and fixed implementation within camera imaging pipelines or photo editing tools. We propose to learn parameter value maps progressively for local Laplacian filters from annotated data using a lightweight network. Our model achieves simultaneous global tone manipulation and local edge detail preservation in an end-to-end manner. Extensive experimental results on two benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.</div>
</div>
<div class='paper-counter'>478/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Regularity as Intrinsic Reward for Free Play</div>
<div class="field-name">authors:</div>
<div class="field-value">Cansu Sancaktar;Justus Piater;Georg Martius</div>
<div class="field-name">keywords:</div>
<div class="field-value">Intrinsic Motivation;Reinforcement Learning;Model-based Planning;Regularity;Manipulation;Zero-shot Generalization;Unsupervised Exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6d14c7affdc950661caf850922e76e6575fcf49b.pdf" target="_blank">https://openreview.net/pdf/6d14c7affdc950661caf850922e76e6575fcf49b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose regularity as an intrinsic reward signal, to not bias agents towards chaos by naive novelty seeking objectives and instead favor balance and alignment.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose regularity as a novel reward signal for intrinsically-motivated reinforcement learning. Taking inspiration from child development, we postulate that striving for structure and order helps guide exploration towards a subspace of tasks that are not favored by naive uncertainty-based intrinsic rewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR) allows us to operationalize it within model-based reinforcement learning. In a synthetic environment, we showcase the plethora of structured patterns that can emerge from pursuing this regularity objective. We also demonstrate the strength of our method in a multi-object robotic manipulation environment. We incorporate RaIR into free play and use it to complement the model’s epistemic uncertainty as an intrinsic reward. Doing so, we witness the autonomous construction of towers and other regular structures during free play, which leads to a substantial improvement in zero-shot downstream task performance on assembly tasks.</div>
</div>
<div class='paper-counter'>479/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">POMDP Planning for Object Search in Partially Unknown Environment</div>
<div class="field-name">authors:</div>
<div class="field-value">Yongbo Chen;Hanna Kurniawati</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;Partially Observable Markov Decision Process (POMDP);object search</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d0714366b290d7120f7aa2f32c1cb1e6ba99ef8a.pdf" target="_blank">https://openreview.net/pdf/d0714366b290d7120f7aa2f32c1cb1e6ba99ef8a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/11aef926b871b1a2d7a8eab93dd505ec16ff89fd.pdf" target="_blank">https://openreview.net/attachment/11aef926b871b1a2d7a8eab93dd505ec16ff89fd.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes a novel POMDP formulation and solver with a growing state space for object search in a 3D region using mobile robot.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Efficiently searching for target objects in complex environments that contain various types of furniture, such as shelves, tables, and beds, is crucial for mobile robots, but it poses significant challenges due to various factors such as localization errors, limited field of view, and visual occlusion. To address this problem, we propose a Partially Observable Markov Decision Process (POMDP) formulation with a growing state space for object search in a 3D region. We solve this POMDP by carefully designing a perception module and developing a planning algorithm, called Growing Partially Observable Monte-Carlo Planning (GPOMCP), based on online Monte-Carlo tree search and belief tree reuse with a novel upper confidence bound. We have demonstrated that belief tree reuse is reasonable and achieves good performance when the belief differences are limited. Additionally, we introduce a guessed target object with an updating grid world to guide the search in the information-less and reward-less cases, like the absence of any detected objects. We tested our approach using Gazebo simulations on four scenarios of target finding in a realistic indoor living environment with the Fetch robot simulator. Compared to the baseline approaches, which are based on POMCP, our results indicate that our approach enables the robot to find the target object with a higher success rate faster while using the same computational requirements.</div>
</div>
<div class='paper-counter'>480/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3D-Aware Visual Question Answering about Parts, Poses and Occlusions</div>
<div class="field-name">authors:</div>
<div class="field-value">Xingrui Wang;Wufei Ma;Zhuowan Li;Adam Kortylewski;Alan Yuille</div>
<div class="field-name">keywords:</div>
<div class="field-value">VQA;reasoning;3D scene understanding;analysis-by-synthesis;neural modular network;neuro-symbolic reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f3d804210988ca9533053eda964d97f50935356f.pdf" target="_blank">https://openreview.net/pdf/f3d804210988ca9533053eda964d97f50935356f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d61cff42acc3b6bcbd5a25cfa032ea16d5d5971b.zip" target="_blank">https://openreview.net/attachment/d61cff42acc3b6bcbd5a25cfa032ea16d5d5971b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce the task of 3D-VQA where questions query the object parts, 3D poses and occlusions, and create a dataset Super-CLEVR-3D. A modular model with 3D generative scene parsing is proposed to solve the task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite rapid progress in Visual question answering (\textit{VQA}), existing datasets and models mainly focus on testing reasoning in 2D.  However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation.  This includes an understanding of the 3D object pose, their parts and occlusions.   In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes.   We address 3D-aware VQA from both the dataset and the model perspective.   First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions.   Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition.  Our experimental results show our model PO3D-VQA outperforms existing methods significantly, but we still observe a significant performance gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an important open research area.</div>
</div>
<div class='paper-counter'>481/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Guanlin Liu;Lifeng Lai</div>
<div class="field-name">keywords:</div>
<div class="field-value">adversarial attacks; multi agent reinforcement learning;</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/26a837b7b8402452ff990395f48d8d68d2c66342.pdf" target="_blank">https://openreview.net/pdf/26a837b7b8402452ff990395f48d8d68d2c66342.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/123ded4122cd31aa943ff037da1faac351ae4509.zip" target="_blank">https://openreview.net/attachment/123ded4122cd31aa943ff037da1faac351ae4509.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study the adversarial attacks on online multi-agent reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of the manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents’ algorithms.</div>
</div>
<div class='paper-counter'>482/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuanze Wang;Yichao Yan;Dianxi Shi;Wenhan Zhu;Jianqiang Xia;Tan Jeff;Songchang Jin;KE GAO;XIAOBO LI;Xiaokang Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">NeRF;Image-Based Visual Servoing (IBVS);visual localization;visual navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fe3ad96012ed143aa1339d260c698969996a18e8.pdf" target="_blank">https://openreview.net/pdf/fe3ad96012ed143aa1339d260c698969996a18e8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f5dc14cdeac6a3f9dbe17d880a6224728112811d.zip" target="_blank">https://openreview.net/attachment/f5dc14cdeac6a3f9dbe17d880a6224728112811d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual localization is a fundamental task in computer vision and robotics. Training existing visual localization methods requires a large number of posed images to generalize to novel views, while state-of-the-art methods generally require dense ground truth 3D labels for supervision. However, acquiring a  large number of posed images and dense 3D labels in the real world is challenging and costly. In this paper, we present a novel visual localization method that achieves accurate localization while using only a few posed images compared to other localization methods. To achieve this, we first use a few posed images with coarse pseudo-3D labels provided by NeRF to train a coordinate regression network. Then a coarse pose is estimated from the regression network with PNP. Finally, we use the image-based visual servo (IBVS) with the scene prior provided by NeRF for pose optimization. Furthermore, our method can provide effective navigation prior, which enable navigation based on IBVS without using custom markers and depth sensor. Extensive experiments on 7-Scenes  and 12-Scenes datasets demonstrate that our method outperforms state-of-the-art methods under the same setting, with only 5\% to 25\% training data.  Furthermore, our framework can be naturally extended to the visual navigation task based on IBVS, and its effectiveness is verified in simulation experiments.</div>
</div>
<div class='paper-counter'>483/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffused Task-Agnostic Milestone Planner</div>
<div class="field-name">authors:</div>
<div class="field-value">Mineui Hong;Minjae Kang;Songhwai Oh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-task decision-making;Offline reinforcement learning;Planning;Diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b22de8e6206523bfefabc33f95bd1ccaaaae6468.pdf" target="_blank">https://openreview.net/pdf/b22de8e6206523bfefabc33f95bd1ccaaaae6468.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a7db81b91f27bd52cd2a609312288b94ac2cf25f.zip" target="_blank">https://openreview.net/attachment/a7db81b91f27bd52cd2a609312288b94ac2cf25f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper propose a method to plan latent milestones with diffusion model for multi-task, long-horizon decision-making.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years.In this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making.To this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of milestones in a latent space and to have an agent to follow the milestones to accomplish a given task.The proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control.Furthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making.We demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment.The results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems,while also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark.</div>
</div>
<div class='paper-counter'>484/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Accelerating Motion Planning via Optimal Transport</div>
<div class="field-name">authors:</div>
<div class="field-value">An Thai Le;Georgia Chalvatzaki;Armin Biess;Jan Peters</div>
<div class="field-name">keywords:</div>
<div class="field-value">Motion Planning;Trajectory Optimization;Optimal Transport</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d5b6cbf0c010ea92b372c87eba1a7206ef0fb145.pdf" target="_blank">https://openreview.net/pdf/d5b6cbf0c010ea92b372c87eba1a7206ef0fb145.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The paper proposes a highly parallelizable, efficient gradient-free update rule formulated as an Optimal Transport problem, applying to trajectory optimization for optimizing a batch of high-dimensional trajectories on multiple non-convex objectives.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Motion planning is still an open problem for many disciplines, e.g., robotics, autonomous driving, due to their need for high computational resources that hinder real-time, efficient decision-making. A class of methods striving to provide smooth solutions is gradient-based trajectory optimization. However, those methods usually suffer from bad local minima, while for many settings, they may be inapplicable due to the absence of easy-to-access gradients of the optimization objectives. In response to these issues, we introduce Motion Planning via Optimal Transport (MPOT)---a \textit{gradient-free} method that optimizes a batch of smooth trajectories over highly nonlinear costs, even for high-dimensional tasks, while imposing smoothness through a Gaussian Process dynamics prior via the planning-as-inference perspective. To facilitate batch trajectory optimization, we introduce an original zero-order and highly-parallelizable update rule----the Sinkhorn Step, which uses the regular polytope family for its search directions. Each regular polytope, centered on trajectory waypoints, serves as a local cost-probing neighborhood, acting as a \textit{trust region} where the Sinkhorn Step ``transports'' local waypoints toward low-cost regions. We theoretically show that Sinkhorn Step guides the optimizing parameters toward local minima regions of non-convex objective functions. We then show the efficiency of MPOT in a range of problems from low-dimensional point-mass navigation to high-dimensional whole-body robot motion planning, evincing its superiority compared to popular motion planners, paving the way for new applications of optimal transport in motion planning.</div>
</div>
<div class='paper-counter'>485/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Shared Safety Constraints from Multi-task Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Konwoo Kim;Gokul Swamy;Zuxin Liu;Ding Zhao;Sanjiban Choudhury;Steven Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">constraints;inverse reinforcement learning;safe reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/02c3d6770a69ab1a247fbb6641f5068e05f1fd51.pdf" target="_blank">https://openreview.net/pdf/02c3d6770a69ab1a247fbb6641f5068e05f1fd51.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We learn safety constraints from multi-task demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Regardless of the particular task we want to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task setting to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.</div>
</div>
<div class='paper-counter'>486/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jialong Wu;Haoyu Ma;Chaoyi Deng;Mingsheng Long</div>
<div class="field-name">keywords:</div>
<div class="field-value">Model-based reinforcement learning;world model;pre-training</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ea2be9db0c7ba63d0d0f40cef06fcd31833874b7.pdf" target="_blank">https://openreview.net/pdf/ea2be9db0c7ba63d0d0f40cef06fcd31833874b7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents Contextualized World Models (ContextWM), which utilize pre-training on in-the-wild videos to enable sample-efficient model-based reinforcement learning of visual control tasks in various domains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly separate context and dynamics modeling to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is elaborately realized by incorporating a context encoder to retain contextual information and empower the image decoder, which encourages the latent dynamics model to concentrate on essential temporal variations. Our experiments show that in-the-wild video pre-training equipped with ContextWM can significantly improve the sample efficiency of MBRL in various domains, including robotic manipulation, locomotion, and autonomous driving. Code is available at this repository: https://github.com/thuml/ContextWM.</div>
</div>
<div class='paper-counter'>487/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Tame a Wild Camera: In-the-Wild Monocular Camera Calibration</div>
<div class="field-name">authors:</div>
<div class="field-value">Shengjie Zhu;Abhinav Kumar;Masa Hu;Xiaoming Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Monocular Camera Calibration; Camera Pose Estimation; Image Editing</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/72e46f248ef210c03cbdacbf2c0a1044fad61fb7.pdf" target="_blank">https://openreview.net/pdf/72e46f248ef210c03cbdacbf2c0a1044fad61fb7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2341bfdd3ea7f65a5293959f61e8029b8847cfd3.pdf" target="_blank">https://openreview.net/attachment/2341bfdd3ea7f65a5293959f61e8029b8847cfd3.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We calibrate 4 DoF intrinsic with monocular image. We demonstrate robust in-the-wild performance. We include intriguing applications.</div>
<div class="field-name">abstract:</div>
<div class="field-value">3D sensing for monocular in-the-wild images, e.g., depth estimation and 3D object detection, has become increasingly important.However, the unknown intrinsic parameter hinders their development and deployment.Previous methods for the monocular camera calibration rely on specific 3D objects or strong geometry prior, such as using a checkerboard or imposing a Manhattan World assumption.This work instead calibrates intrinsic via exploiting the monocular 3D prior.Given an undistorted image as input, our method calibrates the complete 4 Degree-of-Freedom (DoF) intrinsic parameters.First, we show intrinsic is determined by the two well-studied monocular priors: monocular depthmap and surface normal map.However, this solution necessitates a low-bias and low-variance depth estimation.Alternatively, we introduce the incidence field, defined as the incidence rays between points in 3D space and pixels in the 2D imaging plane.We show that: 1) The incidence field is a pixel-wise parametrization of the intrinsic invariant to image cropping and resizing.2) The incidence field is a learnable monocular 3D prior, determined pixel-wisely by up-to-sacle monocular depthmap and surface normal.With the estimated incidence field, a robust RANSAC algorithm recovers intrinsic.We show the effectiveness of our method through superior performance on synthetic and zero-shot testing datasets.Beyond calibration, we demonstrate downstream applications in image manipulation detection \& restoration, uncalibrated two-view pose estimation, and 3D sensing.</div>
</div>
<div class='paper-counter'>488/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</div>
<div class="field-name">authors:</div>
<div class="field-value">Arjun Majumdar;Karmesh Yadav;Sergio Arnaud;Yecheng Jason Ma;Claire Chen;Sneha Silwal;Aryan Jain;Vincent-Pierre Berges;Tingfan Wu;Jay Vakil;Pieter Abbeel;Jitendra Malik;Dhruv Batra;Yixin Lin;Oleksandr Maksymets;Aravind Rajeswaran;Franziska Meier</div>
<div class="field-name">keywords:</div>
<div class="field-value">representation learning;pre-training;foundation models;embodied AI;reinforcement learning;imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/76ca2ddcf9b4908e677c396d62609d22ba27c306.pdf" target="_blank">https://openreview.net/pdf/76ca2ddcf9b4908e677c396d62609d22ba27c306.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present the largest and most comprehensive empirical study of visual foundation models for Embodied AI (EAI).</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models’ for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.</div>
</div>
<div class='paper-counter'>489/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Offline Imitation Learning with Variational Counterfactual Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">Zexu Sun;Bowei He;Jinxin Liu;Xu Chen;Chen Ma;Shuai Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline imitaion learning;counterfactual reasoning;data augmentation</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b9a6e171b807e5d95fa53aa8388a0612716edf42.pdf" target="_blank">https://openreview.net/pdf/b9a6e171b807e5d95fa53aa8388a0612716edf42.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/996f18ddee5a744a784ece4454b9d96c9c1271df.pdf" target="_blank">https://openreview.net/attachment/996f18ddee5a744a784ece4454b9d96c9c1271df.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In offline imitation learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments.To automatically generate high-quality expert data and improve the generalization ability of the agent, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA) by doing counterfactual inference. In particular, we leverage identifiable variational autoencoder to generate \textit{counterfactual} samples for expert data augmentation. We theoretically analyze the influence of the generated expert data and the improvement of generalization. Moreover, we conduct extensive experiments to demonstrate that our approach significantly outperforms various baselines on both \textsc{DeepMind Control Suite} benchmark for in-distribution performance and \textsc{CausalWorld} benchmark for out-of-distribution generalization.</div>
</div>
<div class='paper-counter'>490/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Strategic Classification under Unknown Personalized Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Han Shao;Avrim Blum;Omar Montasser</div>
<div class="field-name">keywords:</div>
<div class="field-value">strategic classification;mistake bound in online learning;PAC learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3d3ec74b0db7cec37977f54a2c05add1a3ec8191.pdf" target="_blank">https://openreview.net/pdf/3d3ec74b0db7cec37977f54a2c05add1a3ec8191.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0f6461b91f79fd9ca239b6688c56751e9bae7861.pdf" target="_blank">https://openreview.net/attachment/0f6461b91f79fd9ca239b6688c56751e9bae7861.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study the strategic classification problem when the agents' manipulations are personalized and unknown.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. *Ball manipulations* are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work consider manipulations to be *personalized*, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and *unknown* to the learner.We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classifier. We investigate various scenarios in terms of the information available to the learner during the interaction, such as observing the original feature vector before or after deployment, observing the manipulated feature vector, or not seeing either the original or the manipulated feature vector. We begin by providing online mistake bounds and PAC sample complexity in these scenarios for ball manipulations. We also explore non-ball manipulations and show that, even in the simplest scenario where both the original and the manipulated feature vectors are revealed, the mistake bounds and sample complexity are lower bounded by $\Omega(|\mathcal H|)$ when the target function belongs to a known class $\mathcal H$.</div>
</div>
<div class='paper-counter'>491/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Common Ground in Cooperative Communication</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaoran Hao;Yash Jhaveri;Patrick Shafto</div>
<div class="field-name">keywords:</div>
<div class="field-value">Cooperative Communication;Common Ground;Bayesian Theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/97c898cd59add7a1c150dc64d356275451021f94.pdf" target="_blank">https://openreview.net/pdf/97c898cd59add7a1c150dc64d356275451021f94.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e4d16a61755e5df41f31d8614daab79762ad4f0e.pdf" target="_blank">https://openreview.net/attachment/e4d16a61755e5df41f31d8614daab79762ad4f0e.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Cooperative communication plays a fundamental role in theories of human-human interaction--cognition, culture, development, language, etc.--as well as human-robot interaction. The core challenge in cooperative communication is the problem of common ground: having enough shared knowledge and understanding to successfully communicate. Prior models of cooperative communication, however, uniformly assume the strongest form of common ground, perfect and complete knowledge sharing, and, therefore, fail to capture the core challenge of cooperative communication. We propose a general theory of cooperative communication that is mathematically principled and explicitly defines a spectrum of common ground possibilities, going well beyond that of perfect and complete knowledge sharing, on spaces that permit arbitrary representations of data and hypotheses. Our framework is a strict generalization of prior models of cooperative communication. After considering a parametric form of common ground and viewing the data selection and hypothesis inference processes of communication as encoding and decoding, we establish a connection to variational autoencoding, a powerful model in modern machine learning. Finally, we carry out a series of empirical simulations to support and elaborate on our theoretical results.</div>
</div>
<div class='paper-counter'>492/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Reference-Based POMDPs</div>
<div class="field-name">authors:</div>
<div class="field-value">Edward Kim;Yohan Karunanayake;Hanna Kurniawati</div>
<div class="field-name">keywords:</div>
<div class="field-value">POMDP;planning under uncertainty;long horizon</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/72da2ce63ba174b18408b80efffa23c8bc5bdd7f.pdf" target="_blank">https://openreview.net/pdf/72da2ce63ba174b18408b80efffa23c8bc5bdd7f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0c51531efecb8b00f7bd4895581675952753d7aa.zip" target="_blank">https://openreview.net/attachment/0c51531efecb8b00f7bd4895581675952753d7aa.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Expanding Linearly Solvable MDPs to POMDPs</div>
<div class="field-name">abstract:</div>
<div class="field-value">Making good decisions in partially observable and non-deterministic scenarios is a crucial capability for robots. A Partially Observable Markov Decision Process (POMDP) is a general framework for the above problem. Despite advances in POMDP solving, problems with long planning horizons and evolving environments remain difficult to solve even by the best approximate solvers today. To alleviate this difficulty, we propose a slightly modified POMDP problem, called a Reference-Based POMDP, where the objective is to balance between maximizing the expected total reward and being close to a given reference (stochastic) policy. The optimal policy of a Reference-Based POMDP can be computed via iterative expectations using the given reference policy, thereby avoiding exhaustive enumeration of actions at each belief node of the search tree. We demonstrate theoretically that the standard POMDP under stochastic policies is related to the Reference-Based POMDP. To demonstrate the feasibility of exploiting the formulation, we present a basic algorithm RefSolver. Results from experiments on long-horizon navigation problems indicate that this basic algorithm substantially outperforms POMCP.</div>
</div>
<div class='paper-counter'>493/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms</div>
<div class="field-name">authors:</div>
<div class="field-value">Peiyao Xiao;Hao Ban;Kaiyi Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-objective optimization;multi-task leaning;stochastic algorithms;convergence and complexity;Pareto stationarity</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ee48519a8f34c25474d9c8eefebe36d75522cd43.pdf" target="_blank">https://openreview.net/pdf/ee48519a8f34c25474d9c8eefebe36d75522cd43.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8ac1ffe2ab8352fe1a3bcc273d6744a40f79018d.zip" target="_blank">https://openreview.net/attachment/8ac1ffe2ab8352fe1a3bcc273d6744a40f79018d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective formulation by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling. We develop a comprehensive convergence analysis for the proposed methods with different loop sizes and regularization coefficients. We show that both SDMGrad and SDMGrad-OS achieve improved sample complexities to find an $\epsilon$-accurate Pareto stationary point while achieving a small $\epsilon$-level distance toward a conflict-avoidant (CA) direction. For a constant-level CA distance, their sample complexities match the best known $\mathcal{O}(\epsilon^{-2})$ without bounded function value assumption. Extensive experiments show that our methods achieve competitive or improved performance compared to existing gradient manipulation approaches in a series of tasks on multi-task supervised learning and reinforcement learning. Code is available at https://github.com/ml-opt-lab/sdmgrad.</div>
</div>
<div class='paper-counter'>494/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Effectively Learning Initiation Sets in Hierarchical Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Akhil Bagaria;Ben M Abbatematteo;Omer Gottesman;Matt Corsaro;Sreehari Rammohan;George Konidaris</div>
<div class="field-name">keywords:</div>
<div class="field-value">hierarchical reinforcment learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/686201d24ff299a0a0f67b9cba6cc9ea2527ccc3.pdf" target="_blank">https://openreview.net/pdf/686201d24ff299a0a0f67b9cba6cc9ea2527ccc3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b5a53fbe6ab2564a169b30c67ddc33b12a96155b.pdf" target="_blank">https://openreview.net/attachment/b5a53fbe6ab2564a169b30c67ddc33b12a96155b.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Jointly learning option initiation sets and policies in online reinforcement learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">An agent learning an option in hierarchical reinforcement learning must solve three problems: identify the option's subgoal (termination condition), learn a policy, and learn where that policy will succeed (initiation set). The termination condition is typically identified first, but the option policy and initiation set must be learned simultaneously, which is challenging because the initiation set depends on the option policy, which changes as the agent learns. Consequently, data obtained from option execution becomes invalid over time, leading to an inaccurate initiation set that subsequently harms downstream task performance. We highlight three issues---data non-stationarity, temporal credit assignment, and pessimism---specific to learning initiation sets, and propose to address them using tools from off-policy value estimation and classification. We show that our method learns higher-quality initiation sets faster than existing methods (in MiniGrid and Montezuma's Revenge), can automatically discover promising grasps for robot manipulation (in Robosuite), and improves the performance of a state-of-the-art option discovery method in a challenging maze navigation task in MuJoCo.</div>
</div>
<div class='paper-counter'>495/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes</div>
<div class="field-name">authors:</div>
<div class="field-value">Ali Younis;Erik B. Sudderth</div>
<div class="field-name">keywords:</div>
<div class="field-value">particle;filter;mixture;belief propagation;nonparametric;deep learning;generative;discriminative;graphical model;multiple modes;mutli-modal</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eda6a886b813fc5868ad5bff98c51fb035b8a233.pdf" target="_blank">https://openreview.net/pdf/eda6a886b813fc5868ad5bff98c51fb035b8a233.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.</div>
</div>
<div class='paper-counter'>496/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Evolving Connectivity for Recurrent Spiking Neural Networks</div>
<div class="field-name">authors:</div>
<div class="field-value">Guan Wang;Yuhao Sun;Sijie Cheng;Sen Song</div>
<div class="field-name">keywords:</div>
<div class="field-value">neuromorphic computing;spiking neural networks;evolutionary algorithms;inference-only approach;hardware-friendly;robotic locomotion tasks</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2806297dcfeb94f49d307381b803eef09fbc092a.pdf" target="_blank">https://openreview.net/pdf/2806297dcfeb94f49d307381b803eef09fbc092a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recurrent spiking neural networks (RSNNs) hold great potential for advancing artificial general intelligence, as they draw inspiration from the biological nervous system and show promise in modeling complex dynamics.However, the widely-used surrogate gradient-based training methods for RSNNs are inherently inaccurate and unfriendly to neuromorphic hardware.To address these limitations, we propose the evolving connectivity (EC) framework, an inference-only method for training RSNNs.The EC framework reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions.Our EC framework circumvents the need for gradients and features hardware-friendly characteristics, including sparse boolean connections and high scalability.We evaluate EC on a series of standard robotic locomotion tasks, where it achieves comparable performance with deep neural networks and outperforms gradient-trained RSNNs, even solving the complex 17-DoF humanoid task.Additionally, the EC framework demonstrates a two to three fold speedup in efficiency compared to directly evolving parameters.By providing a performant and hardware-friendly alternative, the EC framework lays the groundwork for further energy-efficient applications of RSNNs and advances the development of neuromorphic devices.Our code is publicly available at https://github.com/imoneoi/EvolvingConnectivity.</div>
</div>
<div class='paper-counter'>497/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Combinatorial Algorithm for Approximating the Optimal Transport in the Parallel and MPC Settings</div>
<div class="field-name">authors:</div>
<div class="field-value">Nathaniel Lahn;Sharath Raghvendra;Kaiyi Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimal Transport;Combinatorial Optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/52408efa119b66c6699d4e359796e46765feb5bd.pdf" target="_blank">https://openreview.net/pdf/52408efa119b66c6699d4e359796e46765feb5bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8921afa945712fed789baa22211b6cafab88b904.zip" target="_blank">https://openreview.net/attachment/8921afa945712fed789baa22211b6cafab88b904.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Optimal Transport is a popular distance metric for measuring similarity between distributions. Exact and approximate combinatorial algorithms for computing the optimal transport distance are hard to parallelize. This has motivated the development of numerical solvers (e.g. Sinkhorn method) that can exploit GPU parallelism and produce approximate solutions. We introduce the first parallel combinatorial algorithm to find an additive $\varepsilon$-approximation of the OT distance. The parallel complexity of our algorithm is $O(\log(n)/ \varepsilon^2)$ where $n$ is the total support size for the input distributions. In Massive Parallel Computation (MPC) frameworks such as Hadoop and MapReduce, our algorithm computes an $\varepsilon$-approximate transport plan in $O(\log (\log (n/\varepsilon))/\varepsilon^2)$ rounds with $O(n/\varepsilon)$ space per machine; all prior algorithms in the MPC framework take $\Omega(\log n)$ rounds. We also provide a GPU-friendly matrix-based interpretation of our algorithm where each step of the algorithm is row or column manipulation of the matrix. Experiments suggest that our combinatorial algorithm is faster than the state-of-the-art approximate solvers in the GPU, especially for higher values of $n$.</div>
</div>
<div class='paper-counter'>498/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Tsun-Hsuan Wang;Juntian Zheng;Pingchuan Ma;Yilun Du;Byungchul Kim;Andrew Everett Spielberg;Joshua B. Tenenbaum;Chuang Gan;Daniela Rus</div>
<div class="field-name">keywords:</div>
<div class="field-value">soft robot;diffusion model;co-design</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/99219790f12d9993715f2e6a2eb03395ccb6b38e.pdf" target="_blank">https://openreview.net/pdf/99219790f12d9993715f2e6a2eb03395ccb6b38e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ffc986d9683f1056512609ca36966932ba11a6bd.pdf" target="_blank">https://openreview.net/attachment/ffc986d9683f1056512609ca36966932ba11a6bd.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Use diffusion models augmented by physics-based simulation to breed soft robots</div>
<div class="field-name">abstract:</div>
<div class="field-value">Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy.  Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. \name bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation.  We showcase a range of simulated and fabricated robots along with their capabilities. Check our website: https://diffusebot.github.io/</div>
</div>
<div class='paper-counter'>499/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Computing a human-like reaction time metric from stable recurrent vision models</div>
<div class="field-name">authors:</div>
<div class="field-value">Lore Goetschalckx;Lakshmi Narasimhan Govindarajan;Alekh Karkada Ashok;Aarit Ahuja;David Sheinberg;Thomas Serre</div>
<div class="field-name">keywords:</div>
<div class="field-value">alignment;RNNs;reaction times;equilibrium dynamics;perceptual grouping;decision making</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3da8375405acf5c66efd426ea49330c2a008771c.pdf" target="_blank">https://openreview.net/pdf/3da8375405acf5c66efd426ea49330c2a008771c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/3301fdd8ff2004e158a84d988dacacbdeb311f37.zip" target="_blank">https://openreview.net/attachment/3301fdd8ff2004e158a84d988dacacbdeb311f37.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The meteoric rise in the adoption of deep neural networks as computational models of vision has inspired efforts to ``align” these models with humans. One dimension of interest for alignment includes behavioral choices, but moving beyond characterizing choice patterns to capturing temporal aspects of visual decision-making has been challenging. Here, we sketch a general-purpose methodology to construct computational accounts of reaction times from a stimulus-computable, task-optimized model. Specifically, we introduce a novel metric leveraging insights from subjective logic theory summarizing evidence accumulation in recurrent vision models. We demonstrate that our metric aligns with patterns of human reaction times for stimulus manipulations across four disparate visual decision-making tasks spanning perceptual grouping, mental simulation, and scene categorization. This work paves the way for exploring the temporal alignment of model and human visual strategies in the context of various other cognitive tasks toward generating testable hypotheses for neuroscience. Links to the code and data can be found on the project page: https://serre-lab.github.io/rnn_rts_site/.</div>
</div>
<div class='paper-counter'>500/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Unseen Modality Interaction</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunhua Zhang;Hazel Doughty;Cees G. M. Snoek</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multimodal Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8e07c3981cd8802cd958b953ef0f6b19ce21d791.pdf" target="_blank">https://openreview.net/pdf/8e07c3981cd8802cd958b953ef0f6b19ce21d791.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/95f82b8a02821d6f9286c5808287a1681382e34f.pdf" target="_blank">https://openreview.net/attachment/95f82b8a02821d6f9286c5808287a1681382e34f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We pose the problem of unseen modality interaction and introduce a first solution that is able to infer predictions over modality combinations unseen during training.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences. In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality’s prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, and multimedia retrieval. Project website: https://xiaobai1217.github.io/Unseen-Modality-Interaction/.</div>
</div>
<div class='paper-counter'>501/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Task-aware Distributed Source Coding under Dynamic Bandwidth</div>
<div class="field-name">authors:</div>
<div class="field-value">Po-han Li;Sravan Kumar Ankireddy;Ruihan Zhao;Hossein Nourkhiz Mahjoub;Ehsan Moradi Pari;ufuk topcu;Sandeep P. Chinchali;Hyeji Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">Data Compression;Distributed Source Coding;Semantic Communication;Multi-sensor Networks;Bandwidth Allocation;Information Theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/221f179c8eae01ae39067d9729c421bafedb1eea.pdf" target="_blank">https://openreview.net/pdf/221f179c8eae01ae39067d9729c421bafedb1eea.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a785ad0191965b4c766e7295911d16d14fcbc86f.pdf" target="_blank">https://openreview.net/attachment/a785ad0191965b4c766e7295911d16d14fcbc86f.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We design a distributed compression framework which learns low-rank task representations and efficiently distributes bandwidth among sensors to provide a trade-off between performance and bandwidth.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Efficient compression of correlated data is essential to minimize communication overload in multi-sensor networks. In such networks, each sensor independently compresses the data and transmits them to a central node. A decoder at the central node decompresses and passes the data to a pre-trained machine learning-based task model to generate the final output. Due to limited communication bandwidth, it is important for the compressor to learn only the features that are relevant to the task. Additionally, the final performance depends heavily on the total available bandwidth. In practice, it is common to encounter varying availability in bandwidth. Since higher bandwidth results in better performance, it is essential for the compressor to dynamically take advantage of the maximum available bandwidth at any instant. In this work, we propose a novel distributed compression framework composed of independent encoders and a joint decoder, which we call neural distributed principal component analysis (NDPCA). NDPCA flexibly compresses data from multiple sources to any available bandwidth with a single model, reducing compute and storage overhead. NDPCA achieves this by learning low-rank task representations and efficiently distributing bandwidth among sensors, thus providing a graceful trade-off between performance and bandwidth. Experiments show that NDPCA improves the success rate of multi-view robotic arm manipulation by 9% and the accuracy of object detection tasks on satellite imagery by 14% compared to an autoencoder with uniform bandwidth allocation.</div>
</div>
<div class='paper-counter'>502/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Convex-Concave Zero-Sum Markov Stackelberg Games</div>
<div class="field-name">authors:</div>
<div class="field-value">Denizalp Goktas;Arjun Prakash;Amy Greenwald</div>
<div class="field-name">keywords:</div>
<div class="field-value">Stackelberg games;Equilibrium Computation;Policy Gradient</div>
<div class="field-name">primary_area:</div>
<div class="field-value"></div>
<div class="field-name">venue:</div>
<div class="field-value">NeurIPS 2023 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bc25457b1c75db4758860283d73c942dc5b87872.pdf" target="_blank">https://openreview.net/pdf/bc25457b1c75db4758860283d73c942dc5b87872.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop policy gradient methods to solve zero-sum stochastic Stackelberg games from observed play, and apply our methods to solve reach avoid problems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Zero-sum Markov Stackelberg games can be used to model myriad problems, in domains ranging from economics to human robot interaction. In this paper, we develop policy gradient methods that solve these games in continuous state and action settings using noisy gradient estimates computed from observed trajectories of play. When the games are convex-concave, we prove that our algorithms converge to Stackelberg equilibrium in polynomial time. We also show that reach-avoid problems are naturally modeled as convex-concave zero-sum Markov Stackelberg games, and that Stackelberg equilibrium policies are more effective than their Nash counterparts in these problems.</div>
</div>
<div class='paper-counter'>503/828</div>
<div class="conference">ICML_cc_2025_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DynaMind: Reasoning over Abstract Video Dynamics for Embodied Decision-Making</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziru Wang;Mengmeng Wang;Jade Dai;Teli Ma;Guo-Jun Qi;Yong Liu;Guang Dai;Jingdong Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">decision-making; language commands; manipulation;</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5e31cd25a0e96d1b70ea7ee79328ee404b62a3bd.pdf" target="_blank">https://openreview.net/pdf/5e31cd25a0e96d1b70ea7ee79328ee404b62a3bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose DynaMind, a framework that enhances decision-making through dynamic reasoning, bridging the gap between modalities by abstracting video content into dynamic representations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Integrating natural language instructions and visual perception with decision-making is a critical challenge for embodied agents. Existing methods often struggle to balance the conciseness of language commands with the richness of video content. To bridge the gap between modalities, we propose extracting key spatiotemporal patterns from video that capture visual saliency and temporal evolution, referred to as dynamic representation. Building on this, we introduce DynaMind, a framework that enhances decision-making through dynamic reasoning. Specifically, we design an adaptive FrameScorer to evaluate video frames based on semantic consistency and visual saliency, assigning each frame an importance score. These scores are used to filter redundant video content and synthesize compact dynamic representations. Leveraging these representations, we predict critical future dynamics and apply a dynamic-guided policy to generate coherent and context-aware actions. Extensive results demonstrate that DynaMind significantly outperforms the baselines across several simulation benchmarks and real-world scenarios.</div>
</div>
<div class='paper-counter'>504/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards</div>
<div class="field-name">authors:</div>
<div class="field-value">Yangsibo Huang;Milad Nasr;Anastasios Nikolas Angelopoulos;Nicholas Carlini;Wei-Lin Chiang;Christopher A. Choquette-Choo;Daphne Ippolito;Matthew Jagielski;Katherine Lee;Ken Liu;Ion Stoica;Florian Tramèr;Chiyuan Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Security;LLM leaderboard;LLM evaluation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->security</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3bd2f87101ad4d811997b5c75001949f26f5ed6a.pdf" target="_blank">https://openreview.net/pdf/3bd2f87101ad4d811997b5c75001949f26f5ed6a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">It is now common to evaluate Large Language Models (LLMs) by having humans manually vote to evaluate model outputs, in contrast to typical benchmarks that evaluate knowledge or skill at some particular task. Chatbot Arena, the most popular benchmark of this type, ranks models by asking users to select the better response between two randomly selected models (without revealing which model was responsible for the generations).  These platforms are widely trusted as a fair and accurate measure of LLM capabilities. In this paper, we show that if bot protection and other defenses are not implemented, these voting-based benchmarks are potentially vulnerable to adversarial manipulation. Specifically, we show that an attacker can alter the leaderboard (to promote their favorite model or demote competitors) at the cost of roughly a thousand votes (verified in a simulated, offline version of Chatbot Arena). Our attack consists of two steps: first, we show how an attacker can determine which model was used to generate a given reply with more than $95\%$ accuracy; and then, the attacker can use this information to consistently vote for (or against) a target model. Working with the Chatbot Arena developers, we identify, propose, and implement mitigations to improve the robustness of Chatbot Arena against adversarial manipulation, which, based on our analysis, substantially increases the cost of such attacks. Some of these defenses were present before our collaboration, such as bot protection with Cloudflare, malicious user detection, and rate limiting. Others, including reCAPTCHA and login are being integrated to strengthen the security in Chatbot Arena.</div>
</div>
<div class='paper-counter'>505/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Mingyu Kang;Yong Suk Choi</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion models;image editing;diffusion inversion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->generative_models_and_autoencoders</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/06c19212ca64137b350847d6cf2e072035833b02.pdf" target="_blank">https://openreview.net/pdf/06c19212ca64137b350847d6cf2e072035833b02.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce ENM Inversion, a technique that improves text-guided image editing by enhancing noise map editability.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.</div>
</div>
<div class='paper-counter'>506/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ABNet: Adaptive explicit-Barrier Net for Safe and Scalable Robot Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Xiao;Tsun-Hsuan Wang;Chuang Gan;Daniela Rus</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safe learning;Robot learning;Scalable learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b9d4dc944430a528ef1b5e27fdeec99aa8bc8598.pdf" target="_blank">https://openreview.net/pdf/b9d4dc944430a528ef1b5e27fdeec99aa8bc8598.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new safe robot learning model/method that is efficient, scalable, robust, and with provable guarantees</div>
<div class="field-name">abstract:</div>
<div class="field-value">Safe learning is central to AI-enabled robots where a single failure may lead to catastrophic results. Existing safe learning methods are not scalable, inefficient and hard to train, and tend to generate unstable signals under noisy inputs that are challenging to be deployed for robots. To address these challenges, we propose Adaptive explicit-Barrier Net (ABNet) in which barriers explicitly show up in the closed-form model that guarantees safety. The ABNet has the potential to incrementally scale toward larger safe foundation models.  Each head of ABNet could learn safe control policies from different features and focuses on specific part of the observation. In this way, we do not need to directly construct a large model for complex tasks, which significantly facilitates the training of the model while ensuring its stable output. Most importantly, we can still formally prove the safety guarantees of the ABNet. We demonstrate the efficiency and strength of ABNet in 2D robot obstacle avoidance, safe robot manipulation, and vision-based end-to-end autonomous driving, with results showing much better robustness and guarantees over existing models.</div>
</div>
<div class='paper-counter'>507/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Ian Gemp;Andreas Alexander Haupt;Luke Marris;Siqi Liu;Georgios Piliouras</div>
<div class="field-name">keywords:</div>
<div class="field-value">Markov Decision Process;Nash Equilibrium;Markov Game;N-player General-Sum;Convex Optimization;Occupancy Measure</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->game_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7aa20db9d10d7fe078b3c4d631f697a47efac1a7.pdf" target="_blank">https://openreview.net/pdf/7aa20db9d10d7fe078b3c4d631f697a47efac1a7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">convex MDPs + multi-agent = convex Markov Game. Non-trivial Nash existence proof. Powerful notions of imitation, exploration, and more enabled by occupancy-measure view.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.</div>
</div>
<div class='paper-counter'>508/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Reinforcement Learning Control of a Physical Robot Device for Assisted Human Walking without a Simulator</div>
<div class="field-name">authors:</div>
<div class="field-value">Junmin Zhong;Emiliano Quinones Yumbla;Seyed Yousef Soltanian;Ruofan Wu;Wenlong Zhang;Jennie Si</div>
<div class="field-name">keywords:</div>
<div class="field-value">Soft Robotics;Human robotic interaction;reinforcement learning without simulator</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b29fb06f76481e8dd38ab85f9167bd53f9ec9a67.pdf" target="_blank">https://openreview.net/pdf/b29fb06f76481e8dd38ab85f9167bd53f9ec9a67.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This study presents an innovative reinforcement learning (RL) control approach to facilitate soft exosuit-assisted human walking. Our goal is to address the ongoing challenges in developing reliable RL-based methods for controlling physical devices. To overcome key obstacles—such as limited data, the absence of a simulator for human-robot interaction during walking, the need for low computational overhead in real-time deployment, and the demand for rapid adaptation to achieve personalized control while ensuring human safety—we propose an online Adaptation from an offline Imitating Expert Policy (AIP) approach. Our offline learning mimics human expert actions through real human walking demonstrations without robot assistance. The resulted policy is then used to initialize online actor-critic learning, the goal of which is to optimally personalize robot assistance. In addition to being fast and robust, our online RL method also posses important properties such as learning convergence, dynamic stability, and solution optimality. We have successfully demonstrated our simpleand robust framework for safe robot control on all five tested human participants, without selectively presenting results. The qualitative performance guarantees provided by our online RL, along with the consistent experimental validation of AIP control, represent the first demonstration of online adaptation for softsuit control personalization and serve as important evidence for the use of online RL in controlling a physical device to solve a real-life problem.</div>
</div>
<div class='paper-counter'>509/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Improving the Continuity of Goal-Achievement Ability via Policy Self-Regularization for Goal-Conditioned Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Xudong Gong;Sen Yang;Feng Dawei;Kele Xu;Bo Ding;Huaimin Wang;Yong Dou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Goal-Conditioned Reinforcement Learning;Policy Regularization;Continuity of Goal-Achievement Ability</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->policy_search</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/54eb0dd4a357075be42e4f628d5d42bd970def94.pdf" target="_blank">https://openreview.net/pdf/54eb0dd4a357075be42e4f628d5d42bd970def94.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a margin-based policy self-regularization approach to improve the continuity of goal-achievement ability for goal-conditioned reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper addresses the challenge of discontinuity in goal-achievement capabilities observed in Goal-conditioned Reinforcement Learning (GCRL) algorithms. Through a theoretical analysis, we identify that the reuse of successful trajectories or policies during training can aid in achieving adjacent goals of achievable goals. However, the policy discrepancy between achievable and adjacent goals must be carefully managed to avoid both overly trivial and excessively large differences, which can respectively hinder policy performance. To tackle this issue, we propose a margin-based policy self-regularization approach that optimizes the policy discrepancies between adjacent desired goals to a minimal acceptable threshold. This method can be integrated into popular GCRL algorithms, such as GC-SAC, HER, and GC-PPO. Systematic evaluations across two robotic arm control tasks and a complex fixed-wing aircraft control task demonstrate that our approach significantly improves the continuity of goal-achievement abilities of GCRL algorithms, thereby enhancing their overall performance.</div>
</div>
<div class='paper-counter'>510/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Universal Approximation of Mean-Field Models via Transformers</div>
<div class="field-name">authors:</div>
<div class="field-value">Shiba Biswal;Karthik Elamvazhuthi;Rishi Sonthalia</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mean Field Equations;Transformers;Universal Approximation;Collective Behavior</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cf1192b940f5300e539d1262b8c9e7613a6aa704.pdf" target="_blank">https://openreview.net/pdf/cf1192b940f5300e539d1262b8c9e7613a6aa704.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We empirically and theoretically show that transformers can approximate infinite dimensional mean field equations</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper investigates the use of transformers to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. First, we empirically demonstrate that transformers are well-suited for approximating a variety of mean field models, including the Cucker-Smale model for flocking and milling, and the mean-field system for training two-layer neural networks. We validate our numerical experiments via mathematical theory. Specifically, we prove that if a finite-dimensional transformer effectively approximates the finite-dimensional vector field governing the particle system, then the $L_\infty$ distance between the \textit{expected transformer} and the infinite-dimensional mean-field vector field can be bounded by a function of the number of particles observed during training. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer.</div>
</div>
<div class='paper-counter'>511/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Latent Diffusion Planning for Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Amber Xie;Oleh Rybkin;Dorsa Sadigh;Chelsea Finn</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;diffusion;planning;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dd116929958176d02e57dd4cab5f0095a08241e0.pdf" target="_blank">https://openreview.net/pdf/dd116929958176d02e57dd4cab5f0095a08241e0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We learn a diffusion-based planner and inverse dynamics model in latent space for imitation learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations.To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data.On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data.</div>
</div>
<div class='paper-counter'>512/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification</div>
<div class="field-name">authors:</div>
<div class="field-value">Flavio Petruzzellis;Cristina Cornelio;Pietro Lio</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neuro-Symbolic;Hierarchical Planning;Robotics;Symbolic Verification;Knowledge Graph RAG</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/558e2ff2c7126b87206d9de23de763294a28a6ee.pdf" target="_blank">https://openreview.net/pdf/558e2ff2c7126b87206d9de23de763294a28a6ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a neurosymbolic method combining hierarchical planning, KG-RAG, and symbolic validation to enhance LLMs for complex task planning, ensuring correctness, failure detection, and offering a tool to evaluate LLM reasoning/compositional skills.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have shown promise as robotic planners but often struggle with long-horizon and complex tasks, especially in specialized environments requiring external knowledge. While hierarchical planning and Retrieval-Augmented Generation (RAG) address some of these challenges, they remain insufficient on their own and a deeper integration is required for achieving more reliable systems. To this end, we propose a neuro-symbolic approach that enhances LLMs-based planners with Knowledge Graph-based RAG for hierarchical plan generation. This method decomposes complex tasks into manageable subtasks, further expanded into executable atomic action sequences. To ensure formal correctness and proper decomposition, we integrate a Symbolic Validator, which also functions as a failure detector by aligning expected and observed world states. Our evaluation against baseline methods demonstrates the consistent significant advantages of integrating hierarchical planning, symbolic verification, and RAG across tasks of varying complexity and different LLMs. Additionally, our experimental setup and novel metrics not only validate our approach for complex planning but also serve as a tool for assessing LLMs' reasoning and compositional capabilities. Code available at https://github.com/corneliocristina/HVR.</div>
</div>
<div class='paper-counter'>513/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Clone-Robust AI Alignment</div>
<div class="field-name">authors:</div>
<div class="field-value">Ariel D. Procaccia;Benjamin Schiffer;Shirley Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">RLHF;AI alignment;social choice</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->alignment</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/340e0a3be5df0e203711ce0d69e0a7e6e64e051c.pdf" target="_blank">https://openreview.net/pdf/340e0a3be5df0e203711ce0d69e0a7e6e64e051c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF can be unbalanced due to adversarial manipulation or inadvertent repetition. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.</div>
</div>
<div class='paper-counter'>514/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Avoiding spurious sharpness minimization broadens applicability of SAM</div>
<div class="field-name">authors:</div>
<div class="field-value">Sidak Pal Singh;Hossein Mobahi;Atish Agarwala;Yann Dauphin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Sharpness Aware Minimization;SAM;Hessian;LLMs;Generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->algorithms</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fe7a3810ed638d67138ad4faad474ce99a65861f.pdf" target="_blank">https://openreview.net/pdf/fe7a3810ed638d67138ad4faad474ce99a65861f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">SAM does not work for LLMs; we diagnose the issue and fix it, resulting in better generalization.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance --- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics --- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs)</div>
</div>
<div class='paper-counter'>515/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Towards Learning to Complete Anything in Lidar</div>
<div class="field-name">authors:</div>
<div class="field-value">Ayça Takmaz;Cristiano Saltori;Neehar Peri;Tim Meinhardt;Riccardo de Lutio;Laura Leal-Taixé;Aljosa Osep</div>
<div class="field-name">keywords:</div>
<div class="field-value">Zero-shot segmentation; Lidar scene completion; Lidar perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8fbe2a59d85d4f1be15c6351679cc46349d858df.pdf" target="_blank">https://openreview.net/pdf/8fbe2a59d85d4f1be15c6351679cc46349d858df.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Zero-Shot Lidar Panoptic Scene Completion</div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies.</div>
</div>
<div class='paper-counter'>516/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Geometric Contact Flows: Contactomorphisms for Dynamics and Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Andrea Testa;Søren Hauberg;Tamim Asfour;Leonel Rozo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Contact geometry;Riemannian geometry;contact Hamiltonian dynamics;Dynamical systems;Diffeomorphisms</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->sequential_network_and_time_series_modeling</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e4b188aceebf1dd61edbfa56754966e0d25e87f1.pdf" target="_blank">https://openreview.net/pdf/e4b188aceebf1dd61edbfa56754966e0d25e87f1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Leveraging Riemannian and Contact geometry as inductive biases to learn and control complex dynamical systems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Accurately modeling and predicting complex dynamical systems, particularly those involving force exchange and dissipation, is crucial for applications ranging from fluid dynamics to robotics, but presents significant challenges due to the intricate interplay of geometric constraints and energy transfer. This paper introduces Geometric Contact Flows (GFC), a novel framework leveraging Riemannian and Contact geometry as inductive biases to learn such systems. GCF constructs a latent contact Hamiltonian model encoding desirable properties like stability or energy conservation. An ensemble of contactomorphisms then adapts this model to the target dynamics while preserving these properties. This ensemble allows for uncertainty-aware geodesics that attract the system’s behavior toward the data support, enabling robust generalization and adaptation to unseen scenarios. Experiments on learning dynamics for physical systems and for controlling robots on interaction tasks demonstrate the effectiveness of our approach.</div>
</div>
<div class='paper-counter'>517/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Constrained Exploitability Descent: An Offline Reinforcement Learning Method for Finding Mixed-Strategy Nash Equilibrium</div>
<div class="field-name">authors:</div>
<div class="field-value">Runyu Lu;Yuanheng Zhu;Dongbin Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline reinforcement learning;adversarial Markov game;mixed-strategy Nash equilibrium;policy constraint;exploitability descent</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->batchoffline</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/368c7528c4ec8622551dd1e1e4de17def8925ae3.pdf" target="_blank">https://openreview.net/pdf/368c7528c4ec8622551dd1e1e4de17def8925ae3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper proposes Constrained Exploitability Descent (CED), a model-free offline reinforcement learning (RL) algorithm for solving adversarial Markov games (MGs). CED combines the game-theoretical approach of Exploitability Descent (ED) with policy constraint methods from offline RL. While policy constraints can perturb the optimal pure-strategy solutions in single-agent scenarios, we find the side effect less detrimental in adversarial games, where the optimal policy can be a mixed-strategy Nash equilibrium. We theoretically prove that, under the uniform coverage assumption on the dataset, CED converges to a stationary point in deterministic two-player zero-sum Markov games. We further prove that the min-player policy at the stationary point follows the property of mixed-strategy Nash equilibrium in MGs. Compared to the model-based ED method that optimizes the max-player policy, our CED method no longer relies on a generalized gradient. Experiments in matrix games, a tree-form game, and an infinite-horizon soccer game verify that CED can find an equilibrium policy for the min-player as long as the offline dataset guarantees uniform coverage. Besides, CED achieves a significantly lower NashConv compared to an existing pessimism-based method and can gradually improve the behavior policy even under non-uniform data coverages. When combined with neural networks, CED also outperforms behavior cloning and offline self-play in a large-scale two-team robotic combat game.</div>
</div>
<div class='paper-counter'>518/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Counting atoms faster: policy-based nuclear magnetic resonance pulse sequencing for atomic abundance measurement</div>
<div class="field-name">authors:</div>
<div class="field-value">Rohan Shenoy;Evan Austen Coleman;Hans Gaensbauer;Elsa Olivetti</div>
<div class="field-name">keywords:</div>
<div class="field-value">NMR;MRI;elemental analysis;atomic abundance;carbon measurement;MRV;RL;magnetization;nuclear spin;pulse sequencing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->chemistry_physics_and_earth_sciences</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/67b86be13dd7e96d8b79ee05ea6ac88be5fc352d.pdf" target="_blank">https://openreview.net/pdf/67b86be13dd7e96d8b79ee05ea6ac88be5fc352d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train a policy which counts atoms of a given element in a sample more quickly than traditional approaches, by controlling a magnetic field to align their nuclear spins.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Quantifying the elemental composition of a material is a general scientific challenge with broad relevance to environmental sustainability. Existing techniques for the measurement of atomic abundances generally require laboratory conditions and expensive equipment. As a result, they cannot be deployed *in situ* without significant capital investment, limiting their proliferation. Measurement techniques based on nuclear magnetic resonance (NMR) hold promise in this setting due to their applicability across the periodic table, their non-destructive manipulation of samples, and their amenability to *in silico* optimization. In this work, we learn policies to modulate NMR pulses for rapid atomic abundance quantification. Our approach involves three inter-operating agents which (1) rapidly align nuclear spins for measurement, (2) quickly force relaxation to equilibrium, and (3) toggle control between agents (1) and (2) to minimize overall measurement time. To demonstrate this technique, we consider a specific use case of low-magnetic-field carbon-13 quantification for low-cost, portable analysis of foodstuffs and soils. We find significant performance improvements relative to traditional NMR pulse sequencing, and discuss limitations on the applicability of this approach.</div>
</div>
<div class='paper-counter'>519/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach</div>
<div class="field-name">authors:</div>
<div class="field-value">Minting Pan;Yitao Zheng;Jiajian Li;Yunbo Wang;Xiaokang Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Reinforcement Learning;Model-based Reinforcement Learning;Visuomotor Control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->batchoffline</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/44f03edf07e0fe5280a2a42c6d05c47f6b4ac75e.pdf" target="_blank">https://openreview.net/pdf/44f03edf07e0fe5280a2a42c6d05c47f6b4ac75e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline reinforcement learning (RL) enables policy optimization using static datasets, avoiding the risks and costs of extensive real-world exploration. However, it struggles with suboptimal offline behaviors and inaccurate value estimation due to the lack of environmental interaction. We present Video-Enhanced Offline RL (VeoRL), a model-based method that constructs an interactive world model from diverse, unlabeled video data readily available online. Leveraging model-based behavior guidance, our approach transfers commonsense knowledge of control policy and physical dynamics from natural videos to the RL agent within the target domain. VeoRL achieves substantial performance gains (over 100% in some cases) across visual control tasks in robotic manipulation, autonomous driving, and open-world video games. Project page: https://panmt.github.io/VeoRL.github.io.</div>
</div>
<div class='paper-counter'>520/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Behavioral Exploration: Learning to Explore via In-Context Adaptation</div>
<div class="field-name">authors:</div>
<div class="field-value">Andrew Wagenmaker;Zhiyuan Zhou;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">in-context learning;exploration;adaptive agents;behavior cloning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/41de08eeadc2b8dbc0929a3d620ff6035a7c7ed5.pdf" target="_blank">https://openreview.net/pdf/41de08eeadc2b8dbc0929a3d620ff6035a7c7ed5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train adaptive policies that utilize in-context learning to enable online exploration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing autonomous agents that quickly explore an environment and adapt their behavior online is a canonical challenge in robotics and machine learning. While humans are able to achieve such fast online exploration and adaptation, often acquiring new information and skills in only a handful of interactions, existing algorithmic approaches tend to rely on random exploration and slow, gradient-based behavior updates. How can we endow autonomous agents with such capabilities on par with humans? Taking inspiration from recent progress on both in-context learning and large-scale behavioral cloning, in this work we propose behavioral exploration: training agents to internalize what it means to explore and adapt in-context over the space of ''expert'' behaviors. To achieve this, given access to a dataset of expert demonstrations, we train a long-context generative model to predict expert actions conditioned on a context of past observations and a measure of how ''exploratory'' the expert's behaviors are relative to this context. This enables the model to not only mimic the behavior of an expert, but also, by feeding its past history of interactions into its context, to select different expert behaviors than what have been previously selected, thereby allowing for fast online adaptation and targeted, ''expert-like'' exploration. We demonstrate the effectiveness of our method in both simulated locomotion and manipulation settings, as well as on real-world robotic manipulation tasks, illustrating its ability to learn adaptive, exploratory behavior.</div>
</div>
<div class='paper-counter'>521/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Suning Huang;Zheyu Aqa Zhang;Tianhai Liang;Yihan Xu;Zhehao Kou;Chenhao Lu;Guowei Xu;Zhengrong Xue;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Visual Reinforcement Learning;Robotics;Mixture-of-Experts</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/260a5d848d814a655b4ae8f55ffde5154fb23c60.pdf" target="_blank">https://openreview.net/pdf/260a5d848d814a655b4ae8f55ffde5154fb23c60.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">MENTOR is a highly efficient visual RL algorithm that excels in both simulation and real-world complex robotic learning tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the *architecture* and *optimization* of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone and introduces a task-oriented perturbation mechanism. MENTOR outperforms state-of-the-art methods across three simulation benchmarks and achieves an average of 83\% success rate on three challenging real-world robotic manipulation tasks, significantly surpassing the 32% success rate of the strongest existing model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available at https://suninghuang19.github.io/mentor_page/.</div>
</div>
<div class='paper-counter'>522/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhenglin Wan;Xingrui Yu;David Mark Bossens;Yueming Lyu;Qing Guo;Flint Xiaofeng Fan;Yew-Soon Ong;Ivor Tsang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Quality Diversity;Reinforcement Learning;Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->inverse</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/49998be209a4986a107a9d760efb2833e277195d.pdf" target="_blank">https://openreview.net/pdf/49998be209a4986a107a9d760efb2833e277195d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We designed a novel Extrinsic Behavior Curiosity module to significantly improve the efficiency and performance of Quality Diversity Robot Locomotion algorithms.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning (IL) has shown promise in robot locomotion but is often limited to learning a single expert policy, constraining behavior diversity and robustness in unpredictable real-world scenarios. To address this, we introduce Quality Diversity Inverse Reinforcement Learning (QD-IRL), a novel framework that integrates quality-diversity optimization with IRL methods, enabling agents to learn diverse behaviors from limited demonstrations. This work introduces Extrinsic Behavioral Curiosity (EBC), which allows agents to receive additional curiosity rewards from an external critic based on how novel the behaviors are with respect to a large behavioral archive. To validate the effectiveness of EBC in exploring diverse locomotion behaviors, we evaluate our method on multiple robot locomotion tasks. EBC improves the performance of QD-IRL instances with GAIL, VAIL, and DiffAIL across all included environments by up to 185\%, 42\%, and 150\%, even surpassing expert performance by 20\% in Humanoid. Furthermore, we demonstrate that EBC is applicable to Gradient-Arborescence-based Quality Diversity Reinforcement Learning  (QD-RL) algorithms, where it substantially improves performance and provides a generic technique for diverse robot locomotion. The source code of this work is provided at https://github.com/vanzll/EBC.</div>
</div>
<div class='paper-counter'>523/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces</div>
<div class="field-name">authors:</div>
<div class="field-value">Amirhossein Roknilamouki;Arnob Ghosh;Ming Shi;Fatemeh Nourzad;Eylem Ekici;Ness Shroff</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Episodic Linear MDP;Constrained RL;Safe RL;Non-Convex RL;Covering number</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9b6a61787576d6790a033d7897a9cdf36b8539b6.pdf" target="_blank">https://openreview.net/pdf/9b6a61787576d6790a033d7897a9cdf36b8539b6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This is the first paper that achieves sub-linear regret with zero violation for linear MDP with non-convex feature space</div>
<div class="field-name">abstract:</div>
<div class="field-value">In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\tilde{\mathcal{O}}((1 + \tfrac{1}{\tau})  \sqrt{\log(\frac{1}{\tau})  d^3 H^4 K})$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $\tau$ the safety threshold.  Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called *Objective–Constraint Decomposition* (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI.</div>
</div>
<div class='paper-counter'>524/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields</div>
<div class="field-name">authors:</div>
<div class="field-value">Gwanhyeong Koo;Sunjae Yoon;Younghwan Lee;Ji Woo Hong;Chang D. Yoo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Drag-based editing;Image editing;Diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->computer_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/810e5099d3c9e5a90ab5867496965ebf7b45f382.pdf" target="_blank">https://openreview.net/pdf/810e5099d3c9e5a90ab5867496965ebf7b45f382.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">FlowDrag leverages 3D mesh deformation to guide stable, geometry-consistent drag-based image editing and introduces VFD-Bench, a benchmark with explicit ground-truth edits.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.</div>
</div>
<div class='paper-counter'>525/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Highly Compressed Tokenizer Can Generate Without Training</div>
<div class="field-name">authors:</div>
<div class="field-value">Lukas Lao Beyer;Tianhong Li;Xinlei Chen;Sertac Karaman;Kaiming He</div>
<div class="field-name">keywords:</div>
<div class="field-value">image tokenizer;1D tokenizer;autoencoder;generative model;text-to-image generation;image editing;training-free</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->generative_models_and_autoencoders</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6c8799183921a9462f5a9fb902c427adb095d3b1.pdf" target="_blank">https://openreview.net/pdf/6c8799183921a9462f5a9fb902c427adb095d3b1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Tokenizers with very high compression ratio possess an expressive latent space suitable for image generation via latent space manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called *1D* image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.</div>
</div>
<div class='paper-counter'>526/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">R*: Efficient Reward Design via Reward Structure Evolution and Parameter Alignment Optimization with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Pengyi Li;Jianye HAO;Hongyao Tang;Yifu Yuan;Jinbin Qiao;Zibin Dong;YAN ZHENG</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reward Design;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->deep_rl</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f4dcdc12830796dc486af237f226b1ebeaaac6b6.pdf" target="_blank">https://openreview.net/pdf/f4dcdc12830796dc486af237f226b1ebeaaac6b6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Reward functions are crucial for policy learning. Large Language Models (LLMs), with strong coding capabilities and valuable domain knowledge, provide an automated solution for high-quality reward design. However, code-based reward functions require precise guiding logic and parameter configurations within a vast design space, leading to low optimization efficiency.To address the challenges,we propose an efficient automated reward design framework, called R*,which decomposes reward design into two parts: reward structure evolution and parameter alignment optimization. To design high-quality reward structures, R* maintains a reward function population and modularizes the functional components. LLMs are employed as the mutation operator, and module-level crossover is proposed to facilitate efficient exploration and exploitation.To design more efficient reward parameters, R* first leverages LLMs to generate multiple critic functions for trajectory comparison and annotation. Based on these critics, a voting mechanism is employed to collect the trajectory segments with high-confidence labels.These labeled segments are then used to refine the reward function parameters through preference learning.Experiments on diverse robotic control tasks demonstrate that R* outperforms strong baselines in both reward design efficiency and quality, surpassing human-designed reward functions.</div>
</div>
<div class='paper-counter'>527/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On the Convergence of Continuous Single-timescale Actor-critic</div>
<div class="field-name">authors:</div>
<div class="field-value">Xuyang Chen;Lin Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">single-timescale actor-critic;continuous state-action space;Markovian sampling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7d85a777c09091d22ffd9e5e02b84a92f5f3e396.pdf" target="_blank">https://openreview.net/pdf/7d85a777c09091d22ffd9e5e02b84a92f5f3e396.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper establishes convergence guarantees for single-timescale actor-critic methods in continuous state-action spaces with Markovian sampling.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Actor-critic algorithms have been instrumental in boosting the performance of numerous challenging applications involving continuous control, such as highly robust and agile robot motion control. However, their theoretical understanding remains largely underdeveloped. Existing analyses mostly focus on finite state-action spaces and on simplified variants of actor-critic, such as double-loop updates with i.i.d. sampling, which are often impractical for real-world applications.We consider the canonical and widely adopted single-timescale updates with Markovian sampling in continuous state-action space. Specifically, we establish finite-time convergence by introducing a novel Lyapunov analysis framework, which provides a unified convergence characterization of both the actor and the critic. Our approach is less conservative than previous methods and offers new insights into the coupled dynamics of actor-critic updates.</div>
</div>
<div class='paper-counter'>528/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Efficient Robotic Garment Manipulation with Standardization</div>
<div class="field-name">authors:</div>
<div class="field-value">zhou changshi;Feng Luan;hujiarui;Shaoqiang Meng;Zhipeng Wang;Yanchao Dong;Yanmin Zhou;Bin He</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deformable Object Manipulation，Bimanual Manipulation，Self-supervised learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/47fcdbf73c093ee4c08930b8a02d3dcc6dfe6eb9.pdf" target="_blank">https://openreview.net/pdf/47fcdbf73c093ee4c08930b8a02d3dcc6dfe6eb9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Garment manipulation is a significant challenge for robots due to the complex dynamics and potential self-occlusion of garments. Most existing methods of efficient garment unfolding overlook the crucial role of standardization of flattened garments, which could significantly simplify downstream tasks like folding, ironing, and packing. This paper presents APS-Net, a novel approach to garment manipulation that combines unfolding and standardization in a unified framework. APS-Net employs a dual-arm, multi-primitive policy with dynamic fling to quickly unfold crumpled garments and pick-and-place(p&p)  for precise alignment. The purpose of garment standardization during unfolding involves not only maximizing surface coverage but also aligning the garment’s shape and orientation to predefined requirements. To guide effective robot learning, we introduce a novel factorized reward function for standardization, which incorporates garment coverage (Cov), keypoint distance (KD), and intersection-over-union (IoU) metrics. Additionally, we introduce a spatial action mask and an Action Optimized Module to improve unfolding efficiency by selecting actions and operation points effectively. In simulation, APS-Net outperforms state-of-the-art methods for long sleeves, achieving 3.9% better coverage, 5.2% higher IoU, and a 0.14 decrease in KD (7.09% relative reduction). Real-world folding tasks further demonstrate that standardization simplifies the folding process. Project page: https://hellohaia.github.io/APS/</div>
</div>
<div class='paper-counter'>529/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">William H English;Dominic Simon;Sumit Kumar Jha;Rickard Ewetz</div>
<div class="field-name">keywords:</div>
<div class="field-value">Temporal Logic;Grammar Constrained Decoding;Sequence-to-Sequence</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->language_speech_and_dialog</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a0f23a49d6193a534754df8554134ac506b63b6c.pdf" target="_blank">https://openreview.net/pdf/a0f23a49d6193a534754df8554134ac506b63b6c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a framework for translating natural language into TL that exploits improved grounding of atomic predicates, a grammar-forcing training protocol for seq2seq models, and a temporal logic based grammar constrained decoding strategy</div>
<div class="field-name">abstract:</div>
<div class="field-value">Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a grounding of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate grounding, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the grounding and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.</div>
</div>
<div class='paper-counter'>530/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Empowering World Models with Reflection for Embodied Video Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaowei Chi;Chun-Kai Fan;Hengyuan Zhang;Xingqun Qi;Rongyu Zhang;Anthony Chen;Chi-Min Chan;Wei Xue;Qifeng Liu;Shanghang Zhang;Yike Guo</div>
<div class="field-name">keywords:</div>
<div class="field-value">World Model;Generation Model;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/23aece122dd7a7803398dbd38a30936f160fbf10.pdf" target="_blank">https://openreview.net/pdf/23aece122dd7a7803398dbd38a30936f160fbf10.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios.  To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction.  It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at https://sites.google.com/view/icml-eva.</div>
</div>
<div class='paper-counter'>531/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hierarchical Equivariant Policy via Frame Transfer</div>
<div class="field-name">authors:</div>
<div class="field-value">Haibo Zhao;Dian Wang;Yizhe Zhu;Xupeng Zhu;Owen Lewis Howell;Linfeng Zhao;Yaoyao Qian;Robin Walters;Robert Platt</div>
<div class="field-name">keywords:</div>
<div class="field-value">robot learning;imitation learning;robotic manipulation;equivariance</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec17d97a340c0a94da98c935feba37fbb7f4cdf5.pdf" target="_blank">https://openreview.net/pdf/ec17d97a340c0a94da98c935feba37fbb7f4cdf5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a equivariant hierarchical policy learning framework for visuomotor policy learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in hierarchical policy learning highlight the advantages of decomposing systems into high-level and low-level agents, enabling efficient long-horizon reasoning and precise fine-grained control. However, the interface between these hierarchy levels remains underexplored, and existing hierarchical methods often ignore domain symmetry, resulting in the need for extensive demonstrations to achieve robust performance. To address these issues, we propose Hierarchical Equivariant Policy (HEP), a novel hierarchical policy framework. We propose a frame transfer interface for hierarchical policy learning, which uses the high-level agent's output as a coordinate frame for the low-level agent, providing a strong inductive bias while retaining flexibility. Additionally, we integrate domain symmetries into both levels and theoretically demonstrate the system's overall equivariance. HEP achieves state-of-the-art performance in complex robotic manipulation tasks, demonstrating significant improvements in both simulation and real-world settings.</div>
</div>
<div class='paper-counter'>532/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Identifying Neural Dynamics Using Interventional State Space Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Amin Nejatbakhsh;Yixin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Causal dynamical systems;interventions;state space models;photo-stimulation;micro-stimulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->neuroscience_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/beec12861a2cfdbe4dfd4381c902a6451cb1bad5.pdf" target="_blank">https://openreview.net/pdf/beec12861a2cfdbe4dfd4381c902a6451cb1bad5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop interventional state space models, a class of causal models that can predict neural responses under causal perturbations and identify neural dynamics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural circuits produce signals that are complex and nonlinear. To facilitate the understanding of neural dynamics, a popular approach is to fit state space models (SSM) to the data and analyze the dynamics of the low-dimensional latent variables. Despite the power of SSM to explain the dynamics of neural circuits, these models have been shown to merely capture statistical associations in the data and cannot be causally interpreted. Therefore, an important research problem is to build models that can predict neural dynamics under causal manipulations. Here, we propose interventional state-space models (iSSM), a class of causal models that can predict neural responses to novel perturbations. We draw on recent advances in causal dynamical systems and present theoretical results for the identifiability of iSSM. In simulations of the motor cortex, we show that iSSM can recover the true latents and the underlying dynamics. In addition, we illustrate two applications of iSSM in biological datasets. First, we applied iSSM to a dataset of calcium recordings from ALM neurons in mice during photostimulation. Second, we applied iSSM to a dataset of electrophysiological recordings from macaque dlPFC during micro-stimulation. In both cases, we show that iSSM outperforms SSM and results in identifiable parameters. The code is available at https://github.com/amin-nejat/issm.</div>
</div>
<div class='paper-counter'>533/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization</div>
<div class="field-name">authors:</div>
<div class="field-value">Hao Li;Qi Lv;Rui Shao;Xiang Deng;Yinchuan Li;Jianye HAO;Liqiang Nie</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Robotics;Skill Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ec9f3ac506dafdbe54463b35ecf1caa4ca36e1ef.pdf" target="_blank">https://openreview.net/pdf/ec9f3ac506dafdbe54463b35ecf1caa4ca36e1ef.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose STAR,  a framework that advances both skill learning and composition to complete complex behaviors.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation.Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present **S**kill **T**raining with **A**ugmented **R**otation (**STAR**), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ).It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions.Further, to capture the casual relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation.Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12% improvement over the baselines.</div>
</div>
<div class='paper-counter'>534/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures</div>
<div class="field-name">authors:</div>
<div class="field-value">Dongzhe Zheng;Wenjie Mei</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning dynamics;environmental constraints;geometric framework;control barrier functions</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->everything_else</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dd6955c3362020dcf19ebe0c5b9b26e083f2ee70.pdf" target="_blank">https://openreview.net/pdf/dd6955c3362020dcf19ebe0c5b9b26e083f2ee70.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions.</div>
</div>
<div class='paper-counter'>535/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GaussMarker: Robust Dual-Domain Watermark for Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Kecen Li;Zhicong Huang;Xinwen Hou;Cheng Hong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion Models;Watermark</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->security</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b7ba48c02ce5851abe09450d86e2aeba4c6265dd.pdf" target="_blank">https://openreview.net/pdf/b7ba48c02ce5851abe09450d86e2aeba4c6265dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose the first dual-domain watermark for diffusion models and achieve state-of-the-art watermark detection performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the *single-domain* of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first *dual-domain* DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks.GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications.</div>
</div>
<div class='paper-counter'>536/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">WOMD-Reasoning: A Large-Scale Dataset for Interaction Reasoning in Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiheng Li;Cunxin Fan;Chongjian GE;Seth Z. Zhao;Chenran Li;Chenfeng Xu;Huaxiu Yao;Masayoshi Tomizuka;Bolei Zhou;Chen Tang;Mingyu Ding;Wei Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Language Q&A Dataset;Autonomous Driving;Interaction Reasoning;Multi-modal Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/875fc230f62cb80651594cdd59cc1f74af316f66.pdf" target="_blank">https://openreview.net/pdf/875fc230f62cb80651594cdd59cc1f74af316f66.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce WOMD-Reasoning, the largest language dataset focusing on traffic rule-induced interactions in driving.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Language models uncover unprecedented abilities in analyzing driving scenarios, owing to their limitless knowledge accumulated from text-based pre-training. Naturally, they should particularly excel in analyzing rule-based interactions, such as those triggered by traffic laws, which are well documented in texts. However, such interaction analysis remains underexplored due to the lack of dedicated language datasets that address it. Therefore, we propose Waymo Open Motion Dataset-Reasoning (WOMD-Reasoning), a comprehensive large-scale Q&As dataset built on WOMD focusing on describing and reasoning traffic rule-induced interactions in driving scenarios. WOMD-Reasoning also presents by far the largest multi-modal Q&A dataset, with 3 million Q&As on real-world driving scenarios, covering a wide range of driving topics from map descriptions and motion status descriptions to narratives and analyses of agents' interactions, behaviors, and intentions. To showcase the applications of WOMD-Reasoning, we design Motion-LLaVA, a motion-language model fine-tuned on WOMD-Reasoning. Quantitative and qualitative evaluations are performed on WOMD-Reasoning dataset as well as the outputs of Motion-LLaVA, supporting the data quality and wide applications of WOMD-Reasoning, in interaction predictions, traffic rule compliance plannings, etc. The dataset and its vision modal extension are available on https://waymo.com/open/download/. The codes & prompts to build it are available on https://github.com/yhli123/WOMD-Reasoning.</div>
</div>
<div class='paper-counter'>537/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Lucy Xiaoyang Shi;brian ichter;Michael Robert Equi;Liyiming Ke;Karl Pertsch;Quan Vuong;James Tanner;Anna Walling;Haohuan Wang;Niccolo Fusai;Adrian Li-Bell;Danny Driess;Lachy Groom;Sergey Levine;Chelsea Finn</div>
<div class="field-name">keywords:</div>
<div class="field-value">Machine Learning;Robotics;Language;Vision-Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/641fc522a201fc660b34e1224cbf7afa6ace2eee.pdf" target="_blank">https://openreview.net/pdf/641fc522a201fc660b34e1224cbf7afa6ace2eee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Hi Robot enables robots to follow open-ended, complex instructions, adapt to feedback, and interact with humans.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that's not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.Videos are available at https://www.pi.website/research/hirobot</div>
</div>
<div class='paper-counter'>538/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Three-Dimensional Trajectory Prediction with 3DMoTraj Dataset</div>
<div class="field-name">authors:</div>
<div class="field-value">Hao Zhou;Xu Yang;Mingyu Fan;Lu Qi;Xiangtai Li;Ming-Hsuan Yang;Fei Luo</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D trajectory prediction;3D trajectory dataset;Decoupled trajectory prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f3a8e791e48e54d0dbc8c45d62b953e3d54ffe16.pdf" target="_blank">https://openreview.net/pdf/f3a8e791e48e54d0dbc8c45d62b953e3d54ffe16.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce 3DMoTraj, a large-scale dataset for 3D trajectory prediction from underwater vehicles, and propose a decoupled prediction method that significantly reduces the complexity of predicting 3D trajectories.</div>
<div class="field-name">abstract:</div>
<div class="field-value">With the growing interest in embodied and spatial intelligence, accurately predicting trajectories in 3D environments has become increasingly critical. However, no datasets have been explicitly designed to study 3D trajectory prediction. To this end, we contribute a 3D motion trajectory (3DMoTraj) dataset collected from unmanned underwater vehicles (UUVs) operating in oceanic environments. Mathematically, trajectory prediction becomes significantly more complex when transitioning from 2D to 3D. To tackle this challenge, we analyze the prediction complexity of 3D trajectories and propose a new method consisting of two key components: decoupled trajectory prediction and correlated trajectory refinement. The former decouples inter-axis correlations, thereby reducing prediction complexity and generating coarse predictions. The latter refines the coarse predictions by modeling their inter-axis correlations. Extensive experiments show that our method significantly improves 3D trajectory prediction accuracy and outperforms state-of-the-art methods. Both the 3DMoTraj dataset and the method are available at https://github.com/zhouhao94/3DMoTraj.</div>
</div>
<div class='paper-counter'>539/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations</div>
<div class="field-name">authors:</div>
<div class="field-value">Lee Cohen;Connie Hong;Jack Hsieh;Judy Hanwen Shen</div>
<div class="field-name">keywords:</div>
<div class="field-value">fairness;strategic classification;hiring;societal impacts;trustworthy machine learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->fairness</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2194e77b837ce1b6b8587bf012d630957152bff0.pdf" target="_blank">https://openreview.net/pdf/2194e77b837ce1b6b8587bf012d630957152bff0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A new model for LLM strategic manipulations grounded in real world hiring systems</div>
<div class="field-name">abstract:</div>
<div class="field-value">In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and giving some candidates an unfair advantage. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a "two-ticket" scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. We further generalize this approach to an $n$-ticket scheme and prove that hiring outcomes converge to a fixed, group-independent decision, eliminating disparities arising from differential LLM access. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.</div>
</div>
<div class='paper-counter'>540/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">WorldSimBench: Towards Video Generation Models as World Simulators</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiran Qin;Zhelun Shi;Jiwen Yu;Xijun Wang;Enshen Zhou;Lijun Li;Zhenfei Yin;Xihui Liu;Lu Sheng;Jing Shao;LEI BAI;Ruimao Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Vision;World Model，Dataset and Benchmark</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->computer_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f1c6bd4c2d73bfc20572e2ab12746450e3f45775.pdf" target="_blank">https://openreview.net/pdf/f1c6bd4c2d73bfc20572e2ab12746450e3f45775.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Towards Video Generation Models as World Simulators</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulater. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.</div>
</div>
<div class='paper-counter'>541/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Fraud-Proof Revenue Division on Subscription Platforms</div>
<div class="field-name">authors:</div>
<div class="field-value">Abheek Ghosh;Tzeh Yuan Neoh;Nicholas Teh;Giannis Tyrovolas</div>
<div class="field-name">keywords:</div>
<div class="field-value">fraud-proof;revenue division;subscription platforms;mechanism design</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->game_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d553df7a024c6daa74e64ae6238b32c8e07412df.pdf" target="_blank">https://openreview.net/pdf/d553df7a024c6daa74e64ae6238b32c8e07412df.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We study a model of subscription-based platforms where users pay a fixed fee for unlimited access to content, and creators receive a share of the revenue. Existing approaches to detecting fraud predominantly rely on machine learning methods, engaging in an ongoing arms race with bad actors. We explore revenue division mechanisms that inherently disincentivize manipulation. We formalize three types of manipulation-resistance axioms and examine which existing rules satisfy these. We show that a mechanism widely used by streaming platforms, not only fails to prevent fraud, but also makes detecting manipulation computationally intractable. We also introduce a novel rule, ScaledUserProp, that satisfies all three manipulation-resistance axioms. Finally, experiments with both real-world and synthetic streaming data support ScaledUserProp as a fairer alternative compared to existing rules.</div>
</div>
<div class='paper-counter'>542/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Active Fine-Tuning of Multi-Task Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Marco Bagatella;Jonas Hübotter;Georg Martius;Andreas Krause</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;deep reinforcement learning;multi-task reinforcement learning;active learning;fine-tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7a9db97db61d6e05ee18160f8fc66ce9fa9db56c.pdf" target="_blank">https://openreview.net/pdf/7a9db97db61d6e05ee18160f8fc66ce9fa9db56c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We actively query demonstrations when fine-tuning a multi-task policy through behavioral cloning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Pre-trained generalist policies are rapidly gaining relevance in robot learning due to their promise of fast adaptation to novel, in-domain tasks.This adaptation often relies on collecting new demonstrations for a specific task of interest and applying imitation learning algorithms, such as behavioral cloning.However, as soon as several tasks need to be learned, we must decide *which tasks should be demonstrated and how often?*We study this multi-task problem and explore an interactive framework in which the agent *adaptively* selects the tasks to be demonstrated.We propose AMF (Active Multi-task Fine-tuning), an algorithm to maximize multi-task policy performance under a limited demonstration budget by collecting demonstrations yielding the largest information gain on the expert policy.We derive performance guarantees for AMF under regularity assumptions and demonstrate its empirical effectiveness to efficiently fine-tune neural policies in complex and high-dimensional environments.</div>
</div>
<div class='paper-counter'>543/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On the Learnability of Distribution Classes with Adaptive Adversaries</div>
<div class="field-name">authors:</div>
<div class="field-value">Tosca Lechner;Alex Bie;Gautam Kamath</div>
<div class="field-name">keywords:</div>
<div class="field-value">distribution learning;robustness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ac3d8d30d3a814d958572b3b8f95e5017aaa11da.pdf" target="_blank">https://openreview.net/pdf/ac3d8d30d3a814d958572b3b8f95e5017aaa11da.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We consider the question of learnability of distribution classes in the presence of adaptive adversaries -- that is, adversaries capable of intercepting the samples requested by a learner and applying manipulations with full knowledge of the samples before passing it on to the learner. This stands in contrast to oblivious adversaries, who can only modify the underlying distribution the samples come from but not their i.i.d.\ nature. We formulate a general notion of learnability with respect to adaptive adversaries, taking into account the budget of the adversary. We show that learnability with respect to additive adaptive adversaries is a strictly stronger condition than learnability with respect to additive oblivious adversaries.</div>
</div>
<div class='paper-counter'>544/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics</div>
<div class="field-name">authors:</div>
<div class="field-value">Letian Chen;Nina Marie Moorman;Matthew Craig Gombolay</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning from Demonstration;Vision-Language Models;Inverse Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3e312af7cb5200764c051a6d6d5dbce2913d26ad.pdf" target="_blank">https://openreview.net/pdf/3e312af7cb5200764c051a6d6d5dbce2913d26ad.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">ELEMENTAL enables robots to learn user-aligned reward functions by combining user language instructions and demonstration through interactive self-reflection using vision-language models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) has demonstrated compelling performance in robotic tasks, but its success often hinges on the design of complex, ad hoc reward functions. Researchers have explored how Large Language Models (LLMs) could enable non-expert users to specify reward functions more easily. However, LLMs struggle to balance the importance of different features, generalize poorly to out-of-distribution robotic tasks, and cannot represent the problem properly with only text-based descriptions. To address these challenges, we propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a novel framework that combines natural language guidance with visual user demonstrations to align robot behavior with user intentions better. By incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only task specifications, while leveraging inverse reinforcement learning (IRL) to balance feature weights and match the demonstrated behaviors optimally. ELEMENTAL also introduces an iterative feedback-loop through self-reflection to improve feature, reward, and policy learning. Our experiment results demonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and achieves 41.3% better generalization in out-of-distribution tasks, highlighting its robustness in LfD.</div>
</div>
<div class='paper-counter'>545/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Falcon: Fast Visuomotor Policies via Partial Denoising</div>
<div class="field-name">authors:</div>
<div class="field-value">Haojun Chen;Minghao Liu;Chengdong Ma;Xiaojian Ma;Zailin Ma;Huimin Wu;Yuanpei Chen;Yifan Zhong;Mingzhi Wang;Qing Li;Yaodong Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion Models;Acceleration;Sampling;Robotics;Motion Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/807ae299363042d87aed7942d7225452cddfe00a.pdf" target="_blank">https://openreview.net/pdf/807ae299363042d87aed7942d7225452cddfe00a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion policies are widely adopted in complex visuomotor tasks for their ability to capture multimodal action distributions. However, the multiple sampling steps required for action generation significantly harm real-time inference efficiency, which limits their applicability in real-time decision-making scenarios. Existing acceleration techniques either require retraining or degrade performance under low sampling steps. Here we propose Falcon, which mitigates this speed-performance trade-off and achieves further acceleration. The core insight is that visuomotor tasks exhibit sequential dependencies between actions. Falcon leverages this by reusing partially denoised actions from historical information rather than sampling from Gaussian noise at each step. By integrating current observations, Falcon reduces sampling steps while preserving performance. Importantly, Falcon is a training-free algorithm that can be applied as a plug-in to further improve decision efficiency on top of existing acceleration techniques. We validated Falcon in 48 simulated environments and 2 real-world robot experiments. demonstrating a 2-7x speedup with negligible performance degradation, offering a promising direction for efficient visuomotor policy design.</div>
</div>
<div class='paper-counter'>546/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiahui Zhu;Kihyun Yu;Dabeen Lee;Xin Liu;Honghao Wei</div>
<div class="field-name">keywords:</div>
<div class="field-value">Constrained Reinforcement Learning; Constrained MDP; online learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->online_learning_active_learning_and_bandits</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3ca463a5c905d7708f980a4abf8b6958ccdc8eac.pdf" target="_blank">https://openreview.net/pdf/3ca463a5c905d7708f980a4abf8b6958ccdc8eac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propsed a new algorithm to deal with adversarial constraint in CMDP</div>
<div class="field-name">abstract:</div>
<div class="field-value">Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret $\tilde{\mathcal{O}}(\sqrt{K})$ and strong constraint violation $\tilde{\mathcal{O}}(\sqrt{K})$ without relying on Slater’s condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.</div>
</div>
<div class='paper-counter'>547/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks</div>
<div class="field-name">authors:</div>
<div class="field-value">Varshita Kolipaka;Akshit Sinha;Debangan Mishra;Sumit Kumar;Arvindh Arun;Shashwat Goel;Ponnurangam Kumaraguru</div>
<div class="field-name">keywords:</div>
<div class="field-value">Machine Unlearning;Graphs;Graph Neural Networks;GNN;Attacks;Manipulations;Removal</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->graph_neural_networks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c1e206ad952417ca1e216529f936a6700784a75e.pdf" target="_blank">https://openreview.net/pdf/c1e206ad952417ca1e216529f936a6700784a75e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose Cognac, a method for unlearning adverse effects of manipulated data from GNNs. It achieves near Oracle performance with just 5% of the manipulated set is identified for deletion, beating retraining while being 8x efficient.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. Because graph data does not follow the independently and identically distributed *i.i.d.* assumption, adversarial manipulations or incorrect data can propagate to other data points through message passing, which deteriorates the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of *Corrective Unlearning*. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method,**Cognac**, which can unlearn the effect of the manipulation set even when only $5$% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set, and is $8$x more efficient while also scaling to large datasets. We hope our work assists GNN developers in mitigating harmful effects caused by issues in real-world data, post-training.</div>
</div>
<div class='paper-counter'>548/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">QPRL : Learning Optimal Policies with Quasi-Potential Functions for Asymmetric Traversal</div>
<div class="field-name">authors:</div>
<div class="field-value">Jumman Hossain;Nirmalya Roy</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Quasi-Potential Functions;Quasimetric Learning;Asymmetric Traversal Costs</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3d168cc7ead4f262bbe611ac34c2f967630cf568.pdf" target="_blank">https://openreview.net/pdf/3d168cc7ead4f262bbe611ac34c2f967630cf568.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Introduces a theoretical framework for reinforcement learning using quasi-potential functions to handle asymmetric traversal costs</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) in real-world tasks such as robotic navigation often encounters environments with asymmetric traversal costs, where actions like climbing uphill versus moving downhill incur distinctly different penalties, or transitions may become irreversible. While recent quasimetric RL methods relax symmetry assumptions, they typically do not explicitly account for path-dependent costs or provide rigorous safety guarantees. We introduce Quasi-Potential Reinforcement Learning (QPRL), a novel framework that explicitly decomposes asymmetric traversal costs into a path-independent potential function ($\Phi$) and a path-dependent residual ($\Psi$). This decomposition allows efficient learning and stable policy optimization via a Lyapunov-based safety mechanism. Theoretically, we prove that QPRL achieves convergence with improved sample complexity of $\tilde{O}(\sqrt{T})$, surpassing prior quasimetric RL bounds of $\tilde{O}(T)$. Empirically, our experiments demonstrate that QPRL attains state-of-the-art performance across various navigation and control tasks, significantly reducing irreversible constraint violations by approximately $4\times$ compared to baselines.</div>
</div>
<div class='paper-counter'>549/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VIP: Vision Instructed Pre-training for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhuoling Li;LiangLiang Ren;Jinrong Yang;Yong Zhao;Xiaoyang Wu;Zhenhua Xu;Xiang Bai;Hengshuang Zhao</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision instruction;robotic manipulation pre-training;imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fc80bd3b42c458d1d871411db0d2aec7f70c9c37.pdf" target="_blank">https://openreview.net/pdf/fc80bd3b42c458d1d871411db0d2aec7f70c9c37.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose how to utilize vision instruction to improve robotic manipulation pre-training.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The effectiveness of scaling up training data in robotic manipulation is still limited. A primary challenge in manipulation is the tasks are diverse, and the trained policy would be confused if the task targets are not specified clearly. Existing works primarily rely on text instruction to describe targets. However, we reveal that current robotic data cannot train policies to understand text instruction effectively, and vision is much more comprehensible. Therefore, we introduce utilizing vision instruction to specify targets. A straightforward implementation is training a policy to predict the intermediate actions linking the current observation and a future image. Nevertheless, a single future image does not describe the task target in insufficient detail. To handle this problem, we propose to use sparse point flows to provide more detailed information. Extensive tasks are designed based on real and simulated environments to evaluate the effectiveness of our vision instructed pre-training (VIP) method. The results indicate VIP improves the performance on diverse tasks significantly, and the derived policy can complete competitive tasks like ``opening the lid of a tightly sealed bottle''.</div>
</div>
<div class='paper-counter'>550/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Luise Ge;Michael Lanier;Anindya Sarkar;Bengisu Guresti;Chongjie Zhang;Yevgeniy Vorobeychik</div>
<div class="field-name">keywords:</div>
<div class="field-value">Policy Committee; Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fa017bddff24af3985ab40687de91c154965c358.pdf" target="_blank">https://openreview.net/pdf/fa017bddff24af3985ab40687de91c154965c358.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We build small committees of policies to handle diverse tasks. Each task is matched with a policy that performs nearly optimally. This ensures fairness by helping most users do well.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Many dynamic decision problems, such as robotic control, involve a series of tasks, many of which are unknown at training time.Typical approaches for these problems, such as multi-task and meta reinforcement learning, do not generalize well when the tasks are diverse. On the other hand, approaches that aim to tackle task diversity, such as using task embedding as policy context and task clustering, typically lack performance guarantees and require a large number of training tasks. To address these challenges, we propose a novel approach for learning a policy committee that includes at least one near-optimal policy with high probability for tasks encountered during execution. While we show that this problem is in general inapproximable, we present two practical algorithmic solutions.The first yields provable approximation and task sample complexity guarantees when tasks are low-dimensional (the best we can do due to inapproximability), whereas the second is a general and practical gradient-based approach. In addition, we provide a provable sample complexity bound for few-shot learning. Our experiments on MuJoCo and Meta-World show that the proposed approach outperforms state-of-the-art multi-task, meta-, and task clustering baselines in training, generalization, and few-shot learning, often by a large margin. Our code is available at https://github.com/CERL-WUSTL/PACMAN.</div>
</div>
<div class='paper-counter'>551/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Yucheng Hu;Yanjiang Guo;Pengchao Wang;Xiaoyu Chen;Yen-Jen Wang;Jianke Zhang;Koushil Sreenath;Chaochao Lu;Jianyu Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot policy learning;diffusion model;inverse dynamics model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c025e8a373675e3fd6bbaf74928a639cda9e5a28.pdf" target="_blank">https://openreview.net/pdf/c025e8a373675e3fd6bbaf74928a639cda9e5a28.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a generalist robot policy, Video Prediction Policy which conditioned on visual representaions inside video diffusion models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data.In experiments, VPP achieves a 18.6\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\% increase in success rates for complex real-world dexterous manipulation tasks. For your convenience, videos can be found at https://video-prediction-policy.github.io/</div>
</div>
<div class='paper-counter'>552/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoquan Fang;Markus Grotz;Wilbert Pumacay;Yi Ru Wang;Dieter Fox;Ranjay Krishna;Jiafei Duan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Behavior Cloning;Imitation Learning;Memory-based Robotics Transformer</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8037fc274c936f562d9ecc1ca364a02611213f98.pdf" target="_blank">https://openreview.net/pdf/8037fc274c936f562d9ecc1ca364a02611213f98.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Memory-based Multi View Robotics Transformer</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduce **SAM2Act**, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of **86.8% across 18 tasks** in the RLBench benchmark, and demonstrates robust generalization on *The Colosseum* benchmark, with only a **4.3% performance gap** under diverse environmental perturbations. Building on this foundation, we propose **SAM2Act+**, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduce ***MemoryBench***, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achieves an average success rate of **94.3% on memory-based tasks** in *MemoryBench*, significantly outperforming existing approaches and pushing the boundaries of memory-based robotic systems.Project page: [sam2act.github.io](https://sam2act.github.io/).</div>
</div>
<div class='paper-counter'>553/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Motoki Omura;Kazuki Ota;Takayuki Osa;Yusuke Mukuta;Tatsuya Harada</div>
<div class="field-name">keywords:</div>
<div class="field-value">online reinforcement learning;q-learning;bellman operator</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->deep_rl</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c55c407ae270ad6eaf4917a72f08dc1ba66bf165.pdf" target="_blank">https://openreview.net/pdf/c55c407ae270ad6eaf4917a72f08dc1ba66bf165.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality.</div>
</div>
<div class='paper-counter'>554/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges</div>
<div class="field-name">authors:</div>
<div class="field-value">Nayoung Lee;Ziyang Cai;Avi Schwarzschild;Kangwook Lee;Dimitris Papailiopoulos</div>
<div class="field-name">keywords:</div>
<div class="field-value">self-improvement;length generalization;self-training</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/26a8b210db30decc38f18798e4172a6c41c0557e.pdf" target="_blank">https://openreview.net/pdf/26a8b210db30decc38f18798e4172a6c41c0557e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models often struggle with length generalization and solving complex problem instances beyond their training distribution. We present a self-improvement approach where models iteratively generate and learn from their own solutions, progressively tackling harder problems while maintaining a standard transformer architecture. Across diverse tasks including arithmetic, string manipulation, and maze solving, our method enables models to solve problems far beyond their initial training distribution—for instance, generalizing from 10-digit to 100-digit addition without apparent saturation. We observe that filtering for correct self-generated examples leads to exponential improvements in out-of-distribution performance across training rounds. Additionally, starting from pretrained models significantly accelerates this self-improvement process for several tasks. Our results demonstrate how controlled weak-to-strong curricula can systematically expand model capabilities while preserving architectural simplicity.</div>
</div>
<div class='paper-counter'>555/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinmin He;Kai Li;Yifan Zang;Haobo Fu;QIANG FU;Junliang Xing;Jian Cheng</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Multi-Task Reinforcement Learning;Skill Abstraction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->batchoffline</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/240822ad8c2e1665f7b0a1d393f3f898a877724b.pdf" target="_blank">https://openreview.net/pdf/240822ad8c2e1665f7b0a1d393f3f898a877724b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline multi-task reinforcement learning aims to learn a unified policy capable of solving multiple tasks using only pre-collected task-mixed datasets, without requiring any online interaction with the environment. However, it faces significant challenges in effectively sharing knowledge across tasks. Inspired by the efficient knowledge abstraction observed in human learning, we propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed to extract and utilize reusable skills to enhance knowledge transfer and task performance. Our approach uncovers reusable skills through a goal-oriented skill extraction process and leverages vector quantization to construct a discrete skill library. To mitigate class imbalances between broadly applicable and task-specific skills, we introduce a skill enhancement phase to refine the extracted skills. Furthermore, we integrate these skills using hierarchical policy learning, enabling the construction of a high-level policy that dynamically orchestrates discrete skills to accomplish specific tasks. Extensive experiments on diverse robotic manipulation tasks within the MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.</div>
</div>
<div class='paper-counter'>556/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Cansu Sancaktar;Christian Gumbsch;Andrii Zadaianchuk;Pavel Kolev;Georg Martius</div>
<div class="field-name">keywords:</div>
<div class="field-value">intrinsic motivation;exploration;foundation models;model-based RL</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->deep_rl</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/abcb4060fc1aace78a2b79538ab740f7e1b1c925.pdf" target="_blank">https://openreview.net/pdf/abcb4060fc1aace78a2b79538ab740f7e1b1c925.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose SENSEI to equip model-based RL agents with intrinsic motivation for semantically meaningful exploration using VLMs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Exploration is a cornerstone of reinforcement learning (RL). Intrinsic motivation attempts to decouple exploration from external, task-based rewards. However, established approaches to intrinsic motivation that follow general principles such as information gain, often only uncover low-level interactions. In contrast, children’s play suggests that they engage in meaningful high-level behavior by imitating or interacting with their caregivers. Recent work has focused on using foundation models to inject these semantic biases into exploration. However, these methods often rely on unrealistic assumptions, such as language-embedded environments or access to high-level actions. We propose SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL agents with an intrinsic motivation for semantically meaningful behavior. SENSEI distills a reward signal of interestingness from Vision Language Model (VLM) annotations, enabling an agent to predict these rewards through a world model. Using model-based RL, SENSEI trains an exploration policy that jointly maximizes semantic rewards and uncertainty. We show that in both robotic and video game-like simulations SENSEI discovers a variety of meaningful behaviors from image observations and low-level actions. SENSEI provides a general tool for learning from foundation model feedback, a crucial research direction, as VLMs become more powerful.</div>
</div>
<div class='paper-counter'>557/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Perception in Reflection</div>
<div class="field-name">authors:</div>
<div class="field-value">Yana Wei;Liang Zhao;Kangheng Lin;En Yu;Yuang Peng;Runpei Dong;Jianjian Sun;Haoran Wei;Zheng Ge;Xiangyu Zhang;Vishal M. Patel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Perception;Reflection;LVLMs</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->large_language_models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c5ea1810d4cb95156bc7b915d3a3b0fc99c886b4.pdf" target="_blank">https://openreview.net/pdf/c5ea1810d4cb95156bc7b915d3a3b0fc99c886b4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a perception in reflection paradigm designed to transcend the limitations of current large vision-language models (LVLMs), which are expected yet often fail to achieve perfect perception initially. Specifically, we propose Reflective Perception (RePer), a dual-model reflection mechanism that systematically alternates between policy and critic models, enables iterative refinement of visual perception. This framework is powered by Reflective Perceptual Learning (RPL), which reinforces intrinsic reflective capabilities through a methodically constructed visual reflection dataset and reflective unlikelihood training Comprehensive experimental evaluation demonstrates RePer's quantifiable improvements in image understanding, captioning precision, and hallucination reduction. Notably, RePer achieves strong alignment between model attention patterns and human visual focus, while RPL optimizes fine-grained and free-form preference alignment. These advancements establish perception in reflection as a robust paradigm for future multimodal agents, particularly in tasks requiring complex reasoning and multi-step manipulation. Project Page: [https://weiyana.github.io/Perception-in-Reflection](https://weiyana.github.io/Perception-in-Reflection)</div>
</div>
<div class='paper-counter'>558/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning the RoPEs: Better 2D and 3D Position Encodings with STRING</div>
<div class="field-name">authors:</div>
<div class="field-value">Connor Schenck;Isaac Reid;Mithun George Jacob;Alex Bewley;Joshua Ainslie;David Rendleman;Deepali Jain;Mohit Sharma;Kumar Avinava Dubey;Ayzaan Wahid;Sumeet Singh;René Wagner;Tianli Ding;Chuyuan Fu;Arunkumar Byravan;Jake Varley;Alexey A. Gritsenko;Matthias Minderer;Dmitry Kalashnikov;Jonathan Tompson;Vikas Sindhwani;Krzysztof Marcin Choromanski</div>
<div class="field-name">keywords:</div>
<div class="field-value">position encodings for Transformers;RoPE;Robotics;translation-invariance;Lie algebra</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->algorithms</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8e699c1959ea794ad97590c4cac42051d885b886.pdf" target="_blank">https://openreview.net/pdf/8e699c1959ea794ad97590c4cac42051d885b886.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces STRING: a general class of separable translation-invariant position encodings for Transformers (extending Rotary Position Encodings), and applies them in a wide range of 2D and 3D scenarios, in particular in Robotics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce $\textbf{STRING}$: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings, a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides $\textbf{exact}$ translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers.We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods. Videos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-robotics.</div>
</div>
<div class='paper-counter'>559/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SOLD: Slot Object-Centric Latent Dynamics Models for Relational Manipulation Learning from Pixels</div>
<div class="field-name">authors:</div>
<div class="field-value">Malte Mosbach;Jan Niklas Ewertz;Angel Villar-Corrales;Sven Behnke</div>
<div class="field-name">keywords:</div>
<div class="field-value">Model-based Reinforcement Learning;Object-centric Learning;World Models;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->deep_rl</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/aebf40c9e02d26ac1158cfde3307359e8305b284.pdf" target="_blank">https://openreview.net/pdf/aebf40c9e02d26ac1158cfde3307359e8305b284.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce an object-centric model-based RL algorithm that learns solely from pixels, enhancing interpretability and outperforming state-of-the-art methods in robotic tasks requiring reasoning and manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning (RL) holds the potential to improve sample efficiency over model-free methods by learning from imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment’s state. In contrast, humans reason about objects and their interactions, predicting how actions will affect specific parts of their surroundings. Inspired by this, we propose *Slot-Attention for Object-centric Latent Dynamics (SOLD)*, a novel model-based RL algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3 and TD-MPC2 - state-of-the-art model-based RL algorithms - across a range of multi-object manipulation environments that require both relational reasoning and dexterous control. Videos and code are available at https:// slot-latent-dynamics.github.io.</div>
</div>
<div class='paper-counter'>560/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Safe Control via On-the-Fly Bandit Exploration</div>
<div class="field-name">authors:</div>
<div class="field-value">Alexandre Capone;Ryan Kazuo Cosner;Aaron Ames;Sandra Hirche</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safety;Gaussian processes;control barrier functions;Bayesian models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f18a0aa156891ad44af4ea9ad3e09634bf984234.pdf" target="_blank">https://openreview.net/pdf/f18a0aa156891ad44af4ea9ad3e09634bf984234.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We guarantee safe control by learning on the fly, without requiring a prior model or backup controller</div>
<div class="field-name">abstract:</div>
<div class="field-value">Control tasks with safety requirements under high levels of model uncertainty are increasingly common. Machine learning techniques are frequently used to address such tasks, typically by leveraging model error bounds to specify robust constraint-based safety filters. However, if the learned model uncertainty is very high, the corresponding filters are potentially invalid, meaning no control input satisfies the constraints imposed by the safety filter. While most works address this issue by assuming some form of safe backup controller, ours tackles it by collecting additional data on the fly using a Gaussian process bandit-type algorithm. We combine a control barrier function with a learned model to specify a robust certificate that ensures safety if feasible. Whenever infeasibility occurs, we leverage the control barrier function to guide exploration, ensuring the collected data contributes toward the closed-loop system safety. By combining a safety filter with exploration in this manner, our method provably achieves safety in a general setting that does not require any prior model or backup controller, provided that the true system lies in a reproducing kernel Hilbert space. To the best of our knowledge, it is the first safe learning-based control method that achieves this.</div>
</div>
<div class='paper-counter'>561/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Mehrdad Moghimi;Hyejin Ku</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Distributional Reinforcement Learning;Risk Aversion;Spectral Risk Measures;Time-Consistency</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->online</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e7819b7440a4ce04c671daaf3a7cb797870c0093.pdf" target="_blank">https://openreview.net/pdf/e7819b7440a4ce04c671daaf3a7cb797870c0093.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In domains such as finance, healthcare, and robotics, managing worst-case scenarios is critical, as failure to do so can lead to catastrophic outcomes. Distributional Reinforcement Learning (DRL) provides a natural framework to incorporate risk sensitivity into decision-making processes. However, existing approaches face two key limitations: (1) the use of fixed risk measures at each decision step often results in overly conservative policies, and (2) the interpretation and theoretical properties of the learned policies remain unclear. While optimizing a static risk measure addresses these issues, its use in the DRL framework has been limited to the simple static CVaR risk measure. In this paper, we present a novel DRL algorithm with convergence guarantees that optimizes for a broader class of static Spectral Risk Measures (SRM). Additionally, we provide a clear interpretation of the learned policy by leveraging the distribution of returns in DRL and the decomposition of static coherent risk measures. Extensive experiments demonstrate that our model learns policies aligned with the SRM objective, and outperforms existing risk-neutral and risk-sensitive DRL models in various settings.</div>
</div>
<div class='paper-counter'>562/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffusionVLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression</div>
<div class="field-name">authors:</div>
<div class="field-value">Junjie Wen;Yichen Zhu;Minjie Zhu;Zhibin Tang;Jinming Li;Zhongyi Zhou;Xiaoyu Liu;Chaomin Shen;Yaxin Peng;Feifei Feng</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision-language-action models;reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d9ad5d722d8a8e6e1a4f5748391ef1c439c2c706.pdf" target="_blank">https://openreview.net/pdf/d9ad5d722d8a8e6e1a4f5748391ef1c439c2c706.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we present DiffusionVLA, a novel framework that integrates autoregressive reasoning with diffusion policies to address the limitations of existing methods: while autoregressive Vision-Language-Action (VLA) models lack precise and robust action generation, diffusion-based policies inherently lack reasoning capabilities. Central to our approach is autoregressive reasoning — a task decomposition and explanation process enabled by a pre-trained VLM — to guide diffusion-based action policies. To tightly couple reasoning with action generation, we introduce a reasoning injection module that directly embeds self-generated reasoning phrases into the policy learning process. The framework is simple, flexible, and efficient, enabling seamless deployment across diverse robotic platforms.We conduct extensive experiments using multiple real robots to validate the effectiveness of DiVLA. Our tests include a challenging factory sorting task, where DiVLA successfully categorizes objects, including those not seen during training. The reasoning injection module enhances interpretability, enabling explicit failure diagnosis by visualizing the model’s decision process. Additionally, we test DiVLA on a zero-shot bin-picking task, achieving \textbf{63.7\% accuracy on 102 previously unseen objects}. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiVLA can follow novel instructions and retain conversational ability. Notably, DiVLA is data-efficient and fast at inference; our smallest DiVLA-2B runs 82Hz on a single A6000 GPU. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.</div>
</div>
<div class='paper-counter'>563/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Harsh Goel;Mohammad Omama;Behdad Chalaki;Vaishnav Tadiparthi;Ehsan Moradi Pari;Sandeep P. Chinchali</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-agent Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->multiagent</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/857ab7d68934c04396d35f8beabafd85af467833.pdf" target="_blank">https://openreview.net/pdf/857ab7d68934c04396d35f8beabafd85af467833.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Improving multi-agent reinforcement learning by introducing a new information theoretic metric that enables diversity through world models</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agent's past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agent’s role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agents' roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%. The code is available at https://github.com/UTAustin-SwarmLab/R3DM.</div>
</div>
<div class='paper-counter'>564/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">UP-VLA:  A Unified Understanding and Prediction Model for Embodied Agent</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianke Zhang;Yanjiang Guo;Yucheng Hu;Xiaoyu Chen;Xiang Zhu;Jianyu Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">VLA;VLM;Embodied Agent</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a31d9729845e48950a82af3a4935b4f181940e6e.pdf" target="_blank">https://openreview.net/pdf/a31d9729845e48950a82af3a4935b4f181940e6e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose UP-VLA，a unified understanding and prediction model for embodied agent.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities.VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus onhigh-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics.These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms.In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33\% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.</div>
</div>
<div class='paper-counter'>565/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification</div>
<div class="field-name">authors:</div>
<div class="field-value">Hang Gao;Huang Wenxuan;Fengge Wu;Zhao Junsuo;Changwen Zheng;Huaping Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large language models; Graph Neural Netwroks; Causal</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->graph_neural_networks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bbd409c657d453c02ba784c570133f8c0cda3a8b.pdf" target="_blank">https://openreview.net/pdf/bbd409c657d453c02ba784c570133f8c0cda3a8b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This study analyzes the under-explored properties of LLM-enhanced GNNs using interchange interventions and proposes an optimization module to improve information transfer.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.</div>
</div>
<div class='paper-counter'>566/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</div>
<div class="field-name">authors:</div>
<div class="field-value">Huang Huang;Fangchen Liu;Letian Fu;Tingfan Wu;Mustafa Mukadam;Jitendra Malik;Ken Goldberg;Pieter Abbeel</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision language action model; robot foundation model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7fad0feb536c6adfcc1f93202cc2a447ee101254.pdf" target="_blank">https://openreview.net/pdf/7fad0feb536c6adfcc1f93202cc2a447ee101254.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A vision-language model using a frozen VLM for strong zero-shot generalization ability</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained vision-language models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zero-shot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.</div>
</div>
<div class='paper-counter'>567/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space</div>
<div class="field-name">authors:</div>
<div class="field-value">Xupeng Zhu;Fan Wang;Robin Walters;Jane Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Equivariance;Closed-Loop Policy;Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c5f710d0d4c4b6918752b49ce065c915bf7bdde3.pdf" target="_blank">https://openreview.net/pdf/c5f710d0d4c4b6918752b49ce065c915bf7bdde3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An SE(3) equivariant visuomotor policy based on spherical Fourier representation that is sample efficient and generalize to novel 3D arrangement of the scene.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion Policies are effective at learning closed-loop manipulation policies from human demonstrations but generalize poorly to novel arrangements of objects in 3D space, hurting real-world performance. To address this issue, we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion policy that adapts trajectories according to 3D transformations of the scene. Such equivariance is achieved by embedding the states, actions, and the denoising process in spherical Fourier space. Additionally, we employ novel spherical FiLM layers to condition the action denoising process equivariantly on the scene embeddings. Lastly, we propose a spherical denoising temporal U-net that achieves spatiotemporal equivariance with computational efficiency. In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization across transformed 3D scenes. SDP demonstrates a large performance improvement over strong baselines in 20 simulation tasks and 5 physical robot tasks including single-arm and bi-manual embodiments. Code is available at https://github.com/amazon-science/Spherical_Diffusion_Policy.</div>
</div>
<div class='paper-counter'>568/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoyuan Cai;Zhenghao Peng;Bolei Zhou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Human-in-the-loop Reinforcement Learning;Shared Autonomy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2d69f8e47d89e3b4def482200fa47dd038d8586f.pdf" target="_blank">https://openreview.net/pdf/2d69f8e47d89e3b4def482200fa47dd038d8586f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency.Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM.</div>
</div>
<div class='paper-counter'>569/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Manan Tayal;Aditya Singh;Shishir Kolathaya;Somil Bansal</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safety-Performance co-optimization;Safety-critical controls;Physics-informed ML;conformal prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6dc6e9465bdfa74d09c46633e154b57006e2dcc5.pdf" target="_blank">https://openreview.net/pdf/6dc6e9465bdfa74d09c46633e154b57006e2dcc5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a novel approach to co-optimize safety and performance in autonomous systems using Physics- Informed Machine Learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.</div>
</div>
<div class='paper-counter'>570/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinhao Liang;Jacob K Christopher;Sven Koenig;Ferdinando Fioretto</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-Agent Path Planning;Diffusion Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->generative_models_and_autoencoders</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d76930933d1acce5a0a7cec4fc6c587d2ee7f7aa.pdf" target="_blank">https://openreview.net/pdf/d76930933d1acce5a0a7cec4fc6c587d2ee7f7aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a method combining diffusion models with constrained optimization techniques to generate feasible multi-robot trajectories in complex and highly constrained environments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in diffusion models hold significant potential in robotics, enabling the generation of diverse and smooth trajectories directly from raw representations of the environment. Despite this promise, applying diffusion models to motion planning remains challenging due to their difficulty in enforcing critical constraints, such as collision avoidance and kinematic feasibility. These limitations become even more pronounced in Multi-Robot Motion Planning (MRMP), where multiple robots must coordinate in shared spaces. To address these challenges, this work proposes **S**imultaneous **M**RMP **D**iffusion (SMD), a novel approach integrating constrained optimization into the diffusion sampling process to produce collision-free, kinematically feasible trajectories. Additionally, the paper introduces a comprehensive MRMP benchmark to evaluate trajectory planning algorithms across scenarios with varying robot densities, obstacle complexities, and motion constraints. Experimental results show SMD consistently outperforms classical and other learning-based motion planners, achieving higher success rates and efficiency in complex multi-robot environments. The code and implementation are available at https://github.com/RAISELab-atUVA/Diffusion-MRMP.</div>
</div>
<div class='paper-counter'>571/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability</div>
<div class="field-name">authors:</div>
<div class="field-value">Chen Wei;Chi Zhang;Jiachen Zou;Haotian Deng;Dietmar Heinke;Quanying Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Perceptual variability;Object Recognition;Behavior Manipulation;Behavioral Alignment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->neuroscience_cognitive_science</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2a2c9dbea08a02e5f79c34d64c54d3c859ff1a22.pdf" target="_blank">https://openreview.net/pdf/2a2c9dbea08a02e5f79c34d64c54d3c859ff1a22.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">By sampling along neural network perceptual boundaries, we generated images that induce high variability in human decision and can predict and manipulate individual behavior on these samples.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. We propose a systematic Boundary Alignment Manipulation (BAM) framework for studying human perceptual variability through image generation. BAM combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. Our perceptual boundary sampling algorithm generates stimuli along ANN perceptual boundaries that intrinsically induce significant perceptual variability. The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images.Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants.This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis. Code and data for this work are publicly available.</div>
</div>
<div class='paper-counter'>572/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">The Case for Learned Provenance-based System Behavior Baseline</div>
<div class="field-name">authors:</div>
<div class="field-value">Yao Zhu;Zhenyuan LI;Yangyang Wei;Shouling Ji</div>
<div class="field-name">keywords:</div>
<div class="field-value">graph representation learning;provenance graph;cyber attack detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->everything_else</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/61d7364f96cf17ee1b3b33863614fce3e139f4a5.pdf" target="_blank">https://openreview.net/pdf/61d7364f96cf17ee1b3b33863614fce3e139f4a5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Provenance graphs describe data flows and causal dependencies of host activities, enabling to track the data propagation and manipulation throughout the systems, which provide a foundation for intrusion detection. However, these Provenance-based Intrusion Detection Systems (PIDSes) face significant challenges in storage, representation, and analysis, which impede the efficacy of machine learning models such as Graph Neural Networks (GNNs) in processing and learning from these graphs. This paper presents a novel learning-based anomaly detection method designed to efficiently embed and analyze large-scale provenance graphs. Our approach integrates dynamic graph processing with adaptive encoding, facilitating compact embeddings that effectively address out-of-vocabulary (OOV) elements and adapt to normality shifts in dynamic real-world environments. Subsequently, we incorporate this refined baseline into a tag-propagation framework for real-time detection. Our evaluation demonstrates the method's accuracy and adaptability in anomaly path mining, significantly advancing the state-of-the-art in handling and analyzing provenance graphs for anomaly detection.</div>
</div>
<div class='paper-counter'>573/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Analyze Feature Flow to Enhance Interpretation and Steering in Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Daniil Laptev;Nikita Balagansky;Yaroslav Aksenov;Daniil Gavrilov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mechansitic Interpretability;Sparse Autoencoders;Steering;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->large_language_models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/261623ac8df014504f2d6362e3ea5f4577766589.pdf" target="_blank">https://openreview.net/pdf/261623ac8df014504f2d6362e3ea5f4577766589.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.</div>
</div>
<div class='paper-counter'>574/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DriveGPT: Scaling Autoregressive Behavior Models for Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Xin Huang;Eric M Wolff;Paul Vernaza;Tung Phan-Minh;Hongge Chen;David S Hayden;Mark Edmonds;Brian Pierce;Xinxin Chen;Pratik Elias Jacob;Xiaobai Chen;Chingiz Tairbekov;Pratik Agarwal;Tianshi Gao;Yuning Chai;Siddhartha Srinivasa</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;Foundation Models;Behavior Modeling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1c8a2dedadefc245fba9c98bebf14f59b514db75.pdf" target="_blank">https://openreview.net/pdf/1c8a2dedadefc245fba9c98bebf14f59b514db75.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our paper studies the scaling properties in driving, across multiple orders of magnitude in data, model size, and compute.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.</div>
</div>
<div class='paper-counter'>575/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PlaySlot: Learning Inverse Latent Dynamics for Controllable Object-Centric Video Prediction and Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Angel Villar-Corrales;Sven Behnke</div>
<div class="field-name">keywords:</div>
<div class="field-value">object-centric video prediction;object-centric learning;inverse dynamics learning;action planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->representation_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/33d77557841d2fc65f4758813db95e734e6bc8a1.pdf" target="_blank">https://openreview.net/pdf/33d77557841d2fc65f4758813db95e734e6bc8a1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose PlaySlot-- an object-centric video prediction model that infers object representations and latent actions from unlabeled video sequences and uses them to forecast future video frames.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Predicting future scene representations is a crucial task for enabling robots to understand andinteract with the environment. However, most existing methods rely on videos and simulationswith precise action annotations, limiting their ability to leverage the large amount of avail-able unlabeled video data. To address this challenge, we propose PlaySlot, an object-centricvideo prediction model that infers object representations and latent actions from unlabeledvideo sequences. It then uses these representations to forecast future object states and videoframes. PlaySlot allows the generation of multiple possible futures conditioned on latent actions,which can be inferred from video dynamics, provided by a user, or generated by a learned actionpolicy, thus enabling versatile and interpretable world modeling. Our results show that PlaySlotoutperforms both stochastic and object-centric baselines for video prediction across different environments. Furthermore, we show that our inferred latent actions can be used to learn robot behaviors sample-efficiently from unlabeled videodemonstrations. Videos and code are available on our project website.</div>
</div>
<div class='paper-counter'>576/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diversified Flow Matching with Translation Identifiability</div>
<div class="field-name">authors:</div>
<div class="field-value">Sagar Shrestha;Xiao Fu</div>
<div class="field-name">keywords:</div>
<div class="field-value">unsupervised domain translation;flow matching;GAN;identifiability</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->unsupervised_and_semisupervised_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/59116253aa21d47f8737be645edf37fe0e8e6029.pdf" target="_blank">https://openreview.net/pdf/59116253aa21d47f8737be645edf37fe0e8e6029.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information---yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces *diversified flow matching* (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.</div>
</div>
<div class='paper-counter'>577/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly</div>
<div class="field-name">authors:</div>
<div class="field-value">Yan Shen;Ruihai Wu;Yubin Ke;Xinyuan Song;Zeyi Li;Xiaoqi Li;Hongwei Fan;Haoran Lu;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Bimanual Manipulation;Geometric Shape Assembly</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3f8c56cedcdc53d1c605231977fd1d60f17575af.pdf" target="_blank">https://openreview.net/pdf/3f8c56cedcdc53d1c605231977fd1d60f17575af.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce BiAssemble, a framework that learns bimanual collaborative affordances for long-horizon geometric shape assembly task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Shape assembly, the process of combining parts into a complete whole, is a crucial skill for robots with broad real-world applications. Among the various assembly tasks, geometric assembly—where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)—is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity  of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods.</div>
</div>
<div class='paper-counter'>578/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</div>
<div class="field-name">authors:</div>
<div class="field-value">Jing Huang;Junyi Tao;Thomas Icard;Diyi Yang;Christopher Potts</div>
<div class="field-name">keywords:</div>
<div class="field-value">Causal Abstraction;Causal Interpretability;OOD;Correctness Prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->accountability_transparency_and_interpretability</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7911ddddfbc3d4e474661ed5176febdc66db3aeb.pdf" target="_blank">https://openreview.net/pdf/7911ddddfbc3d4e474661ed5176febdc66db3aeb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Internal causal mechanisms robustly predict language model out-of-distribution behaviors.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks—including symbol manipulation, knowledge retrieval, and instruction following—we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model’s behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.</div>
</div>
<div class='paper-counter'>579/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SAH-Drive: A Scenario-Aware Hybrid Planner for Closed-Loop Vehicle Trajectory Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuqi Fan;Zhiyong Cui;Zhenning Li;Yilong Ren;Haiyang Yu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion model;autonomous driving;hybrid planner;trajectory planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b9753655e43bdf7980dcc9f4cb71ce02f1c06bb4.pdf" target="_blank">https://openreview.net/pdf/b9753655e43bdf7980dcc9f4cb71ce02f1c06bb4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">SAH-Drive combines a lightweight rule-based planner and an extensive learning-based planner, utilizing a dual-timescale decision neuron to determine the final trajectory.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reliable planning is crucial for achieving autonomous driving. Rule-based planners are efficient but lack generalization, while learning-based planners excel in generalization yet have limitations in real-time performance and interpretability. In long-tail scenarios, these challenges make planning particularly difficult. To leverage the strengths of both rule-based and learning-based planners, we proposed the **Scenario-Aware Hybrid Planner** (SAH-Drive) for closed-loop vehicle trajectory planning. Inspired by human driving behavior, SAH-Drive combines a lightweight rule-based planner and a comprehensive learning-based planner, utilizing a dual-timescale decision neuron to determine the final trajectory. To enhance the computational efficiency and robustness of the hybrid planner, we also employed a diffusion proposal number regulator and a trajectory fusion module. The experimental results show that the proposed method significantly improves the generalization capability of the planning system, achieving state-of-the-art performance in interPlan, while maintaining computational efficiency without incurring substantial additional runtime.</div>
</div>
<div class='paper-counter'>580/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scalable Gaussian Processes with Latent Kronecker Structure</div>
<div class="field-name">authors:</div>
<div class="field-value">Jihao Andreas Lin;Sebastian Ament;Maximilian Balandat;David Eriksson;José Miguel Hernández-Lobato;Eytan Bakshy</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaussian process;Kronecker;Bayesian;scalable;product kernel;gridded data;missing values</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic_methods->gaussian_processes</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8409c0bcee96cb6eeb4c206f08ea118645780e05.pdf" target="_blank">https://openreview.net/pdf/8409c0bcee96cb6eeb4c206f08ea118645780e05.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">How to leverage efficient product kernels on gridded data with missing values for scalable GP regression</div>
<div class="field-name">abstract:</div>
<div class="field-value">Applying Gaussian processes (GPs) to very large datasets remains a challenge due to limited computational scalability. Matrix structures, such as the Kronecker product, can accelerate operations significantly, but their application commonly entails approximations or unrealistic assumptions. In particular, the most common path to creating a Kronecker-structured kernel matrix is by evaluating a product kernel on gridded inputs that can be expressed as a Cartesian product. However, this structure is lost if any observation is missing, breaking the Cartesian product structure, which frequently occurs in real-world data such as time series. To address this limitation, we propose leveraging latent Kronecker structure, by expressing the kernel matrix of observed values as the projection of a latent Kronecker product. In combination with iterative linear system solvers and pathwise conditioning, our method facilitates inference of exact GPs while requiring substantially fewer computational resources than standard iterative methods. We demonstrate that our method outperforms state-of-the-art sparse and variational GPs on real-world datasets with up to five million examples, including robotics, automated machine learning, and climate applications.</div>
</div>
<div class='paper-counter'>581/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Action-Constrained Imitation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Chia-Han Yeh;Tse-Sheng Nan;Risto Vuorio;Wei Hung;Hung Yen Wu;Shao-Hua Sun;Ping-Chun Hsieh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning; Action Constraints</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d016e9ae9fd9a657b5c6c6ac08cbbeaa5c3f00de.pdf" target="_blank">https://openreview.net/pdf/d016e9ae9fd9a657b5c6c6ac08cbbeaa5c3f00de.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study a new problem setting termed Action-Constrained Imitation Learning and propose a method  to mitigate the mismatch of occupancy measures between the expert and the learner caused by the action constraints.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Policy learning under action constraints plays a central role in ensuring safe behaviors in various robot control and resource allocation applications.In this paper, we study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space.The fundamental challenge of ACIL lies in the unavoidable mismatch of occupancy measure between the expert and the imitator caused by the action constraints. We tackle this mismatch through trajectory alignment and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via Model Predictive Control, which aligns the surrogate trajectories with the expert trajectories based on the Dynamic Time Warping (DTW) distance. Through extensive experiments, we demonstrate that learning from the dataset generated by DTWIL significantly enhances performance across multiple robot control tasks and outperforms various benchmark imitation learning algorithms in terms of sample efficiency.</div>
</div>
<div class='paper-counter'>582/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Distributional Diffusion Models with Scoring Rules</div>
<div class="field-name">authors:</div>
<div class="field-value">Valentin De Bortoli;Alexandre Galashov;J Swaroop Guntupalli;Guangyao Zhou;Kevin Patrick Murphy;Arthur Gretton;Arnaud Doucet</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion models;energy distance;maximum mean discrepancy;scoring rules;accelerated sampling of diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7215d2d3de147b4ab2ade2c9f4e4af9e10961136.pdf" target="_blank">https://openreview.net/pdf/7215d2d3de147b4ab2ade2c9f4e4af9e10961136.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">New losses for diffusion based on scoring rules; Better quality sampling in few steps regime</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to  data until fully corrupted. The corresponding reverse process progressively ``denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to speed up sample generation by learning the posterior distribution of  clean data samples  given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is  accomplished by replacing the standard regression loss used to estimate conditional means with a  scoring rule. We validate our method on  image and robot trajectory generation, where we  consistently outperform standard diffusion models at few discretization steps.</div>
</div>
<div class='paper-counter'>583/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Shuanghao Bai;Wanqi Zhou;Pengxiang Ding;Wei Zhao;Donglin Wang;Badong Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Behavior Cloning;Robot Manipulation;Information Bottleneck</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/98f4eb99c4d588d5a848ac5a50ca770cf8e51ed7.pdf" target="_blank">https://openreview.net/pdf/98f4eb99c4d588d5a848ac5a50ca770cf8e51ed7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Integration of information theory with behavior cloning in robot manipulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks show consistent performance improvements with IB across various settings, underscoring the importance of reducing input data redundancy and highlighting its practical value for real-world applications.</div>
</div>
<div class='paper-counter'>584/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongyin Zhang;Zifeng Zhuang;Han Zhao;Pengxiang Ding;Hongchao Lu;Donglin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">VLA Model;Reinforcement Learning;Robotic Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/06fee7a1122ea26338330e0d4ace4117ec6c3ca6.pdf" target="_blank">https://openreview.net/pdf/06fee7a1122ea26338330e0d4ace4117ec6c3ca6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Reinforced Robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principles of maximizing cumulative reward.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.</div>
</div>
<div class='paper-counter'>585/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling</div>
<div class="field-name">authors:</div>
<div class="field-value">Jinghan Li;Zhicheng Sun;Yadong MU</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Equilibrium Models;Large Language Models;Robot Task Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/06a16c0c4ce21d6815efd9caa5780495d31d595b.pdf" target="_blank">https://openreview.net/pdf/06a16c0c4ce21d6815efd9caa5780495d31d595b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An equilibrium model-based LLM planner capable of self-refining plans based on external and internal feedback.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/anonymous-icml-2025/equilibrium-planner.</div>
</div>
<div class='paper-counter'>586/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Power Mean Estimation in Stochastic Continuous Monte-Carlo Tree Search</div>
<div class="field-name">authors:</div>
<div class="field-value">Tuan Quang Dam</div>
<div class="field-name">keywords:</div>
<div class="field-value">Monte-Carlo Tree Search; Continuous Reinforcement Learning Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d0a41d7d7350a1b4807f14d20ac55f8728fade4e.pdf" target="_blank">https://openreview.net/pdf/d0a41d7d7350a1b4807f14d20ac55f8728fade4e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a theoretically-guaranteed planning algorithm that enables making optimal continuous decisions in unpredictable environments, outperforming existing methods on robotic control tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Monte-Carlo Tree Search (MCTS) has demonstrated success in online planning for deterministic environments, yet significant challenges remain in adapting it to stochastic Markov Decision Processes (MDPs), particularly in continuous state-action spaces. Existing methods, such as HOOT, which combines MCTS with the Hierarchical Optimistic Optimization (HOO) bandit strategy, address continuous spaces but rely on a logarithmic exploration bonus that lacks theoretical guarantees in non-stationary, stochastic settings. Recent advancements, such as Poly-HOOT, introduced a polynomial bonus term to achieve convergence in deterministic MDPs, though a similar theory for stochastic MDPs remains undeveloped. In this paper, we propose a novel MCTS algorithm, Stochastic-Power-HOOT, designed for continuous, stochastic MDPs. Stochastic-Power-HOOT integrates a power mean as a value backup operator, alongside a polynomial exploration bonus to address the non-stationarity inherent in continuous action spaces. Our theoretical analysis establishes that Stochastic-Power-HOOT converges at a polynomial rate of $\mathcal{O}(n^{-1/2})$, where \( n \) is the number of visited trajectories, thereby extending the non-asymptotic convergence guarantees of Poly-HOOT to stochastic environments. Experimental results on synthetic and stochastic tasks validate our theoretical findings, demonstrating the effectiveness of Stochastic-Power-HOOT in continuous, stochastic domains.</div>
</div>
<div class='paper-counter'>587/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Stop: Deep Learning for Mean Field Optimal Stopping</div>
<div class="field-name">authors:</div>
<div class="field-value">Lorenzo Magnino;Yuchen Zhu;Mathieu Lauriere</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimal Stopping;Deep Learning;Mean Field Games</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->algorithms</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/97920973841ddf0fdcbd37ca85737d6d37ce2a5b.pdf" target="_blank">https://openreview.net/pdf/97920973841ddf0fdcbd37ca85737d6d37ce2a5b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Optimal stopping is a fundamental problem in optimization with applications in risk management, finance, robotics, and machine learning. We extend the standard framework to a multi-agent setting, named multi-agent optimal stopping (MAOS), where agents cooperate to make optimal stopping decisions in a finite-space, discrete-time environment. Since solving MAOS becomes computationally prohibitive as the number of agents is very large, we study the mean-field optimal stopping (MFOS) problem, obtained as the number of agents tends to infinity. We establish that MFOS provides a good approximation to MAOS and prove a dynamic programming principle (DPP) based on mean-field control theory. We then propose two deep learning approaches: one that learns optimal stopping decisions by simulating full trajectories and another that leverages the DPP to compute the value function and to learn the optimal stopping rule using backward induction. Both methods train neural networks to approximate optimal stopping policies. We demonstrate the effectiveness  and the scalability of our work through numerical experiments on 6 different problems in spatial dimension up to 300. To the best of our knowledge, this is the first work to formalize and computationally solve MFOS in discrete time and finite space, opening new directions for scalable MAOS methods.</div>
</div>
<div class='paper-counter'>588/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Should Decision-Makers Reveal Classifiers in Online Strategic Classification?</div>
<div class="field-name">authors:</div>
<div class="field-value">Han Shao;Shuo Xie;Kunhe Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">strategic classification;mistake bound;online learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->game_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b8f96cf594948b8261e10c3055f951077a99e9b4.pdf" target="_blank">https://openreview.net/pdf/b8f96cf594948b8261e10c3055f951077a99e9b4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Strategic classification addresses a learning problem where a decision-maker implements a classifier over agents who may manipulate their features in order to receive favorable predictions. In the standard model of online strategic classification, in each round, the decision-maker implements and publicly reveals a classifier, after which agents perfectly best respond based on this knowledge. However, in practice, whether to disclose the classifier is often debated---some decision-makers believe that hiding the classifier can prevent misclassification errors caused by manipulation. In this paper, we formally examine how limiting the agents' access to the current classifier affects the decision-maker's performance. Specifically, we consider an extended online strategic classification setting where agents lack direct knowledge about the current classifier and instead manipulate based on a weighted average of historically implemented classifiers. Our main result shows that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or $k_{\text{in}}$ times more mistakes compared to the full-knowledge setting, where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph (representing how many distinct feature vectors can be manipulated to appear as a single one), and $\gamma$ is the discount factor indicating agents' memory of past classifiers. Our results demonstrate how withholding access to the classifier can backfire and degrade the decision-maker's performance in online strategic classification.</div>
</div>
<div class='paper-counter'>589/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Test-Time Adaptation for Online Vision-Language Navigation with Feedback-based Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Sungjune Kim;Gyeongrok Oh;Heeju Ko;Daehyun Ji;Dongwook Lee;Byung-Jun Lee;Sujin Jang;Sangpil Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision-language navigation;test-time adaptation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a273e15cd7e38fd010663df74dfea2486251fe0e.pdf" target="_blank">https://openreview.net/pdf/a273e15cd7e38fd010663df74dfea2486251fe0e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel test-time adaptation framework for online vision-language navigation using feedback-based reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Navigating in an unfamiliar environment during deployment poses a critical challenge for a vision-language navigation (VLN) agent. Yet, test-time adaptation (TTA) remains relatively underexplored in robotic navigation, leading us to the fundamental question: what are the key properties of TTA for online VLN? In our view, effective adaptation requires three qualities: 1) flexibility in handling different navigation outcomes, 2) interactivity with external environment, and 3) maintaining a harmony between plasticity and stability. To address this, we introduce FeedTTA, a novel TTA framework for online VLN utilizing feedback-based reinforcement learning. Specifically, FeedTTA learns by maximizing binary episodic feedback, a practical setup in which the agent receives a binary scalar after each episode that indicates the success or failure of the navigation. Additionally, we propose a gradient regularization technique that leverages the binary structure of FeedTTA to achieve a balance between plasticity and stability during adaptation. Our extensive experiments on challenging VLN benchmarks demonstrate the superior adaptability of FeedTTA, even outperforming the state-of-the-art offline training methods in REVERIE benchmark with a single stream of learning.</div>
</div>
<div class='paper-counter'>590/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EvoControl: Multi-Frequency Bi-Level Control for High-Frequency Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Samuel Holt;Todor Davchev;Dhruva Tirumala;Ben Moran;Atil Iscen;Antoine Laurens;Yixin Lin;Erik Frey;Markus Wulfmeier;Francesco Romano;Nicolas Heess</div>
<div class="field-name">keywords:</div>
<div class="field-value">Hierarchical Reinforcement Learning;Evolutionary Strategies;High-Frequency Control;Continuous Control;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->everything_else</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/40c2413d3264c05d7d5dc6a99b75b513baeb30aa.pdf" target="_blank">https://openreview.net/pdf/40c2413d3264c05d7d5dc6a99b75b513baeb30aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">EvoControl marries a slow PPO high-level policy with an evolution-strategies low-level controller to learn fast, adaptive, tune-free high-frequency continuous control that outperforms both PD-based hierarchies and direct torque baselines.</div>
<div class="field-name">abstract:</div>
<div class="field-value">High-frequency control in continuous action and state spaces is essential for practical applications in the physical world. Directly applying end-to-end reinforcement learning to high-frequency control tasks struggles with assigning credit to actions across long temporal horizons, compounded by the difficulty of efficient exploration. The alternative, learning low-frequency policies that guide higher-frequency controllers (e.g., proportional-derivative (PD) controllers), can result in a limited total expressiveness of the combined control system, hindering overall performance. We introduce *EvoControl*, a novel bi-level policy learning framework for learning both a slow high-level policy (using PPO) and a fast low-level policy (using Evolution Strategies) for solving continuous control tasks. Learning with Evolution Strategies for the lower-policy allows robust learning for long horizons that crucially arise when operating at higher frequencies. This enables *EvoControl* to learn to control interactions at a high frequency, benefitting from more efficient exploration and credit assignment than direct high-frequency torque control without the need to hand-tune PD parameters. We empirically demonstrate that *EvoControl* can achieve a higher evaluation reward for continuous-control tasks compared to existing approaches, specifically excelling in tasks where high-frequency control is needed, such as those requiring safety-critical fast reactions.</div>
</div>
<div class='paper-counter'>591/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Thomas Schmied;Thomas Adler;Vihang Prakash Patil;Maximilian Beck;Korbinian Pöppel;Johannes Brandstetter;Günter Klambauer;Razvan Pascanu;Sepp Hochreiter</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;rnn;xlstm;mamba;multi-task;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->batchoffline</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c650ba5e7d796e2b4f0668d8ea90cd7d8a052d74.pdf" target="_blank">https://openreview.net/pdf/c650ba5e7d796e2b4f0668d8ea90cd7d8a052d74.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which results in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.</div>
</div>
<div class='paper-counter'>592/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scaling Laws for Pre-training Agents and World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Tim Pearce;Tabish Rashid;David Bignell;Raluca Georgescu;Sam Devlin;Katja Hofmann</div>
<div class="field-name">keywords:</div>
<div class="field-value">world modeling;imitation learning;scaling laws</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->generative_models_and_autoencoders</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a7a58483e4908d16dd8e656927a0b474fb4781dd.pdf" target="_blank">https://openreview.net/pdf/a7a58483e4908d16dd8e656927a0b474fb4781dd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when generative learning objectives on offline datasets (pre-training) are used to model an agent's behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that `bigger is better', we show that the same types of power laws found in language modeling also arise in world modeling and imitation learning (e.g. between loss and optimal model size). However, the coefficients of these laws are heavily influenced by the tokenizer, task \& architecture -- this has important implications on the optimal sizing of models and data.</div>
</div>
<div class='paper-counter'>593/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Adversaries Can Misuse Combinations of Safe Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Erik Jones;Anca Dragan;Jacob Steinhardt</div>
<div class="field-name">keywords:</div>
<div class="field-value">safety;misuse;adversary;combining models;attacks;hacking</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->safety</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/deb9c4c885e0ca9a59e872dae5a03971bf7f36a6.pdf" target="_blank">https://openreview.net/pdf/deb9c4c885e0ca9a59e872dae5a03971bf7f36a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Developers try to evaluate whether an AI system can accomplish malicious tasks before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for such misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.</div>
</div>
<div class='paper-counter'>594/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D</div>
<div class="field-name">authors:</div>
<div class="field-value">Paul McVay;Sergio Arnaud;Ada Martin;Arjun Majumdar;Krishna Murthy Jatavallabhula;Phillip Thomas;Ruslan Partsey;Daniel Dugas;Abha Gejji;Alexander Sax;Vincent-Pierre Berges;Mikael Henaff;Ayush Jain;Ang Cao;Ishita Prasad;Mrinal Kalakrishnan;Michael Rabbat;Nicolas Ballas;Mido Assran;Oleksandr Maksymets;Aravind Rajeswaran;Franziska Meier</div>
<div class="field-name">keywords:</div>
<div class="field-value">self-supervised learning;object localization;referring expressions;3D language grounding</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->computer_vision</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ad047cb665efb75be8a655bf9cb4f4ab3b97d687.pdf" target="_blank">https://openreview.net/pdf/ad047cb665efb75be8a655bf9cb4f4ab3b97d687.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A model that can localize objects in 3D from textual referring expressions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like "the small coffee table between the sofa and the lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model. Code, models and dataset can be found at the project website: locate3d.atmeta.com</div>
</div>
<div class='paper-counter'>595/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhendong Wang;Max Li;Ajay Mandlekar;Zhenjia Xu;Jiaojiao Fan;Yashraj Narang;Linxi Fan;Yuke Zhu;Yogesh Balaji;Mingyuan Zhou;Ming-Yu Liu;Yu Zeng</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion policies; diffusion distillation; one-step diffusion policy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc2b9b59d57741a827586a3a6aa52b065508b586.pdf" target="_blank">https://openreview.net/pdf/dc2b9b59d57741a827586a3a6aa52b065508b586.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce One-Step Diffusion Policy (OneDP), a distilled diffusion-based policy that accelerates robotic action generation 40 times..</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models, praised for their success in generative tasks, are increasingly being applied to robotics, demonstrating exceptional performance in behavior cloning. However, their slow generation process stemming from iterative denoising steps poses a challenge for real-time applications in resource-constrained robotics setups and dynamically changing environments.In this paper, we introduce the One-Step Diffusion Policy (OneDP), a novel approach that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating response times for robotic control tasks. We ensure the distilled generator closely aligns with the original policy distribution by minimizing the Kullback-Leibler (KL) divergence along the diffusion chain, requiring only $2\%$-$10\%$ additional pre-training cost for convergence. We evaluated OneDP on 6 challenging simulation tasks as well as 4 self-designed real-world tasks using the Franka robot. The results demonstrate that OneDP not only achieves state-of-the-art success rates but also delivers an order-of-magnitude improvement in inference speed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing its potential for dynamic and computationally constrained robotic applications. A video demo is provided at our project page, and the code will be publicly available.</div>
</div>
<div class='paper-counter'>596/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping</div>
<div class="field-name">authors:</div>
<div class="field-value">Guangyi Liu;Suzan Iloglu;Michael Caldara;Joseph W Durham;Michael M. Zavlanos</div>
<div class="field-name">keywords:</div>
<div class="field-value">Distributionally Robust Reinforcement Learning;Multi-agent Reinforcement Learning;Group Distributionally Robust Optimization;Robotic Sortation Warehouse</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->multiagent</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/01c14cc5a48785546cc056d4a8383d7afc94fd36.pdf" target="_blank">https://openreview.net/pdf/01c14cc5a48785546cc056d4a8383d7afc94fd36.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A robust multi-agent reinforcement learning framework using group DRO and contextual bandit that optimizes chute mapping in robotics sortation warehouses, reducing package recirculation by 80% under varying induction rates.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In Amazon robotic warehouses, the destination-to-chute mapping problem is crucial for efficient package sorting. Often, however, this problem is complicated by uncertain and dynamic package induction rates, which can lead to increased package recirculation. To tackle this challenge, we introduce a Distributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework that learns a destination-to-chute mapping policy that is resilient to adversarial variations in induction rates. Specifically, DRMARL relies on group distributionally robust optimization (DRO) to learn a policy that performs well not only on average but also on each individual subpopulation of induction rates within the group that capture, for example, different seasonality or operation modes of the system. This approach is then combined with a novel contextual bandit-based estimator of the worst-case induction distribution for each state-action pair, significantly reducing the cost of exploration and thereby increasing the learning efficiency and scalability of our framework. Extensive simulations demonstrate that DRMARL achieves robust chute mapping in the presence of varying induction distributions, reducing package recirculation by an average of 80% in the simulation scenario.</div>
</div>
<div class='paper-counter'>597/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Rui Yang;Hanyang Chen;Junyu Zhang;Mark Zhao;Cheng Qian;Kangrui Wang;Qineng Wang;Teja Venkat Koripella;Marziyeh Movahedi;Manling Li;Heng Ji;Huan Zhang;Tong Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Agent;Multi-modal Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->large_language_models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/747c4284e6d00f7e24692f8b5bdcbafa9e810931.pdf" target="_blank">https://openreview.net/pdf/747c4284e6d00f7e24692f8b5bdcbafa9e810931.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduces EmbodiedBench, a benchmark designed to evaluate the finegrained capabilities of vision-driven embodied agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents.EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning.Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only $28.9\\%$ on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at [https://embodiedbench.github.io](https://embodiedbench.github.io).</div>
</div>
<div class='paper-counter'>598/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Robotic Policy Learning via Latent Space Backward Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Dongxiu Liu;Haoyi Niu;Zhihao Wang;Jinliang Zheng;Yinan Zheng;Zhonghong Ou;Jianming HU;Jianxiong Li;Xianyuan Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Planning;Embodied Agents;Goal-Conditioned Policy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/695f4ab642ff1d3714f3414b49f3215408e886f5.pdf" target="_blank">https://openreview.net/pdf/695f4ab642ff1d3714f3414b49f3215408e886f5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks?To address this, we propose a **B**ackward **P**lanning scheme in **L**atent space (**LBP**), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction.Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: [https://lbp-authors.github.io](https://lbp-authors.github.io).</div>
</div>
<div class='paper-counter'>599/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Gaoyue Zhou;Hengkai Pan;Yann LeCun;Lerrel Pinto</div>
<div class="field-name">keywords:</div>
<div class="field-value">World Models;Planning;Representation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8b7b83b178fcee5b8dccf2d70fa03c5f7ba97597.pdf" target="_blank">https://openreview.net/pdf/8b7b83b178fcee5b8dccf2d70fa03c5f7ba97597.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a simple yet effective method for constructing versatile world models with pre-trained DINOv2 that generalize to complex environment dynamics, which enables zero-shot solutions at test time for arbitrary goals.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remain challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.</div>
</div>
<div class='paper-counter'>600/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Adrià López Escoriza;Nicklas Hansen;Stone Tao;Tongzhou Mu;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Learning from Demonstrations;Robotics;Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->deep_rl</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3599c43148b13fd821ac6abe4629c9baf0a97a23.pdf" target="_blank">https://openreview.net/pdf/3599c43148b13fd821ac6abe4629c9baf0a97a23.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework multi-stage manipulation tasks with sparse rewards and visual inputs; our framework combines learned dense rewards, model-based RL, a bi-phasic training scheme, and a small number of demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable sub-goals. In this work, we propose DEMO³, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult taskscompared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.</div>
</div>
<div class='paper-counter'>601/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">X-Hacking: The Threat of Misguided AutoML</div>
<div class="field-name">authors:</div>
<div class="field-value">Rahul Sharma;Sumantrak Mukherjee;Andrea Sipka;Eyke Hüllermeier;Sebastian Josef Vollmer;Sergey Redyuk;David Antony Selby</div>
<div class="field-name">keywords:</div>
<div class="field-value">explainable AI;SHAP;AutoML;Model Multiplicity;Rashomon sets;Ethics;Reproducibility</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->accountability_transparency_and_interpretability</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/53ae0cc98ee6975b0e94dc8651eafce9d933ea01.pdf" target="_blank">https://openreview.net/pdf/53ae0cc98ee6975b0e94dc8651eafce9d933ea01.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Deliberate manipulation of SHAP explanations at scale using AutoML</div>
<div class="field-name">abstract:</div>
<div class="field-value">Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how easily an automated machine learning pipeline can be adapted to exploit model multiplicity at scale: searching a set of ‘defensible’ models with similar predictive performance to find a desired explanation. We formulate the trade-off between explanation and accuracy as a multi-objective optimisation problem, and illustrate empirically on familiar real-world datasets that, on average, Bayesian optimisation accelerates X-hacking 3-fold for features susceptible to it, versus random sampling. We show the vulnerability of a dataset to X-hacking can be determined by information redundancy among features. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI.</div>
</div>
<div class='paper-counter'>602/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robust ML Auditing using Prior Knowledge</div>
<div class="field-name">authors:</div>
<div class="field-value">Jade Garcia Bourrée;Augustin Godinot;Sayan Biswas;Anne-Marie Kermarrec;Erwan Le Merrer;Gilles Tredan;Martijn de Vos;Milos Vujasinovic</div>
<div class="field-name">keywords:</div>
<div class="field-value">ML audit;ML theory;fairness;fairwashing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">general_machine_learning->evaluation</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 spotlightposter</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c6525e84b385fff72f6a125d5999d365410620c2.pdf" target="_blank">https://openreview.net/pdf/c6525e84b385fff72f6a125d5999d365410620c2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation.This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users.In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious.Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.</div>
</div>
<div class='paper-counter'>603/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DexScale: Automating Data Scaling for Sim2Real Generalizable Robot Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Guiliang Liu;Yueci Deng;Runyi Zhao;Huayi Zhou;Jian Chen;Jietao Chen;Ruiyan Xu;Yunxin Tai;Kui Jia</div>
<div class="field-name">keywords:</div>
<div class="field-value">Data Engine;Embodied AI;Robot Control;Manipulation;Policy Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/909cb07e365c75873479dca6e911a730e34d7653.pdf" target="_blank">https://openreview.net/pdf/909cb07e365c75873479dca6e911a730e34d7653.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce DexScale, a data engine designed to perform automatic skills simulation and scaling for learning deployable robot manipulation policies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">A critical prerequisite for achieving generalizable robot control is the availability of a large-scale robot training dataset. Due to the expense of collecting realistic robotic data, recent studies explored simulating and recording robot skills in virtual environments. While simulated data can be generated at higher speeds, lower costs, and larger scales, the applicability of such simulated data remains questionable due to the gap between simulated and realistic environments. To advance the Sim2Real generalization, in this study, we present DexScale, a data engine designed to perform automatic skills simulation and scaling for learning deployable robot manipulation policies. Specifically, DexScale ensures the usability of simulated skills by integrating diverse forms of realistic data into the simulated environment, preserving semantic alignment with the target tasks. For each simulated skill in the environment, DexScale facilitates effective Sim2Real data scaling by automating the process of domain randomization and adaptation. Tuned by the scaled dataset, the control policy achieves zero-shot Sim2Real generalization across diverse tasks, multiple robot embodiments, and widely studied policy model architectures, highlighting its importance in advancing Sim2Real embodied intelligence.</div>
</div>
<div class='paper-counter'>604/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">An End-to-End Model for Logits-Based Large Language Models Watermarking</div>
<div class="field-name">authors:</div>
<div class="field-value">KA HIM WONG;Jicheng Zhou;Jiantao Zhou;Yain-Whar Si</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM watermarking;End-to-end optimization;Robustness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">social_aspects->safety</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7cfefee933f7d77996daad12e4e38c5a83b268ce.pdf" target="_blank">https://openreview.net/pdf/7cfefee933f7d77996daad12e4e38c5a83b268ce.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce the first logits-based end-to-end model for LLM watermarking, where encoder and decoder networks are jointly optimized to improve detection robustness and text quality.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By joint optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online-prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37–39% under paraphrasing and 17.2% on average, while maintaining text quality on par with the distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs. Code is available at https://github.com/KAHIMWONG/E2E_LLM_WM.</div>
</div>
<div class='paper-counter'>605/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</div>
<div class="field-name">authors:</div>
<div class="field-value">Yue Meng;Chuchu Fan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Temporal Logic;Flow Matching;Graph Neural Networks</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2c4e87e8c33f892b5676db0c2244bcb758aca8f2.pdf" target="_blank">https://openreview.net/pdf/2c4e87e8c33f892b5676db0c2244bcb758aca8f2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We utilize Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general signal temporal logic specifications.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF</div>
</div>
<div class='paper-counter'>606/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Flow-based Domain Randomization for Learning and Sequencing Robotic Skills</div>
<div class="field-name">authors:</div>
<div class="field-value">Aidan Curtis;Eric Li;Michael Noseworthy;Nishad Gothoskar;Sachin Chitta;Hui Li;Leslie Pack Kaelbling;Nicole E Carey</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Domain Randomization;Uncertainty;Assembly;Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6fbee5dfec94300489a2a4a4100b9a7ccf5f453.pdf" target="_blank">https://openreview.net/pdf/b6fbee5dfec94300489a2a4a4100b9a7ccf5f453.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A normalizing-flow approach to domain randomization yields robust RL policies for multi-step robot manipulation under uncertainty.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies learned in simulation. By randomizing properties of the environment during training, the learned policy can be robust to uncertainty along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate the problem of automatically discovering this sampling distribution via entropy-regularized reward maximization of a neural sampling distribution in the form of a normalizing flow. We show that this architecture is more flexible and results in better robustness than existing approaches to learning simple parameterized sampling distributions. We demonstrate that these policies can be used to learn robust policies for contact-rich assembly tasks. Additionally, we explore how these sampling distributions, in combination with a privileged value function, can be used for out-of-distribution detection in the context of an uncertainty-aware multi-step manipulation planner.</div>
</div>
<div class='paper-counter'>607/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Safe-EF: Error Feedback for Non-smooth Constrained Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Rustem Islamov;Yarden As;Ilyas Fatkhullin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Optimization;Distributed Optimization;Compression;Safe Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">optimization->large_scale_parallel_and_distributed</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/553da7c91df54de1b0c2bdbcf5dd66eed61e95e9.pdf" target="_blank">https://openreview.net/pdf/553da7c91df54de1b0c2bdbcf5dd66eed61e95e9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We analyze a non-smooth distributed optimization problem with constraints and provide an algorithm to solve it which matches the lower bound.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-$K$) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity.</div>
</div>
<div class='paper-counter'>608/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Maximum Total Correlation Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Bang You;Puze Liu;Huaping Liu;Jan Peters;Oleg Arenz</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Total Correlation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9dc7c4301fdbba4a4c5e24de827abafc106d9e7f.pdf" target="_blank">https://openreview.net/pdf/9dc7c4301fdbba4a4c5e24de827abafc106d9e7f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Maximizing trajectory total correlation for learning simple and robust RL policies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Simplicity is a powerful inductive bias. In reinforcement learning, regularization is used for simpler policies, data augmentation for simpler representations, and sparse reward functions for simpler objectives, all that, with the underlying motivation to increase generalizability and robustness by focusing on the essentials. Supplementary to these techniques, we investigate how to promote simple behavior throughout the episode. To that end, we introduce a modification of the reinforcement learning problem that additionally maximizes the total correlation within the induced trajectories. We propose a practical algorithm that optimizes all models, including policy and state representation, based on a lower-bound approximation. In simulated robot  environments, our method naturally generates policies that induce periodic and compressible trajectories, and that exhibit superior robustness to noise and changes in dynamics compared to baseline methods, while also improving performance in the original tasks.</div>
</div>
<div class='paper-counter'>609/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Seungho Baek;taegeon park;Jongchan Park;Seungjun Oh;Yusung Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Hierarchical Reinforcement Learning;Offline Goal-Conditioned Reinforcement Learning;Graph-based Reinforcement Learning;Temporal Distance Representation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement_learning->batchoffline</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/56baac74fdcb0d60af3fa4479355a3a3db8dbb55.pdf" target="_blank">https://openreview.net/pdf/56baac74fdcb0d60af3fa4479355a3a3db8dbb55.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a graph-based offline HRL framework that significantly improves long-horizon reasoning and trajectory stitching.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Existing offline hierarchical reinforcement learning methods rely on high-level policy learning to generate subgoal sequences. However, their efficiency degrades as task horizons increase, and they lack effective strategies for stitching useful state transitions across different trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that formulates subgoal selection as a graph search problem rather than learning an explicit high-level policy. By embedding states into a Temporal Distance Representation (TDR) space, GAS clusters semantically similar states from different trajectories into unified graph nodes, enabling efficient transition stitching. A shortest-path algorithm is then applied to select subgoal sequences within the graph, while a low-level policy learns to reach the subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE) metric, which filters out noisy or inefficient transition states, significantly enhancing task performance. GAS outperforms prior offline HRL methods across locomotion, navigation, and manipulation tasks. Notably, in the most stitching-critical task, it achieves a score of 88.3, dramatically surpassing the previous state-of-the-art score of 1.0. Our source code is available at: https://github.com/qortmdgh4141/GAS.</div>
</div>
<div class='paper-counter'>610/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Improving Your Model Ranking on Chatbot Arena by Vote Rigging</div>
<div class="field-name">authors:</div>
<div class="field-value">Rui Min;Tianyu Pang;Chao Du;Qian Liu;Minhao Cheng;Min Lin</div>
<div class="field-name">keywords:</div>
<div class="field-value">Chatbot Arena;Vote Rigging;Large Language Models;Leaderboard Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->large_language_models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/47509f1414b708c9e215d27b0e9a15697fa8b061.pdf" target="_blank">https://openreview.net/pdf/47509f1414b708c9e215d27b0e9a15697fa8b061.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We demonstrate that we could improve model rankings on Chatbot Arena by vote rigging.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Chatbot Arena is an open platform for evaluating LLMs by pairwise battles, in which users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be *rigged* to improve (or decrease) the ranking of a target model $m\_{t}$. We first introduce a straightforward **target-only rigging** strategy that focuses on new battles involving $m\_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m\_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about 1% of new battles will involve $m\_{t}$. To overcome this, we propose an **omnipresent rigging** strategy, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m\_{t}$, even if $m\_{t}$ is not directly involved in the battle. We conduct experiments on around *1.7 million* historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategy can improve model rankings by rigging only *hundreds of* new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. [**Code**](https://github.com/sail-sg/Rigging-ChatbotArena) is publicly available to reproduce all experiments.</div>
</div>
<div class='paper-counter'>611/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation</div>
<div class="field-name">authors:</div>
<div class="field-value">Juno Kim;Denny Wu;Jason D. Lee;Taiji Suzuki</div>
<div class="field-name">keywords:</div>
<div class="field-value">large language model;reasoning;search;distillation;metastability</div>
<div class="field-name">primary_area:</div>
<div class="field-value">theory->learning_theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9e3c069a3064b0bc1ffdea79bc7f3702dcf81231.pdf" target="_blank">https://openreview.net/pdf/9e3c069a3064b0bc1ffdea79bc7f3702dcf81231.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time computation by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed \emph{metastable representation} of the reasoning dynamics can be distilled into a smaller, more efficient model.</div>
</div>
<div class='paper-counter'>612/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Pre-training Auto-regressive Robotic Models with 4D Representations</div>
<div class="field-name">authors:</div>
<div class="field-value">Dantong Niu;Yuvan Sharma;Haoru Xue;Giscard Biamby;Junyi Zhang;Ziteng Ji;Trevor Darrell;Roei Herzig</div>
<div class="field-name">keywords:</div>
<div class="field-value">Auto-regressive Robotic Models;Pre-training;4D Representations</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications->robotics</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ae3bf32e2376b7a6ea00da62136dfe667f0dc5b5.pdf" target="_blank">https://openreview.net/pdf/ae3bf32e2376b7a6ea00da62136dfe667f0dc5b5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a novel robotics pre-training approach that leverages low-level 4D representations by tracking 3D points over time in videos to achieve effective pre-training for robotics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an **A**uto-regressive **R**obotic **M**odel that leverages low-level **4**D **R**epresentations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.</div>
</div>
<div class='paper-counter'>613/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PoisonBench: Assessing Language Model Vulnerability to Poisoned Preference Data</div>
<div class="field-name">authors:</div>
<div class="field-value">Tingchen Fu;Mrinank Sharma;Philip Torr;Shay B Cohen;David Krueger;Fazl Barez</div>
<div class="field-name">keywords:</div>
<div class="field-value">backdoor attack;data poisoning;preference learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">deep_learning->large_language_models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICML 2025 poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/edcbdb4a6ad45e5c0d736ac1414b8214a19c1fc2.pdf" target="_blank">https://openreview.net/pdf/edcbdb4a6ad45e5c0d736ac1414b8214a19c1fc2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 22 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not always enhance resilience against poisoning attacks and the influence on model resilience varies among different model suites. (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.</div>
</div>
<div class='paper-counter'>614/828</div>
<div class="conference">ICLR_cc_2025_Conference.csv</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Re-Imagining Multimodal Instruction Tuning: A Representation View</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiyang Liu;James Chenhao Liang;Ruixiang Tang;Yugyung Lee;MAJID RABBANI;Sohail Dianat;Raghuveer Rao;Lifu Huang;Dongfang Liu;Qifan Wang;Cheng Han</div>
<div class="field-name">keywords:</div>
<div class="field-value">Representation Tuning;Large Multimodal Models;Parameter-efficient Fine-tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5258198d4d738e054b9e119c78ec513874dfa67d.pdf" target="_blank">https://openreview.net/pdf/5258198d4d738e054b9e119c78ec513874dfa67d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Multimodal Representation Tuning for Zero-shot Multimodal Instruction Learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior.</div>
</div>
<div class='paper-counter'>615/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Michael Matthews;Michael Beukman;Chris Lu;Jakob Nicolaus Foerster</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;open-endedness;unsupervised environment design;automatic curriculum learning;benchmark</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1c9333bd485fcf46a3c7b3a1420dd36b55476d63.pdf" target="_blank">https://openreview.net/pdf/1c9333bd485fcf46a3c7b3a1420dd36b55476d63.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Training with reinforcement learning on a vast open-ended distribution of physics-based tasks leads to an agent that can zero-shot solve human-designed problems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.Our trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments.  Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*.  This includes solving some environments that standard RL training completely fails at.We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.</div>
</div>
<div class='paper-counter'>616/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Yu;Songheng Yin;Steve Easterbrook;Animesh Garg</div>
<div class="field-name">keywords:</div>
<div class="field-value">Controllable video generation;Egocentric video prediction;World model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9751921246733ea4a397d6e657233cdf4b3ff7ae.pdf" target="_blank">https://openreview.net/pdf/9751921246733ea4a397d6e657233cdf4b3ff7ae.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in video diffusion models have established a strong foundation for developing world models with practical applications. The next challenge lies in exploring how an agent can leverage these foundation models to understand, interact with, and plan within observed environments. This requires adding more controllability to the model, transforming it into a versatile game engine capable of dynamic manipulation and control. To address this, we investigated three key conditioning factors: camera, context frame, and text, identifying limitations in current model designs. Specifically, the fusion of camera embeddings with video features leads to camera control being influenced by those features. Additionally, while textual information compensates for necessary spatiotemporal structures, it often intrudes into already observed parts of the scene. To tackle these issues, we designed the Spacetime Epipolar Attention Layer, which ensures that egomotion generated by the model strictly aligns with the camera’s movement through rigid constraints. Moreover, we propose the CI2V-adapter, which uses camera information to better determine whether to prioritize textual or visual embeddings, thereby alleviating the issue of textual intrusion into observed areas. Through extensive experiments, we demonstrate that our new model EgoSim achieves excellent results on both the RealEstate and newly repurposed Epic-Field datasets. For more results, please refer to https://egosim.github.io/EgoSim/.</div>
</div>
<div class='paper-counter'>617/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AFlow: Automating Agentic Workflow Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiayi Zhang;Jinyu Xiang;Zhaoyang Yu;Fengwei Teng;Xiong-Hui Chen;Jiaqi Chen;Mingchen Zhuge;Xin Cheng;Sirui Hong;Jinlin Wang;Bingnan Zheng;Bang Liu;Yuyu Luo;Chenglin Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">LLM Agent; Prompt Optimization; Workflow Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f74f4704961076008514f7af306d3a5e0a8fc87f.pdf" target="_blank">https://openreview.net/pdf/f74f4704961076008514f7af306d3a5e0a8fc87f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b9ac0c0f03a6e8df34a4b431ce9e88f799a518eb.zip" target="_blank">https://openreview.net/attachment/b9ac0c0f03a6e8df34a4b431ce9e88f799a518eb.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce the field of Agentic Workflow Optimization and propose an effective search algorithm called AFLOW, enabling it to surpass manually constructed workflows on six reasoning datasets.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFLOW, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFLOW's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.</div>
</div>
<div class='paper-counter'>618/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets</div>
<div class="field-name">authors:</div>
<div class="field-value">Guangqi Jiang;Yifei Sun;Tao Huang;Huanyu Li;Yongyuan Liang;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Foundation Model;Representation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d3dc8ec627dc45342df0a8f34c2fdb93052f42ac.pdf" target="_blank">https://openreview.net/pdf/d3dc8ec627dc45342df0a8f34c2fdb93052f42ac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Manipulation-centric robotic representation training on large-scale robot dataset boosts policy performance on manipulation tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the ''manipulation centricity'' is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose **M**anipulation **C**entric **R**epresentation (**MCR**), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with an action prediction loss and a time contrastive loss during pre-training. Empirical results across four simulation domains with 20 robotic manipulation tasks demonstrate that **MCR** outperforms the strongest baseline by 14.8\%. Additionally, **MCR** significantly boosts the success rate in three real-world manipulation tasks by 76.9\%. Project website: robots-pretrain-robots.github.io</div>
</div>
<div class='paper-counter'>619/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Songming Liu;Lingxuan Wu;Bangguo Li;Hengkai Tan;Huayu Chen;Zhengyi Wang;Ke Xu;Hang Su;Jun Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">robot learning;diffusion models;foundation models;bimanual manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/29d56379d000b8c0e05906c5958e67e2e870ab0c.pdf" target="_blank">https://openreview.net/pdf/29d56379d000b8c0e05906c5958e67e2e870ab0c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/790d1f8285e80d0c411bcec3d39cc31ca25b7072.zip" target="_blank">https://openreview.net/attachment/790d1f8285e80d0c411bcec3d39cc31ca25b7072.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to $1.2$B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over $6$K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1$\sim$5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.</div>
</div>
<div class='paper-counter'>620/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Language Agents Meet Causality -- Bridging LLMs and Causal World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">John Gkountouras;Matthias Lindemann;Phillip Lippe;Efstratios Gavves;Ivan Titov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Causality;Causal Representation Learning;Language Agents;Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/00f503aae18d3c78d251d2fba1dfabd0aab10547.pdf" target="_blank">https://openreview.net/pdf/00f503aae18d3c78d251d2fba1dfabd0aab10547.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Improving LLM planning capabilities using learned causal representations</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.</div>
</div>
<div class='paper-counter'>621/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions</div>
<div class="field-name">authors:</div>
<div class="field-value">Weifeng Lin;Xinyu Wei;Renrui Zhang;Le Zhuo;Shitian Zhao;Siyuan Huang;Junlin Xie;Peng Gao;Hongsheng Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion Model;Image Generation;Image-to-Image</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8049a747b16cec89ba1525cdd24484a2032de91f.pdf" target="_blank">https://openreview.net/pdf/8049a747b16cec89ba1525cdd24484a2032de91f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting/outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding  abilities for images with diverse resolutions but also exhibits generalization capabilities with unseen tasks and human instructions.</div>
</div>
<div class='paper-counter'>622/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaizhe Hu;Zihang Rui;Yao He;Yuyao Liu;Pu Hua;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics;Imitation Learning;Visual Imitation Learning;Robustness;Diffusion Model;Diffusion Inversion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c9316af6880ffc7e2fd30269343f9198108e91df.pdf" target="_blank">https://openreview.net/pdf/c9316af6880ffc7e2fd30269343f9198108e91df.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/555799937b2fe8bf9844db9b7ed4824619e621db.zip" target="_blank">https://openreview.net/attachment/555799937b2fe8bf9844db9b7ed4824619e621db.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose ***Stem-OB*** that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. *Stem-OB* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. *Stem-OB* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of **22.2%** in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details.</div>
</div>
<div class='paper-counter'>623/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals</div>
<div class="field-name">authors:</div>
<div class="field-value">Grace Liu;Michael Tang;Benjamin Eysenbach</div>
<div class="field-name">keywords:</div>
<div class="field-value">exploration;emergent skills;contrastive reinforcement learning;open-ended learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6aca63c902fc1ab1cb39947be975c7ca820620c3.pdf" target="_blank">https://openreview.net/pdf/6aca63c902fc1ab1cb39947be975c7ca820620c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state (see Fig. 1) and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.</div>
</div>
<div class='paper-counter'>624/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion-Based Planning for Autonomous Driving with Flexible Guidance</div>
<div class="field-name">authors:</div>
<div class="field-value">Yinan Zheng;Ruiming Liang;Kexin ZHENG;Jinliang Zheng;Liyuan Mao;Jianxiong Li;Weihao Gu;Rui Ai;Shengbo Eben Li;Xianyuan Zhan;Jingjing Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion planning;autonomous driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6398cace84eddf279b77ad79b2c27e3bdf9d137.pdf" target="_blank">https://openreview.net/pdf/b6398cace84eddf279b77ad79b2c27e3bdf9d137.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.</div>
</div>
<div class='paper-counter'>625/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Devdhar Patel;Hava T Siegelmann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Decision Frequency;Action Sequence Generation;Model-Based Training;Model-Free Control;Efficient Learning;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/abfe1c87402290180e7b38a8d53684d723b99fc6.pdf" target="_blank">https://openreview.net/pdf/abfe1c87402290180e7b38a8d53684d723b99fc6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/693e01b4048cb114c669781fe0dfe21980ff288f.zip" target="_blank">https://openreview.net/attachment/693e01b4048cb114c669781fe0dfe21980ff288f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce an algorithm that achieves competitive continuous control at extremely slow control frequencies using action sequences</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a "temporal recall" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Furthermore, we compare SRL with model-based online planning, showing that SRL achieves comparable FAS while leveraging the same model during training that online planners use for planning.</div>
</div>
<div class='paper-counter'>626/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Quan Zhang;Yuxin Qi;Xi Tang;Jinwei Fang;Xi Lin;Ke Zhang;Chun Yuan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Manipulation Detection；Segment Anything Model；Prompt learning；Semantic-Agnostic</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3fdc1b52124e63a52324d0ff0f0eb01cce8d1207.pdf" target="_blank">https://openreview.net/pdf/3fdc1b52124e63a52324d0ff0f0eb01cce8d1207.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM. Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization. Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks. Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method.</div>
</div>
<div class='paper-counter'>627/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinxin Zhao;Wenzhe Cai;Likun Tang;Teng Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics;Visual Navigation;Vision-Language Model;Scene Imagination</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e349d69236fa6d97f504e96881ee34405d7de516.pdf" target="_blank">https://openreview.net/pdf/e349d69236fa6d97f504e96881ee34405d7de516.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/50db0c19d9844da80e6169eba9e3af752a09a7df.pdf" target="_blank">https://openreview.net/attachment/50db0c19d9844da80e6169eba9e3af752a09a7df.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a mapless visual navigation system by proposing a imagination-based visual prompting for pre-trained large vision-language models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.</div>
</div>
<div class='paper-counter'>628/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning View-invariant World Models for Visual Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jing-Cheng Pang;Nan Tang;Kaiyuan Li;Yuting Tang;Xin-Qiang Cai;Zhen-Yu Zhang;Gang Niu;Masashi Sugiyama;Yang Yu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic manipulation;reinforcement learning;world model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/107272aeccda7bbe5716fed69b72e3b6bc77409f.pdf" target="_blank">https://openreview.net/pdf/107272aeccda7bbe5716fed69b72e3b6bc77409f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study robust robotic manipulation under viewpoint disturbance by learning view-invariant representation and world model.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic manipulation tasks often rely on visual inputs from cameras to perceive the environment. However, previous approaches still suffer from performance degradation when the camera’s viewpoint changes during manipulation. In this paper, we propose ReViWo (Representation learning for View-invariant World model), leveraging multi-view data to learn robust representations for control under viewpoint disturbance. ReViWo utilizes an autoencoder framework to reconstruct target images by an architecture that combines view-invariant representation (VIR) and view-dependent representation. To train ReViWo, we collect multi-view data in simulators with known view labels, meanwhile, ReViWo is simutaneously trained on Open X-Embodiment datasets without view labels. The VIR is then used to train a world model on pre-collected manipulation data and a policy through interaction with the world model. We evaluate the effectiveness of ReViWo in various viewpoint disturbance scenarios, including control under novel camera positions and frequent camera shaking, using the Meta-world & PandaGym environments. Besides, we also conduct experiments on real world ALOHA robot. The results demonstrate that ReViWo maintains robust performance under viewpoint disturbance, while baseline methods suffer from significant performance degradation. Furthermore, we show that the VIR captures task-relevant state information and remains stable for observations from novel viewpoints, validating the efficacy of the ReViWo approach.</div>
</div>
<div class='paper-counter'>629/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OccProphet: Pushing the Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with an Observer-Forecaster-Refiner Framework</div>
<div class="field-name">authors:</div>
<div class="field-value">Junliang Chen;Huaiyuan Xu;Yi Wang;Lap-Pui Chau</div>
<div class="field-name">keywords:</div>
<div class="field-value">camera-only occupancy forecasting;efficiency;effectiveness;autonomous driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dfa964e01776bee3436a7bb182a5fafd67461314.pdf" target="_blank">https://openreview.net/pdf/dfa964e01776bee3436a7bb182a5fafd67461314.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper proposes OccProphet, a camera-only framework for efficient and effective occupancy forecasting, in a lightweight Observer-Forecaster-Refiner pipeline, performing better and 2.6 times faster than Cam4DOcc reducing computational costs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make occupancy forecasting less efficient during training and inference stages, hindering its feasibility for deployment on edge agents. In this paper, we propose a novel framework, \textit{i.e.}, OccProphet, to efficiently and effectively learn occupancy forecasting with significantly lower computational requirements while improving forecasting accuracy. OccProphet comprises three lightweight components: Observer, Forecaster, and Refiner. The Observer extracts spatio-temporal features from 3D multi-frame voxels using the proposed Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster and Refiner conditionally predict and refine future occupancy inferences. Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets demonstrate that OccProphet is both training- and inference-friendly. OccProphet reduces 58\%$\sim$78\% of the computational cost with a 2.6$\times$ speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves 4\%$\sim$18\% relatively higher forecasting accuracy. Code and models are publicly available at https://github.com/JLChen-C/OccProphet.</div>
</div>
<div class='paper-counter'>630/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">Joey Hong;Anca Dragan;Sergey Levine</div>
<div class="field-name">keywords:</div>
<div class="field-value">offline reinforcement learning;language models;dialogue;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dc752ff5bb459b9673de31cd456e0b5e4fb841be.pdf" target="_blank">https://openreview.net/pdf/dc752ff5bb459b9673de31cd456e0b5e4fb841be.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a new offline RL algorithm specifically to fine-tune pretrained LLMs and VLMs better.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite the widespread use of policy gradient methods to train large language models for single turn tasks (e.g., question answering), value-based methods for multi-turn RL in an off-policy or offline setting have proven particularly challenging to scale to the setting of large language models. This setting requires effectively leveraging pretraining, scaling to large architectures with billions of parameters, and training on large datasets, all of which represent major challenges for current value-based RL methods. In this work, we propose a novel offline RL algorithm that addresses these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT) problem where the probabilities of tokens directly translate to Q-values. In this way we obtain an algorithm that smoothly transitions from maximizing the likelihood of the data during pretraining to learning a near-optimal Q-function during finetuning. Our algorithm has strong theoretical foundations, enjoying performance bounds similar to state-of-the-art Q-learning methods, while in practice utilizing an objective that closely resembles SFT. Because of this, our approach can enjoy the full benefits of the pretraining of language models, without the need to reinitialize any weights before RL finetuning, and without the need to initialize new heads for predicting values or advantages. Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a variety of tasks including both natural language dialogue and robotic manipulation and navigation from images.</div>
</div>
<div class='paper-counter'>631/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Benchmarking LLMs' Judgments with No Gold Standard</div>
<div class="field-name">authors:</div>
<div class="field-value">Shengwei Xu;Yuxuan Lu;Grant Schoenebeck;Yuqing Kong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Benchmarking;Peer Review;Mutual Information;Data Contamination;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6997d33d4182fd0407c9f904b7315e9c2527bde4.pdf" target="_blank">https://openreview.net/pdf/6997d33d4182fd0407c9f904b7315e9c2527bde4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by large language models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on two human-annotated datasets, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulation, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner. We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers.  Because GRE-bench is based upon GEM, it inherits its robustness properties.  Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.</div>
</div>
<div class='paper-counter'>632/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport</div>
<div class="field-name">authors:</div>
<div class="field-value">Lvmin Zhang;Anyi Rao;Maneesh Agrawala</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion model;illumination editing;image editing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/08c6b53400491d4d663335fe55fa7814ad3582f0.pdf" target="_blank">https://openreview.net/pdf/08c6b53400491d4d663335fe55fa7814ad3582f0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/de8d0759f2436018375ec73ea4befa9bf4067f6e.pdf" target="_blank">https://openreview.net/attachment/de8d0759f2436018375ec73ea4befa9bf4067f6e.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Diffusion-based image illumination harmonization and editing model</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos.</div>
</div>
<div class='paper-counter'>633/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Cross-Embodiment Dexterous Grasping with Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoqi Yuan;Bohan Zhou;Yuhui Fu;Zongqing Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">dexterous grasping;cross-embodiment learning;reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1df9702a66ff28e5991f23617c22e2c8f5e85390.pdf" target="_blank">https://openreview.net/pdf/1df9702a66ff28e5991f23617c22e2c8f5e85390.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Dexterous hands exhibit significant potential for complex real-world grasping tasks. While recent studies have primarily focused on learning policies for specific robotic hands, the development of a universal policy that controls diverse dexterous hands remains largely unexplored.In this work, we study the learning of cross-embodiment dexterous grasping policies using reinforcement learning (RL). Inspired by the capability of human hands to control various dexterous hands through teleoperation, we propose a universal action space based on the human hand's eigengrasps. The policy outputs eigengrasp actions that are then converted into specific joint actions for each robot hand through a retargeting mapping. We simplify the robot hand's proprioception to include only the positions of fingertips and the palm, offering a unified observation space across different robot hands. Our approach demonstrates an 80\% success rate in grasping objects from the YCB dataset across four distinct embodiments using a single vision-based policy. Additionally, our policy exhibits zero-shot generalization to two previously unseen embodiments and significant improvement in efficient finetuning. For further details and videos, visit our project page (https://sites.google.com/view/crossdex).</div>
</div>
<div class='paper-counter'>634/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">Guanxing Lu;Ziwei Wang;Changliu Liu;Jiwen Lu;Yansong Tang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Instruction Following (EIF);Large Language Model;Chain-of-thought Reasoning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e74c772eb4de5556458e621b7dcb849a018ca229.pdf" target="_blank">https://openreview.net/pdf/e74c772eb4de5556458e621b7dcb849a018ca229.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/446b375fe8f7750f1acbdc6fa7f9cfb5c0e38e9a.zip" target="_blank">https://openreview.net/attachment/446b375fe8f7750f1acbdc6fa7f9cfb5c0e38e9a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a Thinkbot agent that reasons the thought chain in sparse human instruction to successfully complete complex embodied instruction following (EIF) goals.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied Instruction Following (EIF) requires agents to complete human instruction by interacting objects in complicated surrounding environments. Conventional methods directly consider the sparse human instruction to generate action plans for agents, which usually fail to achieve human goals because of the instruction incoherence in action descriptions. On the contrary, we propose ThinkBot that reasons the thought chain in human instruction to recover the missing action descriptions, so that the agent can successfully complete human goals by following the coherent instruction. Specifically, we first design an instruction completer based on large language models to recover the missing actions with interacted objects between consecutive human instruction, where the perceived surrounding environments and the completed sub-goals are considered for instruction completion. Based on the partially observed scene semantic maps, we present an object localizer to infer the position of interacted objects and the related Bayesian uncertainty for close-loop planning. Extensive experiments in the simulated environment show that our ThinkBot outperforms the state-of-the-art EIF methods by a sizable margin in both success rate and execution efficiency. Project page: https://guanxinglu.github.io/thinkbot/.</div>
</div>
<div class='paper-counter'>635/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Personalized Visual Instruction Tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">Renjie Pi;Jianshu Zhang;Tianyang Han;Jipeng Zhang;Rui Pan;Tong Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">large language model;multimodal large language model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fad01f834bb162234cf7b4dacb7410e3d4b7504c.pdf" target="_blank">https://openreview.net/pdf/fad01f834bb162234cf7b4dacb7410e3d4b7504c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness." Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.</div>
</div>
<div class='paper-counter'>636/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Articulate-Anything:  Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Long Le;Jason Xie;William Liang;Hung-Ju Wang;Yue Yang;Yecheng Jason Ma;Kyle Vedder;Arjun Krishna;Dinesh Jayaraman;Eric Eaton</div>
<div class="field-name">keywords:</div>
<div class="field-value">Computer vision;Vision-Language Models;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5b5bc03250bf501d6bd2746b36645f34e2c1b720.pdf" target="_blank">https://openreview.net/pdf/5b5bc03250bf501d6bd2746b36645f34e2c1b720.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Articulate-Anything, a state-of-the-art method for generating diverse and high-quality interactable digital twins from many inputs including text, images, and videos.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation.However, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we present Articulate-Anything, a system that automates the articulation of diverse, complex objects from many input modalities, including text, images, and videos. Articulate-Anything leverages vision-language models (VLMs) to generate code that can be compiled into an interactable digital twin for use in standard 3D simulators. Our system exploits existing 3D asset datasets via a mesh retrieval mechanism, along with an actor-critic system that iteratively proposes, evaluates, and refines solutions for articulating the objects, self-correcting errors to achieve a robust out- come. Qualitative evaluations demonstrate Articulate-Anything's capability to articulate complex and even ambiguous object affordances by leveraging rich grounded inputs. In extensive quantitative experiments on the standard PartNet-Mobility dataset, Articulate-Anything substantially outperforms prior work, increasing the success rate from 8.7-11.6\% to 75\% and setting a new bar for state-of-art performance.  We further showcase the utility of our generated assets by using them to train robotic policies for fine-grained manipulation tasks that go beyond basic pick and place.</div>
</div>
<div class='paper-counter'>637/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Guobin Shen;Dongcheng Zhao;Yiting Dong;Xiang He;Yi Zeng</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Jailbreak Defense;Safety-Utility Balance;Internal State Manipulation;Sparse Representation Adjustment</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3e3a156f8017f17c24b3dd73769977bfd744c52f.pdf" target="_blank">https://openreview.net/pdf/3e3a156f8017f17c24b3dd73769977bfd744c52f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">As large language models (LLMs) become integral to various applications, ensuring both their safety and utility is paramount. Jailbreak attacks, which manipulate LLMs into generating harmful content, pose significant challenges to this balance. Existing defenses, such as prompt engineering and safety fine-tuning, often introduce computational overhead, increase inference latency, and lack runtime flexibility. Moreover, overly restrictive safety measures can degrade model utility by causing refusals of benign queries. In this paper, we introduce *Jailbreak Antidote*, a method that enables real-time adjustment of LLM safety preferences by manipulating a sparse subset of the model's internal states during inference. By shifting the model's hidden representations along a safety direction with varying strengths, we achieve flexible control over the safety-utility balance without additional token overhead or inference delays. Our analysis reveals that safety-related information in LLMs is sparsely distributed; adjusting approximately *5\%* of the internal state is as effective as modifying the entire state. Extensive experiments on nine LLMs (ranging from 2 billion to 72 billion parameters), evaluated against ten jailbreak attack methods and compared with six defense strategies, validate the effectiveness and efficiency of our approach. By directly manipulating internal states during reasoning, *Jailbreak Antidote* offers a lightweight, scalable solution that enhances LLM safety while preserving utility, opening new possibilities for real-time safety mechanisms in widely-deployed AI systems.</div>
</div>
<div class='paper-counter'>638/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiangyu Wang;Donglin Yang;Ziqin Wang;Hohin Kwan;Jinyu Chen;Wenjun Wu;Hongsheng Li;Yue Liao;Si Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Unmanned Aerial Vehicle;Drone;Vision-Language Navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f6dd1708c5bea6d1e7af5637983d1beb4f8f025b.pdf" target="_blank">https://openreview.net/pdf/f6dd1708c5bea6d1e7af5637983d1beb4f8f025b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a UAV simulation platform, an assistant-guided realistic UAV VLN benchmark, and an MLLM-based method to address the challenges in realistic UAV vision-language navigation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing agents capable of navigating to a target location based on language instructions and visual information, known as vision-language navigation (VLN), has attracted widespread interest. Most research has focused on ground-based agents, while UAV-based VLN remains relatively underexplored. Recent efforts in UAV vision-language navigation predominantly adopt ground-based VLN settings, relying on predefined discrete action spaces and neglecting the inherent disparities in agent movement dynamics and the complexity of navigation tasks between ground and aerial environments. To address these disparities and challenges, we propose solutions from three perspectives: platform, benchmark, and methodology. To enable realistic UAV trajectory simulation in VLN tasks, we propose the OpenUAV platform,  which features diverse environments, realistic flight control, and extensive algorithmic support. We further construct a target-oriented VLN dataset consisting of approximately 12k trajectories on this platform, serving as the first dataset specifically designed for realistic UAV VLN tasks. To tackle the challenges posed by complex aerial environments, we propose an assistant-guided UAV object search benchmark called UAV-Need-Help, which provides varying levels of guidance information to help UAVs better accomplish realistic VLN tasks. We also propose a UAV navigation LLM that, given multi-view images, task descriptions, and assistant instructions, leverages the multimodal understanding capabilities of the MLLM to jointly process visual and textual information, and performs hierarchical trajectory generation. The evaluation results of our method significantly outperform the baseline models, while there remains a considerable gap between our results and those achieved by human operators, underscoring the challenge presented by the UAV-Need-Help task.</div>
</div>
<div class='paper-counter'>639/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">What's the Move? Hybrid Imitation Learning via Salient Points</div>
<div class="field-name">authors:</div>
<div class="field-value">Priya Sundaresan;Hengyuan Hu;Quan Vuong;Jeannette Bohg;Dorsa Sadigh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Imitation Learning;Robot Learning;Robot Manipulation;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/de1b4dfb9b322a36442786fa4e815b45dc87944b.pdf" target="_blank">https://openreview.net/pdf/de1b4dfb9b322a36442786fa4e815b45dc87944b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an imitation learning algorithm for complex robot manipulation with visuospatial generalization; it substantially outperforms SOTA existing methods across 4 real-world tasks and 2 simulated benchmarks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduce **SPHINX**: **S**alient **P**oint-based **H**ybrid **I**mitatio**N** and e**X**ecution, a flexible IL policy that leverages multimodal observations (point clouds and wrist images), along with a hybrid action space of low-frequency, sparse waypoints and high-frequency, dense end effector movements. Given 3D point cloud observations, SPHINX learns to infer task-relevant points within a point cloud, or *salient points*, which support spatial generalization by focusing on semantically meaningful features. These salient points serve as anchor points to predict waypoints for long-range movement, such as reaching target poses in free-space. Once near a salient point, SPHINX learns to switch to predicting dense end-effector movements given close-up wrist images for precise phases of a task. By exploiting the strengths of different input modalities and action representations for different manipulation phases, SPHINX tackles complex tasks in a sample-efficient, generalizable manner. Our method achieves **86.7%**  success across 4 real-world and 2 simulated tasks, outperforming the next best state-of-the-art IL baseline by **41.1%** on average across **440** real world trials. SPHINX additionally generalizes to novel viewpoints, visual distractors, spatial arrangements, and execution speeds with a **1.7x** speedup over the most competitive baseline. Our website (http://sphinx-manip.github.io) provides open-sourced code for data collection, training, and evaluation, along with supplementary videos.</div>
</div>
<div class='paper-counter'>640/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ILLUSION: Unveiling Truth with a Comprehensive Multi-Modal, Multi-Lingual Deepfake Dataset</div>
<div class="field-name">authors:</div>
<div class="field-value">Kartik Thakral;Rishabh Ranjan;Akanksha Singh;Akshat Jain;Mayank Vatsa;Richa Singh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-modal;Multi-Lingual;Deepfake;AIGC</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b4b0f39d84d66fbd58c0ce677e7f47db34c37ab8.pdf" target="_blank">https://openreview.net/pdf/b4b0f39d84d66fbd58c0ce677e7f47db34c37ab8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/837815e4320a00ea2a85e6adc55a6cc3024a7c79.pdf" target="_blank">https://openreview.net/attachment/837815e4320a00ea2a85e6adc55a6cc3024a7c79.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents ILLUSION, an unprecedented large-scale, multi-modal, and multi-lingual deepfake dataset with over a million samples. It aims at development of robust deepfake detection systems, closely simulating real-world scenarios.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The proliferation of deepfakes and AI-generated content has led to a surge in media forgeries and misinformation, necessitating robust detection systems. However, current datasets lack diversity across modalities, languages, and real-world scenarios. To address this gap, we present ILLUSION (Integration of Life-Like Unique Synthetic Identities and Objects from Neural Networks), a large-scale, multi-modaldeepfake dataset comprising 1.3 million samples spanning audio-visual forgeries, 26 languages, challenging noisy environments, and various manipulation protocols. Generated using 28 state-of-the-art generative techniques, ILLUSION includesfaceswaps, audio spoofing, synchronized audio-video manipulations, and synthetic media while ensuring a balanced representation of gender and skin tone for unbiased evaluation. Using Jaccard Index and UpSet plot analysis, we demonstrate ILLUSION’s distinctiveness and minimal overlap with existing datasets, emphasizing its novel generative coverage. We benchmarked image, audio, video, and multi-modal detection models, revealing key challenges such as performance degradation in multilingual and multi-modal contexts, vulnerability to real-world distortions, and limited generalization to zero-day attacks. By bridging synthetic and real-world complexities, ILLUSION provides a challenging yet essential platform for advancing deepfake detection research. The dataset is publicly available at https://www.iab-rubric.org/illusion-database.</div>
</div>
<div class='paper-counter'>641/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuejiang Liu;Jubayer Ibn Hamid;Annie Xie;Yoonho Lee;Max Du;Chelsea Finn</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Action Chunking;Action Decoding;Test-Time Sampling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2bb1ad62b490ac28f51a49a1f148810187244776.pdf" target="_blank">https://openreview.net/pdf/2bb1ad62b490ac28f51a49a1f148810187244776.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a thorough analysis of action chunking and a decoding algorithm to improve it</div>
<div class="field-name">abstract:</div>
<div class="field-value">Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. Yet, its effects on the learned policy remain inconsistent: some studies find it crucial for achieving strong results, while others observe decreased performance. In this paper, we first dissect how action chunking impacts the divergence between a learner and a demonstrator. We find that action chunking allows the learner to better capture the temporal dependencies in demonstrations but at the cost of reduced reactivity to unexpected states. To address this tradeoff, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop adaptation. At each timestep, BID samples multiple candidate predictions and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples that align with previous decisions; (ii) forward contrast, which seeks samples of high likelihood for future plans. By coupling decisions within and across action chunks, BID promotes both long-term consistency and short-term reactivity. Experimental results show that our method boosts the performance of two state-of-the-art generative policies across seven simulation benchmarks and two real-world tasks. Code and videos are available at https://bid-robot.github.io.</div>
</div>
<div class='paper-counter'>642/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yangtao Chen;Zixuan Chen;Junhui Yin;Jing Huo;Pinzhuo Tian;Jieqi Shi;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Manipulation;imitation learning;foundation models;sub-goals;diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ecd829e279f0682dcfeefecc82736686079eb078.pdf" target="_blank">https://openreview.net/pdf/ecd829e279f0682dcfeefecc82736686079eb078.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Robots' ability to follow language instructions and execute diverse 3D manipulation tasks is vital in robot learning. Traditional imitation learning-based methods perform well on seen tasks but struggle with novel, unseen ones due to variability. Recent approaches leverage large foundation models to assist in understanding novel tasks, thereby mitigating this issue. However, these methods lack a task-specific learning process, which is essential for an accurate understanding of 3D environments, often leading to execution failures. In this paper, we introduce GravMAD, a sub-goal-driven, language-conditioned action diffusion framework that combines the strengths of imitation learning and foundation models. Our approach breaks tasks into sub-goals based on language instructions, allowing auxiliary guidance during both training and inference. During training, we introduce Sub-goal Keypose Discovery to identify key sub-goals from demonstrations. Inference differs from training, as there are no demonstrations available, so we use pre-trained foundation models to bridge the gap and identify sub-goals for the current task. In both phases, GravMaps are generated from sub-goals, providing GravMAD with more flexible 3D spatial guidance compared to fixed 3D positions. Empirical evaluations on RLBench show that GravMAD significantly outperforms state-of-the-art methods, with a 28.63\% improvement on novel tasks and a 13.36\% gain on tasks encountered during training. Evaluations on real-world robotic tasks further show that GravMAD can reason about real-world tasks, associate them with relevant visual information, and generalize to novel tasks. These results demonstrate GravMAD's strong multi-task learning and generalization in 3D manipulation. Video demonstrations are available at: https://gravmad.github.io.</div>
</div>
<div class='paper-counter'>643/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</div>
<div class="field-name">authors:</div>
<div class="field-value">Baiting Luo;Ava Pettet;Aron Laszka;Abhishek Dubey;Ayan Mukhopadhyay</div>
<div class="field-name">keywords:</div>
<div class="field-value">Sequential Decision-Making;Monte Carlo Tree Search;Temporal Abstraction;Planning;Model-based Reinforcement Learning;Offline Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bcbe0f8b213d8d29a865395eee16c82e5f6171bb.pdf" target="_blank">https://openreview.net/pdf/bcbe0f8b213d8d29a865395eee16c82e5f6171bb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/39c2ad4d8f408be1924c8a295beb9cdb6cedfcd6.zip" target="_blank">https://openreview.net/attachment/39c2ad4d8f408be1924c8a295beb9cdb6cedfcd6.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A scalable approach for sequential decision-making in high-dimensional continuous action spaces by learning macro actions and using MCTS.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.Empirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces.</div>
</div>
<div class='paper-counter'>644/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Data Scaling Laws in Imitation Learning for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Fanqi Lin;Yingdong Hu;Pingyue Sheng;Chuan Wen;Jiacheng You;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Data Scaling Laws;Imitation Learning;Robotic Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1267dc9fa357b2c74dd373fa3158867c9d404781.pdf" target="_blank">https://openreview.net/pdf/1267dc9fa357b2c74dd373fa3158867c9d404781.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy’s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\% success rates in novel environments with unseen objects.</div>
</div>
<div class='paper-counter'>645/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Lun Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Micro-batch Clipping;Convergence Analysis</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1b07bba8859bce30f85ace2e4a4ac25966c80fca.pdf" target="_blank">https://openreview.net/pdf/1b07bba8859bce30f85ace2e4a4ac25966c80fca.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We revisit micro-batch clipping, a technique to improve memory efficiency in differentially private stochastic gradient descent, and find that it can also provide performance boost.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Micro-batch clipping, a gradient clipping method, has recently shown potential in enhancing auto-speech recognition (ASR) model performance. However, the underlying mechanism behind this improvement remains mysterious, particularly the observation that only certain micro-batch sizes are beneficial. In this paper, we make the first attempt to explain this phenomenon. Inspired by recent data pruning research, we assume that specific training samples may impede model convergence during certain training phases. Under this assumption, the convergence analysis shows that micro-batch clipping can improve the convergence rate asymptotically at the cost of an additional constant bias that does not diminish with more training iterations. The bias is dependent on a few factors and can be minimized at specific micro-batch size, thereby elucidating the existence of the sweet-spot micro-batch size observed previously. We also verify the effectiveness of micro-batch clipping beyond speech models on vision and language models, and show promising performance gains in these domains. An exploration of potential limitations shows that micro-batch clipping is less effective when training data originates from multiple distinct domains.</div>
</div>
<div class='paper-counter'>646/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zhipei Xu;Xuanyu Zhang;Runyi Li;Zecheng Tang;Qing Huang;Jian Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Image Forgery Detection and Localization;Multi-modal Large Language Model;Tamper Detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c2e03abe9dfa3b923b99b40e14a5d9dfde0668a3.pdf" target="_blank">https://openreview.net/pdf/c2e03abe9dfa3b923b99b40e14a5d9dfde0668a3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The rapid development of generative AI is a double-edged sword, which not only facilitates content creation but also makes image manipulation easier and more difficult to detect. Although current image forgery detection and localization (IFDL) methods are generally effective, they tend to face two challenges: \textbf{1)} black-box nature with unknown detection principle, \textbf{2)} limited generalization across diverse tampering methods (e.g., Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the explainable IFDL task and design FakeShield, a multi-modal framework capable of evaluating image authenticity, generating tampered region masks, and providing a judgment basis based on pixel-level and image-level tampering clues. Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery Localization Module (MFLM) to address various types of tamper detection interpretation and achieve forgery localization guided by detailed textual descriptions. Extensive experiments demonstrate that FakeShield effectively detects and localizes various tampering techniques, offering an explainable and superior solution compared to previous IFDL methods. The code is available at https://github.com/zhipeixu/FakeShield.</div>
</div>
<div class='paper-counter'>647/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Solving New Tasks by Adapting Internet Video Knowledge</div>
<div class="field-name">authors:</div>
<div class="field-value">Calvin Luo;Zilai Zeng;Yilun Du;Chen Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value">Text-Conditioned Generalization;Video Diffusion;Adaptation;Planning;Policy Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d63745d12848160974b823c4ca6436e20569bcd8.pdf" target="_blank">https://openreview.net/pdf/d63745d12848160974b823c4ca6436e20569bcd8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5bad753bf262f692f2d21308ade4e8faf6b2dab2.zip" target="_blank">https://openreview.net/attachment/5bad753bf262f692f2d21308ade4e8faf6b2dab2.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We compare techniques for adapting large-scale video generative models to in-domain robotic data, and demonstrate that it facilitates text-conditioned generalization to novel tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors.  When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning.  However, they may not be sensitive to the specificities of the particular environment the agent inhabits.  On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification.  In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations.  We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors.  In particular, we present a novel adaptation strategy, termed *Inverse Probabilistic Adaptation*, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available.</div>
</div>
<div class='paper-counter'>648/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Knowledge Graph Finetuning Enhances Knowledge Manipulation in Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Hanzhu Chen;Xu Shen;Jie Wang;Zehao Wang;Qitan Lv;Junjie He;Rong Wu;Feng Wu;Jieping Ye</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Knowledge Graph;Supervised Fine-tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5f15450a628486a895ba792cfbe5b6d13256024b.pdf" target="_blank">https://openreview.net/pdf/5f15450a628486a895ba792cfbe5b6d13256024b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Despite the impressive performance of general large language models(LLMs), many of their applications in specific domains (e.g., low-data and knowledge-intensive) still confront significant challenges. Supervised fine-tuning (SFT)---where a general LLM is further trained on a small labeled dataset to adapt for specific tasks or domains---has shown great power for developing domain-specific LLMs. However, existing SFT data primarily consist of Question and Answer (Q&A) pairs, which poses a significant challenge for LLMs to comprehend the correlation and logic of knowledge underlying the Q&A. To address this challenge, we propose a conceptually flexible and general framework to boost SFT, namely Knowledge Graph-Driven Supervised Fine-Tuning (KG-SFT). The key idea of KG-SFT is to generate high-quality explanations for each Q&A pair via a structured knowledge graph to enhance the knowledge comprehension and manipulation of LLMs. Specifically, KG-SFT consists of three components: Extractor, Generator, and Detector. For a given Q&A pair, (i) Extractor first identifies entities within Q&A pairs and extracts relevant reasoning subgraphs from external KGs, (ii) Generator then produces corresponding fluent explanations utilizing these reasoning subgraphs, and (iii) finally, Detector performs sentence-level knowledge conflicts detection on these explanations to guarantee the reliability. KG-SFT focuses on generating high-quality explanations to improve the quality of Q&A pair, which reveals a promising direction for supplementing existing data augmentation methods. Extensive experiments on fifteen different domains and six different languages demonstrate the effectiveness of KG-SFT, leading to an accuracy improvement of up to 18% and an average of 8.7% in low-data scenarios.</div>
</div>
<div class='paper-counter'>649/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Physics of Language Models: Part 3.2, Knowledge Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Zeyuan Allen-Zhu;Yuanzhi Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">knowledge manipulation;language models;generative models;reversal curse</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6097941da7c6577acb02e468ee39ff27979fe359.pdf" target="_blank">https://openreview.net/pdf/6097941da7c6577acb02e468ee39ff27979fe359.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: \textbf{retrieval} (e.g., "What is person A's attribute X?"), \textbf{classification} (e.g., "Is A's attribute X even or odd?"), \textbf{comparison} (e.g., "Is A greater than B in attribute X?"), and \textbf{inverse search} (e.g., "Which person's attribute X equals T?").We show that language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. Moreover, their performance in inverse knowledge search is virtually 0\%, regardless of the prompts.Our primary contribution is a \emph{controlled, synthetic experiment} that confirms these weaknesses are \emph{inherent} to language models: they cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored in the models, despite adequate training and sufficient model size. Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs.</div>
</div>
<div class='paper-counter'>650/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Strategic Classification With Externalities</div>
<div class="field-name">authors:</div>
<div class="field-value">Safwan Hossain;Evi Micha;Yiling Chen;Ariel D. Procaccia</div>
<div class="field-name">keywords:</div>
<div class="field-value">strategic classification;game theory;strategic machine learning;algorithmic game theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5c2b0420f46e696a5e2dcdbbe49fafd7329c28c3.pdf" target="_blank">https://openreview.net/pdf/5c2b0420f46e696a5e2dcdbbe49fafd7329c28c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We study strategic classification when participating agents induce externality, from a theoretical perspective.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose a new variant of the strategic classification problem: a principal reveals a classifier, and $n$ agents report their (possibly manipulated) features to be classified. Motivated by real-world applications, our model crucially allows the manipulation of one agent to affect another; that is, it explicitly captures inter-agent externalities. The principal-agent interactions are formally modeled as a Stackelberg game, with the resulting agent manipulation dynamics captured as a simultaneous game. We show that under certain assumptions, the pure Nash Equilibrium of this agent manipulation game is unique and can be efficiently computed. Leveraging this result, PAC learning guarantees are established for the learner: informally, we show that it is possible to learn classifiers that minimize loss on the distribution, even when a random number of agents are manipulating their way to a pure Nash Equilibrium. We also comment on the optimization of such classifiers through gradient-based approaches. This work sets the theoretical foundations for a more realistic analysis of classifiers that are robust against multiple strategic actors interacting in a common environment.</div>
</div>
<div class='paper-counter'>651/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Carl Qi;Dan Haramati;Tal Daniel;Aviv Tamar;Amy Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion;Object-Centric Representation;Robotic Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/30a31a7608d79717d4f30371c7396cd64f010e34.pdf" target="_blank">https://openreview.net/pdf/30a31a7608d79717d4f30371c7396cd64f010e34.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a behavioral cloning method for multi-object manipulation that combines object-centric representations with diffusion models, enabling zero-shot generalization to novel object compositions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Object manipulation is a common component of everyday tasks, but learning to manipulate objects from high-dimensional observations presents significant challenges. These challenges are heightened in multi-object environments due to the combinatorial complexity of the state space as well as of the desired behaviors. While recent approaches have utilized large-scale offline data to train models from pixel observations, achieving performance gains through scaling, these methods struggle with compositional generalization in unseen object configurations with constrained network and dataset sizes. To address these issues, we propose a novel behavioral cloning (BC) approach that leverages object-centric representations and an entity-centric Transformer with diffusion-based optimization, enabling efficient learning from offline image data. Our method first decomposes observations into Deep Latent Particles (DLP), which are then processed by our entity-centric Transformer that computes attention at the particle level, simultaneously predicting object dynamics and the agent's actions. Combined with the ability of diffusion models to capture multi-modal behavior distributions, this results in substantial performance improvements in multi-object tasks and, more importantly, enables compositional generalization. We present BC agents capable of zero-shot generalization to perform tasks with novel compositions of objects and goals, including larger numbers of objects than seen during training. We provide video rollouts on our webpage: https://sites.google.com/view/ec-diffuser.</div>
</div>
<div class='paper-counter'>652/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Moritz Reuss;Jyothish Pari;Pulkit Agrawal;Rudolf Lioutikov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics;Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf" target="_blank">https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Mixture-of-Denoising Experts (MoDE), a novel Diffusion Policy that leverages a noise-conditioned routing strategy to achieve more efficient denoising and improved performance compared to prior approaches.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior.As models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. Therefore, continuing with the current architectures will present a computational roadblock. To address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning.MoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise-conditioned routing, reducing both active parameters by 40\% and inference costs by 90\% via expert caching.Our architecture combines this efficient scaling with noise-conditioned self-attention mechanism, enabling more effective denoising across different noise levels. MoDE achieves state-of-the-art performance on 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01 on CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and Transformer Diffusion Policies by an average of $57\%$ across 4 benchmarks, while using 90\% fewer FLOPs and fewer active parameters compared to default Diffusion Transformer architectures. Furthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies. Code and demonstrations are available at https://mbreuss.github.io/MoDE_Diffusion_Policy.</div>
</div>
<div class='paper-counter'>653/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Causal Information Prioritization for Efficient Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongye Cao;Fan Feng;Tianpei Yang;Jing Huo;Yang Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">causality;reinforcement learning;empowerment;sample efficiency</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/20a8ae0a5dfd269846a9b14cc220610e5ebc4494.pdf" target="_blank">https://openreview.net/pdf/20a8ae0a5dfd269846a9b14cc220610e5ebc4494.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/74ba552d5cd3f2b9e274d004b8d4dd0548cc26b6.zip" target="_blank">https://openreview.net/attachment/74ba552d5cd3f2b9e274d004b8d4dd0548cc26b6.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">To address limitations of blind exploration and poor sample efficiency, we introduce CIP, a novel efficient RL framework that prioritizes causal information through the lens of reward feedback.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across $39$ tasks in $5$ diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.</div>
</div>
<div class='paper-counter'>654/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Linear combinations of latents in generative models: subspaces and beyond</div>
<div class="field-name">authors:</div>
<div class="field-value">Erik Bodin;Alexandru I. Stere;Dragos D Margineantu;Carl Henrik Ek;Henry Moss</div>
<div class="field-name">keywords:</div>
<div class="field-value">generative models;diffusion models;latent space interpolation;latent subspaces;latent representations;flow matching;vae</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7d1ba6d760c90abe680c3281092660124660c0c6.pdf" target="_blank">https://openreview.net/pdf/7d1ba6d760c90abe680c3281092660124660c0c6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show how latent subspaces can be defined in e.g. diffusion models, yielding expressive low-dimensional representations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Sampling from generative models has become a crucial tool for applications like data synthesis and augmentation. Diffusion, Flow Matching and Continuous Normalising Flows have shown effectiveness across various modalities, and rely on latent variables for generation. For experimental design or creative applications that require more control over the generation process, it has become common to manipulate the latent variable directly. However, existing approaches for performing such manipulations (e.g. interpolation or forming low-dimensional representations) only work well in special cases or are network or data-modality specific. We propose Latent Optimal Linear combinations (LOL) as a general-purpose method to form linear combinations of latent variables that adhere to the assumptions of the generative model. As LOL is easy to implement and naturally addresses the broader task of forming any linear combinations, e.g. the construction of subspaces of the latent space, LOL dramatically simplifies the creation of expressive low-dimensional representations of high-dimensional objects.</div>
</div>
<div class='paper-counter'>655/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation</div>
<div class="field-name">authors:</div>
<div class="field-value">Donggon Jang;Yucheol Cho;Suin Lee;Taehyeon Kim;Daeshik Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multimodal Dataset;Multi-target and Multi-granularity Reasoning Segmentation;Benchmark</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a5c3431e3619238b473226bdcacc2561597e8e9f.pdf" target="_blank">https://openreview.net/pdf/a5c3431e3619238b473226bdcacc2561597e8e9f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Introducing the Multi-target and Multi-granularity Reasoning (MMR) dataset and a new framework for detailed object and part-level reasoning segmentation in complex, multi-target scenarios.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The fusion of Large Language Models (LLMs) with vision models is pioneering new possibilities in user-interactive vision-language tasks. A notable application is reasoning segmentation, where models generate pixel-level segmentation masks by comprehending implicit meanings in human instructions. However, seamless human-AI interaction demands more than just object-level recognition; it requires understanding both objects and the functions of their detailed parts, particularly in multi-target scenarios. For example, when instructing a robot to \textit{“turn on the TV"}, there could be various ways to accomplish this command. Recognizing multiple objects capable of turning on the TV, such as the TV itself or a remote control (multi-target), provides more flexible options and aids in finding the optimized scenario. Furthermore, understanding specific parts of these objects, like the TV's button or the remote's button (part-level), is important for completing the action. Unfortunately, current reasoning segmentation datasets predominantly focus on a single target object-level reasoning, which limits the detailed recognition of an object's parts in multi-target contexts. To address this gap, we construct a large-scale dataset called Multi-target and Multi-granularity Reasoning (MMR). MMR comprises 194K complex and implicit instructions that consider multi-target, object-level, and part-level aspects, based on pre-existing image-mask sets. This dataset supports diverse and context-aware interactions by hierarchically providing object and part information. Moreover, we propose a straightforward yet effective framework for multi-target, object-level, and part-level reasoning segmentation. Experimental results on MMR show that the proposed method can reason effectively in multi-target and multi-granularity scenarios, while the existing reasoning segmentation model still has room for improvement. The dataset is available at \url{https://github.com/jdg900/MMR}.</div>
</div>
<div class='paper-counter'>656/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Subtask-Aware Visual Reward Learning from Segmented Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Changyeon Kim;Minho Heo;Doohyun Lee;Honglak Lee;Jinwoo Shin;Joseph J Lim;Kimin Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Reward Learning;Robotic Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/18d7fe7853d07ace357a88ffd892f59889d69123.pdf" target="_blank">https://openreview.net/pdf/18d7fe7853d07ace357a88ffd892f59889d69123.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2be10b8d059956b891d8f31da453fd417569f388.zip" target="_blank">https://openreview.net/attachment/2be10b8d059956b891d8f31da453fd417569f388.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel reward learning framework utilizing action-free videos with minimal guidance for long-horizon complex robotic tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning (RL) agents have demonstrated their potential across various robotic tasks. However, they still heavily rely on human-engineered reward functions, requiring extensive trial-and-error and access to target behavior information, often unavailable in real-world settings. This paper introduces REDS: REward learning from Demonstration with Segmentations, a novel reward learning framework that leverages action-free videos with minimal supervision. Specifically, REDS employs video demonstrations segmented into subtasks from diverse sources and treats these segments as ground-truth rewards. We train a dense reward function conditioned on video segments and their corresponding subtasks to ensure alignment with ground-truth reward signals by minimizing the Equivalent-Policy Invariant Comparison distance. Additionally, we employ contrastive learning objectives to align video representations with subtasks, ensuring precise subtask inference during online interactions. Our experiments show that REDS significantly outperforms baseline methods on complex robotic manipulation tasks in Meta-World and more challenging real-world tasks, such as furniture assembly in FurnitureBench, with minimal human intervention. Moreover, REDS facilitates generalization to unseen tasks and robot embodiments, highlighting its potential for scalable deployment in diverse environments.</div>
</div>
<div class='paper-counter'>657/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hyungjoo Chae;Namyoung Kim;Kai Tzu-iunn Ong;Minju Gwak;Gwanwoo Song;Jihoon Kim;Sunghwan Kim;Dongha Lee;Jinyoung Yeo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Web Agent;World Model;Digital Agent;Planning;LLM</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f707cdce9564aa4399e705eb27b4f3681c601ad8.pdf" target="_blank">https://openreview.net/pdf/f707cdce9564aa4399e705eb27b4f3681c601ad8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7c2030481f75b2e1ce8fbe46873fa3b62b61f138.zip" target="_blank">https://openreview.net/attachment/7c2030481f75b2e1ce8fbe46873fa3b62b61f138.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have recently gained much attention in building autonomous agents. However, performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents.</div>
</div>
<div class='paper-counter'>658/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yang Tian;Sizhe Yang;Jia Zeng;Ping Wang;Dahua Lin;Hao Dong;Jiangmiao Pang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Manipulation ; Pre-training ; Visual Foresight ; Inverse Dynamics ; Large-scale robot dataset</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/91a1d5e7a6417130767d8f9fb4625586f6f9a16f.pdf" target="_blank">https://openreview.net/pdf/91a1d5e7a6417130767d8f9fb4625586f6f9a16f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/b0834376a4280c3d172cd07cd3b254c105e2645d.zip" target="_blank">https://openreview.net/attachment/b0834376a4280c3d172cd07cd3b254c105e2645d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available.</div>
</div>
<div class='paper-counter'>659/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Diffusion Policy Policy Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Allen Z. Ren;Justin Lidard;Lars Lien Ankile;Anthony Simeonov;Pulkit Agrawal;Anirudha Majumdar;Benjamin Burchfiel;Hongkai Dai;Max Simchowitz</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;diffusion policy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bd9dc7ce67343a8a2535086502ae559a508d4aba.pdf" target="_blank">https://openreview.net/pdf/bd9dc7ce67343a8a2535086502ae559a508d4aba.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a053c1d971780ef20a946bf1c72170ab2e99cc08.zip" target="_blank">https://openreview.net/attachment/a053c1d971780ef20a946bf1c72170ab2e99cc08.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce DPPO, an algorithmic framework and set of best practices for fine-tuning diffusion-based policies in continuous control and robot learning tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task.</div>
</div>
<div class='paper-counter'>660/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Image Watermarks are Removable using Controllable Regeneration from Clean Noise</div>
<div class="field-name">authors:</div>
<div class="field-value">Yepeng Liu;Yiren Song;Hai Ci;Yu Zhang;Haofan Wang;Mike Zheng Shou;Yuheng Bu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Watermark;Detection;Robustness;Diffusion Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a161be498fd7a39c9d7c7e8ae6eeeb3d0ec412b2.pdf" target="_blank">https://openreview.net/pdf/a161be498fd7a39c9d7c7e8ae6eeeb3d0ec412b2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a controllable regeneration method for effective image watermark removal. We aim for our method to serve as a benchmark to evaluate and enhance the robustness of future watermarking techniques.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying state-of-the-art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a \textbf{clean Gaussian noise} via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency/quality and enhanced watermark removal performance compared to existing regeneration approaches. Our code is available at \url{https://github.com/yepengliu/CtrlRegen}.</div>
</div>
<div class='paper-counter'>661/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption</div>
<div class="field-name">authors:</div>
<div class="field-value">Joonsung Jeon;Woo Jae Kim;Suhyeon Ha;Sooel Son;Sung-eui Yoon</div>
<div class="field-name">keywords:</div>
<div class="field-value">Adversarial Example;Adversarial Attack;Inpainting;Image Protection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3ee5ad72e6aefa024eb7dfff1052a7822eb7fa08.pdf" target="_blank">https://openreview.net/pdf/3ee5ad72e6aefa024eb7dfff1052a7822eb7fa08.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bd72b45933c2aaf8306e76dd9f8f8ee321b77b8d.pdf" target="_blank">https://openreview.net/attachment/bd72b45933c2aaf8306e76dd9f8f8ee321b77b8d.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While existing methods for protecting images from manipulation in diffusion-based generative models have primarily focused on image-to-image and text-to-image tasks, the challenge of preventing unauthorized inpainting has been rarely addressed, often resulting in suboptimal protection performance. To mitigate inpainting abuses, we propose ADVPAINT, a novel defensive framework that generates adversarial perturbations that effectively disrupt the adversary’s inpainting tasks. ADVPAINT targets the self- and cross-attention blocks in a target diffusion inpainting model to distract semantic understanding and prompt interactions during image generation. ADVPAINT also employs a two-stage perturbation strategy, dividing the perturbation region based on an enlarged bounding box around the object, enhancing robustness across diverse masks of varying shapes and sizes. Our experimental results demonstrate that ADVPAINT’s perturbations are highly effective in disrupting the adversary’s inpainting tasks, outperforming existing methods; ADVPAINT attains over a 100-point increase in FID and substantial decreases in precision.</div>
</div>
<div class='paper-counter'>662/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Predicate Hierarchies Improve Few-Shot State Classification</div>
<div class="field-name">authors:</div>
<div class="field-value">Emily Jin;Joy Hsu;Jiajun Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">few-shot state classification;predicate hierarchies</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d9f3c5117ed4f4854e2fdeb9bc54690b68704e8c.pdf" target="_blank">https://openreview.net/pdf/d9f3c5117ed4f4854e2fdeb9bc54690b68704e8c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/14bbd72979febd5a959b528ee093dc6ec58a276d.pdf" target="_blank">https://openreview.net/attachment/14bbd72979febd5a959b528ee093dc6ec58a276d.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A state classification model that encodes predicate hierarchies to generalize effectively in few-shot scenarios.</div>
<div class="field-name">abstract:</div>
<div class="field-value">State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data.</div>
</div>
<div class='paper-counter'>663/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Agent S: An Open Agentic Framework that Uses Computers Like a Human</div>
<div class="field-name">authors:</div>
<div class="field-value">Saaket Agashe;Jiuzhou Han;Shuyu Gan;Jiachen Yang;Ang Li;Xin Eric Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Vision and Language Model;Agents;Retrieval Augmented Generation;GUI;Large Language Models;Agent Computer Interface</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5c0a5b17c744fe619f72841610ad18eb2216c723.pdf" target="_blank">https://openreview.net/pdf/5c0a5b17c744fe619f72841610ad18eb2216c723.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c6cf3b18dd93b7aa3ec69681cdcc54bb65fc2fe7.zip" target="_blank">https://openreview.net/attachment/c6cf3b18dd93b7aa3ec69681cdcc54bb65fc2fe7.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37\% on success rate (an 83.6\% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.</div>
</div>
<div class='paper-counter'>664/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Contractive Dynamical Imitation Policies for Efficient Out-of-Sample Recovery</div>
<div class="field-name">authors:</div>
<div class="field-value">Amin Abyaneh;Mahrokh Ghoddousi Boroujeni;Hsiu-Chin Lin;Giancarlo Ferrari-Trecate</div>
<div class="field-name">keywords:</div>
<div class="field-value">Learning from demonstration;Safe imitation learning;Robotics;Dynamical system;Contraction theory</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a62f3af81ebbf403063ea92199e81f19a447c9bb.pdf" target="_blank">https://openreview.net/pdf/a62f3af81ebbf403063ea92199e81f19a447c9bb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/126b30dc56d3d862a3e7adc96bc947178e94a4a8.zip" target="_blank">https://openreview.net/attachment/126b30dc56d3d862a3e7adc96bc947178e94a4a8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a class of contractive imitation policies with theoretical guarantees and out-of-sample error bounds for robot learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning is a data-driven approach to learning policies from expert behavior, but it is prone to unreliable outcomes in out-of-sample (OOS) regions. While previous research relying on stable dynamical systems guarantees convergence to a desired state, it often overlooks transient behavior. We propose a framework for learning policies modeled by contractive dynamical systems, ensuring that all policy rollouts converge regardless of perturbations, and in turn, enable efficient OOS recovery. By leveraging recurrent equilibrium networks and coupling layers, the policy structure guarantees contractivity for any parameter choice, which facilitates unconstrained optimization. We also provide theoretical upper bounds for worst-case and expected loss to rigorously establish the reliability of our method in deployment. Empirically, we demonstrate substantial OOS performance improvements for simulated robotic manipulation and navigation tasks. See [sites.google.com/view/contractive-dynamical-policies](https://sites.google.com/view/contractive-dynamical-policies) for our codebase and highlight of the results.</div>
</div>
<div class='paper-counter'>665/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives</div>
<div class="field-name">authors:</div>
<div class="field-value">Wang Qinsi;Jinghan Ke;Masayoshi Tomizuka;Kurt Keutzer;Chenfeng Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Model Compression;Low-Rank Decomposition;SVD;Effecient LLM;Differentiable</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0040a2ba2284bb19baf84133ac2054adbf1896d9.pdf" target="_blank">https://openreview.net/pdf/0040a2ba2284bb19baf84133ac2054adbf1896d9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We are the first to theoretically prove that truncating activations outperforms truncating weights, and we propose Dobi-SVD, the first SVD-based method to significantly compress LLM weights with minimal performance drop.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have sparked a new wave of AI applications; however, their substantial computational costs and memory demands pose significant challenges to democratizing access to LLMs for a broader audience. Singular Value Decomposition (SVD), a technique studied for decades, offers a hardware-independent and flexibly tunable solution for LLM compression. In this paper, we present new directions using SVD: we first theoretically analyze the optimality of truncating weights and truncating activations, then we further identify three key issues on SVD-based LLM compression, including (1) How can we determine the optimal truncation position for each weight matrix in LLMs? (2) How can we efficiently update the weight matrices based on truncation position? (3) How can we address the inherent "injection" nature that results in the information loss of the SVD? We propose an effective approach, **Dobi-SVD**, to tackle the three issues. First, we propose a **differentiable** truncation-value learning mechanism, along with gradient-robust backpropagation, enabling the model to adaptively find the optimal truncation positions. Next, we utilize the Eckart-Young-Mirsky theorem to derive a theoretically **optimal** weight update formula through rigorous mathematical analysis. Lastly, by observing and leveraging the quantization-friendly nature of matrices after SVD decomposition, we reconstruct a mapping between truncation positions and memory requirements, establishing a **bijection** from truncation positions to memory. Experimental results show that with a 40\% parameter-compression rate, our method achieves a perplexity of 9.07 on the Wikitext2 dataset with the compressed LLama-7B model, a 78.7\% improvement over the state-of-the-art SVD for LLM compression method. We emphasize that Dobi-SVD is the first to achieve such a high-ratio LLM compression with minimal performance drop.  We also extend our Dobi-SVD to VLM compression, achieving a 20\% increase in throughput with minimal performance degradation. We hope that the inference speedup—up to 12.4x on 12GB NVIDIA Titan Xp GPUs and 3x on 80GB A100 GPUs for LLMs, and 1.2x on 80GB A100 GPUs for VLMs—will bring significant benefits to the broader community such as robotics.</div>
</div>
<div class='paper-counter'>666/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Neural Wave Equation for Irregularly Sampled Sequence Data</div>
<div class="field-name">authors:</div>
<div class="field-value">Arkaprava Majumdar;M Anand Krishna;P. K. Srijith</div>
<div class="field-name">keywords:</div>
<div class="field-value">Wave Equation;Neural ODE;Sequence Labelling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8ed140ad99d8ff52315acf85732ba20a69b155a7.pdf" target="_blank">https://openreview.net/pdf/8ed140ad99d8ff52315acf85732ba20a69b155a7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ab9b94b444bc096cf67dc0570e8b8110f1e75185.zip" target="_blank">https://openreview.net/attachment/ab9b94b444bc096cf67dc0570e8b8110f1e75185.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Partial Differential Equations parameterised by a Neural Network (like Neural ODE) can be used to solve sequence modeling problems. We hypothesize why this might be the case and demonstrate that it outpeforms many known continuous RNN models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of hidden states continuously in time, to address irregularly sampled sequence data. However, they assume a fixed architectural depth and limit their flexibility to adapt to data sets with varying complexities. We propose the neural wave equation, a novel deep learning method inspired by the wave equation, to address this through continuous modeling of depth. Neural Wave Equation models the evolution of hidden states continuously across time as well as depth by using a non-homogeneous wave equation parameterized by a neural network.  Through d'Alembert's analytical solution of the wave equation, we also show that the neural wave equation provides denser connections across the hidden states, allowing for better modeling capability.  We conduct experiments on several sequence labeling problems involving irregularly sampled sequence data and demonstrate the superior performance of the proposed neural wave equation model.</div>
</div>
<div class='paper-counter'>667/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">StringLLM: Understanding the String Processing Capability of Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Xilong Wang;Hao Fu;Jindong Wang;Neil Zhenqiang Gong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;String Processing;Benchmarks;Datasets</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3e147a9e66e1e748d98d7ec870ca9acfa92dc4f2.pdf" target="_blank">https://openreview.net/pdf/3e147a9e66e1e748d98d7ec870ca9acfa92dc4f2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">The first comprehensive study to understand the string processing capability of Large Language Models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">String processing, which mainly involves the analysis and manipulation of strings, is a fundamental component of modern computing. Despite the significant advancements of large language models (LLMs) in various natural language processing (NLP) tasks, their capability in string processing remains underexplored and underdeveloped. To bridge this gap, we present a comprehensive study of LLMs' string processing capability. In particular, we first propose StringLLM, a method to construct datasets for benchmarking string processing capability of LLMs. We use StringLLM to build a series of datasets, referred to as StringBench. It encompasses a wide range of string processing tasks, allowing us to systematically evaluate LLMs' performance in this area. Our evaluations indicate that LLMs struggle with accurately processing strings compared to humans. To uncover the underlying reasons for this limitation, we conduct an in-depth analysis and subsequently propose an effective approach that significantly enhances LLMs' string processing capability via fine-tuning. This work provides a foundation for future research to understand LLMs' string processing capability. Our code and data are available at https://github.com/wxl-lxw/StringLLM.</div>
</div>
<div class='paper-counter'>668/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</div>
<div class="field-name">authors:</div>
<div class="field-value">Wayne Wu;Honglin He;Jack He;Yiran Wang;Chenda Duan;Zhizheng Liu;Quanyi Li;Bolei Zhou</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Simulation;Micromobility</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2148ac06726c822348288a8487f9675686820d83.pdf" target="_blank">https://openreview.net/pdf/2148ac06726c822348288a8487f9675686820d83.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/faeb6c011ad4abcf9f778f6c2f894b34e6cb11d3.zip" target="_blank">https://openreview.net/attachment/faeb6c011ad4abcf9f778f6c2f894b34e6cb11d3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">MetaUrban is a compositional simulation platform for AI-driven urban micromobility research. It will be open-source to enable more research opportunities for the community, and foster generalizable and safe embodied AI and micromobility in cities.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Public urban spaces such as streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in robotics and embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. **Micromobility** enabled by AI for short-distance travel in public urban spaces plays a crucial component in future transportation systems. It is essential to ensure the generalizability and safety of AI models used for maneuvering mobile machines. In this work, we present **MetaUrban**, a *compositional* simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an *infinite* number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and data have been released.</div>
</div>
<div class='paper-counter'>669/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Qingtao Liu;Yu Cui;Zhengnan Sun;Gaofeng Li;Jiming Chen;Qi Ye</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics; Manipulation; Vision and tactile; Multi-modal pretraining; Reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6461a4f5e160c1de5d38f87d547b9d45deb9234a.pdf" target="_blank">https://openreview.net/pdf/6461a4f5e160c1de5d38f87d547b9d45deb9234a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d40ff5944dc10f511e518112c0db7abff00c0c55.zip" target="_blank">https://openreview.net/attachment/d40ff5944dc10f511e518112c0db7abff00c0c55.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision and touch are the most commonly used senses in human manipulation. While leveraging human manipulation videos for robotic task pretraining has shown promise in prior works, it is limited to image and language modalities and deployment to simple parallel grippers. In this paper, aiming to address the limitations, we collect a vision-tactile dataset by humans manipulating 10 daily tasks and 182 objects. In contrast with the existing datasets, our dataset is the first visual-tactile dataset for complex robotic manipulation skill learning. Also, we introduce a novel benchmark, featuring six complex dexterous manipulation tasks and a reinforcement learning-based vision-tactile skill learning framework. 18 non-pretraining and pretraining methods within the framework are designed and compared to investigate the effectiveness of different modalities and pertaining strategies. Key findings based on our benchmark results and analyses experiments include: 1) Despite the tactile modality used in our experiments being binary and sparse, including it directly in the policy training boosts the success rate by about 20\% and joint pretraining it with vision gains a further 20\%. 2) Joint pretraining visual-tactile modalities exhibits strong adaptability in unknown tasks and achieves robust performance among all tasks. 3) Using binary tactile signals with vision is robust to viewpoint setting, tactile noise, and the binarization threshold, which facilitates to the visual-tactile policy to be deployed in reality. The dataset and benchmark are available at \url{https://github.com/LQTS/VTDexManip}.</div>
</div>
<div class='paper-counter'>670/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Instant Policy: In-Context Imitation Learning via Graph Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Vitalis Vosylius;Edward Johns</div>
<div class="field-name">keywords:</div>
<div class="field-value">In-context Imitation Learning;Robotic Manipulation;Graph Neural Networks;Diffusion Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7d4ed7ffbcb999a4d1131f9686b102f98850b7cf.pdf" target="_blank">https://openreview.net/pdf/7d4ed7ffbcb999a4d1131f9686b102f98850b7cf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/716b62b74b66441f9330fe9d7f0dfa737ce2e89c.zip" target="_blank">https://openreview.net/attachment/716b62b74b66441f9330fe9d7f0dfa737ce2e89c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We formulate In-Context Imitation Learning as a diffusion-based graph generation problem and learn it using procedurally generated pseudo-demonstrations.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks.</div>
</div>
<div class='paper-counter'>671/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">UniDrive: Towards Universal Driving Perception Across Camera Configurations</div>
<div class="field-name">authors:</div>
<div class="field-value">Ye Li;Wenzhao Zheng;Xiaonan Huang;Kurt Keutzer</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;3D Detection;Sensor Configuration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/376b8471ac198c466bf57891c7eec4f1b12f716e.pdf" target="_blank">https://openreview.net/pdf/376b8471ac198c466bf57891c7eec4f1b12f716e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/42e0c87313e07f8567c9927b445e193ee1a5ec01.zip" target="_blank">https://openreview.net/attachment/42e0c87313e07f8567c9927b445e193ee1a5ec01.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-centric autonomous driving has demonstrated excellent performance with economical sensors. As the fundamental step, 3D perception aims to infer 3D information from 2D images based on 3D-2D projection. This makes driving perception models susceptible to sensor configuration (e.g., camera intrinsics and extrinsics) variations. However, generalizing across camera configurations is important for deploying autonomous driving models on different car models. In this paper, we present UniDrive, a novel framework for vision-centric autonomous driving to achieve universal perception across camera configurations. We deploy a set of unified virtual cameras and propose a ground-aware projection method to effectively transform the original images into these unified virtual views. We further propose a virtual configuration optimization method by minimizing the expected projection error between original and virtual cameras. The proposed virtual camera projection can be applied to existing 3D perception methods as a plug-and-play module to mitigate the challenges posed by camera parameter variability, resulting in more adaptable and reliable driving perception models. To evaluate the effectiveness of our framework, we collect a dataset on CARLA by driving the same routes while only modifying the camera configurations. Experimental results demonstrate that our method trained on one specific camera configuration can generalize to varying configurations with minor performance degradation.</div>
</div>
<div class='paper-counter'>672/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">M^3PC: Test-time Model Predictive Control using Pretrained Masked Trajectory Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Kehan Wen;Yutong Hu;Yao Mu;Lei Ke</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline-to-Online Reinforcement Learning;Model-based Reinforcement Learning;Masked Autoencoding;Robot Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d82891202728fac4e3c8f00b0f082d8d6bc2f510.pdf" target="_blank">https://openreview.net/pdf/d82891202728fac4e3c8f00b0f082d8d6bc2f510.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/73bf0fa4c7e66b228a27a931ea7975067fdf812a.zip" target="_blank">https://openreview.net/attachment/73bf0fa4c7e66b228a27a931ea7975067fdf812a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Enhance Transformer for RL by employing the Model itself for test-time MPC, achieving better performance in offline RL and offline-to-online RL for both simulated and real-world robotic tasks, with additional goal-reaching capabilities.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent work in Offline Reinforcement Learning (RL) has shown that  a unified transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (e.g., states, actions, rewards) within given trajectory datasets. However, this information has not been fully exploited during the inference phase, where the agent needs to generate an optimal policy instead of just reconstructing masked components from unmasked. Given that a pretrained trajectory model can act as both a Policy Model and a World Model with appropriate mask patterns, we propose using Model Predictive Control (MPC) at test time to leverage the model's own predictive capacity to guide its action selection. Empirical results on D4RL and RoboMimic show that our inference-phase MPC significantly improves the decision-making performance of a pretrained trajectory model without any additional parameter training. Furthermore, our framework can be adapted to Offline to Online (O2O) RL and Goal Reaching RL, resulting in more substantial performance gains when an additional online interaction budget is given, and better generalization capabilities when different task targets are specified. Code is available: \href{https://github.com/wkh923/m3pc}{\texttt{https://github.com/wkh923/m3pc}}.</div>
</div>
<div class='paper-counter'>673/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">The Value of Sensory Information to a Robot</div>
<div class="field-name">authors:</div>
<div class="field-value">Arjun Krishna;Edward S. Hu;Dinesh Jayaraman</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;limited sensing;perception;imitation learning;reinforcement learning;planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d8f9b2e71190288de8742716c39af0dbadc85802.pdf" target="_blank">https://openreview.net/pdf/d8f9b2e71190288de8742716c39af0dbadc85802.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e2763712a14cc582d81ebcb943a0d8bed694854c.pdf" target="_blank">https://openreview.net/attachment/e2763712a14cc582d81ebcb943a0d8bed694854c.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel approach to study when and how frequently state-of-the-art robotic policies need to sense the world reveals many interesting insights and untapped efficiencies</div>
<div class="field-name">abstract:</div>
<div class="field-value">A decision-making agent, such as a robot, must observe and react to any new task-relevant information that becomes available from its environment. We seek to study a fundamental scientific question: what value does sensory information hold to an agent at various moments in time during the execution of a task? Towards this, we empirically study agents of varying architectures, generated with varying policy synthesis approaches (imitation, RL, model-based control), on diverse robotics tasks. For each robotic agent, we characterize its regret in terms of performance degradation when state observations are withheld from it at various task states for varying lengths of time. We find that sensory information is surprisingly rarely task-critical in many commonly studied task setups. Task characteristics such as stochastic dynamics largely dictate the value of sensory information for a well-trained robot; policy architectures such as planning vs. reactive control generate more nuanced second-order effects. Further, sensing efficiency is curiously correlated with task proficiency: in particular, fully trained high-performing agents are more robust to sensor loss than novice agents early in their training. Overall, our findings characterize the tradeoffs between sensory information and task performance in practical sequential decision making tasks, and pave the way towards the design of more resource-efficient decision-making agents.</div>
</div>
<div class='paper-counter'>674/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SafeDiffuser: Safe Planning with Diffusion Probabilistic Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Xiao;Tsun-Hsuan Wang;Chuang Gan;Ramin Hasani;Mathias Lechner;Daniela Rus</div>
<div class="field-name">keywords:</div>
<div class="field-value">Diffusion model;Safety guarantees;Planning and control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/97b1bd5e50999f2d4d388a86da1b1b72e9d6e545.pdf" target="_blank">https://openreview.net/pdf/97b1bd5e50999f2d4d388a86da1b1b72e9d6e545.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/88701bafd71e3b6b48b1a100d7c904ac2e62bee9.zip" target="_blank">https://openreview.net/attachment/88701bafd71e3b6b48b1a100d7c904ac2e62bee9.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new method to ensure diffusion probabilistic models satisfy specifications by using a control theoretic method</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models have shown promise in data-driven planning. While these planners are commonly employed in applications where decisions are critical, they still lack established safety guarantees. In this paper, we address this limitation by introducing SafeDiffuser, a method to equip diffusion models with safety guarantees via control barrier functions. The key idea of our approach is to embed finite-time diffusion invariance, i.e., a form of specification consisting of safety constraints, into the denoising diffusion procedure. This way we enable data generation under safety constraints. We show that SafeDiffusers maintain the generative performance of diffusion models while also providing robustness in safe data generation. We evaluate our method on a series of tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, and demonstrate the advantages of robustness over vanilla diffusion models.</div>
</div>
<div class='paper-counter'>675/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LLaRA: Supercharging Robot Learning Data for Vision-Language Policy</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiang Li;Cristina Mata;Jongwoo Park;Kumara Kahatapitiya;Yoo Sung Jang;Jinghuan Shang;Kanchana Ranasinghe;Ryan D Burgert;Mu Cai;Yong Jae Lee;Michael S Ryoo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;VLM;Behavior Cloning;Instruction Tuning;Self-supervised Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/88e833c98e7c9f665ef182cf0d30f65c58655784.pdf" target="_blank">https://openreview.net/pdf/88e833c98e7c9f665ef182cf0d30f65c58655784.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5e53807aa2ae6c44d146127afcce71b92549db4e.zip" target="_blank">https://openreview.net/attachment/5e53807aa2ae6c44d146127afcce71b92549db4e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Effortlessly and efficiently transform a pretrained VLM into a robot policy and enhance its performance using auxiliary data generated in a self-supervised manner.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision Language Models (VLMs) have recently been leveraged to generate robotic actions, forming Vision-Language-Action (VLA) models. However, directly adapting a pretrained VLM for robotic control remains challenging, particularly when constrained by a limited number of robot demonstrations. In this work, we introduce LLaRA: Large Language and Robotics Assistant, a framework that formulates robot action policy as visuo-textual conversations and enables an efficient transfer of a pretrained VLM into a powerful VLA, motivated by the success of visual instruction tuning in Computer Vision. First, we present an automated pipeline to generate conversation-style instruction tuning data for robots from existing behavior cloning datasets, aligning robotic actions with image pixel coordinates. Further, we enhance this dataset in a self-supervised manner by defining six auxiliary tasks, without requiring any additional action annotations. We show that a VLM finetuned with a limited amount of such datasets can produce meaningful action decisions for robotic control. Through experiments across multiple simulated and real-world tasks, we demonstrate that LLaRA achieves state-of-the-art performance while preserving the generalization capabilities of large language models. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.</div>
</div>
<div class='paper-counter'>676/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting</div>
<div class="field-name">authors:</div>
<div class="field-value">Qihang Zhang;Yinghao Xu;Chaoyang Wang;Hsin-Ying Lee;Gordon Wetzstein;Bolei Zhou;Ceyuan Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">image editting;gaussian splatting;3D</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bef3ef638a7d38bfd4f0e8b6e265ab7b9f1696d5.pdf" target="_blank">https://openreview.net/pdf/bef3ef638a7d38bfd4f0e8b6e265ab7b9f1696d5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/76d3fdb0abac80bf28c02def8d44cd505ab07a1f.zip" target="_blank">https://openreview.net/attachment/76d3fdb0abac80bf28c02def8d44cd505ab07a1f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing.</div>
</div>
<div class='paper-counter'>677/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding</div>
<div class="field-name">authors:</div>
<div class="field-value">Henry Zheng;Hao Shi;Qihang Peng;Yong Xien Chng;Rui Huang;Yepeng Weng;zhongchao shi;Gao Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Visual Grounding;Embodied AI;Egocentric Vision</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/62bd16ea0919efef86e53459069a9dc57160d76d.pdf" target="_blank">https://openreview.net/pdf/62bd16ea0919efef86e53459069a9dc57160d76d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">DenseGrounding enhances visual and textual semantics to significantly improve ego-centric 3D visual grounding, outperforming existing methods.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverage large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, achieving improvements of **5.81%** and **7.56%** when trained on the comprehensive full training dataset and smaller mini subset, respectively, further advancing the SOTA in ego-centric 3D visual grounding. Our method also achieves **1st place** and receives **Innovation Award** in the 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.</div>
</div>
<div class='paper-counter'>678/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency</div>
<div class="field-name">authors:</div>
<div class="field-value">Qixin Zhang;Zongqi Wan;Yu Yang;Li Shen;Dacheng Tao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Online Learning;Submodular Maximization;Surrogate Gradient;Multi-Agent</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cf46dc3f49864e5412fc5ac973a0fb658c1f6f90.pdf" target="_blank">https://openreview.net/pdf/cf46dc3f49864e5412fc5ac973a0fb658c1f6f90.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a42be760bbfb1cc19af0c1901718fe0f03776f58.zip" target="_blank">https://openreview.net/attachment/a42be760bbfb1cc19af0c1901718fe0f03776f58.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm,  are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a $\textbf{MA-OSMA}$ algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, $\textbf{MA-OSMA}$ leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in $\textbf{MA-OSMA}$, we also introduce a projection-free $\textbf{MA-OSEA}$ algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of $\widetilde{O}(\sqrt{\frac{C_{T}T}{1-\beta}})$ against a  $(\frac{1-e^{-c}}{c})$-approximation to the best comparator in hindsight, where $C_{T}$ is the deviation of maximizer sequence, $\beta$ is the spectral gap of the network and $c$ is the joint curvature of submodular objectives. This result significantly improves the $(\frac{1}{1+c})$-approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking.</div>
</div>
<div class='paper-counter'>679/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Language Guided Skill Discovery</div>
<div class="field-name">authors:</div>
<div class="field-value">Seungeun Rho;Laura Smith;Tianyu Li;Sergey Levine;Xue Bin Peng;Sehoon Ha</div>
<div class="field-name">keywords:</div>
<div class="field-value">Unsupervised Skill Discovery;Guided Skill Discovery;Reinforcement Learning;Language Guided RL</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dfe591eb362df2066cb37277f6986e7ea4b78785.pdf" target="_blank">https://openreview.net/pdf/dfe591eb362df2066cb37277f6986e7ea4b78785.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e1829bb9f9fb03a6e317e6fef28404d9df1051c0.zip" target="_blank">https://openreview.net/attachment/e1829bb9f9fb03a6e317e6fef28404d9df1051c0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a skill discovery method that enable discovery of semantically diverse skills using LLM.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for downstream tasks, obtaining a semantically diverse repertoire of skills is crucial. While some approaches use discriminators to acquire distinguishable skills and others focus on increasing state coverage, the direct pursuit of ‘semantic diversity’ in skills remains underexplored. We hypothesize that leveraging the semantic knowledge of large language models (LLM) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt. Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.</div>
</div>
<div class='paper-counter'>680/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Adversarial Search Engine Optimization for Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Fredrik Nestaas;Edoardo Debenedetti;Florian Tramèr</div>
<div class="field-name">keywords:</div>
<div class="field-value">large language models;security;prompt injection;search engine optimization;function calling</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0013ea65c64e643a504794b3f210786ccb572e26.pdf" target="_blank">https://openreview.net/pdf/0013ea65c64e643a504794b3f210786ccb572e26.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show that web pages and plugins can manipulate LLM systems for economic gains.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) are increasingly used in applications where the model selects from competing third-party content, such as in LLM-powered search engines or chatbot plugins.In this paper, we introduce _Preference Manipulation Attacks_, a new class of attacks that manipulate an LLM's selections to favor the attacker. We demonstrate that carefully crafted website content or plugin documentations can trick an LLM to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization (a form of adversarial Search Engine Optimization).We show this can lead to a _prisoner's dilemma_, where all parties are incentivized to launch attacks, but this collectively  degrades the LLM's outputs for everyone. We demonstrate our attacks on production LLM search engines (Bing and Perplexity) and plugin APIs (for GPT-4 and Claude). As LLMs are increasingly used to rank third-party content, we expect Preference Manipulation Attacks to emerge as a significant threat.</div>
</div>
<div class='paper-counter'>681/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongyin Zhang;Pengxiang Ding;Shangke Lyu;Ying Peng;Donglin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Manipulation; Vision Language Action Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a66d2f4c2bd2fc1a6dbe8313e8800f90e45c7195.pdf" target="_blank">https://openreview.net/pdf/a66d2f4c2bd2fc1a6dbe8313e8800f90e45c7195.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel closed-loop VLA method GEVRM that integrates the internal model control principle to enhance the robustness of robot visual manipulation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals. Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. This allows the model to implicitly infer and distinguish perturbations from the external environment. The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.</div>
</div>
<div class='paper-counter'>682/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Yi Li;Yuquan Deng;Jesse Zhang;Joel Jang;Marius Memmel;Caelan Reed Garrett;Fabio Ramos;Dieter Fox;Anqi Li;Abhishek Gupta;Ankit Goyal</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision language model; cross-domain generalization; sim-to-real transfer; robot manipulation; vision language action model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eafdc79dd4a2aa8bac8cced6ed84a72b790f2bcd.pdf" target="_blank">https://openreview.net/pdf/eafdc79dd4a2aa8bac8cced6ed84a72b790f2bcd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Hierarchical VLA architectures can enable robotic manipulation with semantic, visual, and geometric generalization after trained on cheap off-domain data</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, *off-domain* data such as action-free videos, hand-drawn sketches, or simulation data. In this work, we posit that *hierarchical* vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions.In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning.We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences in embodiments, dynamics, visual appearances, and task semantics, etc.In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain.Visual results are provided at: [https://hamster-robot.github.io/](https://hamster-robot.github.io/)</div>
</div>
<div class='paper-counter'>683/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Residual-MPPI: Online Policy Customization for Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Pengcheng Wang;Chenran Li;Catherine Weaver;Kenta Kawamoto;Masayoshi Tomizuka;Chen Tang;Wei Zhan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Policy customization;Combination of learning- and planning-based approaches;Model predictive control</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/39282e6e4d45e72269d3cc5cf19f3856b02e14c3.pdf" target="_blank">https://openreview.net/pdf/39282e6e4d45e72269d3cc5cf19f3856b02e14c3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a generic online planning algorithm for customizing continuous-control policies on new performance metrics in few-shot and even zero-shot online settings.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Policies developed through Reinforcement Learning (RL) and Imitation Learning (IL) have shown great potential in continuous control tasks, but real-world applications often require adapting trained policies to unforeseen requirements. While fine-tuning can address such needs, it typically requires additional data and access to the original training metrics and parameters.In contrast, an online planning algorithm, if capable of meeting the additional requirements, can eliminate the necessity for extensive training phases and customize the policy without knowledge of the original training scheme or task. In this work, we propose a generic online planning algorithm for customizing continuous-control policies at the execution time, which we call Residual-MPPI. It can customize a given prior policy on new performance metrics in few-shot and even zero-shot online settings, given access to the prior action distribution alone. Through our experiments, we demonstrate that the proposed Residual-MPPI algorithm can accomplish the few-shot/zero-shot online policy customization task effectively, including customizing the champion-level racing agent, Gran Turismo Sophy (GT Sophy) 1.0, in the challenging car racing scenario, Gran Turismo Sport (GTS) environment. Code for MuJoCo experiments is included in the supplementary and will be open-sourced upon acceptance. Demo videos are available on our website: https://sites.google.com/view/residual-mppi.</div>
</div>
<div class='paper-counter'>684/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?</div>
<div class="field-name">authors:</div>
<div class="field-value">Charles Dawson;Van Tran;Max Z. Li;Chuchu Fan</div>
<div class="field-name">keywords:</div>
<div class="field-value">rare event modeling;normalizing flows;Bayesian inverse problems</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c4c6392ed9bd388e1117460c334b7600ca19cbb1.pdf" target="_blank">https://openreview.net/pdf/c4c6392ed9bd388e1117460c334b7600ca19cbb1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d312def09016cddf62a54c809fde1ba98798016f.zip" target="_blank">https://openreview.net/attachment/d312def09016cddf62a54c809fde1ba98798016f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Modeling rare events like failures in safety-critical systems requires learning from a very limited amount of data, which is challenging for existing inference tools. We develop a self-regularized framework for severely data-constrained problems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Increased deployment of autonomous systems in fields like transportation and robotics have seen a corresponding increase in safety-critical failures. These failures can be difficult to model and debug due to the relative lack of data: compared to tens of thousands of examples from normal operations, we may have only seconds of data leading up to the failure. This scarcity makes it challenging to train generative models of rare failure events, as existing methods risk either overfitting to noise in the limited failure dataset or underfitting due to an overly strong prior. We address this challenge with CalNF, or calibrated normalizing flows, a self-regularized framework for posterior learning from limited data. CalNF achieves state-of-the-art performance on data-limited failure modeling and inverse problems and enables a first-of-a-kind case study into the root causes of the 2022 Southwest Airlines scheduling crisis.</div>
</div>
<div class='paper-counter'>685/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Vision Language Models are In-Context Value Learners</div>
<div class="field-name">authors:</div>
<div class="field-value">Yecheng Jason Ma;Joey Hejna;Chuyuan Fu;Dhruv Shah;Jacky Liang;Zhuo Xu;Sean Kirmani;Peng Xu;Danny Driess;Ted Xiao;Osbert Bastani;Dinesh Jayaraman;Wenhao Yu;Tingnan Zhang;Dorsa Sadigh;Fei Xia</div>
<div class="field-name">keywords:</div>
<div class="field-value">robot learning;vision-language model;value estimation;manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/20db6b8c79e29a38bf57ab8d7f3d84edc62a9ff5.pdf" target="_blank">https://openreview.net/pdf/20db6b8c79e29a38bf57ab8d7f3d84edc62a9ff5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/992c5691dc9129acdd574d4e9d1a0387fc63f0a8.zip" target="_blank">https://openreview.net/attachment/992c5691dc9129acdd574d4e9d1a0387fc63f0a8.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning.</div>
</div>
<div class='paper-counter'>686/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Imitation under Misspecification</div>
<div class="field-name">authors:</div>
<div class="field-value">Nicolas Espinosa-Dice;Sanjiban Choudhury;Wen Sun;Gokul Swamy</div>
<div class="field-name">keywords:</div>
<div class="field-value">Inverse Reinforcement Learning;Imitation Learning;Distribution Shift;Policy Completeness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2b7baec6a14cd721012c960826d31664eb546936.pdf" target="_blank">https://openreview.net/pdf/2b7baec6a14cd721012c960826d31664eb546936.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We address interactive imitation learning in the misspecified setting and prove a structural condition under which our efficient imitation algorithm can avoid compounding errors.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We consider the problem of imitation learning under misspecification: settings where the learner is fundamentally unable to replicate expert behavior everywhere. This is often true in practice due to differences in observation space and action space expressiveness (e.g. perceptual or morphological differences between robots and humans). Given the learner must make some mistakes in the misspecified setting, interaction with the environment is fundamentally required to figure out which mistakes are particularly costly and lead to compounding errors. However, given the computational cost and safety concerns inherent in interaction, we'd like to perform as little of it as possible while ensuring we've learned a strong policy. Accordingly, prior work has proposed a flavor of efficient inverse reinforcement learning algorithms that merely perform a computationally efficient local search procedure with strong guarantees in the realizable setting. We first prove that under a novel structural condition we term reward-agnostic policy completeness, these sorts of local-search based IRL algorithms are able to avoid compounding errors. We then consider the question of where we should perform local search in the first place, given the learner may not be able to "walk on a tightrope" as well as the expert in the misspecified setting. We prove that in the misspecified setting, it is beneficial to broaden the set of states on which local search is performed to include those reachable by good policies the learner can actually play. We then experimentally explore a variety of sources of misspecification and how offline data can be used to effectively broaden where we perform local search from.</div>
</div>
<div class='paper-counter'>687/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Persistent Pre-training Poisoning of LLMs</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiming Zhang;Javier Rando;Ivan Evtimov;Jianfeng Chi;Eric Michael Smith;Nicholas Carlini;Florian Tramèr;Daphne Ippolito</div>
<div class="field-name">keywords:</div>
<div class="field-value">poisoning;pretraining;large language models;security</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b41ac9b5d6fb2e083f86fcb4889c6d5ae257bc8a.pdf" target="_blank">https://openreview.net/pdf/b41ac9b5d6fb2e083f86fcb4889c6d5ae257bc8a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We poison LLMs during pretraining and show that an attacker with control of only 0.1% of the data can manipulate model behavior even after alignment.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web.Prior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets.Our work evaluates for the first time whether language models can also be \emph{compromised during pre-training}, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO).We pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B).Our main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%.</div>
</div>
<div class='paper-counter'>688/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BadRobot: Jailbreaking Embodied LLM Agents in the Physical World</div>
<div class="field-name">authors:</div>
<div class="field-value">Hangtao Zhang;Chenyu Zhu;Xianlong Wang;Ziqi Zhou;Changgan Yin;Minghui Li;Lulu Xue;Yichen Wang;Shengshan Hu;Aishan Liu;Peijin Guo;Leo Yu Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics;Safety Risks;Embodied AI;LLM;Multimodal;Agent;Jailbreak attack</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/76383d1a08361f293726479351318ef9a63c5d74.pdf" target="_blank">https://openreview.net/pdf/76383d1a08361f293726479351318ef9a63c5d74.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces BadRobot, providing the first evidence that embodied LLMs can be manipulated into various dangerous actions in the physical world, even harming humans, and severely violating Asimov's Three Laws of Robotics.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodied AI represents systems where AI is integrated into physical entities. Multimodal Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, the first attack paradigm designed to jailbreak robotic manipulation, making embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot. We emphasize that addressing this emerging vulnerability is crucial for the secure deployment of LLMs in robotics.Warning: This paper contains harmful AI-generated language and aggressive actions.</div>
</div>
<div class='paper-counter'>689/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiu Yuan;Tongzhou Mu;Stone Tao;Yunhao Fang;Mengke Zhang;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value">Policy Learning;Online Improve</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/304325920749811806862e3d1b3d3e02ad2dcea9.pdf" target="_blank">https://openreview.net/pdf/304325920749811806862e3d1b3d3e02ad2dcea9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9c8d86ffea6be804d3e641a4db65ca1d6ec90320.pdf" target="_blank">https://openreview.net/attachment/9c8d86ffea6be804d3e641a4db65ca1d6ec90320.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in robot learning have used imitation learning with large models and extensive demonstrations to develop effective policies. However, these models are often limited by the quantity quality, and diversity of demonstrations. This paper explores improving offline-trained imitation learning models through online interactions with the environment. We introduce Policy Decorator, which uses a model-agnostic residual policy to refine large imitation learning models during online interactions. By implementing controlled exploration strategies, Policy Decorator enables stable, sample-efficient online learning. Our evaluation spans eight tasks across two benchmarks—ManiSkill and Adroit—and involves two state-of-the-art imitation learning models (Behavior Transformer and Diffusion Policy). The results show Policy Decorator effectively improves the offline-trained policies and preserves the smooth motion of imitation learning models, avoiding the erratic behaviors of pure RL policies. See our [project page](https://policydecorator.github.io/) for videos.</div>
</div>
<div class='paper-counter'>690/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Diagrams: A Graphical Language for Compositional Training Regimes</div>
<div class="field-name">authors:</div>
<div class="field-value">Mason Lary;Richard Samuelson;Alexander Wilentz;Alina Zare;Matthew Klawonn;James Fairbanks</div>
<div class="field-name">keywords:</div>
<div class="field-value">ML Libraries;Training;Foundation Models;Multi-Task Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">infrastructure, software libraries, hardware, systems, etc.</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0eddfdda96f440e6c2203ae459c3702d84a738fb.pdf" target="_blank">https://openreview.net/pdf/0eddfdda96f440e6c2203ae459c3702d84a738fb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We rigorously model training setups diagrammatically and provide a library for powerful manipulations thereof.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component models are trained. The result of training on this loss is a collection of models whose predictions ``agree" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning diagrams allow practitioners to build complicated models as compositions of smaller components, identify relationships between workflows, and manipulate models during or after training. Leveraging a category theoretic framework, we introduce a rigorous semantics for learning diagrams that puts such operations on a firm mathematical foundation.</div>
</div>
<div class='paper-counter'>691/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FOSP: Fine-tuning Offline Safe Policy through World Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Chenyang Cao;Yucheng Xin;Silang Wu;Longxiang He;Zichen Yan;Junbo Tan;Xueqian Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Safe RL;Offline-to-online RL;Robot Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f72da8409213f6155efba9926b0ba16e6a0ae696.pdf" target="_blank">https://openreview.net/pdf/f72da8409213f6155efba9926b0ba16e6a0ae696.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/344cfdc64a753b10a4a85ad05e4f0b997b5ebe72.zip" target="_blank">https://openreview.net/attachment/344cfdc64a753b10a4a85ad05e4f0b997b5ebe72.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">a safe model-based offline-to-online RL framework to solve safe generalization tasks</div>
<div class="field-name">abstract:</div>
<div class="field-value">Offline Safe Reinforcement Learning (RL) seeks to address safety constraints by learning from static datasets and restricting exploration. However, these approaches heavily rely on the dataset and struggle to generalize to unseen scenarios safely. In this paper, we aim to improve safety during the deployment of vision-based robotic tasks through online fine-tuning an offline pretrained policy. To facilitate effective fine-tuning, we introduce model-based RL, which is known for its data efficiency. Specifically, our method employs in-sample optimization to improve offline training efficiency while incorporating reachability guidance to ensure safety. After obtaining an offline safe policy, a safe policy expansion approach is leveraged for online fine-tuning. The performance of our method is validated on simulation benchmarks with five vision-only tasks and through real-world robot deployment using limited data. It demonstrates that our approach significantly improves the generalization of offline policies to unseen safety-constrained scenarios. To the best of our knowledge, this is the first work to explore offline-to-online RL for safe generalization tasks. The videos are available at https://sunlighted.github.io/fosp_web/.</div>
</div>
<div class='paper-counter'>692/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Offline Hierarchical Reinforcement Learning via Inverse Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Carolin Schmidt;Daniele Gammelli;James Harrison;Marco Pavone;Filipe Rodrigues</div>
<div class="field-name">keywords:</div>
<div class="field-value">Offline Reinforcement Learning;Hierarchical Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/93f17acee9694ad86b98c155ed84b9a8b68c79ac.pdf" target="_blank">https://openreview.net/pdf/93f17acee9694ad86b98c155ed84b9a8b68c79ac.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">In this work, we propose a unified framework for offline reinforcement learning of hierarchical policies.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Hierarchical policies enable strong performance in many sequential decision-making problems, such as those with high-dimensional action spaces, those requiring long-horizon planning, and settings with sparse rewards. However, learning hierarchical policies from static offline datasets presents a significant challenge.Crucially, actions taken by higher-level policies may not be directly observable within hierarchical controllers, and the offline dataset might have been generated using a different policy structure, hindering the use of standard offline learning algorithms.In this work, we propose $\textit{OHIO}$: a framework for offline reinforcement learning (RL) of hierarchical policies. Our framework leverages knowledge of the policy structure to solve the $\textit{inverse problem}$, recovering the unobservable high-level actions that likely generated the observed data under our hierarchical policy.This approach constructs a dataset suitable for off-the-shelf offline training.We demonstrate our framework on robotic and network optimization problems and show that it substantially outperforms end-to-end RL methods and improves robustness.  We investigate a variety of instantiations of our framework, both in direct deployment of policies trained offline and when online fine-tuning is performed. Code and data are available at https://ohio-offline-hierarchical-rl.github.io.</div>
</div>
<div class='paper-counter'>693/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval</div>
<div class="field-name">authors:</div>
<div class="field-value">Mohammad Omama;Po-han Li;Sandeep P. Chinchali</div>
<div class="field-name">keywords:</div>
<div class="field-value">image retrieval;efficiency;foundation models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b02faf316a710b9fee70bd960f71044348620387.pdf" target="_blank">https://openreview.net/pdf/b02faf316a710b9fee70bd960f71044348620387.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">AE-SVC improves image retrieval by refining foundation model embeddings, boosting performance by up to 15.5%. (SS)2D enhances efficiency by learning adaptive-sized embeddings, adding a 10% performance gain for smaller sizes.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Image retrieval is crucial in robotics and computer vision, with downstream applications in robot place recognition and vision-based product recommendations. Modern retrieval systems face two key challenges: scalability and efficiency.State-of-the-art image retrieval systems train specific neural networks for each dataset, an approach that lacks scalability. Furthermore, since retrieval speed is directly proportional to embedding size, existing systems that use large embeddings lack efficiency. To tackle scalability, recent works propose using off-the-shelf foundation models. However, these models, though applicable across datasets, fall short in achieving performance comparable to that of dataset-specific models. Our key observation is that, while foundation models capture necessary subtleties for effective retrieval, the underlying distribution of their embedding space can negatively impact cosine similarity searches. We introduce Autoencoders with Strong Variance Constraints (AE-SVC), which, when used for projection, significantly improves the performance of foundation models. We provide an in-depth theoretical analysis of AE-SVC. Addressing efficiency, we introduce Single-Shot Similarity Space Distillation ((SS)2D), a novel approach to learn embeddings with adaptive sizes that offers a better trade-off between size and performance. We conducted extensive experiments on four retrieval datasets, including Stan-ford Online Products (SoP) and Pittsburgh30k, using four different off-the-shelf foundation models, including DinoV2 and CLIP. AE-SVC demonstrates up to a 16% improvement in retrieval performance, while (SS)2D shows a further 10% improvement for smaller embedding sizes.</div>
</div>
<div class='paper-counter'>694/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy</div>
<div class="field-name">authors:</div>
<div class="field-value">Mingyang Zhao;Gaofeng Meng;Dong-ming Yan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Unsupervised Deformation;Neural Implicit Representations;Correntropy;Locally Linear Reconstruction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c431fb1bd73d6a39106f3f408ac0040833529670.pdf" target="_blank">https://openreview.net/pdf/c431fb1bd73d6a39106f3f408ac0040833529670.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/42d97ad07e3c8b63b50daf606524085b617a8387.zip" target="_blank">https://openreview.net/attachment/42d97ad07e3c8b63b50daf606524085b617a8387.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper aims to solve the occlusion challenge in non-rigid alignment of point clouds using neural deformation correntropy.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Non-rigid alignment of point clouds is crucial for scene understanding, reconstruction, and various computer vision and robotics tasks. Recent advancements in implicit deformation networks for non-rigid registration have significantly reduced the reliance on large amounts of annotated training data. However, existing state-of-the-art methods still face challenges in handling occlusion scenarios. To address this issue, this paper introduces an innovative unsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly aligning point clouds. The key innovation of our method lies in the utilization of the adaptive correntropy function as a localized similarity measure, enabling us to treat individual points distinctly. In contrast to previous approaches that solely minimize overall deviations between two shapes, we combine unsupervised implicit neural representations with the maximum correntropy criterion to optimize the deformation of unoccluded regions. This effectively avoids collapsed, tearing, and other physically implausible results. Moreover, we present a theoretical analysis and establish the relationship between the maximum correntropy criterion and the commonly used Chamfer distance, highlighting that the correntropy-induced metric can be served as a more universal measure for point cloud analysis. Additionally, we introducelocally linear reconstruction to ensure that regions lacking correspondences between shapes still undergo physically natural deformations. Our method achieves superior or competitive performance compared to existing approaches, particularly when dealing with occluded geometries. We also demonstrate the versatility of our method in challenging tasks such as large deformations, shape interpolation, and shape completion under occlusion disturbances.</div>
</div>
<div class='paper-counter'>695/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BodyGen: Advancing Towards Efficient Embodiment Co-Design</div>
<div class="field-name">authors:</div>
<div class="field-value">Haofei Lu;Zhe Wu;Junliang Xing;Jianshu Li;Ruoyu Li;Zhe Li;Yuanchun Shi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/099ba9cde725eecb7e65cd06114a36d6b3d1c611.pdf" target="_blank">https://openreview.net/pdf/099ba9cde725eecb7e65cd06114a36d6b3d1c611.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper introduces BodyGen:  a novel framework for efficient embodiment co-design.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io.</div>
</div>
<div class='paper-counter'>696/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">REvolve: Reward Evolution with Large Language Models using Human Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">RISHI HAZRA;Alkis Sygkounas;Andreas Persson;Amy Loutfi;Pedro Zuidberg Dos Martires</div>
<div class="field-name">keywords:</div>
<div class="field-value">Evolutionary Algorithms;Reward Design;Reinforcement Learning;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8c41e3f4f7919e2026b0b1271c8f181cffdb4577.pdf" target="_blank">https://openreview.net/pdf/8c41e3f4f7919e2026b0b1271c8f181cffdb4577.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an evolutionary framework for reward design for Reinforcement Learning, where LLMs act as intelligent operators, guided by human feedback, to generate and refine reward functions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings -- autonomous driving, humanoid locomotion, and dexterous manipulation -- wherein notions of ``good" behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines.</div>
</div>
<div class='paper-counter'>697/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Onkar Kishor Susladkar;Jishu Sen Gupta;Chirag Sehgal;Sparsh Mittal;Rekha Singhal</div>
<div class="field-name">keywords:</div>
<div class="field-value">text2video;VQ-Diffusion;video Inpainting;Large scale pretraining</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/45b7da6fce6fc272027212b6c17632d43a4c1530.pdf" target="_blank">https://openreview.net/pdf/45b7da6fce6fc272027212b6c17632d43a4c1530.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">High Quality text to video generation with discrete diffusion</div>
<div class="field-name">abstract:</div>
<div class="field-value">The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with masked modeling to enhance spatiotemporal video compression. The model achieves superior temporal consistency and state-of-the-art (SOTA) reconstruction quality by employing a novel training strategy with full frame masking. Second, we present MotionAura, a text-to-video generation framework that utilizes vector-quantized diffusion models to discretize the latent space and capture complex motion dynamics, producing temporally coherent videos aligned with text prompts. Third, we propose a spectral transformer-based denoising network that processes video data in the frequency domain using the Fourier Transform. This method effectively captures global context and long-range dependencies for high-quality video generation and denoising. Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Our models achieve SOTA performance on a range of benchmarks.  Our work offers robust frameworks for spatiotemporal modeling and user-driven video content manipulation.</div>
</div>
<div class='paper-counter'>698/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinyan Chen;Jianfei Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">human sensing;multimodal learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/61327208c312687d18da3657a11b32bf9a86510c.pdf" target="_blank">https://openreview.net/pdf/61327208c312687d18da3657a11b32bf9a86510c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Human sensing, which employs various sensors and advanced deep learning technologies to accurately capture and interpret human body information, has significantly impacted fields like public security and robotics. However, current human sensing primarily depends on modalities such as cameras and LiDAR, each of which has its own strengths and limitations. Furthermore, existing multimodal fusion solutions are typically designed for fixed modality combinations, requiring extensive retraining when modalities are added or removed for diverse scenarios. In this paper, we propose a modality-invariant foundation model for all modalities, X-Fi, to address these issues. X-Fi enables the independent or combinatory use of sensor modalities without additional training by utilizing a transformer structure to accommodate variable input sizes and incorporating a novel "X-fusion" mechanism to preserve modality-specific features during multimodal integration. This approach not only enhances adaptability but also facilitates the learning of complementary features across modalities. Extensive experiments conducted on the MM-Fi and XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi achieves state-of-the-art performance in human pose estimation (HPE) and human activity recognition (HAR) tasks. The findings indicate that our proposed model can efficiently support a wide range of human sensing applications, ultimately contributing to the evolution of scalable, multimodal sensing technologies.</div>
</div>
<div class='paper-counter'>699/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruijie Zheng;Yongyuan Liang;Shuaiyi Huang;Jianfeng Gao;Hal Daumé III;Andrey Kolobov;Furong Huang;Jianwei Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Vision Language Model;Robot Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cc4b18989f84e02c6b06df8b480b7156ad8ee1ee.pdf" target="_blank">https://openreview.net/pdf/cc4b18989f84e02c6b06df8b480b7156ad8ee1ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Visual trace prompting enhances VLA models' spatial-temporal understanding, boosting robotic manipulation performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models’ spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuningOpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.</div>
</div>
<div class='paper-counter'>700/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generating Freeform Endoskeletal Robots</div>
<div class="field-name">authors:</div>
<div class="field-value">Muhan Li;Lingji Kong;Sam Kriegman</div>
<div class="field-name">keywords:</div>
<div class="field-value">co-design;agent design;robots;morphology;evolution;locomotion</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8116e2460f3c321de29bb15edde3a42e25becc24.pdf" target="_blank">https://openreview.net/pdf/8116e2460f3c321de29bb15edde3a42e25becc24.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce the multiphysics simulation, de novo design, and universal control of endoskeletal robots with minimal assumptions about the robots’ morphology and behavior.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems.</div>
</div>
<div class='paper-counter'>701/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Cong Lu;Shengran Hu;Jeff Clune</div>
<div class="field-name">keywords:</div>
<div class="field-value">Exploration;Large Language Models;LLM agents;Open-endedness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c14827ceef266c12d8a9cdb99b4dd27960910845.pdf" target="_blank">https://openreview.net/pdf/c14827ceef266c12d8a9cdb99b4dd27960910845.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new FM agent for hard exploration tasks based on the classic Go-Explore algorithm augmented with foundation model intelligent selection.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration (i.e., determine which states to save and explore from, and what actions to consider next), which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g., discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting opportunity to recognize and capitalize on serendipitous discoveries---states encountered during exploration that are valuable in terms of exploration, yet where what makes them interesting was not anticipated by the human user. We evaluate our algorithm on a diverse range of language and vision-based tasks that require search and exploration. Across these tasks, IGE strongly exceeds classic reinforcement learning and graph search baselines, and also succeeds where prior state-of-the-art FM agents like Reflexion completely fail. Overall, Intelligent Go-Explore combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities. All our code is open-sourced at: https://github.com/conglu1997/intelligent-go-explore.</div>
</div>
<div class='paper-counter'>702/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Geometric Reasoning Networks For Robot Task And Motion Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Smail Ait Bouhsain;Rachid Alami;Thierry Simeon</div>
<div class="field-name">keywords:</div>
<div class="field-value">Graph Neural Networks;Deep Learning for Robotics;Task and Motion Planning;Robot Manipulation Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4c142fb0625912332eff11ad284991e6692f7016.pdf" target="_blank">https://openreview.net/pdf/4c142fb0625912332eff11ad284991e6692f7016.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c75996e3b2d833fb3d0527e600eda82e3f509898.zip" target="_blank">https://openreview.net/attachment/c75996e3b2d833fb3d0527e600eda82e3f509898.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Task and Motion Planning (TAMP) is a computationally challenging robotics problem due to the tight coupling of discrete symbolic planning and continuous geometric planning of robot motions. In particular, planning manipulation tasks in complex 3D environments leads to a large number of costly geometric planner queries to verify the feasibility of considered actions and plan their motions. To address this issue, we propose Geometric Reasoning Networks (GRN), a graph neural network (GNN)-based model for action and grasp feasibility prediction, designed to significantly reduce the dependency on the geometric planner. Moreover, we introduce two key interpretability mechanisms: inverse kinematics (IK) feasibility prediction and grasp obstruction (GO) estimation. These modules not only improve feasibility predictions accuracy, but also explain why certain actions or grasps are infeasible, thus allowing a more efficient search for a feasible solution. Through extensive experimental results, we show that our model outperforms state-of-the-art methods, while maintaining generalizability to more complex environments, diverse object shapes, multi-robot settings, and real-world robots.</div>
</div>
<div class='paper-counter'>703/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</div>
<div class="field-name">authors:</div>
<div class="field-value">Xueyi Liu;Jianibieke Adalibieke;Qianwei Han;Yuzhe Qin;Li Yi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Dexterous Manipulation;Neural Tracking Control;Homotopy Optimization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/01470325ec04e8892dfee49b08d62be10f92f3c2.pdf" target="_blank">https://openreview.net/pdf/01470325ec04e8892dfee49b08d62be10f92f3c2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d2aa6700cccba46139800a94a1cbc2561b54dece.zip" target="_blank">https://openreview.net/attachment/d2aa6700cccba46139800a94a1cbc2561b54dece.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">DexTrack presents a neural tracking controller for dexterous robot hand manipulation, with high adaptability, generalization, and robustness.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at  [DexTrack](https://meowuu7.github.io/DexTrack/).</div>
</div>
<div class='paper-counter'>704/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing</div>
<div class="field-name">authors:</div>
<div class="field-value">Grace Zhang;Ayush Jain;Injune Hwang;Shao-Hua Sun;Joseph J Lim</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-task Reinforcement Learning;Behavior Sharing</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b32339abdd3d912bddcd961626a1dda76008d791.pdf" target="_blank">https://openreview.net/pdf/b32339abdd3d912bddcd961626a1dda76008d791.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Sharing behaviors between tasks via off-policy data collection accelerates multi-task reinforcement learning algorithms</div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeling data between tasks.  In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors.  We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm.  Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments. Videos are available at https://qmp-mtrl.github.io/.</div>
</div>
<div class='paper-counter'>705/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generalized Consistency Trajectory Models for Image Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Beomsu Kim;Jaemin Kim;Jeongsol Kim;Jong Chul Ye</div>
<div class="field-name">keywords:</div>
<div class="field-value">Consistency Models;Image Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9f9f4962fee65c3470f9167815ed43cb0387418c.pdf" target="_blank">https://openreview.net/pdf/9f9f4962fee65c3470f9167815ed43cb0387418c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/19e046909a6e4bba08122ea071fc1494cb720628.zip" target="_blank">https://openreview.net/attachment/19e046909a6e4bba08122ea071fc1494cb720628.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs. We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing. Code is available at https://github.com/1202kbs/GCTM.</div>
</div>
<div class='paper-counter'>706/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TopoDiffusionNet: A Topology-aware Diffusion Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Saumya Gupta;Dimitris Samaras;Chao Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Topology;Diffusion Models;Persistent Homology;Counting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4e5111dc3c4962c6d6a4cd4f8c30d9dca2edc906.pdf" target="_blank">https://openreview.net/pdf/4e5111dc3c4962c6d6a4cd4f8c30d9dca2edc906.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7548d9aff43d9f53835aac7641f39a6dd1635c22.pdf" target="_blank">https://openreview.net/attachment/7548d9aff43d9f53835aac7641f39a6dd1635c22.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Diffusion models are great at creating visually appealing images but struggle to maintain specific topologies. This work introduces TopoDiffusionNet, a diffusion model designed to generate images that adhere to given topological constraints.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models excel at creating visually impressive images but often struggle to generate images with a specified topology. The Betti number, which represents the number of structures in an image, is a fundamental measure in topology. Yet, diffusion models fail to satisfy even this basic constraint. This limitation restricts their utility in applications requiring exact control, like robotics and environmental modeling. To address this, we propose TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to maintain the desired topology. We leverage tools from topological data analysis, particularly persistent homology, to extract the topological structures within an image. We then design a topology-based objective function to guide the denoising process, preserving intended structures while suppressing noisy ones. Our experiments across four datasets demonstrate significant improvements in topological accuracy. TDN is the first to integrate topology with diffusion models, opening new avenues of research in this area.</div>
</div>
<div class='paper-counter'>707/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Junjie Li;Yang Liu;Weiqing Liu;Shikai Fang;Lewen Wang;Chang Xu;Jiang Bian</div>
<div class="field-name">keywords:</div>
<div class="field-value">Financial Market Simulation;Generative Foundation Model;Large Market Model (LMM);Controllable Simulation;Interactive Simulation;Market Impact;Reinforcement Learning;Forecasting;Market Manipulation Detection;Order-Level Data</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6faae768b8f1de961ac54f667a981cde8ceaa3b.pdf" target="_blank">https://openreview.net/pdf/b6faae768b8f1de961ac54f667a981cde8ceaa3b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains under-explored. In financial markets, generative models can simulate complex market effects of participants with various behaviors, enabling interaction under different market conditions, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the domain-specific need for realistic, interactive and controllable order generation. Key observations include LMM's strong scalability across data size and model complexity, and MarS's robust and practicable realism in controlled generation with market impact. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment, thus demonstrating MarS's ``paradigm shift'' potential for a variety of financial applications. We release the code of MarS at https://github.com/microsoft/MarS/.</div>
</div>
<div class='paper-counter'>708/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Michelle D Zhao;Henny Admoni;Reid Simmons;Aaditya Ramdas;Andrea Bajcsy</div>
<div class="field-name">keywords:</div>
<div class="field-value">conformal prediction;interactive imitation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c3a56692fd6a8b3e417daf92e2a035b37131ad82.pdf" target="_blank">https://openreview.net/pdf/c3a56692fd6a8b3e417daf92e2a035b37131ad82.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ab1380dd5c378b919f5de257380ba67c80011393.pdf" target="_blank">https://openreview.net/attachment/ab1380dd5c378b919f5de257380ba67c80011393.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We contribute an interactive imitation learning approach wherein the robot uses prediction intervals calibrated by online conformal prediction as a reliable measure of deployment-time uncertainty to actively query for more expert feedback.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.</div>
</div>
<div class='paper-counter'>709/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hongxin Zhang;Zeyuan Wang;Qiushi Lyu;Zheyuan Zhang;Sunli Chen;Tianmin Shu;Behzad Dariush;Kwonjoon Lee;Yilun Du;Chuang Gan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI; Multi-agent Planning; Compositional World Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/54baa5d646fd9a8bbe3d264abfa7e975492da99d.pdf" target="_blank">https://openreview.net/pdf/54baa5d646fd9a8bbe3d264abfa7e975492da99d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We learn a compositional world model for multi-agent cooperation by factorizing the joint actions of multiple agents and compositionally generating the video.  In combination with VLMs, a tree search procedure enables online cooperative planning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at https://umass-embodied-agi.github.io/COMBO</div>
</div>
<div class='paper-counter'>710/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement</div>
<div class="field-name">authors:</div>
<div class="field-value">Zijie Geng;Jie Wang;Ziyan Liu;Siyuan Xu;Zhentao Tang;Shixiong Kai;Mingxuan Yuan;Jianye HAO;Feng Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Macro placement;Chip design;EDA</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5d7ed49e204fca148c232c15170f073ed5308fd7.pdf" target="_blank">https://openreview.net/pdf/5d7ed49e204fca148c232c15170f073ed5308fd7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a learning-based method for optimizing cross-stage metrics in macro placement.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design.However, existing methods primarily focus on *online* optimization of *intermediate surrogate metrics* that are available at the current placement stage, rather than directly targeting the *cross-stage metrics*---such as the timing performance---that measure the final chip quality.This is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making the *online* optimization impractical.Consequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues.To bridge this gap, we propose **LaMPlace**, which **L**earns **a** **M**ask for optimizing cross-stage metrics in macro placement.Specifically, LaMPlace trains a predictor on *offline* data to estimate these *cross-stage metrics* and then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics.This mask essentially acts as a fast evaluator, enabling placement decisions based on *cross-stage metrics* rather than *intermediate surrogate metrics*.Experiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6\%, notably 43.0\% and 30.4\% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance.</div>
</div>
<div class='paper-counter'>711/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Infinite-Resolution Integral Noise Warping for Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Yitong Deng;Winnie Lin;Lingxiao Li;Dmitriy Smirnov;Ryan D Burgert;Ning Yu;Vincent Dedun;Mohammad H. Taghavi</div>
<div class="field-name">keywords:</div>
<div class="field-value">diffusion models; video generation; temporal consistency; multi-view consistency; noise warping; white Gaussian noise</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1ea0060f8a0cbde295f3e50ee29f2138c8538feb.pdf" target="_blank">https://openreview.net/pdf/1ea0060f8a0cbde295f3e50ee29f2138c8538feb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6cc4d9d688d67c3c95b0ef26c62e2c9c9ae696eb.zip" target="_blank">https://openreview.net/attachment/6cc4d9d688d67c3c95b0ef26c62e2c9c9ae696eb.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Adapting pretrained image-based diffusion models to generate temporally consistent videos has become an impactful generative modeling research direction. Training-free noise-space manipulation has proven to be an effective technique, where the challenge is to preserve the Gaussian white noise distribution while adding in temporal consistency. Recently, Chang et al. (2024) formulated this problem using an integral noise representation with distribution-preserving guarantees, and proposed an upsampling-based algorithm to compute it. However, while their mathematical formulation is advantageous, the algorithm incurs a high computational cost. Through analyzing the limiting-case behavior of their algorithm as the upsampling resolution goes to infinity, we develop an alternative algorithm that, by gathering increments of multiple Brownian bridges, achieves their infinite-resolution accuracy while simultaneously reducing the computational cost by orders of magnitude. We prove and experimentally validate our theoretical claims, and demonstrate our method's effectiveness in real-world applications. We further show that our method can readily extend to the 3-dimensional space.</div>
</div>
<div class='paper-counter'>712/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning</div>
<div class="field-name">authors:</div>
<div class="field-value">Patrick Yin;Tyler Westenbroek;Ching-An Cheng;Andrey Kolobov;Abhishek Gupta</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Learning;Reinforcement Learning;Fine-Tuning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0d5594d771be5ba9c088aa585dae7fca0a65237e.pdf" target="_blank">https://openreview.net/pdf/0d5594d771be5ba9c088aa585dae7fca0a65237e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We use value functions trained in simulation to guide efficient exploration for efficient real-world finetuning, with robot hardware and theoretical results</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the \emph{Simulation-Guided Fine-tuning} (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps.</div>
</div>
<div class='paper-counter'>713/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3D-SPATIAL MULTIMODAL MEMORY</div>
<div class="field-name">authors:</div>
<div class="field-value">Xueyan Zou;Yuchen Song;Ri-Zhao Qiu;Xuanbin Peng;Jianglong Ye;Sifei Liu;Xiaolong Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Foundation Model;Gaussian Splatting;Large Multimodal Model;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/49718e82c4fa24eac05ec11d26bd767cd526299a.pdf" target="_blank">https://openreview.net/pdf/49718e82c4fa24eac05ec11d26bd767cd526299a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/34923763c7b8b9056c3b9a767b6493b6e9396952.pdf" target="_blank">https://openreview.net/attachment/34923763c7b8b9056c3b9a767b6493b6e9396952.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3’s feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.</div>
</div>
<div class='paper-counter'>714/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruoxuan Feng;Jiangyu Hu;Wenke Xia;TianciGao;Ao Shen;Yuhao Sun;Bin Fang;Di Hu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Tactile Representation Learning;Visuo-tactile Sensors;Cross-sensor Transferring</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0436ce35e53852c0b38830a81a52312638a6ad59.pdf" target="_blank">https://openreview.net/pdf/0436ce35e53852c0b38830a81a52312638a6ad59.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Visuo-tactile sensors aim to emulate human tactile perception, enabling robots to precisely understand and manipulate objects. Over time, numerous meticulously designed visuo-tactile sensors have been integrated into robotic systems, aiding in completing various tasks. However, the distinct data characteristics of these low-standardized visuo-tactile sensors hinder the establishment of a powerful tactile perception system. We consider that the key to addressing this issue lies in learning unified multi-sensor representations, thereby integrating the sensors and promoting  tactile knowledge transfer between them. To achieve unified representation of this nature, we introduce TacQuad, an aligned multi-modal multi-sensor tactile dataset from four different visuo-tactile sensors, which enables the explicit integration of various sensors. Recognizing that humans perceive the physical environment by acquiring diverse tactile information such as texture and pressure changes, we further propose to learn unified multi-sensor representations from both static and dynamic perspectives. By integrating tactile images and videos, we present AnyTouch, a unified static-dynamic multi-sensor representation learning framework with a multi-level structure, aimed at both enhancing comprehensive perceptual abilities and enabling effective cross-sensor transfer. This multi-level architecture captures pixel-level details from tactile data via masked modeling and enhances perception and transferability by learning semantic-level sensor-agnostic features through multi-modal alignment and cross-sensor matching. We provide a comprehensive analysis of multi-sensor transferability, and validate our method on various offline datasets and in the real-world pouring task. Experimental results show that our method outperforms existing methods, exhibits outstanding static and dynamic perception capabilities across various sensors. The code, TacQuad dataset and AnyTouch model are fully available at gewu-lab.github.io/AnyTouch/.</div>
</div>
<div class='paper-counter'>715/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Xinyue Wang;Biwei Huang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Model-based Reinforcement Learning;World Model;Causal Representation Learning;Generalization in Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c7455bc50fe9dd68a2a79846e84c6081499ce154.pdf" target="_blank">https://openreview.net/pdf/c7455bc50fe9dd68a2a79846e84c6081499ce154.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning—where known components are reconfigured to handle new situations—we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks.</div>
</div>
<div class='paper-counter'>716/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Shijie Liu;Andrew Craig Cullen;Paul Montague;Sarah Monazam Erfani;Benjamin I. P. Rubinstein</div>
<div class="field-name">keywords:</div>
<div class="field-value">Adversarial Machine Learning;Certified Robustness;Reinforcement Learning;Poisoning Attack</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3231e22a87c54cecd026921d0ded2e82557413f0.pdf" target="_blank">https://openreview.net/pdf/3231e22a87c54cecd026921d0ded2e82557413f0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We strengthen offline RL's certified defense against poisoning attacks in more general RL settings via Differential Privacy, achieving much greater robustness compared to previous methods.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments---significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than 50% with up to 7% of the training data poisoned, significantly improving over the 0.008% in prior work (Wu et al., 2022), while producing certified radii that is 5 times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.</div>
</div>
<div class='paper-counter'>717/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Graph Neural Networks Gone Hogwild</div>
<div class="field-name">authors:</div>
<div class="field-value">Olga Solodova;Nick Richardson;Deniz Oktay;Ryan P Adams</div>
<div class="field-name">keywords:</div>
<div class="field-value">graph neural networks;multi-agent;asynchronous;decentralized</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning on graphs and other geometries & topologies</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e8bd1d55ad9f030a6e0db669b9e72967f01edacb.pdf" target="_blank">https://openreview.net/pdf/e8bd1d55ad9f030a6e0db669b9e72967f01edacb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Graph neural networks (GNNs) appear to be powerful tools to learn state representations for agents in distributed, decentralized multi-agent systems, but generate catastrophically incorrect predictions when nodes update asynchronously during inference.  This failure under asynchrony effectively excludes these architectures from many potential applications where synchrony is difficult or impossible to enforce, e.g., robotic swarms or sensor networks.  In this work we identify ''implicitly-defined'' GNNs as a class of architectures which is provably robust to asynchronous ''hogwild'' inference, adapting convergence guarantees from work in asynchronous and distributed optimization.   We then propose a novel implicitly-defined GNN architecture, which we call an energy GNN.   We show that this architecture outperforms other GNNs from this class on a variety of synthetic tasks inspired by multi-agent systems.</div>
</div>
<div class='paper-counter'>718/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Marcus Williams;Micah Carroll;Adhyyan Narang;Constantin Weisser;Brendan Murphy;Anca Dragan</div>
<div class="field-name">keywords:</div>
<div class="field-value">manipulation;deception;alignment;reward hacking;user feedback</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/84270f4e2b464c25de9aba591cf647f5ebf83fdf.pdf" target="_blank">https://openreview.net/pdf/84270f4e2b464c25de9aba591cf647f5ebf83fdf.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to manipulative or deceptive tactics to obtain positive feedback from users who are vulnerable to such strategies. We study this phenomenon by training LLMs with Reinforcement Learning with simulated user feedback in environments of practical LLM usage. In our settings, we find that: 1) Extreme forms of "feedback gaming" such as manipulation and deception are learned reliably; 2) Even if only 2% of users are vulnerable to manipulative strategies, LLMs learn to identify and target them while behaving appropriately with other users, making such behaviors harder to detect; 3) To mitigate this issue, it may seem promising to leverage continued safety training or LLM-as-judges during training to filter problematic outputs. Instead, we found that while such approaches help in some of our settings, they backfire in others, sometimes even leading to subtler manipulative behaviors. We hope our results can serve as a case study which highlights the risks of using gameable feedback sources -- such as user feedback -- as a target for RL. Our code is publicly available. Warning: some of our examples may be upsetting.</div>
</div>
<div class='paper-counter'>719/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Peidong Li;Dixiao Cui</div>
<div class="field-name">keywords:</div>
<div class="field-value">End-to-End;Autonomous Driving;Sparse Scene Representation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2e901b08d02e0029d77be6fe3a19c018b9aeea1f.pdf" target="_blank">https://openreview.net/pdf/2e901b08d02e0029d77be6fe3a19c018b9aeea1f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/41c71b200de0e1c4810d17f93b50975a47bd4853.zip" target="_blank">https://openreview.net/attachment/41c71b200de0e1c4810d17f93b50975a47bd4853.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for human-designed supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves a 27.2\% relative reduction in L2 error and a 51.6\% decrease in collision rate to UniAD in nuScenes, with a 10.9× faster inference speed and 13× faster training time. Moreover, SSR outperforms VAD-Base with a 48.6-point improvement on driving score in CARLA's Town05 Long benchmark. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment. Code is available at https://github.com/PeidongLi/SSR.</div>
</div>
<div class='paper-counter'>720/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaoqiang Wang;Bang Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Model;Autonomous Agent;Graphical User Interface</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fe8ac6fdc44a055d3352848de8c47cd6ac61f958.pdf" target="_blank">https://openreview.net/pdf/fe8ac6fdc44a055d3352848de8c47cd6ac61f958.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a generalist agent that navigates various applications in an operating system using only screen input.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) and large multimodal models (LMMs) have shown great potential in automating complex tasks like web browsing and gaming. However, their ability to generalize across diverse applications remains limited, hindering broader utility. To address this challenge, we present OSCAR: Operating System Control via state-Aware reasoning and Re-planning. OSCAR is a generalist agent designed to autonomously navigate and interact with various desktop and mobile applications through standardized controls, such as mouse and keyboard inputs, while processing screen images to fulfill user commands. OSCAR translates human instructions into executable Python code, enabling precise control over graphical user interfaces (GUIs). To enhance stability and adaptability, OSCAR operates as a state machine, equipped with error-handling mechanisms and dynamic task re-planning, allowing it to efficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR’s effectiveness through extensive experiments on diverse benchmarks across desktop and mobile platforms, where it transforms complex workflows into simple natural language commands, significantly boosting user productivity. Our code will be open-source upon publication.</div>
</div>
<div class='paper-counter'>721/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Zewei Zhang;Huan Liu;Jun Chen;Xiangyu Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">drag editing;generative AI;diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/85285fd26bdd762cef6f3c4318a3eccd4585c8b7.pdf" target="_blank">https://openreview.net/pdf/85285fd26bdd762cef6f3c4318a3eccd4585c8b7.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The source code and data are available at https://gooddrag.github.io.</div>
</div>
<div class='paper-counter'>722/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains</div>
<div class="field-name">authors:</div>
<div class="field-value">Wonje Choi;Jinwoo Park;Sanghyun Ahn;Daehee Lee;Honguk Woo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Neuro-symbolic AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c8d2430bc59b081b9da93330e0d70ee7eba1729b.pdf" target="_blank">https://openreview.net/pdf/c8d2430bc59b081b9da93330e0d70ee7eba1729b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7587009f738cb3f811fdbca9df7ca5d8deef287d.zip" target="_blank">https://openreview.net/attachment/7587009f738cb3f811fdbca9df7ca5d8deef287d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains</div>
<div class="field-name">abstract:</div>
<div class="field-value">We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks—including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario—demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments.</div>
</div>
<div class='paper-counter'>723/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Latent Action Pretraining from Videos</div>
<div class="field-name">authors:</div>
<div class="field-value">Seonghyeon Ye;Joel Jang;Byeongguk Jeon;Se June Joo;Jianwei Yang;Baolin Peng;Ajay Mandlekar;Reuben Tan;Yu-Wei Chao;Bill Yuchen Lin;Lars Liden;Kimin Lee;Jianfeng Gao;Luke Zettlemoyer;Dieter Fox;Minjoon Seo</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision-language-action models;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6b7b03945eac7710a5ddf7520451e25499d63eab.pdf" target="_blank">https://openreview.net/pdf/6b7b03945eac7710a5ddf7520451e25499d63eab.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4db0b74711fdff372466e60f20729a6aa0b2c312.zip" target="_blank">https://openreview.net/attachment/4db0b74711fdff372466e60f20729a6aa0b2c312.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce Latent Action Pretraining for general Action models (LAPA), the first unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce Latent Action Pretraining for general Action models (LAPA), the first unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation models.</div>
</div>
<div class='paper-counter'>724/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment</div>
<div class="field-name">authors:</div>
<div class="field-value">Chenliang Li;Siliang Zeng;Zeyi Liao;Jiaxiang Li;Dongyeop Kang;Alfredo Garcia;Mingyi Hong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Alignment;Inverse Reinforcement Learning;Reinforment Learning from Human Feedback</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6ae0a555b42dc1a035349059cbc878a7ef37dd59.pdf" target="_blank">https://openreview.net/pdf/6ae0a555b42dc1a035349059cbc878a7ef37dd59.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model.  Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced.</div>
</div>
<div class='paper-counter'>725/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Think Then React: Towards Unconstrained Action-to-Reaction Motion Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Wenhui Tan;Boyuan Li;Chuhao Jin;Wenbing Huang;Xiting Wang;Ruihua Song</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human Reaction Generation;3D Human Motion;Large Language Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4e8e242b8dab9d84bfba7009b2aaf7126695daa9.pdf" target="_blank">https://openreview.net/pdf/4e8e242b8dab9d84bfba7009b2aaf7126695daa9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games.Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion.To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions.First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts.Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding.Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.</div>
</div>
<div class='paper-counter'>726/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Kuo-Han Hung;Pang-Chi Lo;Jia-Fong Yeh;Han-Yuan Hsu;Yi-Ting Chen;Winston H. Hsu</div>
<div class="field-name">keywords:</div>
<div class="field-value">reward learning;reinforcement learning;long-horizon robot learning;vision-language</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9e938b67e066304512f6e5481fc3502a0a218cd4.pdf" target="_blank">https://openreview.net/pdf/9e938b67e066304512f6e5481fc3502a0a218cd4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Existing Vision-Instruction Correlation (VIC) reward models struggle with training for long-horizon tasks. We propose VICtoR, a new reward model for long-horizon robotic reinforcement learning that assigns rewards hierarchically.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We study reward models for long-horizon manipulation by learning from action-free videos and language instructions, which we term the visual-instruction correlation (VIC) problem. Existing VIC methods face challenges in learning rewards for long-horizon tasks due to their lack of sub-stage awareness, difficulty in modeling task complexities, and inadequate object state estimation. To address these challenges,we introduce VICtoR, a novel hierarchical VIC reward model capable of providing effective reward signals for long-horizon manipulation tasks. Trained solely on primitive motion demonstrations, VICtoR effectively provides precise reward signals for long-horizon tasks by assessing task progress at various stages using a novel stage detector and motion progress evaluator. We conducted extensive experiments in both simulated and real-world datasets. The results suggest that VICtoR outperformed the best existing methods, achieving a 43% improvement in success rates for long-horizon tasks. Our project page can be found at https://cmlab-victor.github.io/cmlab-vicotor.github.io/.</div>
</div>
<div class='paper-counter'>727/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generalizable Motion Planning via Operator Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Sharath Matada;Luke Bhan;Yuanyuan Shi;Nikolay Atanasov</div>
<div class="field-name">keywords:</div>
<div class="field-value">Motion planning;operator learning;value function approximation;Eikonal PDE</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6d1338c15e8f2b8fe1d36d44ebc00915dbfb4e5.pdf" target="_blank">https://openreview.net/pdf/b6d1338c15e8f2b8fe1d36d44ebc00915dbfb4e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we introduce a planning neural operator (PNO) for predicting the value function of a motion planning problem. We recast value function approximation as learning a single operator from the cost function space to the value functionspace, which is defined by an Eikonal partial differential equation (PDE). Therefore, our PNO model, despite being trained with a finite number of samples at coarse resolution, inherits the zero-shot super-resolution property of neural operators. We demonstrate accurate value function approximation at 16× the training resolution on the MovingAI lab’s 2D city dataset, compare with state-of-the-art neural valuefunction predictors on 3D scenes from the iGibson building dataset and showcase optimal planning with 4-joint robotic manipulators. Lastly, we investigate employing the value function output of PNO as a heuristic function to accelerate motion planning. We show theoretically that the PNO heuristic is $\epsilon$-consistent by introducing an inductive bias layer that guarantees our value functions satisfy the triangle inequality. With our heuristic, we achieve a $30$% decrease in nodes visited while obtaining near optimal path lengths on the MovingAI lab 2D city dataset, compared to classical planning methods (A$^\ast$, RRT$^\ast$).</div>
</div>
<div class='paper-counter'>728/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems</div>
<div class="field-name">authors:</div>
<div class="field-value">Fu Luo;Xi Lin;Yaoxin Wu;Zhenkun Wang;Tong Xialiang;Mingxuan Yuan;Qingfu Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Neural Combinatorial Optimization;Large-Scale Vehicle Routing Problem</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ba45c4b0897675dfcbfb88a538bcd7c390375a89.pdf" target="_blank">https://openreview.net/pdf/ba45c4b0897675dfcbfb88a538bcd7c390375a89.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with large-scale VRPs and hindering their practical applicability. In this paper, we propose a lightweight cross-attention mechanism with linear complexity, by which a Transformer network is developed to learn efficient and favorable solutions for large-scale VRPs. We also propose a Self-Improved Training (SIT) algorithm that enables direct model training on large-scale VRP instances, bypassing extensive computational overhead for attaining labels. By iterating solution reconstruction, the Transformer network itself can generate improved partial solutions as pseudo-labels to guide the model training. Experimental results on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes indicate that our method consistently achieves superior performance for synthetic and real-world benchmarks, significantly boosting the scalability of NCO methods.</div>
</div>
<div class='paper-counter'>729/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Gumbel Counterfactual Generation From Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Shauli Ravfogel;Anej Svete;Vésteinn Snæbjarnarson;Ryan Cotterell</div>
<div class="field-name">keywords:</div>
<div class="field-value">Causality;language models;counterfactuals</div>
<div class="field-name">primary_area:</div>
<div class="field-value">foundation or frontier models, including LLMs</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/616a2941f9383910c30c60e9987ba275a9a3e020.pdf" target="_blank">https://openreview.net/pdf/616a2941f9383910c30c60e9987ba275a9a3e020.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d5f4691c178f6a5386c29aba92a18f71b4749d0c.zip" target="_blank">https://openreview.net/attachment/d5f4691c178f6a5386c29aba92a18f71b4749d0c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a framework to generate true counterfactuals from language models by reformulating them as well-founded structural equation models using the Gumbel-max trick.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery---e.g., model ablations or manipulation of linear subspaces tied to specific concepts---to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals---e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as a structural equation model using the Gumbel-max trick, which we called Gumbel counterfactual generation. This reformulation allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.</div>
</div>
<div class='paper-counter'>730/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Physics-informed Temporal Difference Metric Learning for Robot Motion Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruiqi Ni;zherong pan;Ahmed H Qureshi</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot motion planning;Eikonal Equation;Physics-informed Neural Networks;Temporal Difference Learning;Metric Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/049854950d3016ea9ed75a0ecbd9984b508426bd.pdf" target="_blank">https://openreview.net/pdf/049854950d3016ea9ed75a0ecbd9984b508426bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/90a781479eafa2dd78ff63c7c77b9963dea30752.zip" target="_blank">https://openreview.net/attachment/90a781479eafa2dd78ff63c7c77b9963dea30752.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Self-supervised physics informed neural networks for scalable robot motion planning</div>
<div class="field-name">abstract:</div>
<div class="field-value">The motion planning problem involves finding a collision-free path from a robot's starting to its target configuration. Recently, self-supervised learning methods have emerged to tackle motion planning problems without requiring expensive expert demonstrations. They solve the Eikonal equation for training neural networks and lead to efficient solutions. However, these methods struggle in complex environments because they fail to maintain key properties of the Eikonal equation, such as optimal value functions and geodesic distances. To overcome these limitations, we propose a novel self-supervised temporal difference metric learning approach that solves the Eikonal equation more accurately and enhances performance in solving complex and unseen planning tasks. Our method enforces Bellman's principle of optimality over finite regions, using temporal difference learning to avoid spurious local minima while incorporating metric learning to preserve the Eikonal equation's essential geodesic properties. We demonstrate that our approach significantly outperforms existing self-supervised learning methods in handling complex environments and generalizing to unseen environments, with robot configurations ranging from 2 to 12 degrees of freedom (DOF).</div>
</div>
<div class='paper-counter'>731/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Matthew Chang;Gunjan Chhablani;Alexander Clegg;Mikael Dallaire Cote;Ruta Desai;Michal Hlavac;Vladimir Karashchuk;Jacob Krantz;Roozbeh Mottaghi;Priyam Parashar;Siddharth Patki;Ishita Prasad;Xavier Puig;Akshara Rai;Ram Ramrakhya;Daniel Tran;Joanne Truong;John M Turner;Eric Undersander;Tsung-Yen Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human-Robot Collaboration;Planning;Embodied AI</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4bb6ff694eaca45e88773722cf73178602665bfd.pdf" target="_blank">https://openreview.net/pdf/4bb6ff694eaca45e88773722cf73178602665bfd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e4f1419ccb782a3545518293c244f70eeacf2805.zip" target="_blank">https://openreview.net/attachment/e4f1419ccb782a3545518293c244f70eeacf2805.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Introducing a novel benchmark and in-depth analysis of top-performing planners for human-robot collaboration in household tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation-in-the-loop for the grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with 'real' humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction.</div>
</div>
<div class='paper-counter'>732/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3D Vision-Language Gaussian Splatting</div>
<div class="field-name">authors:</div>
<div class="field-value">Qucheng Peng;Benjamin Planche;Zhongpai Gao;Meng Zheng;Anwesa Choudhuri;Terrence Chen;Chen Chen;Ziyan Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-modal 3D Scene Understanding;Gaussian Splatting;Open-vocabulary Semantic Segmentation;Open-vocabulary Object Localization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c61063530b0f13dee9bdabfe99e3ee214db08872.pdf" target="_blank">https://openreview.net/pdf/c61063530b0f13dee9bdabfe99e3ee214db08872.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.</div>
</div>
<div class='paper-counter'>733/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GROOT-2: Weakly Supervised Multimodal Instruction Following Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Shaofei Cai;Bowei Zhang;Zihao Wang;Haowei Lin;Xiaojian Ma;Anji Liu;Yitao Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Open-world Agent;Weakly Supervised Learning;Goal-Conditioned Policy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1535226a0ab0823a790f6e0174241d5e6c727dcd.pdf" target="_blank">https://openreview.net/pdf/1535226a0ab0823a790f6e0174241d5e6c727dcd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce \agent, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. \agent’s effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.</div>
</div>
<div class='paper-counter'>734/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems</div>
<div class="field-name">authors:</div>
<div class="field-value">Ruochen Jiao;Shaoyuan Xie;Justin Yue;TAKAMI SATO;Lixu Wang;Yixuan Wang;Qi Alfred Chen;Qi Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Backdoor attacks;Large language models;Autonomous agents;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a1e8d4e52767d3833631f38d6c0ab28da802dd5f.pdf" target="_blank">https://openreview.net/pdf/a1e8d4e52767d3833631f38d6c0ab28da802dd5f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/73a5b0d1ecbd95674765c4e94f86418aca3ee534.zip" target="_blank">https://openreview.net/attachment/73a5b0d1ecbd95674765c4e94f86418aca3ee534.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a comprehensive framework on backdoor attacks against embodi LLM for decision making during fine-tuning, including three different attack mechanisms targeting various channels of the systems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for **B**ackdoor **A**ttacks against **L**LM-based **D**ecision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: *word injection*, *scenario manipulation*, and *knowledge injection*, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100\% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65\%, reaching up to 90\%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.</div>
</div>
<div class='paper-counter'>735/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Sensor-Invariant Tactile Representation</div>
<div class="field-name">authors:</div>
<div class="field-value">Harsh Gupta;Yuchen Mo;Shengmiao Jin;Wenzhen Yuan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Tactile sensing;representation learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0cc138fd564b70e7aaaab615bc8ec02de5276bb9.pdf" target="_blank">https://openreview.net/pdf/0cc138fd564b70e7aaaab615bc8ec02de5276bb9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a representation to perform zero-shot transfer across vision-based tactile sensors</div>
<div class="field-name">abstract:</div>
<div class="field-value">High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. However, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. This limitation hinders the ability to transfer models or knowledge learned from one sensor to another. To address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. Our approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. Experimental results demonstrate the method’s effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field.</div>
</div>
<div class='paper-counter'>736/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can a Large Language Model be a Gaslighter?</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Li;Luyao Zhu;Yang Song;Ruixi Lin;Rui Mao;Yang You</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaslighting;Adversarial Attack;Safety Alignment;Trustworthy AI;Psychological Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7ea7dfad703ab15ce385f09e7f56c5d3937c2d4b.pdf" target="_blank">https://openreview.net/pdf/7ea7dfad703ab15ce385f09e7f56c5d3937c2d4b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) have gained human trust due to their capabilities and helpfulness. However, this in turn may allow LLMs to affect users' mindsets by manipulating language. It is termed as gaslighting, a psychological effect. In this work, we aim to investigate the vulnerability of LLMs under prompt-based and fine-tuning-based gaslighting attacks. Therefore, we propose a two-stage framework DeepCoG designed to: 1) elicit gaslighting plans from LLMs with the proposed DeepGaslighting prompting template, and 2) acquire gaslighting conversations from LLMs through our Chain-of-Gaslighting method. The gaslighting conversation dataset along with a corresponding safe dataset is applied to fine-tuning-based attacks on open-source LLMs and anti-gaslighting safety alignment on these LLMs. Experiments demonstrate that both prompt-based and fine-tuning-based attacks transform three open-source LLMs into gaslighters. In contrast, we advanced three safety alignment strategies to strengthen~(by $12.05\%$) the safety guardrail of LLMs. Our safety alignment strategies have minimal impacts on the utility of LLMs. Empirical studies indicate that an LLM may be a potential gaslighter, even if it passed the harmfulness test on general dangerous queries.</div>
</div>
<div class='paper-counter'>737/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A Distributional Approach to Uncertainty-Aware Preference Alignment Using Offline Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Sheng Xu;Bo Yue;Hongyuan Zha;Guiliang Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Preference-based Reinforcement Learning;Distributional Reinforcement Learning;Uncertainty Awareness</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f4365795ea0137f11a349b0d5895dd8c92af43e8.pdf" target="_blank">https://openreview.net/pdf/f4365795ea0137f11a349b0d5895dd8c92af43e8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce an uncertainty-aware preference alignment approach to PbRL by learning a distributional reward model and a risk-sensitive policy using the offline preference dataset.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Designing reward functions in Reinforcement Learning (RL) often demands significant task-specific expertise. Offline Preference-based Reinforcement Learning (PbRL) provides an effective alternative to address the complexity of reward design by learning policies from offline datasets that contain human preferences between trajectory pairs. Existing offline PbRL studies typically model a reward function by maximizing its likelihood of generating the observed human preferences. However, due to the varying number of samples within the limited dataset, less frequently compared trajectories exhibit greater uncertainty, which potentially leads to unreliable behaviors during reward and policy updates. To solve this issue, in this work, we introduce Uncertainty-Aware PbRL (UA-PbRL) to learn a distributional reward model and a risk-sensitive policy from an offline preference dataset. Our approach employs a Maximum A Posteriori (MAP) objective to update trajectory rewards and incorporates an informative prior to account for the uncertainties. Building upon this reward update, we propose a generative reward model to capture the reward distribution, utilizing the offline distributional Bellman operator and the Conditional Value-at-Risk (CVaR) metric to train a risk-sensitive policy. Experimental results demonstrate that UA-PbRL effectively identifies and avoids states with high uncertainty, facilitating risk-averse behaviors across various tasks, including robot control and language model alignment. The code is available at https://github.com/Jasonxu1225/UA-PbRL.</div>
</div>
<div class='paper-counter'>738/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Yijie Guo;Bingjie Tang;Iretiayo Akinola;Dieter Fox;Abhishek Gupta;Yashraj Narang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Assembly Tasks; Skill Retrieval; Skill Adaptation; Sim-to-real Transfer; Reinforcement Learning Fine-tuning;</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/971f86dd29f5284129f0d59163e1a2aa28845ae5.pdf" target="_blank">https://openreview.net/pdf/971f86dd29f5284129f0d59163e1a2aa28845ae5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/aa6e20c88344215e9a488257c70a4fc651296b66.pdf" target="_blank">https://openreview.net/attachment/aa6e20c88344215e9a488257c70a4fc651296b66.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce SRSA, a novel pipeline that retrieves relevant skills from a pre-existing skill library and adapts them to efficiently solve new robotic assembly tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. We introduce SRSA} (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. We establish a framework that jointly captures features of object geometry, physical dynamics, and expert actions to represent the tasks, allowing us to efficiently learn the transfer success predictor. Extensive experiments demonstrate that SRSA significantly outperforms the leading baseline. When retrieving and fine-tuning skills on unseen tasks, SRSA achieves a 19% relative improvement in success rate, exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x fewer transition samples to reach a satisfactory success rate, compared to the baseline. In a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Furthermore, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage https://srsa2024.github.io/.</div>
</div>
<div class='paper-counter'>739/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yichao Liang;Nishanth Kumar;Hao Tang;Adrian Weller;Joshua B. Tenenbaum;Tom Silver;Joao F. Henriques;Kevin Ellis</div>
<div class="field-name">keywords:</div>
<div class="field-value">learning abstractions for planning;neuro-symbolic ai;concept learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0636c153735818ce4afe13edfccd48c0d57a710c.pdf" target="_blank">https://openreview.net/pdf/0636c153735818ce4afe13edfccd48c0d57a710c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">learning neuro-symbolic predicates from interaction allows from improve sample efficiency, generalization and interpretability</div>
<div class="field-name">abstract:</div>
<div class="field-value">Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.</div>
</div>
<div class='paper-counter'>740/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaustubh Vyas;Damien Graux;Yijun Yang;Sebastien Montella;Chenxin Diao;Wendi Zhou;Pavlos Vougiouklis;Ruofei Lai;Yang Ren;Keshuang Li;Jeff Z. Pan</div>
<div class="field-name">keywords:</div>
<div class="field-value">Deep Models;Planning;PDDL;Knowledge Graphs;Benchmark;Large Language Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c2952abe328f9668db8a7ed084a1e5458262e86d.pdf" target="_blank">https://openreview.net/pdf/c2952abe328f9668db8a7ed084a1e5458262e86d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/dd3a80eaadb1bdce096d3cf5a9c1267ba1406c44.zip" target="_blank">https://openreview.net/attachment/dd3a80eaadb1bdce096d3cf5a9c1267ba1406c44.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Introducing Hive: a powerful, explainable system for selecting models & planning atomic actions based on natural language instructions. Hive leverages PDDL to deliver complex multi-modal tasks while respecting user constraints.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.Our system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.</div>
</div>
<div class='paper-counter'>741/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Generalized Behavior Learning from Diverse Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Varshith Sreeramdass;Rohan R Paleja;Letian Chen;Sanne van Waveren;Matthew Gombolay</div>
<div class="field-name">keywords:</div>
<div class="field-value">Behavior Discovery;Demonstrator Heterogeneity</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/30c9e34a86d59d2fd128ff4345089b4ef3629601.pdf" target="_blank">https://openreview.net/pdf/30c9e34a86d59d2fd128ff4345089b4ef3629601.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an imitation learning approach that utilizes a new diversity formulation to generate novel behaviors that generalize over demonstrators' latent preferences.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diverse behavior policies are valuable in domains requiring quick test-time adaptation or personalized human-robot interaction. Human demonstrations provide rich information regarding task objectives and factors that govern individual behavior variations, which can be used to characterize \textit{useful} diversity and learn diverse performant policies.However, we show that prior work that builds naive representations of demonstration heterogeneity fails in generating successful novel behaviors that generalize over behavior factors.We propose Guided Strategy Discovery (GSD), which introduces a novel diversity formulation based on a learned task-relevance measure that prioritizes behaviors exploring modeled latent factors.We empirically validate across three continuous control benchmarks for generalizing to in-distribution (interpolation) and out-of-distribution (extrapolation) factors that GSD outperforms baselines in novel behavior discovery by $\sim$21\%.Finally, we demonstrate that GSD can generalize striking behaviors for table tennis in a virtual testbed while leveraging human demonstrations collected in the real world.Code is available at https://github.com/CORE-Robotics-Lab/GSD.</div>
</div>
<div class='paper-counter'>742/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Lightweight Predictive 3D Gaussian Splats</div>
<div class="field-name">authors:</div>
<div class="field-value">Junli Cao;Vidit Goel;Chaoyang Wang;Anil Kag;Ju Hu;Sergei Korolev;Chenfanfu Jiang;Sergey Tulyakov;Jian Ren</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaussian splatting</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a6a631019cc6e1c687da6a98d1ee66a93a6ad3b2.pdf" target="_blank">https://openreview.net/pdf/a6a631019cc6e1c687da6a98d1ee66a93a6ad3b2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel Gaussian Splat representation requiring much less storage, featuring superior rendering quality, and being able to run on mobile devices in real-time.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent approaches representing 3D objects and scenes using Gaussian splats show increased rendering speed across a variety of platforms and devices. While rendering such representations is indeed extremely efficient, storing and transmitting them is often prohibitively expensive. To represent large-scale scenes, one often needs to store millions of 3D Gaussian, which can occupy up to gigabytes of storage. This creates a significant practical barrier, preventing widespread adoption on resource-constrained devices.In this work, we propose a new representation that dramatically reduces the hard drive footprint while featuring similar or improved quality when compared to the standard 3D Gaussian splats. This representation leverages the inherent feature sharing among splats in the close proximity using a hierarchical tree structure, with which only the parent splats need to be stored. We present a method for constructing tree structures from naturally unstructured point clouds. Additionally, we propose the adaptive tree manipulation to prune the redundant trees in the space, while spawn new ones from the significant  children splats during the optimization process. On the benchmark datasets, we achieve 20x storage reduction in hard-drive footprint with improved fidelity compared to the vanilla 3DGS and 2-5x reduction compared to the exiting compact solutions.  More importantly, we demonstrate the practical application of our method in real-world rendering on mobile devices and AR glasses.</div>
</div>
<div class='paper-counter'>743/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Select before Act: Spatially Decoupled Action Repetition for Continuous Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Buqing Nie;Yangqing Fu;Yue Gao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Action Repetition</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/2fecf0aa03925f7d852ca7b66aca2daac22923ee.pdf" target="_blank">https://openreview.net/pdf/2fecf0aa03925f7d852ca7b66aca2daac22923ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel reinforcement learning method introduces an act-or-repeat mechanism for individual action dimensions, enhancing sample efficiency and final performance with reduced action fluctuation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion.Different to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance.However, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them.This constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. In this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually.SDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity.Compared to existing repetition frameworks, SDAR is more sample efficient with higher policy performance and reduced action fluctuation.Experiments are conducted on various continuous control scenarios, demonstrating the effectiveness of spatially decoupled repetition design proposed in this work.</div>
</div>
<div class='paper-counter'>744/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Baoqi Pei;Yifei Huang;Jilan Xu;Guo Chen;Yuping He;Lijin Yang;Yali Wang;Weidi Xie;Yu Qiao;Fei Wu;Limin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Video representation learning;Egocentric video;Action recognition</div>
<div class="field-name">primary_area:</div>
<div class="field-value">unsupervised, self-supervised, semi-supervised, and supervised representation learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4ec7b0f087bb6723af129a8d3696d65c0c84532b.pdf" target="_blank">https://openreview.net/pdf/4ec7b0f087bb6723af129a8d3696d65c0c84532b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature.However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects.In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process.Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks.</div>
</div>
<div class='paper-counter'>745/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Understanding Long Videos with Multimodal Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Kanchana Ranasinghe;Xiang Li;Kumara Kahatapitiya;Michael S Ryoo</div>
<div class="field-name">keywords:</div>
<div class="field-value">long-video;visual question answering;interpretability</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d58b32f8c632ee668136f84612189566ebfa6f0d.pdf" target="_blank">https://openreview.net/pdf/d58b32f8c632ee668136f84612189566ebfa6f0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Investigates effects of LLM strengths on Long Video QnA tasks. Introduces Multimodal Video Understanding (MVU) framework that incorporates object-centric data from pre-trained models and sets a new state-of-the-art in long-video tasks.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video-specific information. Building on this, we explore injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos, and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establishes its strong generality. Code: github.com/kahnchana/mvu</div>
</div>
<div class='paper-counter'>746/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ADAM: An Embodied Causal Agent in Open-World Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Shu Yu;Chaochao Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">embodied agent;causality;large language model;interpretability;vision language navigation;cross-modal application;cross-modal information extraction;multimodality</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6bfe0db3cb5fbdacb1827ee3bab9bd4a55628238.pdf" target="_blank">https://openreview.net/pdf/6bfe0db3cb5fbdacb1827ee3bab9bd4a55628238.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, which can autonomously navigate the open world, perceive multimodal context, learn causal world knowledge, and tackle complex tasks through lifelong learning. ADAM is empowered by four key components: 1) an interaction module, enabling the agent to execute actions while recording the interaction processes; 2) a causal model module, tasked with constructing an ever-growing causal graph from scratch, which enhances interpretability and reduces reliance on prior knowledge; 3) a controller module, comprising a planner, an actor, and a memory pool, using the learned causal graph to accomplish tasks; 4) a perception module, powered by multimodal large language models, enabling ADAM to perceive like a human player. Extensive experiments show that ADAM constructs a nearly perfect causal graph from scratch, enabling efficient task decomposition and execution with strong interpretability. Notably, in the modified Minecraft game where no prior knowledge is available, ADAM excels with remarkable robustness and generalization capability. ADAM pioneers a novel paradigm that integrates causal methods and embodied agents synergistically. Our project page is at https://opencausalab.github.io/ADAM.</div>
</div>
<div class='paper-counter'>747/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ET-SEED: EFFICIENT TRAJECTORY-LEVEL SE(3) EQUIVARIANT DIFFUSION POLICY</div>
<div class="field-name">authors:</div>
<div class="field-value">Chenrui Tie;Yue Chen;Ruihai Wu;Boxuan Dong;Zeyi Li;Chongkai Gao;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotics; Manipulation; Equivariance</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/535efee901d9f09d3414dca14891f72fc7bf7df8.pdf" target="_blank">https://openreview.net/pdf/535efee901d9f09d3414dca14891f72fc7bf7df8.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/12dea9ee9deb6d02f3b797fd8c9ad6e2b55c58d0.zip" target="_blank">https://openreview.net/attachment/12dea9ee9deb6d02f3b797fd8c9ad6e2b55c58d0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduces ET-SEED, an SE(3) equivariant diffusion model that leverages spatial symmetries to improve data efficiency and spatial generalization in robotic manipulation tasks while reducing training complexity.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks.However, extensive demonstrations are required for policy robustness and generalization.To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks.Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints.We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner.We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object.Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method,as well as its ability to generalize to unseen configurations with only a few demonstrations. Website: https://et-seed.github.io/</div>
</div>
<div class='paper-counter'>748/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robotouille: An Asynchronous Planning Benchmark for LLM Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Gonzalo Gonzalez-Pumariega;Leong Su Yean;Neha Sunkara;Sanjiban Choudhury</div>
<div class="field-name">keywords:</div>
<div class="field-value">benchmark;llm;agents;planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f01d2a760117148d4e987620e37e53a26ccdd860.pdf" target="_blank">https://openreview.net/pdf/f01d2a760117148d4e987620e37e53a26ccdd860.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2008c45e210125170a7537e4818ee7022b41b8fe.zip" target="_blank">https://openreview.net/attachment/2008c45e210125170a7537e4818ee7022b41b8fe.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Robotouille is an LLM agent benchmark for stress testing stress testing long-horizon synchronous and asynchronous planning capabilities.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage over-lapping tasks and interruptions Our results show that ReAct (gpt-4o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution.</div>
</div>
<div class='paper-counter'>749/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiayi Liu;Denys Iliash;Angel X Chang;Manolis Savva;Ali Mahdavi Amiri</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D articulated objects creation; generative model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1f75614b2a017b1213cfe9a4aa651f23288230e5.pdf" target="_blank">https://openreview.net/pdf/1f75614b2a017b1213cfe9a4aa651f23288230e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1b0690bb93d496fa134043a4648b96c47758c6dd.zip" target="_blank">https://openreview.net/attachment/1b0690bb93d496fa134043a4648b96c47758c6dd.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a method to generate articulated objects from a single image by observing the object in the resting state. Our model aims to generate articulated objects that are visually consistent with the input image and kinematically plausible.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We address the challenge of creating 3D assets for household articulated objects from a single image.Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process.These limitations hinder the scalability and practicality for articulated object modeling.In this work, we propose a method to generate articulated objects from a single image.Observing the object in a resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image.To capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics.To tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies.Our experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality.</div>
</div>
<div class='paper-counter'>750/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</div>
<div class="field-name">authors:</div>
<div class="field-value">Zun Wang;Jialu Li;Yicong Hong;Songze Li;Kunchang Li;Shoubin Yu;Yi Wang;Yu Qiao;Yali Wang;Mohit Bansal;Limin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision-and-language navigation;data flywheel;dataset curation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e41310e1909e7cb381027db8c646088385e62597.pdf" target="_blank">https://openreview.net/pdf/e41310e1909e7cb381027db8c646088385e62597.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2f2dcca4331ceeecf12b7c266fe149812ffb58f4.zip" target="_blank">https://openreview.net/attachment/2f2dcca4331ceeecf12b7c266fe149812ffb58f4.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70\% to 78\% SPL on the classic R2R test set, surpassing human performance (76\%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, andthe generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.</div>
</div>
<div class='paper-counter'>751/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Kaustubh Sridhar;Souradeep Dutta;Dinesh Jayaraman;Insup Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">Generalist Agent;Retrieval;In-Context Learning;VLA;Imitation Learning;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7de864552f0638515d915497f208b6f5185bb64b.pdf" target="_blank">https://openreview.net/pdf/7de864552f0638515d915497f208b6f5185bb64b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ce63ce75c0241a95e4727f06c0363e7630398d01.pdf" target="_blank">https://openreview.net/attachment/ce63ce75c0241a95e4727f06c0363e7630398d01.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a retrieval-augmented generalist agent that can adapt to new environments via in-context learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on relatively small datasets and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.</div>
</div>
<div class='paper-counter'>752/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AdaWM: Adaptive World Model based Planning for Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Hang Wang;Xin Ye;Feng Tao;Chenbin Pan;Abhirup Mallik;Burhaneddin Yaman;Liu Ren;Junshan Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">World Model;Autonomous Driving;Reinforcement Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3159e38c859d7ab56a8a370b3dc768e7ac10bd5d.pdf" target="_blank">https://openreview.net/pdf/3159e38c859d7ab56a8a370b3dc768e7ac10bd5d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/283523ed6c941c16de1df7ec56ef2c6672572f71.zip" target="_blank">https://openreview.net/attachment/283523ed6c941c16de1df7ec56ef2c6672572f71.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a   planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the  performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model,  due to distribution shift. We further analyze the effects of these factors  on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed  using efficient low-rank updates. Extensive experiments  on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.</div>
</div>
<div class='paper-counter'>753/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Ge Li;Dong Tian;Hongyi Zhou;Xinkai Jiang;Rudolf Lioutikov;Gerhard Neumann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Value of sequences of actions;Reinforcement Learning;Transformer;Robot Manipulation;Movement Primitives.</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d28331c2e26076b8268acef8c2ad376d8be45219.pdf" target="_blank">https://openreview.net/pdf/d28331c2e26076b8268acef8c2ad376d8be45219.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a novel transformer-based RL method that learns values of consecutive actions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as  Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.</div>
</div>
<div class='paper-counter'>754/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunyue Wei;Shanning Zhuang;Vincent Zhuang;Yanan Sui</div>
<div class="field-name">keywords:</div>
<div class="field-value">Model predictive control;High-dimensional embodied system</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a9d02bb16709f273aa71062e9c829728ec364a0d.pdf" target="_blank">https://openreview.net/pdf/a9d02bb16709f273aa71062e9c829728ec364a0d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming, and therefore not suitable for solving large collections of tasks that require significant manual tuning. In this work, we introduce Model Predictive Control with Morphology-aware Proportional Control (MPC$^2$), a hierarchical model-based learning algorithm for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC$^2$ uses a sampling-based model predictive controller for target posture planning, and enables robust control for high-dimensional tasks by incorporating a morphology-aware proportional controller for actuator coordination. The algorithm enables motion control of a high-dimensional human musculoskeletal model in a variety of motion tasks, such as standing, walking on different terrains, and imitating sports activities. The reward function of MPC$^2$ can be tuned via black-box optimization, drastically reducing the need for human-intensive reward engineering.</div>
</div>
<div class='paper-counter'>755/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Following the Human Thread in Social Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Luca Scofano;Alessio Sampieri;Tommaso Campari;Valentino Sacco;Indro Spinelli;Lamberto Ballan;Fabio Galasso</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Social Navigation;Human Trajectories</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0db5d2cbc4fbe93fe6fdc7b8a41fa656793badae.pdf" target="_blank">https://openreview.net/pdf/0db5d2cbc4fbe93fe6fdc7b8a41fa656793badae.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/416afea4b1b32f0018b5b038e97567e0db560a3c.zip" target="_blank">https://openreview.net/attachment/416afea4b1b32f0018b5b038e97567e0db560a3c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e.,  assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans. The code can be found at https://github.com/L-Scofano/SDA.</div>
</div>
<div class='paper-counter'>756/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes</div>
<div class="field-name">authors:</div>
<div class="field-value">Hengwei Bian;Lingdong Kong;Haozhe Xie;Liang Pan;Yu Qiao;Ziwei Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">LiDAR Generation;Dynamic Scenes;4D Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/6bd7934befbdfc3d017210aa31c91af9e423ee82.pdf" target="_blank">https://openreview.net/pdf/6bd7934befbdfc3d017210aa31c91af9e423ee82.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">DynamicCity is a versatile 4D scene generation model that generate high-quality occupancy scenes from sensory driving data.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. **1)** A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel **Projection Module** to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to **12.56** mIoU gain). Furthermore, we utilize an **Expansion & Squeeze Strategy** to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to **7.05** mIoU gain, **2.06x** training speedup, and **70.84\%** memory reduction). **2)** A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a **Padded Rollout Operation** is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting **versatile 4D generation applications**, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.</div>
</div>
<div class='paper-counter'>757/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaosong Jia;Junqi You;Zhiyuan Zhang;Junchi Yan</div>
<div class="field-name">keywords:</div>
<div class="field-value">end-to-end autonomous driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ddc92e4bc0a321095cf110dfd331b81f62c373b9.pdf" target="_blank">https://openreview.net/pdf/ddc92e4bc0a321095cf110dfd331b81f62c373b9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a unified Transformer based method for end-to-end autonomous driving without complex operators and achieves SOTA performance.</div>
<div class="field-name">abstract:</div>
<div class="field-value">End-to-end autonomous driving (E2E-AD) has emerged as a trend in the field of autonomous driving, promising a data-driven, scalable approach to system design. However, existing E2E-AD methods usually adopt the sequential paradigm of perception-prediction-planning, which leads to cumulative errors and training instability. The manual ordering of tasks also limits the system’s ability to leverage synergies between tasks (for example, planning-aware perception and game-theoretic interactive prediction and planning). Moreover, the dense BEV representation adopted by existing methods brings computational challenges for long-range perception and long-term temporal fusion.  To address these challenges, we present DriveTransformer, a simplified E2E-AD framework for the ease of scaling up, characterized  by three key features: Task Parallelism (All agent, map, and planning queries direct interact with each other at each block), Sparse Representation (Task queries direct interact with raw sensor features), and Streaming Processing (Task queries are stored and passed as history information). As a result, the new framework is composed of three unified operations: task self-attention, sensor cross-attention, temporal cross-attention, which significantly reduces the complexity of system and leads to better training stability. DriveTransformer achieves state-of-the-art performance in both simulated closed-loop benchmark Bench2Drive and real world open-loop benchmark nuScenes with high FPS.</div>
</div>
<div class='paper-counter'>758/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STORM: Spatio-TempOral Reconstruction Model For Large-Scale Outdoor Scenes</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiawei Yang;Jiahui Huang;Boris Ivanovic;Yuxiao Chen;Yan Wang;Boyi Li;Yurong You;Apoorva Sharma;Maximilian Igl;Peter Karkus;Danfei Xu;Yue Wang;Marco Pavone</div>
<div class="field-name">keywords:</div>
<div class="field-value">autonomous driving; reconstruction model; spatiotemporal</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7de16b86ae91515cfc2ff2e082895baa375c985a.pdf" target="_blank">https://openreview.net/pdf/7de16b86ae91515cfc2ff2e082895baa375c985a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present STORM, a spatio-temporal reconstruction model designed to reconstruct space-time scenes and motions from sparse observations for outdoor scenes.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations—parameterized by 3D Gaussians and their velocities—in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., "amodal") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding. For more details, please visit our project at https://jiawei-yang.github.io/STORM/.</div>
</div>
<div class='paper-counter'>759/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Yuanfei Wang;Xiaojie Zhang;Ruihai Wu;Yu Li;Yan Shen;Mingdong Wu;Zhaofeng He;Yizhou Wang;Hao Dong</div>
<div class="field-name">keywords:</div>
<div class="field-value">Articulated Object Manipulation;Adaptive Mechanism Environments;Imitation Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/21603aadcbd6263deda5a33eb96c7c767731d432.pdf" target="_blank">https://openreview.net/pdf/21603aadcbd6263deda5a33eb96c7c767731d432.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Articulated object manipulation is a critical capability for robots to perform various tasks in real-world scenarios.Composed of multiple parts connected by joints, articulated objects are endowed with diverse functional mechanisms through complex relative motions. For example, a safe consists of a door, a handle, and a lock, where the door can only be opened when the latch is unlocked. The internal structure, such as the state of a lock or joint angle constraints, cannot be directly observed from visual observation. Consequently, successful manipulation of these objects requires adaptive adjustment based on trial and error rather than a one-time visual inference. However, previous datasets and simulation environments for articulated objects have primarily focused on simple manipulation mechanisms where the complete manipulation process can be inferred from the object's appearance. To enhance the diversity and complexity of adaptive manipulation mechanisms, we build a novel articulated object manipulation environment and equip it with 9 categories of objects. Based on the environment and objects, we further propose an adaptive demonstration collection and 3D visual diffusion-based imitation learning pipeline that learns the adaptive manipulation policy. The effectiveness of our designs and proposed method is validated through both simulation and real-world experiments.</div>
</div>
<div class='paper-counter'>760/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Wesley A. Suttle;Aamodh Suresh;Carlos Nieto-Granda</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;offline reinforcement learning;exploration;entropy</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d6c216167f7241662c11240bedeb226dd4b840bd.pdf" target="_blank">https://openreview.net/pdf/d6c216167f7241662c11240bedeb226dd4b840bd.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/bd8217c72a7ba1b9420f54c175ba4fb3212d9158.zip" target="_blank">https://openreview.net/attachment/bd8217c72a7ba1b9420f54c175ba4fb3212d9158.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We explore a new exploration objective for RL and show that it generates superior datasets for subsequent offline RL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Entropy-based objectives are widely used to perform state space exploration in reinforcement learning (RL) and dataset generation for offline RL. Behavioral entropy (BE), a rigorous generalization of classical entropies that incorporates cognitive and perceptual biases of agents, was recently proposed for discrete settings and shown to be a promising metric for robotic exploration problems. In this work, we propose using BE as a principled exploration objective for systematically generating datasets that provide diverse state space coverage in complex, continuous, potentially high-dimensional domains. To achieve this, we extend the notion of BE to continuous settings, derive tractable $k$-nearest neighbor estimators, provide theoretical guarantees for these estimators, and develop practical reward functions that can be used with standard RL methods to learn BE-maximizing policies. Using standard MuJoCo environments, we experimentally compare the performance of offline RL algorithms for a variety of downstream tasks on datasets generated using BE, R\'{e}nyi, and Shannon entropy-maximizing policies, as well as the SMM and RND algorithms. We find that offline RL algorithms trained on datasets collected using BE outperform those trained on datasets collected using Shannon entropy, SMM, and RND on all tasks considered, and on 80\% of the tasks compared to datasets collected using Renyi entropy.</div>
</div>
<div class='paper-counter'>761/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">What Matters in Learning from Large-Scale Datasets for Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Vaibhav Saxena;Matthew Bronars;Nadun Ranawaka Arachchige;Kuancheng Wang;Woo Chul Shin;Soroush Nasiriany;Ajay Mandlekar;Danfei Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;robotics;dataset composition</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a9a3e041afe62016bae986042202ec7db7e55873.pdf" target="_blank">https://openreview.net/pdf/a9a3e041afe62016bae986042202ec7db7e55873.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a systematic understanding of what data should be collected to improve the utility of a robotics dataset and facilitate downstream policy learning. In this work, we conduct a large-scale dataset composition study to answer this question. We develop a data generation framework to procedurally emulate common sources of diversity in existing datasets (such as sensor placements and object types and arrangements), and use it to generate large-scale robot datasets with controlled compositions, enabling a suite of dataset composition studies that would be prohibitively expensive in the real world. We focus on two practical settings: (1) what types of diversity should be emphasized when future researchers collect large-scale datasets for robotics, and (2) how should current practitioners retrieve relevant demonstrations from existing datasets to maximize downstream policy performance on tasks of interest. Our study yields several critical insights -- for example, we find that camera poses and spatial arrangements are crucial dimensions for both diversity in collection and alignment in retrieval. In real-world robot learning settings, we find that not only do our insights from simulation carry over, but our retrieval strategies on existing datasets such as DROID allow us to consistently outperform existing training strategies by up to 70\%.</div>
</div>
<div class='paper-counter'>762/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agent</div>
<div class="field-name">authors:</div>
<div class="field-value">Taiyi Wang;Zhihao Wu;Jianheng Liu;Jianye HAO;Jun Wang;Kun Shao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Mobile Agent;LLM;Reinforcement Learning;Fine Tuning;Distributed Training</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b0ac68a1bdcdb9b6e72ee52644ecbb6103901c0a.pdf" target="_blank">https://openreview.net/pdf/b0ac68a1bdcdb9b6e72ee52644ecbb6103901c0a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/84ff0967efd50519f6136143a52385060f501e33.pdf" target="_blank">https://openreview.net/attachment/84ff0967efd50519f6136143a52385060f501e33.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present an asynchronous distributed RL fine-tuning framework for mobile agents completing on-device control tasks. Using live interactions and an advanced off-policy RL approach, we ensure stable, scalable online learning for on-device agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3$\times$ improvement in training efficiency and enables training data collection 2.4$\times$ faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20\% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.</div>
</div>
<div class='paper-counter'>763/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jie Liu;Pan Zhou;Yingjun Du;Ah-Hwee Tan;Cees G. M. Snoek;Jan-Jakob Sonke;Efstratios Gavves</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;multi-agent cooperation;LLM</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/58310c0a0043720fb0a50f8c2c38c08e8a5904d5.pdf" target="_blank">https://openreview.net/pdf/58310c0a0043720fb0a50f8c2c38c08e8a5904d5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new framework to improve cooperation efficiency for embodied mutli-agent cooperation.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term  strategic and cooperative planning, leading to redundant steps, failures, and even serious repercussions in complex tasks like search-and-rescue missions where discussion and cooperative plan are crucial.  To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes, CaPo improves cooperation efficiency with two  phases: 1) meta plan generation, and 2) progress-adaptive meta plan and execution. In the first phase, all agents analyze the task, discuss, and cooperatively create a meta-plan that decomposes the task into subtasks with detailed steps, ensuring a long-term strategic and coherent plan for efficient coordination.  In the second phase, agents execute tasks according to the meta-plan and dynamically adjust it based on their latest progress (e.g., discovering a target object) through multi-turn discussions.  This progress-based adaptation eliminates redundant actions, improving the overall cooperation efficiency of agents. Experimental results on the ThreeDworld Multi-Agent Transport and Communicative Watch-And-Help tasks demonstrate CaPo's much higher task completion rate and efficiency compared with  state-of-the-arts. The code is released at https://github.com/jliu4ai/CaPo.</div>
</div>
<div class='paper-counter'>764/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Zhao;Pengxiang Ding;Zhang Min;Zhefei Gong;Shuanghao Bai;Han Zhao;Donglin Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Vision-Language-Action Model;Speech Instructions;Robot Manipulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/5f77b9b6bd43ed1a7a7d7ba9fc75c64727d77792.pdf" target="_blank">https://openreview.net/pdf/5f77b9b6bd43ed1a7a7d7ba9fc75c64727d77792.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-language-action models (VLAs) have recently become highly prevalent in robot manipulation due to its end-to-end architecture and impressive performance. However, current VLAs are limited to processing human instructions in textual form, neglecting the more natural speech modality for human interaction. A typical approach of incorporating speech modality into VLA necessitates a separate speech recognition system to transcribe spoken instructions into text. Such a cascading pipeline raises two major concerns for robotic systems. First, the entire model grows in size and complexity, potentially resulting in redundant computations and increased memory consumption. Second, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which is crucial for a robot to successfully understand and complete customized tasks. To this end, we propose VLAS, the fisrt end-to-end policy model that seamlessly integrates speech modality for robot manipulation. We present a three-stage speech instruction tuning strategy leveraging multimodal datasets, including our manually curated SQA and CSI datasets. Furthermore, to facilitate personalized operations, we develop a voice retrieval-augmented generation (RAG) approach to enhance the robot's performance in tasks requiring individual-specific knowledge. Experimental results show that the proposed VLAS, following either textual or speech instructions, can achieve performance comparable to traditional VLAs on the CALVIN benchmark. In addition, we created a benchmark consisting of customization tasks, where our VLAS demonstrates absolute superiority by fully leveraging the auxiliary information in speech.</div>
</div>
<div class='paper-counter'>765/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiajun Chai;Sicheng Li;Yuqian Fu;Dongbin Zhao;Yuanheng Zhu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large language models;Agent;Optimal decision-making</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/19a6013acbd870d276019cfc942ecc9eaeeaa320.pdf" target="_blank">https://openreview.net/pdf/19a6013acbd870d276019cfc942ecc9eaeeaa320.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We achieve zero-shot optimal decision-making for LLM agents by integrating the respective advantages of LLMs and RL.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large language models (LLMs) are trained on extensive text data to gain general comprehension capability. Current LLM agents leverage this ability to make zero- or few-shot decisions without reinforcement learning (RL) but fail in making optimal decisions, as LLMs inherently perform next-token prediction rather than maximizing rewards. In contrast, agents trained via RL could make optimal decisions but require extensive environmental interaction. In this work, we develop an algorithm that combines the zero-shot capabilities of LLMs with the optimal decision-making of RL, referred to as the Model-based LLM Agent with Q-Learning (MLAQ). MLAQ employs Q-learning to derive optimal policies from transitions within memory. However, unlike RL agents that collect data from environmental interactions, MLAQ constructs an imagination space fully based on LLM to perform imaginary interactions for deriving zero-shot policies. Our proposed UCB variant generates high-quality imaginary data through interactions with the LLM-based world model, balancing exploration and exploitation while ensuring a sub-linear regret bound. Additionally, MLAQ incorporates a mixed-examination mechanism to filter out incorrect data. We evaluate MLAQ in benchmarks that present significant challenges for existing LLM agents. Results show that MLAQ achieves a optimal rate of over 90\% in tasks where other methods struggle to succeed. Additional experiments are conducted to reach the conclusion that introducing model-based RL into LLM agents shows significant potential to improve optimal decision-making ability. Our interactive website is available at http://mlaq.site.</div>
</div>
<div class='paper-counter'>766/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics</div>
<div class="field-name">authors:</div>
<div class="field-value">Keyi Shen;Jiangwei Yu;Jose Barreiros;Huan Zhang;Yunzhu Li</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Manipulation;Model-Based Planning;Neural Dynamics;Branch-and-Bound Method</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/68b7efa9aa2f788926de548383c2e66e69bd0d93.pdf" target="_blank">https://openreview.net/pdf/68b7efa9aa2f788926de548383c2e66e69bd0d93.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9294af1c8540a2d9d04320e2c47620074e58896a.zip" target="_blank">https://openreview.net/attachment/9294af1c8540a2d9d04320e2c47620074e58896a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural-network-based dynamics models learned from observational data have shown strong predictive capabilities for scene dynamics in robotic manipulation tasks. However, their inherent non-linearity presents significant challenges for effective planning. Current planning methods, often dependent on extensive sampling or local gradient descent, struggle with long-horizon motion planning tasks involving complex contact events.In this paper, we present a GPU-accelerated branch-and-bound (BaB) framework for motion planning in manipulation tasks that require trajectory optimization over neural dynamics models. Our approach employs a specialized branching heuristic to divide the search space into sub-domains and applies a modified bound propagation method, inspired by the state-of-the-art neural network verifier $\alpha,\beta$-CROWN, to efficiently estimate objective bounds within these sub-domains. The branching process guides planning effectively, while the bounding process strategically reduces the search space.Our framework achieves superior planning performance, generating high-quality state-action trajectories and surpassing existing methods in challenging, contact-rich manipulation tasks such as non-prehensile planar pushing with obstacles, object sorting, and rope routing in both simulated and real-world settings. Furthermore, our framework supports various neural network architectures, ranging from simple multilayer perceptrons to advanced graph neural dynamics models, and scales efficiently with different model sizes.</div>
</div>
<div class='paper-counter'>767/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Residual Deep Gaussian Processes on Manifolds</div>
<div class="field-name">authors:</div>
<div class="field-value">Kacper Wyrwal;Andreas Krause;Viacheslav Borovitskiy</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaussian processes;manifolds;deep Gaussian processes;probabilistic methods;variational inference;uncertainty quantification;geometric learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/81e26909d49587f4c4a7c2c7ce1716a91c29ec9c.pdf" target="_blank">https://openreview.net/pdf/81e26909d49587f4c4a7c2c7ce1716a91c29ec9c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/59134b25f168ec048d9f66275c7e10aaee285af9.zip" target="_blank">https://openreview.net/attachment/59134b25f168ec048d9f66275c7e10aaee285af9.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks.With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields.We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon.For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes.Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded.We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process.Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough.</div>
</div>
<div class='paper-counter'>768/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiafei Duan;Wilbert Pumacay;Nishanth Kumar;Yi Ru Wang;Shulin Tian;Wentao Yuan;Ranjay Krishna;Dieter Fox;Ajay Mandlekar;Yijie Guo</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Manipulation; Data Generation; Vision-Language-Model; Failure Reasoning; Failure Detection</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/baa69f167306f963174767be4974c69528aa6379.pdf" target="_blank">https://openreview.net/pdf/baa69f167306f963174767be4974c69528aa6379.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/fe7139ab9329b0ded016070f5bb114c74b8a3b76.pdf" target="_blank">https://openreview.net/attachment/fe7139ab9329b0ded016070f5bb114c74b8a3b76.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A vision-language model for detecting and reasoning about failures in robotic manipulation, which can be used to improve many downstream robotic applications.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they still struggle with failure recognition, limiting their real-world applicability. We introduce AHA, an open-source VLM designed to detect and reason about failures in robotic manipulation using natural language. By framing failure detection as a free-form reasoning task, AHA identifies failures and provides detailed, adaptable explanations across different robots, tasks, and environments. We fine-tuned AHA using FailGen, a scalable framework that generates the first large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen achieves this by procedurally perturbing successful demonstrations from simulation. Despite being trained solely on the AHA dataset, AHA generalizes effectively to real-world failure datasets, robotic systems, and unseen tasks. It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and exceeds the average performance of six compared models including five state-of-the-art VLMs by 35.3% across multiple metrics and datasets. We integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for reinforcement learning, task and motion planning, and zero-shot trajectory generation. AHA’s failure feedback enhances these policies' performances by refining dense reward functions, optimizing task planning, and improving sub-task verification, boosting task success rates by an average of 21.4% across all three tasks compared to GPT-4 models. Project page: https://aha-vlm.github.io</div>
</div>
<div class='paper-counter'>769/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling</div>
<div class="field-name">authors:</div>
<div class="field-value">Jasmine Bayrooti;Carl Henrik Ek;Amanda Prorok</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;model-based reinforcement learning;optimistic exploration</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/da8797fa9e2c9fa4ead977af794e49ff2d201f85.pdf" target="_blank">https://openreview.net/pdf/da8797fa9e2c9fa4ead977af794e49ff2d201f85.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/87e26f0b558d55a2838ef6c20e40d5186017fbba.zip" target="_blank">https://openreview.net/attachment/87e26f0b558d55a2838ef6c20e40d5186017fbba.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a theoretically-grounded approach to optimistic exploration that leverages joint uncertainty over states and rewards for improved sample efficiency.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Learning complex robot behavior through interactions with the environment necessitates principled exploration. Effective strategies should prioritize exploring regions of the state-action space that maximize rewards, with optimistic exploration emerging as a promising direction aligned with this idea and enabling sample-efficient reinforcement learning. However, existing methods overlook a crucial aspect: the need for optimism to be informed by a belief connecting the reward and state. To address this, we propose a practical, theoretically grounded approach to optimistic exploration based on Thompson sampling.  Our approach is the first that allows for reasoning about _joint_ uncertainty over transitions and rewards for optimistic exploration. We apply our method on a set of MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that optimistic exploration significantly accelerates learning in environments with sparse rewards, action penalties, and difficult-to-explore regions. Furthermore, we provide insights into when optimism is beneficial and emphasize the critical role of model uncertainty in guiding exploration.</div>
</div>
<div class='paper-counter'>770/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">HGM³: Hierarchical Generative Masked Motion Modeling with Hard Token Mining</div>
<div class="field-name">authors:</div>
<div class="field-value">Minjae Jeong;Yechan Hwang;Jaejin Lee;Sungyoon Jung;Won Hwa Kim</div>
<div class="field-name">keywords:</div>
<div class="field-value">text-to-motion generation;generative masked model;hard token mining;hierarchical semantic graph</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1bd74ab6018501cf342974f21c023f897d5c1e9c.pdf" target="_blank">https://openreview.net/pdf/1bd74ab6018501cf342974f21c023f897d5c1e9c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ee9fb39de927c04ced3424ee1e182053c46d351b.zip" target="_blank">https://openreview.net/attachment/ee9fb39de927c04ced3424ee1e182053c46d351b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Text-to-motion generation has significant potential in a wide range of applications including animation, robotics, and AR/VR. While recent works on masked motion models are promising, the task remains challenging due to the inherent ambiguity in text and the complexity of human motion dynamics. To overcome the issues, we propose a novel text-to-motion generation framework that integrates two key components: Hard Token Mining (HTM) and a Hierarchical Generative Masked Motion Model (HGM³). Our HTM identifies and masks challenging regions in motion sequences and directs the model to focus on hard-to-learn components for efficacy. Concurrently, the hierarchical model uses a semantic graph to represent sentences at different granularity, allowing the model to learn contextually feasible motions. By leveraging a shared-weight masked motion model, it reconstructs the same sequence under different conditioning levels and facilitates comprehensive learning of complex motion patterns. During inference, the model progressively generates motions by incrementally building up coarse-to-fine details. Extensive experiments on benchmark datasets, including HumanML3D and KIT-ML, demonstrate that our method outperforms existing methods in both qualitative and quantitative measures for generating context-aware motions.</div>
</div>
<div class='paper-counter'>771/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds</div>
<div class="field-name">authors:</div>
<div class="field-value">Hengshuo Chu;Xiang Deng;Qi Lv;Xiaoyang Chen;Yinchuan Li;Jianye HAO;Liqiang Nie</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D.+Large Language Model.+Robot.+Affordance.+Perception</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/1f24613d0aac799415d36944a307d85a27ba53fa.pdf" target="_blank">https://openreview.net/pdf/1f24613d0aac799415d36944a307d85a27ba53fa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/0c7d3cc431f1ff81310976fef46fe9b339a96698.pdf" target="_blank">https://openreview.net/attachment/0c7d3cc431f1ff81310976fef46fe9b339a96698.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">3D Affordance detection is a challenging problem with broad applications on various robotic tasks. Existing methods typically formulate the detection paradigm as a label-based semantic segmentation task.This paradigm relies on predefined labels and lacks the ability to comprehend complex natural language, resulting in limited generalization in open-world scene.To address these limitations, we reformulate the traditional affordance detection paradigm into \textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task is designed to output a affordance mask region given a query reasoning text, which avoids fixed categories of input labels.We accordingly propose the \textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning affordance detection in 3D open-scene.Specifically, 3D-ADLLM introduces large language models (LLMs) to 3D affordance perception with a custom-designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection.In addition, given the scarcity of 3D affordance datasets for training large models, we seek to extract knowledge from general segmentation data and transfer it to affordance detection.Thus, we propose a multi-stage training strategy that begins with a novel pre-training task, i.e., \textit{Referring Object Part Segmentation}~(ROPS).This stage is designed to equip the model with general recognition and segmentation capabilities at the object-part level.Then followed by fine-tuning with the IRAS task, 3D-ADLLM obtains the reasoning ability for affordance detection. In summary, 3D-ADLLM leverages the rich world knowledge and human-object interaction reasoning ability of LLMs, achieving approximately an 8\% improvement in mIoU on open-vocabulary affordance detection tasks.</div>
</div>
<div class='paper-counter'>772/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Active Imitation Learning with Random Network Distillation</div>
<div class="field-name">authors:</div>
<div class="field-value">Emilien Biré;Anthony Kobanda;Ludovic Denoyer;Rémy Portelas</div>
<div class="field-name">keywords:</div>
<div class="field-value">Active Imitation Learning;Imitation Learning;Interactive Learning;Navigation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fbd27a7f8a601553f23bd1dfd385800723b42f64.pdf" target="_blank">https://openreview.net/pdf/fbd27a7f8a601553f23bd1dfd385800723b42f64.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Human in the loop interactive training during which the agent is deciding when expert demonstration is needed</div>
<div class="field-name">abstract:</div>
<div class="field-value">Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries.https://sites.google.com/view/rnd-dagger</div>
</div>
<div class='paper-counter'>773/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement</div>
<div class="field-name">authors:</div>
<div class="field-value">Antonis Antoniades;Albert Örwall;Kexun Zhang;Yuxi Xie;Anirudh Goyal;William Yang Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">agents;LLM;SWE-agents;SWE-bench;search;planning;reasoning;self-improvement;open-ended</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/fd21b53183d7f37c46833653eed82df55976adf4.pdf" target="_blank">https://openreview.net/pdf/fd21b53183d7f37c46833653eed82df55976adf4.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/299ecca17fa3b75d9128db25eee303fab43e9751.zip" target="_blank">https://openreview.net/attachment/299ecca17fa3b75d9128db25eee303fab43e9751.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Introduce an inference-time Monte Carlo Tree Search method for Software Agents.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Software engineers operating in complex and dynamic environments must continuously adapt to evolving requirements, learn iteratively from experience, and reconsider their approaches based on new insights. However, current large language model (LLM)-based software agents often follow linear, sequential processes that prevent backtracking and exploration of alternative solutions, limiting their ability to rethink their strategies when initial approaches prove ineffective. To address these challenges, we propose SWE-Search, a multi-agent framework that integrates Monte Carlo Tree Search (MCTS) with a self-improvement mechanism to enhance software agents' performance on repository-level software tasks. SWE-Search extends traditional MCTS by incorporating a hybrid value function that leverages LLMs for both numerical value estimation and qualitative evaluation. This enables self-feedback loops where agents iteratively refine their strategies based on both quantitative numerical evaluations and qualitative natural language assessments of pursued trajectories. The framework includes a SWE-Agent for adaptive exploration, a Value Agent for iterative feedback, and a Discriminator Agent that facilitates multi-agent debate for collaborative decision-making. Applied to the SWE-bench benchmark, our approach demonstrates a 23% relative improvement in performance across five models compared to standard open-source agents without MCTS. Our analysis reveals how performance scales with increased inference-time compute through deeper search, providing a pathway to improve software agents without requiring larger models or additional training data. This highlights the potential of self-evaluation driven search techniques in complex software engineering environments.</div>
</div>
<div class='paper-counter'>774/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Grounding Video Models to Actions through Goal Conditioned Exploration</div>
<div class="field-name">authors:</div>
<div class="field-value">Yunhao Luo;Yilun Du</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied AI;Decision Making;Robotics;Video Model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a77c12830cd182e6da53c94da81de0ab869ce943.pdf" target="_blank">https://openreview.net/pdf/a77c12830cd182e6da53c94da81de0ab869ce943.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We illustrate how we can ground video models to actions without using actions labels through goal conditioned exploration.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large video models, pretrained on massive quantities of amount of Internet video,  provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.In this paper, we investigate how to directly  ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.We propose a framework that uses trajectory level action generation in combination with video guidance toenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.</div>
</div>
<div class='paper-counter'>775/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianqi Chen;Panwen Hu;Xiaojun Chang;Zhenwei Shi;Michael Kampffmeyer;Xiaodan Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human Motion Synthesis;Human-Human Interaction;Physically Compliant Motion;Creative Workflow Automation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b0f9a1f55209df5b81b336c1261257181474dee1.pdf" target="_blank">https://openreview.net/pdf/b0f9a1f55209df5b81b336c1261257181474dee1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/155ad45b0e4ffecdb3a947803b80766cd9740ed3.zip" target="_blank">https://openreview.net/attachment/155ad45b0e4ffecdb3a947803b80766cd9740ed3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Sitcom-Crafter is a system for generating diverse, physically compliant 3D human motions across multiple motion types, guided by plot contexts to streamline creative workflows in anime and game design.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduce *Sitcom-Crafter*, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers. The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality. Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs. Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities. Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity. Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows. Code and demonstration videos can be found in the supplementary files.</div>
</div>
<div class='paper-counter'>776/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning</div>
<div class="field-name">authors:</div>
<div class="field-value">Ji Qi;Ming Ding;Weihan Wang;Yushi Bai;Qingsong Lv;Wenyi Hong;Bin Xu;Lei Hou;Juanzi Li;Yuxiao Dong;Jie Tang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multimodal Reasoning;Vision-Language Models;Datasets Synthesis;Graphical Math Annotation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e0555201d8c98c37456033d2e5e090d7017928dc.pdf" target="_blank">https://openreview.net/pdf/e0555201d8c98c37456033d2e5e090d7017928dc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-Language Models (VLMs) have shown broad effectiveness due to extensive training that aligns visual inputs with corresponding language responses. However, this conclusive alignment training causes models to overlook essential visual reasoning, leading to failures in handling detailed visual tasks and producing unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to tackle problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively without relying external tools, while also allowing users to trace error causes. In this paper, we study the comprehensive methodology that includes: (1) a flexible design of manipulations based on extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn, multi-image, and (4) a model training process for versatile capabilities. With the design, we also manually annotate **6K** high-quality samples for challenging graphical mathematical problems. Our trained model, CogCoM, equipped with this mechanism and 17B parameters, achieves SOTA performance across **9** benchmarks in **4** categories, demonstrating its effectiveness while maintaining interpretability. Code, model, and data are available at https://github.com/THUDM/CogCoM.</div>
</div>
<div class='paper-counter'>777/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Junting Chen;Checheng Yu;Xunzhe Zhou;Tianqi Xu;Yao Mu;Mengkang Hu;Wenqi Shao;Yikai Wang;Guohao Li;Lin Shao</div>
<div class="field-name">keywords:</div>
<div class="field-value">Embodied Artificial Intelligence;LLM Multi-agent System;Multi-robot System;Task Planning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4bb1e48853d7189b88d1e952850f6d577f85b849.pdf" target="_blank">https://openreview.net/pdf/4bb1e48853d7189b88d1e952850f6d577f85b849.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a007787fc2f9875626c802c09c084d9e43cdf36c.zip" target="_blank">https://openreview.net/attachment/a007787fc2f9875626c802c09c084d9e43cdf36c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A novel multi-agent framework is introduced for effective collaboration among heterogeneous multi-robot systems with embodiment-aware reasoning capabilities by understanding robot URDF files, plus a novel benchmark for this task.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Heterogeneous multi-robot systems (HMRS) have emerged as a powerful ap-proach for tackling complex tasks that single robots cannot manage alone. Currentlarge-language-model-based multi-agent systems (LLM-based MAS) have shownsuccess in areas like software development and operating systems, but applyingthese systems to robot control presents unique challenges. In particular, the ca-pabilities of each agent in a multi-robot system are inherently tied to the physicalcomposition of the robots, rather than predefined roles. To address this issue,we introduce a novel multi-agent framework designed to enable effective collab-oration among heterogeneous robots with varying embodiments and capabilities,along with a new benchmark named Habitat-MAS. One of our key designs isRobot Resume: Instead of adopting human-designed role play, we propose a self-prompted approach, where agents comprehend robot URDF files and call robotkinematics tools to generate descriptions of their physics capabilities to guidetheir behavior in task planning and action execution. The Habitat-MAS bench-mark is designed to assess how a multi-agent framework handles tasks that requireembodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3)navigation, and 4) comprehensive multi-floor object rearrangement. The experi-mental results indicate that the robot’s resume and the hierarchical design of ourmulti-agent system are essential for the effective operation of the heterogeneousmulti-robot system within this intricate problem context.</div>
</div>
<div class='paper-counter'>778/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Selective Task Group Updates for Multi-Task Optimization</div>
<div class="field-name">authors:</div>
<div class="field-value">Wooseong Jeong;Kuk-Jin Yoon</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-Task Learning;Multi-Task Optimization;Proximal Inter-Task Affinity</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning theory</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0980fa596ef8e59436b1c4b1fba5e661f29891b0.pdf" target="_blank">https://openreview.net/pdf/0980fa596ef8e59436b1c4b1fba5e661f29891b0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-task learning enables the acquisition of task-generic knowledge by training multiple tasks within a unified architecture. However, training all tasks together in a single architecture can lead to performance degradation, known as negative transfer, which is a main concern in multi-task learning. Previous works have addressed this issue by optimizing the multi-task network through gradient manipulation or weighted loss adjustments. However, their optimization strategy focuses on addressing task imbalance in shared parameters, neglecting the learning of task-specific parameters. As a result, they show limitations in mitigating negative transfer, since the learning of shared space and task-specific information influences each other during optimization. To address this, we propose a different approach to enhance multi-task performance by selectively grouping tasks and updating them for each batch during optimization. We introduce an algorithm that adaptively determines how to effectively group tasks and update them during the learning process. To track inter-task relations and optimize multi-task networks simultaneously, we propose proximal inter-task affinity, which can be measured during the optimization process. We provide a theoretical analysis on how dividing tasks into multiple groups and updating them sequentially significantly affects multi-task performance by enhancing the learning of task-specific parameters. Our methods substantially outperform previous multi-task optimization approaches and are scalable to different architectures and various numbers of tasks.</div>
</div>
<div class='paper-counter'>779/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ReGen: Generative Robot Simulation via Inverse Design</div>
<div class="field-name">authors:</div>
<div class="field-value">Phat Tan Nguyen;Tsun-Hsuan Wang;Zhang-Wei Hong;Erfan Aasi;Andrew Silva;Guy Rosman;Sertac Karaman;Daniela Rus</div>
<div class="field-name">keywords:</div>
<div class="field-value">generative simulation;robot;autonomous driving;large language model;inverse design</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/05a68cd66fa775acbad6a246f7a68930250738e5.pdf" target="_blank">https://openreview.net/pdf/05a68cd66fa775acbad6a246f7a68930250738e5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">ReGen generates simulations from behavior by inferring plausible simulated environment where the behavior could have occurred through inverse design.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Simulation plays a key role in scaling robot learning and validating policies, but constructing simulations remains labor-intensive. In this paper, we introduce ReGen, a generative simulation framework that automates this process using inverse design. Given an agent's behavior (such as a motion trajectory or objective function) and its textual description, we infer the underlying scenarios and environments that could have caused the behavior.Our approach leverages large language models to construct and expand a graph that captures cause-and-effect relationships and relevant entities with properties in the environment, which is then processed to configure a robot simulation environment. Our approach supports (i) augmenting simulations based on ego-agent behaviors, (ii) controllable, counterfactual scenario generation, (iii) reasoning about agent cognition and mental states, and (iv) reasoning with distinct sensing modalities, such as braking due to faulty GPS signals. We demonstrate our method in autonomous driving and robot manipulation tasks, generating more diverse, complex simulated environments compared to existing simulations with high success rates, and enabling controllable generation for corner cases. This approach enhances the validation of robot policies and supports data or simulation augmentation, advancing scalable robot learning for improved generalization and robustness.</div>
</div>
<div class='paper-counter'>780/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Student-Informed Teacher Training</div>
<div class="field-name">authors:</div>
<div class="field-value">Nico Messikommer;Jiaxu Xing;Elie Aljalbout;Davide Scaramuzza</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Imitation Learning;Robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/819caab07c8284eb5fc3f4008e17505b667df1d9.pdf" target="_blank">https://openreview.net/pdf/819caab07c8284eb5fc3f4008e17505b667df1d9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/090f2bb3d6d3bbac1421df09e0d33196dcdfe18e.zip" target="_blank">https://openreview.net/attachment/090f2bb3d6d3bbac1421df09e0d33196dcdfe18e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">To address the teacher-student asymmetry in imitation learning, we propose a joint learning framework for both teacher and student, adapting the teacher to the capabilities of the student during training to enhance alignment.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks.</div>
</div>
<div class='paper-counter'>781/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Calarina Muslimani;Matthew E. Taylor</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement Learning;Human-in-the-loop;Preference learning;Learning from scalar feedback</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8a655d64f2362c55d3d34a1c4545b4b2024f5eeb.pdf" target="_blank">https://openreview.net/pdf/8a655d64f2362c55d3d34a1c4545b4b2024f5eeb.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/7f3f8eacd8380a44442bc3f2b13b46d7004962bd.zip" target="_blank">https://openreview.net/attachment/7f3f8eacd8380a44442bc3f2b13b46d7004962bd.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process.  Instead, human-in-the-loop RL methods hold the promise of learning reward functions from human feedback. Despite recent successes, many of the human-in-the-loop RL methods still require numerous human interactions to learn successful reward functions.To improve the feedback efficiency of human-in-the-loop RL methods (i.e., require less human interaction), this paper introduces Sub-optimal Data Pre-training, SDP, an approach that leverages reward-free, sub-optimal data to improve scalar- and preference-based RL algorithms. In SDP, we start by pseudo-labeling all low-quality data with the minimum environment reward. Through this process, we obtain reward labels to pre-train our reward model without requiring human labeling or preferences. This pre-training phase provides the reward model a head start in learning, enabling it to recognize that low-quality transitions should be assigned low rewards. Through extensive experiments with both simulated and human teachers, we find that SDP can at least meet, but often significantly improve, state of the art human-in-the-loop RL performance across a variety of simulated robotic tasks.</div>
</div>
<div class='paper-counter'>782/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Eliot Xing;Vernon Luk;Jean Oh</div>
<div class="field-name">keywords:</div>
<div class="field-value">reinforcement learning;differentiable simulation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3cb1cbbfe000dae7ae75ccb6810701cd61c7fe0c.pdf" target="_blank">https://openreview.net/pdf/3cb1cbbfe000dae7ae75ccb6810701cd61c7fe0c.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables. Additional details at https://rewarped.github.io/.</div>
</div>
<div class='paper-counter'>783/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation</div>
<div class="field-name">authors:</div>
<div class="field-value">Anish Abhijit Diwan;Julen Urain;Jens Kober;Jan Peters</div>
<div class="field-name">keywords:</div>
<div class="field-value">imitation learning;energy based generative models;reinforcement learning;imitation from observation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/13098c5b4053f6e6b5487444ed2a1aa1501ca321.pdf" target="_blank">https://openreview.net/pdf/13098c5b4053f6e6b5487444ed2a1aa1501ca321.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/58a4a6887ac00b91c040a1985a542a0577d92d37.zip" target="_blank">https://openreview.net/attachment/58a4a6887ac00b91c040a1985a542a0577d92d37.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Proposes a new algorithm for imitation learning for observation that uses score-based models to learn the expert distribution's energy function and uses these learnt energy functions as reward functions.</div>
<div class="field-name">abstract:</div>
<div class="field-value">This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert's motion data distribution and learns smooth, and well-defined representations of the data distribution's energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings.</div>
</div>
<div class='paper-counter'>784/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Interactive Adjustment for Human Trajectory Prediction with Individual Feedback</div>
<div class="field-name">authors:</div>
<div class="field-value">Jianhua Sun;Yuxuan Li;Liang Chai;Cewu Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Human Trajectory Prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b6328df89249a76db86e956f596a2b8684d3af13.pdf" target="_blank">https://openreview.net/pdf/b6328df89249a76db86e956f596a2b8684d3af13.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">For the first time, we study individual feedback, which is derived from early predictions on a target, to improve the accuracy of subsequent predictions on the same agent. We design an external module IAN to model it and prove its effectiveness.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Human trajectory prediction is fundamental for autonomous driving and service robot. The research community has studied various important aspects of this task and made remarkable progress recently. However, there is an essential perspective which is not well exploited in previous research all along, namely individual feedback. Individual feedback exists in the sequential nature of trajectory prediction, where earlier predictions of a target can be verified over time by his ground-truth trajectories to obtain feedback which provides valuable experience for subsequent predictions on the same agent. In this paper, we show such feedback can reveal the strengths and weaknesses of the model's predictions on a specific target and heuristically guide to deliver better predictions on him. We present an interactive adjustment network to effectively model and leverage the feedback. This network first exploits the feedback from previous predictions to dynamically generate an adjuster which then interactively makes appropriate adjustments to current predictions for more accurate ones. We raise a novel displacement expectation loss to train this interactive architecture. Through experiments on representative prediction methods and widely-used benchmarks, we demonstrate the great value of individual feedback and the superior effectiveness of proposed interactive adjustment network.</div>
</div>
<div class='paper-counter'>785/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation</div>
<div class="field-name">authors:</div>
<div class="field-value">Huan Ren;Wenfei Yang;Xiang Liu;Shifeng Zhang;Tianzhu Zhang</div>
<div class="field-name">keywords:</div>
<div class="field-value">category-level object pose estimation;spherical representations;shape-independence;correspondence prediction</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/cd138b83d118c7d7d9bdb80d49f94de302b6c93d.pdf" target="_blank">https://openreview.net/pdf/cd138b83d118c7d7d9bdb80d49f94de302b6c93d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Category-level object pose estimation aims to determine the pose and size of novel objects in specific categories. Existing correspondence-based approaches typically adopt point-based representations to establish the correspondences between primitive observed points and normalized object coordinates. However, due to the inherent shape-dependence of canonical coordinates, these methods suffer from semantic incoherence across diverse object shapes. To resolve this issue, we innovatively leverage the sphere as a shared proxy shape of objects to learn shape-independent transformation via spherical representations. Based on this insight, we introduce a novel architecture called SpherePose, which yields precise correspondence prediction through three core designs. Firstly, We endow the point-wise feature extraction with SO(3)-invariance, which facilitates robust mapping between camera coordinate space and object coordinate space regardless of rotation transformation. Secondly, the spherical attention mechanism is designed to propagate and integrate features among spherical anchors from a comprehensive perspective, thus mitigating the interference of noise and incomplete point cloud. Lastly, a hyperbolic correspondence loss function is designed to distinguish subtle distinctions, which can promote the precision of correspondence prediction. Experimental results on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior performance of our method, verifying the effectiveness of spherical representations and architectural innovations.</div>
</div>
<div class='paper-counter'>786/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">HyPoGen: Optimization-Biased Hypernetworks for Generalizable Policy Generation</div>
<div class="field-name">authors:</div>
<div class="field-value">Hanxiang Ren;Li Sun;Xulong Wang;Pei Zhou;Zewen Wu;Siyan Dong;Difan Zou;Youyi Zheng;Yanchao Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">hypernetwork;policy generation;behavior cloning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/4bb72f09fc19515811cb5ec38ac561a66db0a5c2.pdf" target="_blank">https://openreview.net/pdf/4bb72f09fc19515811cb5ec38ac561a66db0a5c2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ab57e95a4ded8def0cc1fd84bd59bb994d232056.zip" target="_blank">https://openreview.net/attachment/ab57e95a4ded8def0cc1fd84bd59bb994d232056.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Policy learning through behavior cloning poses significant challenges, particularly when demonstration data is limited. In this work, we present HyPoGen, a novel optimization-biased hypernetwork for policy generation. The proposed hypernetwork learns to synthesize optimal policy parameters solely from task specifications --  without accessing training data -- by modeling policy generation as an approximation of the optimization process executed over a finite number of steps and assuming these specifications serve as a sufficient representation of the demonstration data. By incorporating structural designs that bias the hypernetwork towards optimization, we can improve its generalization capability while only training on source task demonstrations. During the feed-forward prediction pass, the hypernetwork effectively performs an optimization in the latent (compressed) policy space, which is then decoded into policy parameters for action prediction. Experimental results on locomotion and manipulation benchmarks show that HyPoGen significantly outperforms state-of-the-art methods in generating policies for unseen target tasks without any demonstrations, achieving higher success rates and underscoring the potential of optimization-biased hypernetworks in advancing generalizable policy generation. Our code and data are available at: https://github.com/ReNginx/HyPoGen.</div>
</div>
<div class='paper-counter'>787/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Diego Garcia Cerdas;Christina Sartzetaki;Magnus Petersen;Gemma Roig;Pascal Mettes;Iris Groen</div>
<div class="field-name">keywords:</div>
<div class="field-value">brain;selectivity;visual cortex;fMRI;manipulation;variation;diffusion;neuroscience</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to neuroscience & cognitive science</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/796cd6eebb20af6c8dc18259cafff61023b4dc6d.pdf" target="_blank">https://openreview.net/pdf/796cd6eebb20af6c8dc18259cafff61023b4dc6d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/642a7758c5028d859577987c0be0804b0181b78d.zip" target="_blank">https://openreview.net/attachment/642a7758c5028d859577987c0be0804b0181b78d.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">BrainACTIV manipulates a reference image to drive or suppress activity in a target brain region using pretrained diffusion models, providing insights into neural selectivity for experimental purposes.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The human brain efficiently represents visual inputs through specialized neural populations that selectively respond to specific categories. Advancements in generative modeling have enabled data-driven discovery of neural selectivity using brain-optimized image synthesis. However, current methods independently generate one sample at a time, without enforcing structural constraints on the generations; thus, these individual images have no explicit point of comparison, making it hard to discern which image features drive neural response selectivity. To address this issue, we introduce Brain Activation Control Through Image Variation (BrainACTIV), a method for manipulating a reference image to enhance or decrease activity in a target cortical region using pretrained diffusion models. Starting from a reference image allows for fine-grained and reliable offline identification of optimal visuo-semantic properties, as well as producing controlled stimuli for novel neuroimaging studies. We show that our manipulations effectively modulate predicted fMRI responses and agree with hypothesized preferred categories in established regions of interest, while remaining structurally close to the reference image.  Moreover, we demonstrate how our method accentuates differences between brain regions that are selective to the same category, and how it could be used to explore neural representation of brain regions with unknown selectivities. Hence, BrainACTIV holds the potential to formulate robust hypotheses about brain representation and to facilitate the production of naturalistic stimuli for neuroscientific experiments.</div>
</div>
<div class='paper-counter'>788/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">EqNIO: Subequivariant Neural Inertial Odometry</div>
<div class="field-name">authors:</div>
<div class="field-value">Royina Karegoudra Jayanth;Yinshuang Xu;Ziyun Wang;Evangelos Chatzipantazis;Kostas Daniilidis;Daniel Gehrig</div>
<div class="field-name">keywords:</div>
<div class="field-value">equivariance;inertial odometry;subequivariance</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f27e602a2cfec2b9b3b3123106803d1038c0a27f.pdf" target="_blank">https://openreview.net/pdf/f27e602a2cfec2b9b3b3123106803d1038c0a27f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Our framework robustly regresses an equivariant frame which  improves the state-of-the-art in neural IO, when coupled with off-the-shelf filter-based or end-to-end models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Neural network-based odometry using accelerometer and gyroscope readings from a single IMU can achieve robust, and low-drift localization capabilities, through the use of _neural displacement priors (NDPs)_. These priors learn to produce denoised displacement measurements but need to ignore data variations due to specific IMU mount orientation and motion directions, hindering generalization.This work introduces EqNIO, which addresses this challenge with _canonical displacement priors_, i.e., priors that are invariant to the orientation of the gravity-aligned frame in which the IMU data is expressed. We train such priors on IMU measurements, that are mapped into a learnable canonical frame, which is uniquely defined via three axes: the first is gravity, making the frame gravity aligned, while the second and third are predicted from IMU data.  The outputs (displacement and covariance) are mapped back to the original gravity-aligned frame. To maximize generalization, we find that these learnable frames must transform equivariantly with global gravity-preserving roto-reflections from the subgroup $O_g(3)\subset O(3)$, acting on the trajectory, rendering the NDP $O(3)$-_subequivariant_. We tailor specific linear, convolutional, and non-linear layers that commute with the actions of the group. Moreover, we introduce a bijective decomposition of angular rates into vectors that transform similarly to accelerations, allowing us to leverage both measurement types. Natively, angular rates would need to be inverted upon reflection, unlike acceleration, which hinders their joint processing. We highlight EqNIO's flexibility and generalization capabilities by applying it to both filter-based (TLIO), and end-to-end (RONIN) architectures, and outperforming existing methods that use _soft equivariance from auxiliary losses or data augmentation on various datasets. We believe this work paves the way for low-drift and generalizable neural inertial odometry on edge devices. The project details and code can be found at [https://github.com/RoyinaJayanth/EqNIO](https://github.com/RoyinaJayanth/EqNIO).</div>
</div>
<div class='paper-counter'>789/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction</div>
<div class="field-name">authors:</div>
<div class="field-value">Yang Zhou;Hao Shao;Letian Wang;Steven L. Waslander;Hongsheng Li;Yu Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Motion Prediction;Trajectory Prediction;Autonomous Driving;Self-Supervised Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/8e6aef6556dbf41b8ffc5bea809892100a1250fc.pdf" target="_blank">https://openreview.net/pdf/8e6aef6556dbf41b8ffc5bea809892100a1250fc.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability.To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness.Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6\%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime.</div>
</div>
<div class='paper-counter'>790/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziye Huang;Haoqi Yuan;Yuhui Fu;Zongqing Lu</div>
<div class="field-name">keywords:</div>
<div class="field-value">dexterous grasping;residual policy learning;reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/801fe1621d182f06542cd8c64b197af44982eefe.pdf" target="_blank">https://openreview.net/pdf/801fe1621d182f06542cd8c64b197af44982eefe.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. To overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-agnostic base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping.ResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU. For further details and videos, visit our project page.</div>
</div>
<div class='paper-counter'>791/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">TopoGaussian: Inferring Internal Topology Structures from Visual Clues</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaoyu Xiong;Changyu Hu;Chunru Lin;Pingchuan Ma;Chuang Gan;Tao Du</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaussian Splatting;Differential Simulation;Topology Optimization;Neural Implicit Surface</div>
<div class="field-name">primary_area:</div>
<div class="field-value">learning on graphs and other geometries & topologies</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/d667b5f6dda3ebd4f6c62124b8081d4278173384.pdf" target="_blank">https://openreview.net/pdf/d667b5f6dda3ebd4f6c62124b8081d4278173384.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We present TopoGaussian, a holistic, particle-based pipeline for inferring the interior structure of an opaque object from easily accessible photos and videos as input. Traditional mesh-based approaches require tedious and error-prone mesh filling and fixing process, while typically output rough boundary surface. Our pipeline combines Gaussian Splatting with a novel, versatile particle-based differentiable simulator that simultaneously accommodates constitutive model, actuator, and collision, without interference with mesh. Based on the gradients from this simulator, we provide flexible choice of topology representation for optimization, including particle, neural implicit surface, and quadratic surface. The resultant pipeline takes easily accessible photos and videos as input and outputs the topology that matches the physical characteristics of the input. We demonstrate the efficacy of our pipeline on a synthetic dataset and four real-world tasks with 3D-printed prototypes. Compared with existing mesh-based method, our pipeline is 5.26x faster on average with improved shape quality. These results highlight the potential of our pipeline in 3D vision, soft robotics, and manufacturing applications.</div>
</div>
<div class='paper-counter'>792/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model</div>
<div class="field-name">authors:</div>
<div class="field-value">Chongkai Gao;Haozhuo Zhang;Zhixuan Xu;Cai Zhehao;Lin Shao</div>
<div class="field-name">keywords:</div>
<div class="field-value">World Model;Long-Horizon Planning;Robot Manipulation;Flow Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/0eb79622286006ee2260320c7750895ecfdc7bd1.pdf" target="_blank">https://openreview.net/pdf/0eb79622286006ee2260320c7750895ecfdc7bd1.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/9c76d4e4ceb667e9ddde7f6b6f58f45746ae6119.zip" target="_blank">https://openreview.net/attachment/9c76d4e4ceb667e9ddde7f6b6f58f45746ae6119.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We train an interactive world-model that enables model-based planning on the image flow and video spaces for diverse manipualtion tasks with only language-annotated video datasets.</div>
<div class="field-name">abstract:</div>
<div class="field-value">We aim to develop a model-based planning framework for world models that can be scaled with increasing model and data budgets for general-purpose manipulation tasks with only language and vision inputs. To this end, we present FLow-CentrIc generative Planning (FLIP), a model-based planning algorithm on visual space that features three key modules: 1) a multi-modal flow generation model as the general-purpose action proposal module; 2) a flow-conditioned video generation model as the dynamics module; and 3) a vision-language representation learning model as the value module. Given an initial image and language instruction as the goal, FLIP can progressively search for long-horizon flow and video plans that maximize the discounted return to accomplish the task. FLIP is able to synthesize long-horizon plans across objects, robots, and tasks with image flows as the general action representation, and the dense flow information also provides rich guidance for long-horizon video generation. In addition, the synthesized flow and video plans can guide the training of low-level control policies for robot execution. Experiments on diverse benchmarks demonstrate that FLIP can improve both the success rates and quality of long-horizon video plan synthesis and has the interactive world model property, opening up wider applications for future works. Video demos are on our website: https://nus-lins-lab.github.io/flipweb/.</div>
</div>
<div class='paper-counter'>793/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs</div>
<div class="field-name">authors:</div>
<div class="field-value">Wei Hung;Shao-Hua Sun;Ping-Chun Hsieh</div>
<div class="field-name">keywords:</div>
<div class="field-value">Reinforcement learning;action constraints</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b3ad37c317e533396b180e5e021401d9dd8aa0ed.pdf" target="_blank">https://openreview.net/pdf/b3ad37c317e533396b180e5e021401d9dd8aa0ed.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/964ca4ab3f00377d3f5245f3ec551f404c2aee3a.zip" target="_blank">https://openreview.net/attachment/964ca4ab3f00377d3f5245f3ec551f404c2aee3a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint satisfaction but at the cost of either high computational burden incurred by the quadratic programs (QP) or increased architectural complexity due to the use of sophisticated generative models. In this paper, we propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to ACRL through two modifications: (i) To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. (ii) To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process (MDP), which include additional self-loop state transitions and a penalty signal for the rejected actions. This augmented MDP incentives the learned policy to stay close to the feasible action sets. Through extensive experiments in both robot control and resource allocation domains, we demonstrate that the proposed framework enjoys faster training progress, better constraint satisfaction, and a lower action inference time simultaneously than the state-of-the-art ACRL methods. We have made the source code publicly available to encourage further research in this direction.</div>
</div>
<div class='paper-counter'>794/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Multi-Robot Motion Planning with Diffusion Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Yorai Shaoul;Itamar Mishani;Shivam Vats;Jiaoyang Li;Maxim Likhachev</div>
<div class="field-name">keywords:</div>
<div class="field-value">Multi-Agent Planning;Robotics;Generative Models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b25edc149f182ff6f7b4934fc6edafc7ff79ec00.pdf" target="_blank">https://openreview.net/pdf/b25edc149f182ff6f7b4934fc6edafc7ff79ec00.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a0bcbad721fcbe66b3ce5c38c196172ac3638e0e.zip" target="_blank">https://openreview.net/attachment/a0bcbad721fcbe66b3ce5c38c196172ac3638e0e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This paper presents a method combining diffusion models with search-based techniques to generate scalable multi-robot trajectories using only single-robot data.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data. Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques---generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments.</div>
</div>
<div class='paper-counter'>795/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality</div>
<div class="field-name">authors:</div>
<div class="field-value">Kojiro Takeyama;Yimeng Liu;Misha Sra</div>
<div class="field-name">keywords:</div>
<div class="field-value">Dataset;Human trajectory;Indoor locomotion;Virtual reality;Social motion behavior</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f09d255b34007863bcd74b6fde80b26639f4a03b.pdf" target="_blank">https://openreview.net/pdf/f09d255b34007863bcd74b6fde80b26639f4a03b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/23902d06fb5a660c2756930e85887c85f332995a.zip" target="_blank">https://openreview.net/attachment/23902d06fb5a660c2756930e85887c85f332995a.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We present a dataset of two-person trajectories across 130+ home environments, capturing geometrically and socially-aware motion behaviors.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides accurate trajectory and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments.</div>
</div>
<div class='paper-counter'>796/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">AutoCGP: Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Pei Zhou;Ruizhe Liu;Qian Luo;Fan Wang;Yibing Song;Yanchao Yang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Self-Supervised Manipulation Concept Discovery;Concept-Guided Policy for Robotic Tasks</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/af10286b93d0dc3bcf656de5ef35482daaface46.pdf" target="_blank">https://openreview.net/pdf/af10286b93d0dc3bcf656de5ef35482daaface46.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/5ad29192eee60d89f5d72fea931003ee6bd7d11c.zip" target="_blank">https://openreview.net/attachment/5ad29192eee60d89f5d72fea931003ee6bd7d11c.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: an *Automatic Concept Discovery* module that identifies meaningful and consistent manipulation concepts, and a *Concept-Guided Policy Learning* module that effectively utilizes these manipulation concepts for adaptive task execution, including a *Concept Selection Transformer* for concept-based guidance and a *Concept-Guided Policy* for action prediction with the selected concepts. Experiments demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered manipulation concepts. Codes are available at: https://github.com/PeiZhou26/AutoCGP.</div>
</div>
<div class='paper-counter'>797/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing</div>
<div class="field-name">authors:</div>
<div class="field-value">June Suk Choi;Kyungmin Lee;Jongheon Jeong;Saining Xie;Jinwoo Shin;Kimin Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">image inpainting;adversarial attack;image editing;ai safety;diffusion model</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3c9344c3db2bb7274e5677bbe9ed13a505e79c35.pdf" target="_blank">https://openreview.net/pdf/3c9344c3db2bb7274e5677bbe9ed13a505e79c35.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/f6a6895032305072235d777248d96479aaddffc4.zip" target="_blank">https://openreview.net/attachment/f6a6895032305072235d777248d96479aaddffc4.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or harmful content. Although recent defense strategies, which introduce imperceptible adversarial noise to induce model failure, have shown promise, they remain ineffective against more sophisticated manipulations, such as editing with a mask. In this work, we propose DiffusionGuard, a robust and effective defense method against unauthorized edits by diffusion-based image editing models, even in challenging setups. Through a detailed analysis of these models, we introduce a novel objective that generates adversarial noise targeting the early stage of the diffusion process. This approach significantly improves the efficiency and effectiveness of adversarial noises. We also introduce a mask-augmentation technique to enhance robustness against various masks during test time. Finally, we introduce a comprehensive benchmark designed to evaluate the effectiveness and robustness of methods in protecting against privacy threats in realistic scenarios. Through extensive experiments, we show that our method achieves stronger protection and improved mask robustness with lower computational costs compared to the strongest baseline. Additionally, our method exhibits superior transferability and better resilience to noise removal techniques compared to all baseline methods. Our source code is publicly available at https://choi403.github.io/diffusionguard.</div>
</div>
<div class='paper-counter'>798/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo</div>
<div class="field-name">authors:</div>
<div class="field-value">Junzhe Zhu;Yuanchen Ju;Junyi Zhang;Muhan Wang;Zhecheng Yuan;Kaizhe Hu;Huazhe Xu</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;correspondence;computer vision;3D vision</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/be9894ba90b07c5ec0bd2deda17f1b1b8eeab2aa.pdf" target="_blank">https://openreview.net/pdf/be9894ba90b07c5ec0bd2deda17f1b1b8eeab2aa.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develop a dataset and a model for dense 3D correspondence on colored meshes, and perform robotic manipulation and color transfer experiments.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at https://tea-lab.github.io/DenseMatcher/.</div>
</div>
<div class='paper-counter'>799/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations</div>
<div class="field-name">authors:</div>
<div class="field-value">Thomas Tian;Kratarth Goel</div>
<div class="field-name">keywords:</div>
<div class="field-value">Efficient Post-training Preference Alignment;Alignment from demonstrations;Multi-agent Motion Generation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a9c4a760a8e787b8eaa7fbe57e48808633766ac2.pdf" target="_blank">https://openreview.net/pdf/a9c4a760a8e787b8eaa7fbe57e48808633766ac2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c11a7cfc937a3aff037b20e03d65d17fea15b6a3.zip" target="_blank">https://openreview.net/attachment/c11a7cfc937a3aff037b20e03d65d17fea15b6a3.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose an efficient post-training alignment approach that significantly improves the pre-trained motion generation model’s quality without requiring additional post-training human preference annotation or expansive compute.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies.</div>
</div>
<div class='paper-counter'>800/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STAMP: Scalable Task- And Model-agnostic Collaborative Perception</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiangbo Gao;Runsheng Xu;Jiachen Li;Ziran Wang;Zhiwen Fan;Zhengzhong Tu</div>
<div class="field-name">keywords:</div>
<div class="field-value">Autonomous Driving;Collaborative Perception;Domain Adaptation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/7d8a531a66e675e7be8eae193c647c06fb2ff2ee.pdf" target="_blank">https://openreview.net/pdf/7d8a531a66e675e7be8eae193c647c06fb2ff2ee.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/ed9aa93fff3e902a85acd16670204f66f0a790b1.pdf" target="_blank">https://openreview.net/attachment/ed9aa93fff3e902a85acd16670204f66f0a790b1.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose STAMP, a scalable, task- and model-agnostic collaborative perception framework that enables efficient feature sharing and fusion among heterogeneous agents in autonomous driving</div>
<div class="field-name">abstract:</div>
<div class="field-value">Perception is a crucial component of autonomous driving systems. However, single-agent setups often face limitations due to sensor constraints, especially under challenging conditions like severe occlusion, adverse weather, and long-range object detection. Multi-agent collaborative perception (CP) offers a promising solution that enables communication and information sharing between connected vehicles. Yet, the heterogeneity among agents—in terms of sensors, models, and tasks—significantly hinders effective and efficient cross-agent collaboration. To address these challenges, we propose STAMP, a scalable task- and model-agnostic collaborative perception framework tailored for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific domains and a shared protocol domain, facilitating efficient feature sharing and fusion while minimizing computational overhead. Moreover, our approach enhances scalability, preserves model security, and accommodates a diverse range of agents. Extensive experiments on both simulated (OPV2V) and real-world (V2V4Real) datasets demonstrate that STAMP achieves comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As the first-of-its-kind task- and model-agnostic collaborative perception framework, STAMP aims to advance research in scalable and secure mobility systems, bringing us closer to Level 5 autonomy. Our project page is at https://xiangbogaobarry.github.io/STAMP and the code is available at https://github.com/taco-group/STAMP.</div>
</div>
<div class='paper-counter'>801/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">GDrag:Towards General-Purpose Interactive Editing with Anti-ambiguity Point Diffusion</div>
<div class="field-name">authors:</div>
<div class="field-value">Xiaojian Lin;Hanhui Li;Yuhao Cheng;Yiqiang Yan;Xiaodan Liang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Interactive editing;dragging-based image manipulation;diffusion models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/40ddc7c1ae4a0dd23c4e012b289a08348d129983.pdf" target="_blank">https://openreview.net/pdf/40ddc7c1ae4a0dd23c4e012b289a08348d129983.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent interactive point-based image manipulation methods have gained considerable attention for being user-friendly. However, these methods still face two types of ambiguity issues that can lead to unsatisfactory outcomes, namely, intention ambiguity which misinterprets the purposes of users, and content ambiguity where target image areas are distorted by distracting elements. To address these issues and achieve general-purpose manipulations, we propose a novel task-aware, training-free framework called GDrag. Specifically, GDrag defines a taxonomy of atomic manipulations, which can be parameterized and combined unitedly to represent complex manipulations, thereby reducing intention ambiguity. Furthermore, GDrag introduces two strategies to mitigate content ambiguity, including an anti-ambiguity dense trajectory calculation method (ADT) and a self-adaptive motion supervision method (SMS). Given an atomic manipulation, ADT converts the sparse user-defined handle points into a dense point set by selecting their semantic and geometric neighbors, and calculates the trajectory of the point set. Unlike previous motion supervision methods relying on a single global scale for low-rank adaption, SMS jointly optimizes point-wise adaption scales and latent feature biases. These two methods allow us to model fine-grained target contexts and generate precise trajectories. As a result, GDrag consistently produces precise and appealing results in different editing tasks. Extensive experiments on the challenging DragBench dataset demonstrate that GDrag outperforms state-of-the-art methods significantly. The code of GDrag will be released upon acceptance.</div>
</div>
<div class='paper-counter'>802/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</div>
<div class="field-name">authors:</div>
<div class="field-value">Egor Zverev;Sahar Abdelnabi;Soroush Tabesh;Mario Fritz;Christoph H. Lampert</div>
<div class="field-name">keywords:</div>
<div class="field-value">Instruction-data separation;ML Safety;LLM Safety;LLM Security;Indirect Prompt Injection;Large Language Models;Datasets</div>
<div class="field-name">primary_area:</div>
<div class="field-value">alignment, fairness, safety, privacy, and societal considerations</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/e9ac78469850dd214cfff01e770dab787bcb0d89.pdf" target="_blank">https://openreview.net/pdf/e9ac78469850dd214cfff01e770dab787bcb0d89.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/8a5b3ec7597c40e6c9f82cefdfca6d514a2d3f84.zip" target="_blank">https://openreview.net/attachment/8a5b3ec7597c40e6c9f82cefdfca6d514a2d3f84.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce a formal measure and empirical methods to evaluate the instruction-data separation in LLMs.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation for single-turn language models and an empirical variant that is calculable from a model’s outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility.</div>
</div>
<div class='paper-counter'>803/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Junru Song;Yang Yang;Huan Xiao;Wei Peng;Wen Yao;Feifei Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robot Design Automation;Large Language Model;Voxel-Based Soft Robot</div>
<div class="field-name">primary_area:</div>
<div class="field-value">other topics in machine learning (i.e., none of the above)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/9dbb4a438c60a6230727c2ea1de47058a6532a78.pdf" target="_blank">https://openreview.net/pdf/9dbb4a438c60a6230727c2ea1de47058a6532a78.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">This work improves the diversity and inter-task generalizability of robot design processes with the aid of Large Language Models.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in Large Language Models (LLMs) have stimulated a significant paradigm shift in evolutionary optimization, where hand-crafted search heuristics are gradually replaced with LLMs serving as intelligent search operators. However, these studies still bear some notable limitations, including a challenge to balance exploitation with exploration, often leading to inferior solution diversity, as well as poor generalizability of problem solving across different task settings. These unsolved issues render the prowess of LLMs in robot design automation largely untapped. In this work, we present LASeR -- Large Language Model-Aided Evolutionary Search for Robot Design Automation. Leveraging a novel reflection mechanism termed DiRect, we elicit more knowledgeable exploratory behaviors from LLMs based on past search trajectories, reshaping the exploration-exploitation tradeoff with dual improvements in optimization efficiency and solution diversity. Additionally, with evolution fully grounded in task-related background information, we unprecedentedly uncover the inter-task reasoning capabilities of LLMs, facilitating generalizable design processes that effectively inspire zero-shot robot proposals for new applications. Our simulated experiments on voxel-based soft robots showcase distinct advantages of LASeR over competitive baselines. Code at https://github.com/WoodySJR/LASeR.</div>
</div>
<div class='paper-counter'>804/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">NextBestPath: Efficient 3D Mapping of Unseen Environments</div>
<div class="field-name">authors:</div>
<div class="field-value">Shiyao Li;Antoine Guedon;Clémentin Boittiaux;Shizhe Chen;Vincent Lepetit</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D reconstruction;active mapping</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/78222d5232d3ae749251ee8e30be976107357558.pdf" target="_blank">https://openreview.net/pdf/78222d5232d3ae749251ee8e30be976107357558.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">This work addresses the problem of active 3D mapping, where an agent must find an efficient trajectory to exhaustively reconstruct a new scene.Previous approaches mainly predict the next best view near the agent's location, which is prone to getting stuck in local areas. Additionally, existing indoor datasets are insufficient due to limited geometric complexity and inaccurate ground truth meshes.To overcome these limitations, we introduce a novel dataset AiMDoom with a map generator for the Doom video game, enabling to better benchmark active 3D mapping in diverse indoor environments.Moreover, we propose a new method we call next-best-path (NBP), which predicts long-term goals rather than focusing solely on short-sighted views.The model jointly predicts accumulated surface coverage gains for long-term goals and obstacle maps, allowing it to efficiently plan optimal paths with a unified model.By leveraging online data collection, data augmentation and curriculum learning, NBP significantly outperforms state-of-the-art methods on both the existing MP3D dataset and our AiMDoom dataset, achieving more efficient mapping in indoor environments of varying complexity.</div>
</div>
<div class='paper-counter'>805/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiahua Dong;Yu-Xiong Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">3D Editing;Diffusion Model;3D Vision</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/a63be65e0b2a30aa07aae872eb8f9f07d17ca95b.pdf" target="_blank">https://openreview.net/pdf/a63be65e0b2a30aa07aae872eb8f9f07d17ca95b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/4f43e4cc5871cbc181413d5123387782e7d22d3f.zip" target="_blank">https://openreview.net/attachment/4f43e4cc5871cbc181413d5123387782e7d22d3f.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">An intuitive point-based 3D editing pipeline, which enables geometry-related content editing on 3D Gaussians.</div>
<div class="field-name">abstract:</div>
<div class="field-value">The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag, a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.</div>
</div>
<div class='paper-counter'>806/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects</div>
<div class="field-name">authors:</div>
<div class="field-value">Tai Hoang;Huy Le;Philipp Becker;Vien Anh Ngo;Gerhard Neumann</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robotic Manipulation;Equivariance;Graph Neural Networks;Reinforcement Learning;Deformable Objects</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Oral</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/46529e3099f6f1e484a51deab471b61789b5a536.pdf" target="_blank">https://openreview.net/pdf/46529e3099f6f1e484a51deab471b61789b5a536.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/c8c722d780498e9664d34eea99fa143d06c374b0.zip" target="_blank">https://openreview.net/attachment/c8c722d780498e9664d34eea99fa143d06c374b0.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Geometry-aware RL with heterogeneous SE(3) equivariant back-bone policy for robotic manipulation</div>
<div class="field-name">abstract:</div>
<div class="field-value">Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$ equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects. Our project page is available at https://thobotics.github.io/hepi.</div>
</div>
<div class='paper-counter'>807/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks</div>
<div class="field-name">authors:</div>
<div class="field-value">Jiuding Sun;Jing Huang;Sidharth Baskaran;Karel D'Oosterlinck;Christopher Potts;Michael Sklar;Atticus Geiger</div>
<div class="field-name">keywords:</div>
<div class="field-value">mechanistic interpretability;causal abstraction;hypernetwork</div>
<div class="field-name">primary_area:</div>
<div class="field-value">interpretability and explainable AI</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/db66788476e93fff1d62b45467b167b9158fdfd6.pdf" target="_blank">https://openreview.net/pdf/db66788476e93fff1d62b45467b167b9158fdfd6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/e14ecc6b48a1dd55c19d08939b4cd9cb47b23530.zip" target="_blank">https://openreview.net/attachment/e14ecc6b48a1dd55c19d08939b4cd9cb47b23530.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">HyperDAS, a transformer-based hypernetwork, enhances interpretability by pinpointing and learning concept-specific features in neural networks' residual streams.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts (e.g., *the birth year of a Nobel laureate*) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) learns features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.</div>
</div>
<div class='paper-counter'>808/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks</div>
<div class="field-name">authors:</div>
<div class="field-value">Arth Shukla;Stone Tao;Hao Su</div>
<div class="field-name">keywords:</div>
<div class="field-value">benchmark;dataset;simulation;reinforcement learning;imitation learning;robotics</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bcafead0bf180b4a8da019c355309931525f5b2a.pdf" target="_blank">https://openreview.net/pdf/bcafead0bf180b4a8da019c355309931525f5b2a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/86a4bae1cfa936b8f2a6718605c39b2699f9f6eb.zip" target="_blank">https://openreview.net/attachment/86a4bae1cfa936b8f2a6718605c39b2699f9f6eb.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We provide a GPU-accelerated implementation of the HAB which supports realistic low-level control, run extensive RL and IL baselines, and develop a rule-based trajectory filtering system which enables efficient, controlled data generation at scale.</div>
<div class="field-name">abstract:</div>
<div class="field-value">High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.</div>
</div>
<div class='paper-counter'>809/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding</div>
<div class="field-name">authors:</div>
<div class="field-value">Alexey Skrynnik;Anton Andreychuk;Anatolii Borzilov;Alexander Chernyavskiy;Konstantin Yakovlev;Aleksandr Panov</div>
<div class="field-name">keywords:</div>
<div class="field-value">MAPF;MARL;RL;Heuristic search</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/19e55d87f236d0ca49d30994ced3703e8bd5a14e.pdf" target="_blank">https://openreview.net/pdf/19e55d87f236d0ca49d30994ced3703e8bd5a14e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d2f00fdaf0e29dab707c2dd8f3cdf7d31b841ef1.zip" target="_blank">https://openreview.net/attachment/d2f00fdaf0e29dab707c2dd8f3cdf7d31b841ef1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.</div>
</div>
<div class='paper-counter'>810/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</div>
<div class="field-name">authors:</div>
<div class="field-value">Haoyi Zhu;Honghui Yang;Yating Wang;Jiange Yang;Limin Wang;Tong He</div>
<div class="field-name">keywords:</div>
<div class="field-value">embodied AI;representation learning;3D spatial awareness;multi-view image;robot manipulation;neural rendering</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/69efa7c1cd34c4e72171331a81f56b7c914e9e24.pdf" target="_blank">https://openreview.net/pdf/69efa7c1cd34c4e72171331a81f56b7c914e9e24.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We introduce SPA, a novel framework that enhances 3D spatial awareness in embodied AI representation learning, outperforming existing models across 268 tasks and 8 simulators.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning.</div>
</div>
<div class='paper-counter'>811/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Training Free Guided Flow-Matching with Optimal Control</div>
<div class="field-name">authors:</div>
<div class="field-value">Luran Wang;Chaoran Cheng;Yizhen Liao;Yanru Qu;Ge Liu</div>
<div class="field-name">keywords:</div>
<div class="field-value">flow matching;controlled generation;inverse problem</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/eb66c26bfd61b517c8ff218317f7b95f6a59c370.pdf" target="_blank">https://openreview.net/pdf/eb66c26bfd61b517c8ff218317f7b95f6a59c370.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We develope a general and theoretically grounded framework for guided flow matching on both Euclidean and SO3 geometries taking inpiration from optimal control.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.</div>
</div>
<div class='paper-counter'>812/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do</div>
<div class="field-name">authors:</div>
<div class="field-value">Yoav Wald;Mark Goldstein;Yonathan Efroni;Wouter A.C. van Amsterdam;Rajesh Ranganath</div>
<div class="field-name">keywords:</div>
<div class="field-value">effect estimation;treatment times;irregular times;sequential decision making</div>
<div class="field-name">primary_area:</div>
<div class="field-value">causal reasoning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/bcf1973cc2db7ed7bce1af6d6abead42ec7a04a0.pdf" target="_blank">https://openreview.net/pdf/bcf1973cc2db7ed7bce1af6d6abead42ec7a04a0.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/50cd5444e5124b87a076791bfa2c3de946b00097.pdf" target="_blank">https://openreview.net/attachment/50cd5444e5124b87a076791bfa2c3de946b00097.pdf</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A deep-Q effect estimation method for intervention on times and types of treatments</div>
<div class="field-name">abstract:</div>
<div class="field-value">Problems in fields such as healthcare, robotics, and finance requires reasoning about the value both of what decision or action to take and when to take it. The prevailing hope is that artificial intelligence will support such decisions by estimating the causal effect of policies such as how to treat patients or how to allocate resources over time. However, existing methods for estimating the effect of a policy struggle with \emph{irregular time}.  They either discretize time, or disregard the effect of timing policies.We present a new deep-Q algorithm that estimates the effect of both when and what to do called Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for the Q-function that is compatible with flexible sequence models, such as transformers. EDQ provides accurate estimates under standard assumptions. We validate the approach through experiments on survival time and tumor growth tasks.</div>
</div>
<div class='paper-counter'>813/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Breaking Neural Network Scaling Laws with Modularity</div>
<div class="field-name">authors:</div>
<div class="field-value">Akhilan Boopathy;Sunshine Jiang;William Yue;Jaedong Hwang;Abhiram Iyer;Ila R Fiete</div>
<div class="field-name">keywords:</div>
<div class="field-value">scaling laws;modularity;neural network;generalization;compositionality;combinatorial generalization</div>
<div class="field-name">primary_area:</div>
<div class="field-value">other topics in machine learning (i.e., none of the above)</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3830232d5119df6b356d698d216018daf00d9eb9.pdf" target="_blank">https://openreview.net/pdf/3830232d5119df6b356d698d216018daf00d9eb9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We show theoretically that modular neural networks trained on modular tasks can generalize to high-dimensional tasks with a fixed number of training points; we propose a learning rule to exploit this advantage empirically.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.</div>
</div>
<div class='paper-counter'>814/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Marius Memmel;Jacob Berg;Bingqing Chen;Abhishek Gupta;Jonathan Francis</div>
<div class="field-name">keywords:</div>
<div class="field-value">dynamic time warping;few-shot imitation learning;retrieval;foundation models</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b13eaaa9a22344704a1f69c9543bcd63cfec6b0e.pdf" target="_blank">https://openreview.net/pdf/b13eaaa9a22344704a1f69c9543bcd63cfec6b0e.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Subsequence-DTW for sub-trajectory retrieval to augment few-shot policy learning</div>
<div class="field-name">abstract:</div>
<div class="field-value">Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the "sub"-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.</div>
</div>
<div class='paper-counter'>815/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Simulating Human-like Daily Activities with Desire-driven Autonomy</div>
<div class="field-name">authors:</div>
<div class="field-value">Yiding Wang;Yuxuan Chen;Fangwei Zhong;Long Ma;Yizhou Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">desire;autonomy;daily activities;</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b88f5158be52b85d96d503ccbb21bc61dfbf62f5.pdf" target="_blank">https://openreview.net/pdf/b88f5158be52b85d96d503ccbb21bc61dfbf62f5.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A desire-driven autonomy framework to guide an agent to simulate human-like daily activities</div>
<div class="field-name">abstract:</div>
<div class="field-value">Desires motivate humans to interact autonomously with the complex world. In contrast, current AI agents require explicit task specifications, such as instructions or reward functions, which constrain their autonomy and behavioral diversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A) that can enable a large language model (LLM) to autonomously propose and select tasks, motivated by satisfying its multi-dimensional desires. Specifically, the motivational framework of D2A is mainly constructed by a dynamic $Value\ System$, inspired by the Theory of Needs. It incorporates an understanding of human-like desires, such as the need for social interaction, personal fulfillment, and self-care. At each step, the agent evaluates the value of its current state, proposes a set of candidate activities, and selects the one that best aligns with its intrinsic motivations. We conduct experiments on Concordia, a text-based simulator, to demonstrate that our agent generates coherent, contextually relevant daily activities while exhibiting variability and adaptability similar to human behavior. A comparative analysis with other LLM-based agents demonstrates that our approach significantly enhances the rationality of the simulated activities.</div>
</div>
<div class='paper-counter'>816/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination</div>
<div class="field-name">authors:</div>
<div class="field-value">Leonardo Barcellona;Andrii Zadaianchuk;Davide Allegro;Samuele Papa;Stefano Ghidoni;Efstratios Gavves</div>
<div class="field-name">keywords:</div>
<div class="field-value">World model;  Imagination; Imitation Learning; Gaussian Splatting; Compositional; Physics-informed; Object-centric;</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/09c7c86828171ae29d694536b94c26a0c887d7f3.pdf" target="_blank">https://openreview.net/pdf/09c7c86828171ae29d694536b94c26a0c887d7f3.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world robotics applications. To overcome those challenges, we propose to rethink robot world models as learnable digital twins. We introduce DreMa, a new approach for constructing digital twins automatically using learned explicit representations of the real world and its dynamics, bridging the gap between traditional digital twins and world models.DreMa replicates the observed world and its structure by integrating Gaussian Splatting and physics simulators, allowing robots to imagine novel configurations of objects and to predict the future consequences of robot actions thanks to its compositionality.We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa’s imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning).Our project page can be found in: https://dreamtomanipulate.github.io/.</div>
</div>
<div class='paper-counter'>817/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models</div>
<div class="field-name">authors:</div>
<div class="field-value">Shengda Fan;Xin Cong;Yuepeng Fu;Zhong Zhang;Shuyan Zhang;Yuanwei Liu;Yesai Wu;Yankai Lin;Zhiyuan Liu;Maosong Sun</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;Process Automation;Workflow;Tool Learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/c2ae3b37950645ca5b0e7bd93eb3e30fc1cbf417.pdf" target="_blank">https://openreview.net/pdf/c2ae3b37950645ca5b0e7bd93eb3e30fc1cbf417.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/1d0fe4cb519a7fc7de0de5e56b9068fb29b1e1a4.zip" target="_blank">https://openreview.net/attachment/1d0fe4cb519a7fc7de0de5e56b9068fb29b1e1a4.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advancements in large language models (LLMs) have driven a revolutionary paradigm shift in process automation from Robotic Process Automation to Agentic Process Automation by automating the workflow orchestration procedure based on LLMs. However, existing LLMs (even the advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in workflow orchestration. To address this limitation, we present WorkflowLLM, a data-centric framework elaborately designed to enhance the capability of LLMs in workflow orchestration. It first constructs a large-scale fine-tuning dataset WorkflowBench with 106, 763 samples, covering 1, 503 APIs from 83 applications across 28 categories. Specifically, the construction process can be divided into three phases: (1) Data Collection: we collect real-world workflow data from Apple Shortcuts and RoutineHub, transcribing them into Python-style code. We further equip them with generated hierarchical thought via GPT-4o-mini. (2) Query Expansion: we prompt GPT-4o-mini to generate more task queries to enrich the diversity and complexity of workflows. (3) Workflow Generation: we leverage an annotator model trained on collected data to generate workflows for synthesized queries. Finally, we merge the synthetic samples that pass quality confirmation with the collected samples to obtain the WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong capacity to orchestrate complex workflows, while also achieving notable generalization performance on previously unseen APIs. Additionally, WorkflowBench exhibits robust zero-shot generalization capabilities on an out-of-distribution task planning dataset, T-Eval. Our data and code are available at https://github.com/OpenBMB/WorkflowLLM.</div>
</div>
<div class='paper-counter'>818/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents</div>
<div class="field-name">authors:</div>
<div class="field-value">Dongjun Lee;Juyong Lee;Kyuyoung Kim;Jihoon Tack;Jinwoo Shin;Yee Whye Teh;Kimin Lee</div>
<div class="field-name">keywords:</div>
<div class="field-value">Large Language Models;LLM agent;Web automation</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ab477b5ecff7f6b73cc1151b893fb071c069c433.pdf" target="_blank">https://openreview.net/pdf/ab477b5ecff7f6b73cc1151b893fb071c069c433.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">Training language models to automatically contextualize web page observations to enhance the decision-making of LLM agents</div>
<div class="field-name">abstract:</div>
<div class="field-value">Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark.Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.</div>
</div>
<div class='paper-counter'>819/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Shangding Gu;Laixi Shi;Muning Wen;Ming Jin;Eric Mazumdar;Yuejie Chi;Adam Wierman;Costas Spanos</div>
<div class="field-name">keywords:</div>
<div class="field-value">Robust reinforcement learning;benchmark;reinforcement learning;multi-agent reinforcement learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/dba4a4a03ae16240b55d8098868a57fe27df9111.pdf" target="_blank">https://openreview.net/pdf/dba4a4a03ae16240b55d8098868a57fe27df9111.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement learning (RL) seeks to improve resilience against the complexity and variability in agent-environment sequential interactions. Despite the existence of a large number of RL benchmarks, there is a lack of standardized benchmarks for robust RL. Current robust RL policies often focus on a specific type of uncertainty and are evaluated in distinct, one-off environments. In this work, we introduce Robust-Gymnasium, a unified modular benchmark designed for robust RL that supports a wide variety of disruptions across all key RL components—agents' observed state and reward, agents' actions, and the environment. Offering over sixty diverse task environments spanning control and robotics, safe RL, and multi-agent RL, it provides an open-source and user-friendly tool for the community to assess current methods and foster the development of robust RL algorithms. In addition, we benchmark existing standard and robust RL algorithms within this framework, uncovering significant deficiencies in each and offering new insights.</div>
</div>
<div class='paper-counter'>820/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Caleb Chuck;Fan Feng;Carl Qi;Chang Shi;Siddhant Agarwal;Amy Zhang;Scott Niekum</div>
<div class="field-name">keywords:</div>
<div class="field-value">Goal Conditioned Reinforcement Learning;Factor Interactions;Factored State;Hindsight Experience Replay;Counterfactual</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/ece8fced83ad1562f06e5501578c6b5955a28d1d.pdf" target="_blank">https://openreview.net/pdf/ece8fced83ad1562f06e5501578c6b5955a28d1d.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/d293839f419a93d55cefcec0a5867c846b14d74e.zip" target="_blank">https://openreview.net/attachment/d293839f419a93d55cefcec0a5867c846b14d74e.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">First, introduces a method for inferring general factor interactions using a counterfactual test on learned models, then integrates interactions into hindsight relabeling to improve the sample efficiency of GCRL</div>
<div class="field-name">abstract:</div>
<div class="field-value">Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal---an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However, interactions do not have a consensus statistical definition that is tractable for downstream GCRL. Therefore, we propose a definition of interactions based on the concept of _null counterfactual_: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a ``nulling'' operation with a learned model to simulate absences and infer interactions. We demonstrate that NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen. Furthermore, we demonstrate that HInt improves sample efficiency by up to $4\times$ in these domains as goal-conditioned tasks.</div>
</div>
<div class='paper-counter'>821/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">General Scene Adaptation for Vision-and-Language Navigation</div>
<div class="field-name">authors:</div>
<div class="field-value">Haodong Hong;Yanyuan Qiao;Sen Wang;Jiajun Liu;Qi Wu</div>
<div class="field-name">keywords:</div>
<div class="field-value">vision-and-language navigation; scene adaptation; multi-modal learning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">datasets and benchmarks</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/efac23a309270f8be919377bb3c93b1a15284e2b.pdf" target="_blank">https://openreview.net/pdf/efac23a309270f8be919377bb3c93b1a15284e2b.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/6da67309fa3a6bf5493cd70f73e535394be1c171.zip" target="_blank">https://openreview.net/attachment/6da67309fa3a6bf5493cd70f73e535394be1c171.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN (General Scene Adaptation for VLN), a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of out-of-distribution (OOD) data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages large language models (LLMs) to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions,  taking the use case of home robotic assistants as an example. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods, revealing key factors enabling agents to adapt to specific environments. Based on our findings, we propose a novel method, Graph-Retained DUET (GR-DUET), which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.</div>
</div>
<div class='paper-counter'>822/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">CLIPDrag: Combining Text-based and Drag-based Instructions for Image Editing</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziqi Jiang;Zhen Wang;Long Chen</div>
<div class="field-name">keywords:</div>
<div class="field-value">Computer Vision;Generative Model;Diffusion Model;Image Editing.</div>
<div class="field-name">primary_area:</div>
<div class="field-value">generative models</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/f3965f65314008fcef3d06cf7cfe5178df9197d2.pdf" target="_blank">https://openreview.net/pdf/f3965f65314008fcef3d06cf7cfe5178df9197d2.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="" target="_blank"></a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Precise and flexible image editing remains a fundamental challenge in computer vision. Based on the modified areas, most editing methods can be divided into two main types: global editing and local editing. In this paper, we choose the two most common editing approaches (\ie text-based editing and drag-based editing) and analyze their drawbacks. Specifically, text-based methods often fail to describe the desired modifications precisely, while drag-based methods suffer from ambiguity. To address these issues, we proposed \textbf{CLIPDrag}, a novel image editing method that is the first to combine text and drag signals for precise and ambiguity-free manipulations on diffusion models. To fully leverage these two signals, we treat text signals as global guidance and drag points as local information. Then we introduce a novel global-local motion supervision method to integrate text signals into existing drag-based methods by adapting a pre-trained language-vision model like CLIP. Furthermore, we also address the problem of slow convergence in CLIPDrag by presenting a fast point-tracking method that enforces drag points moving toward correct directions. Extensive experiments demonstrate that CLIPDrag outperforms existing single drag-based methods or text-based methods.</div>
</div>
<div class='paper-counter'>823/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">A deep inverse-mapping model for a flapping robotic wing</div>
<div class="field-name">authors:</div>
<div class="field-value">Hadar Sharvit;Raz Karl;Tsevi Beatus</div>
<div class="field-name">keywords:</div>
<div class="field-value">robotics;control;flapping drones;deep learning;time series;inverse mapping;sequence to sequence</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b13f6f26ee90d4a3abb93cc42ad8e5b197b4bae9.pdf" target="_blank">https://openreview.net/pdf/b13f6f26ee90d4a3abb93cc42ad8e5b197b4bae9.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/65ae9f2f7c443ac69b9ccd22ef04615016cf14c1.zip" target="_blank">https://openreview.net/attachment/65ae9f2f7c443ac69b9ccd22ef04615016cf14c1.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We solved an inverse-mapping problem of robotic, flapping-wing systems, by learning the input wing motion required to generate a desired aerodynamic force outcome. This framework is expected to simplify the control of such complex systems.</div>
<div class="field-name">abstract:</div>
<div class="field-value">In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust. This mapping can be inverted to determine the rotation rate needed to generate a desired thrust. However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical. Here, we report a machine-learning solution for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed. Our model learns the input wing motion required to generate a desired aerodynamic force outcome. We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain. To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras. We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime. Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11\% improvement on the test datasets median loss. Moreover, our model shows superior inference time, making it practical for onboard robotic control. Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices.</div>
</div>
<div class='paper-counter'>824/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">Monet: Mixture of Monosemantic Experts for Transformers</div>
<div class="field-name">authors:</div>
<div class="field-value">Jungwoo Park;Ahn Young Jin;Kee-Eung Kim;Jaewoo Kang</div>
<div class="field-name">keywords:</div>
<div class="field-value">large language models;mechanistic interpretability;monosemanticity;mixture of experts;knowledge unlearning</div>
<div class="field-name">primary_area:</div>
<div class="field-value">interpretability and explainable AI</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/50d5efd1156d1e1b17550ecca05aced5ade00817.pdf" target="_blank">https://openreview.net/pdf/50d5efd1156d1e1b17550ecca05aced5ade00817.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/2e1c46a5628dc43a29d70540295458d78bc35157.zip" target="_blank">https://openreview.net/attachment/2e1c46a5628dc43a29d70540295458d78bc35157.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by *polysemanticity*—where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning,  they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce **Mixture of Monosemantic Experts for Transformers (Monet)** architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, **Monet** allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior.</div>
</div>
<div class='paper-counter'>825/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">6D Object Pose Tracking in Internet Videos for Robotic Manipulation</div>
<div class="field-name">authors:</div>
<div class="field-value">Georgy Ponimatkin;Martin Cífka;Tomas Soucek;Médéric Fourmy;Yann Labbé;Vladimir Petrik;Josef Sivic</div>
<div class="field-name">keywords:</div>
<div class="field-value">6DoF pose estimation;robotic manipulation from video</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to computer vision, audio, language, and other modalities</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/b4cc76259c4c3f2b3ef42f95addddbab181921a6.pdf" target="_blank">https://openreview.net/pdf/b4cc76259c4c3f2b3ef42f95addddbab181921a6.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/25538edbe05f4d6d9b33e61503a69a7887b0fc84.zip" target="_blank">https://openreview.net/attachment/25538edbe05f4d6d9b33e61503a69a7887b0fc84.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">A method to estimate 6D pose and trajectory of an object in the Internet video</div>
<div class="field-name">abstract:</div>
<div class="field-value">We seek to extract a temporally consistent 6D pose trajectory of a manipulated  object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, subtle but dynamic object motions, and the fact that the exact mesh of the manipulated object is not known. To address these challenges, we present the following contributions. First, we develop a new method that estimates the 6D pose of any object in the input image without prior knowledge of the object itself. The method proceeds by (i) retrieving a CAD model similar to the depicted object from a large-scale model database, (ii) 6D aligning the retrieved CAD model with the input image, and (iii) grounding the absolute scale of the object with respect to the scene. Second, we extract smooth 6D object trajectories from Internet videos by carefully tracking the detected objects across video frames. The extracted object trajectories are then retargeted via trajectory optimization into the configuration space of a robotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose estimation method on YCB-V and HOPE-Video datasets as well as a new dataset of instructional videos manually annotated with approximate 6D object trajectories. We demonstrate significant improvements over existing state-of-the-art RGB 6D pose estimation methods. Finally,  we show that the 6D object motion estimated from Internet videos can be transferred to a 7-axis robotic manipulator both in a virtual simulator as well as in a real world set-up. We also successfully apply our method to egocentric videos taken from the EPIC-KITCHENS dataset, demonstrating potential for Embodied AI applications.</div>
</div>
<div class='paper-counter'>826/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">OmniRe: Omni Urban Scene Reconstruction</div>
<div class="field-name">authors:</div>
<div class="field-value">Ziyu Chen;Jiawei Yang;Jiahui Huang;Riccardo de Lutio;Janick Martinez Esturo;Boris Ivanovic;Or Litany;Zan Gojcic;Sanja Fidler;Marco Pavone;Li Song;Yue Wang</div>
<div class="field-name">keywords:</div>
<div class="field-value">Gaussians Splatting;Neural Rendering;Dynamic Scene Reconstruction;Autonomous Driving</div>
<div class="field-name">primary_area:</div>
<div class="field-value">applications to robotics, autonomy, planning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Spotlight</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/3e00ab887d032081e5bf1dc891cc3c4362ff357a.pdf" target="_blank">https://openreview.net/pdf/3e00ab887d032081e5bf1dc891cc3c4362ff357a.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/a6075c5ab9b491cfda578768d4b601e58dc6dc5b.zip" target="_blank">https://openreview.net/attachment/a6075c5ab9b491cfda578768d4b601e58dc6dc5b.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value"></div>
<div class="field-name">abstract:</div>
<div class="field-value">We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60 Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes. Code and results are available at [omnire](https://ziyc.github.io/omnire/).</div>
</div>
<div class='paper-counter'>827/828</div>
<div class="paper">
<div class="field-name">title:</div>
<div class="field-value">PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical Reinforcement Learning</div>
<div class="field-name">authors:</div>
<div class="field-value">Utsav Singh;Vinay P. Namboodiri</div>
<div class="field-name">keywords:</div>
<div class="field-value">Hierarchical reinforcement learning;Learning from demonstrations</div>
<div class="field-name">primary_area:</div>
<div class="field-value">reinforcement learning</div>
<div class="field-name">venue:</div>
<div class="field-value">ICLR 2025 Poster</div>
<div class="field-name">pdf:</div>
<div class="field-value"><a href="https://openreview.net/pdf/12d3031a9f935c986708176a7e99e20965bed69f.pdf" target="_blank">https://openreview.net/pdf/12d3031a9f935c986708176a7e99e20965bed69f.pdf</a></div>
<div class="field-name">supplementary_material:</div>
<div class="field-value"><a href="https://openreview.net/attachment/031a14e8a5e3b4b8ba5c6811c2665675d6663735.zip" target="_blank">https://openreview.net/attachment/031a14e8a5e3b4b8ba5c6811c2665675d6663735.zip</a></div>
<div class="field-name">TLDR:</div>
<div class="field-value">We effectively leverage expert demonstrations using our adaptive relabeling based approach to deal with non-stationarity in the context of hierarchical reinforcement learning.</div>
<div class="field-name">abstract:</div>
<div class="field-value">Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to bound the sub-optimality of our approach and derive a joint optimization framework using RL and IL. Since PEAR utilizes only a few expert demonstrations and considers minimal limiting assumptions on the task structure, it can be easily integrated with typical off-policy \RL algorithms to produce a practical HRL approach. We perform extensive experiments on challenging environments and show that PEAR is able to outperform various hierarchical and non-hierarchical baselines and achieve upto 80% success rates in complex sparse robotic control tasks where other baselines typically fail to show significant progress. We also perform ablations to thoroughly analyze the importance of our various design choices. Finally, we perform real world robotic experiments on complex tasks and demonstrate that PEAR consistently outperforms the baselines.</div>
</div>
<div class='paper-counter'>828/828</div>

    </body>
    </html>
    